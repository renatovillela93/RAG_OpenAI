BIM Education in Landscape Architecture: The Rapperswil Model \nAbstract\nIn 2016, Switzerland decided to embrace digitization in every industry, in all sectors, and at \r\nall  levels.  Within  the  Landscape  Architecture  program  at  the  OST  University  of  Applied  Sciences, teaching and research with strong emphasis on Digital Construction had already been carried out for years. In view of the country’s vigorous drive towards digitization and being the only bachelor's pro-\r\ngram in the German-speaking part of Switzerland, Rapperswil Landscape Architecture is committed to \r\npartake and to aid in this digitization process. The paper gives an overview of Digital Construction in \r\nLandscape Architecture education in Rapperswil, status 2022.\n\nIntroduction \nIn the coming years, the construction industry worldwide will undergo digital transformation. \nBuilding Information Modeling (BIM) plays a central role with the digital twin and with a \nclearly defined process. The strategy of the Swiss Federal Council from 2016 for digital Swit-\nzerland (SCHWEIZERISCHE EIDGENOSSENSCHAFT 2016) and the establishment of the frame-\nwork “Bauen Digital Schweiz – Digital Construction  Switzerland”, the country chapter of \nbuildingSMART International, gave the Swiss construction industry a big boost in the direc-\ntion of digitization The Swiss government  even  went one step further and defined a clear \ntime frame. By 2025, Switzerland will implement the BIM process not only in architecture, \nbut in all infrastructure construction projects. The Swiss Federal railway company Schwei-\nzerische Bundesbahnen (SBB), a government-owned company, must apply it in all projects \nby then. SBB is also a big player in real estate architecture projects where greenspaces are \nalways required. Therefore, this is not only applicable to engineers, but also landscape archi-\ntects have to get ready for digital construction. As result, Rapperswil is consistently pursuing \nthe goal of implementing “Digital Construction” in Landscape Architecture education. \n\nDigital Construction \nDigital Construction is the general term for using different digital technologies to build more \nefficiently. The below methods and processes belong to Digital Construction and are taught \nat Rapperswil. \n\nBIM Construction \nRapperswil students work three-dimensionally from day one. In the first semester, the BIM \narchitecture construction software (Revit) and the principle of a digital twin are taught. The \nstudents  are  required  to  build  an  existing  structure  (pavilion,  pergola,  water  feature,  etc.). \nThis approach has the following advantages: \nFast learning of architecture representation principles (floor plans, elevations, sections, 3D views), \n“One single source of truth” – the 3D model is the basis for all (representations, schedules\/quantities, etc.),\nObject oriented construction only with 3D objects, which belong to building categories, \nVery structured modelling with parametric objects (standard families, external families and project families), The model has container function for fundamental construction data. It is used for structural engineering (formwork\/reinforcement) and for MEP (Mechanical, Electric, Plumbing) modelling. \n\nIn the 2nd semester, the students integrate a building into an existing site based on GIS data \nwith  civil  engineering  programs  (InfraWorks  \/  Civil  3D).  They  locate  driveways,  parking \nlots, retention\/infiltration basins in the project. Students get to know the 3D Global Naviga-\ntion Satellite System (GNSS) excavator control system and learn what to look out for when \npreparing data for it. In principal students learn to use the correct tools for different chal-\nlenges, as BIM construction in Landscape Architecture consists of tools for architecture pro-\njects and tools for civil engineering projects. The scheme BIM4RainWaterManament (PET-\nSCHEK 2019) gives a good overview of the Rapperswil teaching in Digital Construction. \n\nBefore the students model the terrain digitally, they learn the craft by hand for one semester. \nIn the construction course in the first semester, the students have to solve numerous small \ntasks on terrain modeling with elevation points\/contour lines and prove their skills in a 1 ½ \nhour exam at the end of the semester. The exercises are based on U.S. grading courses, which \nare common at every north American landscape architecture program. Richard Untermann, \nwith the book “Grade Easy” (UNTERMANN 1972), was an important pioneer for several text-\nbooks on the English-language book market on terrain modelling and serves as the basis for \nthe Rapperwil grading education. \n\nMore complex architectural constructions, which serve as the basis for structural calculations, \nare the subject of the “Constructive Design” module in the 4th semester. This is also where \nthe cooperation with the civil engineers of the newly formed School of Architecture, Civil \nEngineering, Landscape Architecture, Spatial Planning of the OST is practiced for the first \ntime. \n\nThe previously often time-consuming visualizations are becoming a sideshow thanks to the \nBIM construction training. Software like Enscape3D or Twinmotion enables the virtual real-\nity (VR) inspection of the model with a headset or on the screen. Individual images in differ-\nent resolutions can be created from any point. Students can check out headsets and VR capa-\nble computers from a central IT service at OST. \n\nCommon Data Environment (CDE) \nIn addition to the 3D model, the management of processes and information is central to Digital Construction. Internet-based platforms, called Common Data Environment (CDE), are \nresponsible for data exchange, collaboration on a model, costs, quantities, materials, delivery, \ntesting and acceptance processes in all life cycle phases of the structure. \n\nIn the spring semester of 2020, like the other Swiss universities, all of Rapperswil’s landscape \narchitecture courses took place online. Since the first wave of the pandemic, Teams \/ Zoom \nconferences had been part of everyday life for all students. However, Rapperswil went a step \nfurther. It was made mandatory for all students to submit their BIM construction projects via \nthe CDE Autodesk Construction Cloud (ACC). A project folder is dedicated to each course, with student subfolders. There the students store their models, photos, sketches and text. The \nlecturers evaluate the work digitally, with the 3D model being the starting point. \n\nThomas  Putscher,  lecturer  at  OST,  writes,  “I  consider  the  submission  to  the  Construction \nCloud to be a good thing. The students found their way around quickly and it went smoothly. \nMeetings took place directly via Teams with the open 3D cloud model. The corrections to \nthe  model  were very easy  for me because I could do them directly online. I uploaded my \nevaluation sheets and informed all students via email about their grades. If necessary, there \nwere debriefings online with the open 3D model. All in all, the construction cloud has saved \nme a lot of time.” For Rapperswil, the full integration of the Common Data Environment into \nlandscape architecture training was the next logical step towards Digital Construction. From \nfall semester 2021, all students in the construction courses worked with the CDE platform \nright from the first semester. The cloud solution is now used for submission and evaluation \nin all construction courses up to the bachelor thesis. Paper plans are no longer used as submissions. \n\nBIM4RainwaterManagement \n“Climate change is leading to heavier and more frequent precipitation. In urban areas, where \ndevelopment means the total impervious surface area is increasing, there is a growing risk of \nflooding from surface run-off after heavy rainfall. In climate-adapted and risk-based urban \ndevelopment,  there  is  an  increasing  need  to  manage  rainwater  resources  sustainably.  The \nconcept of 'sponge cities', which focus on increasing evaporation, infiltration, retention, controlled  temporary  flooding  and  providing  emergency  waterways,  is  a  planning  solution  to \nprevent damage from surface run-off and to reduce the effects of heat” (BAFU\/ARE 2022). \n\nLandscape Architects must take over a leading role in building 'sponge cities'. How can the \nprinciple of the sponge city be realized as part of Digital Construction? Although it belongs to  BIM  Construction,  this  very  important  topic  is  specifically  addressed  under  the  title \nBIM4RainWaterMangement. It is taught in the construction course in the 5th semester. The \nbasic goals of BIM4RainWaterManagement are: \nReturn of clean rainwater to the groundwater\nOptimization of a slow percolation\nUsage of a digital twin. \n\nWhen building the digital twin, the students apply the steps of the BIM4RainWaterManagement scheme (PETSCHEK 2019) and of course, they  use digital terrain  modelling. It is the \nbasis for quickly testing precise alternatives of retention and infiltration and thus find the \nideal solution for allowing rainwater percolation on site. The civil engineering software and \nthe  architecture  software,  combined  with  their  respective  strengths,  are  used  to  set  up  the \nBIM4RainWaterManagement project. \n\nThe 3D model has the following advantages: \nProof of retention areas in the event of heavy rain events. All water from roof tops and surfaces percolates on site, \nPrecise modeling of pavement surfaces and subsurface structures, \nParametric inlets, sludges collectors, manholes and pipes are included in the digital twin, \nClash detection between tree root balls and subsurface drainage elements, \nPrecise stakeout of all elements using BIM2Field, \nStudy of site grading alternatives with the help of automized grading (Grading Optimization in the civil engineering software). \n\nIn the next step, a link between the digital twin and numeric stormwater management soft-\nware will be established to validate the model and integrate it into a larger GIS analysis con-\ntext. Also extracting sustainability data from the digital twin will be an important topic. \n\nBIM4Trees \nStudents “plant” as part of the course described under point 2.3 BIMTrees. The trees were developed by Andreas Luka Consulting in close cooperation with Rapperswil, and the Swiss landscape contractor association Jardin Suisse, in which the Swiss tree nurseries are organized. \n\n3D trees are a major challenge for BIM in Landscape Architecture, as their geometry and \nproperties change significantly over the entire life cycle (LUKA & GUO 2021). Existing solutions did not adequately meet this challenge and could only insufficiently exploit the potential \nof BIM. Rapperswil therefore, supported the implementation of dynamic BIM trees with a \nresearch project. \n\nThe focus on the outer shells for crown, tree trunk and root, which are important for BIM, \nand their representation as solids allow both efficient clash detections and the extraction of \nvolumes  and  masses  for  further  analyses  (structural  engineering  calculations  and  shadow \ncasting). The tree size can be predicted interactively and quickly within the BIM software \nusing initial values for size when planting and growth functions for any point in time after \nplanting, complete with root in species, variety and age-specific shape and size. By simply \nlinking the very technical looking trees with Enscape3D, convincing visual representations \nof tree planting are created. \n\nThe feedback from independent study and bachelor projects flows directly into further development. There are currently 80 species\/varieties with the shapes and sizes according to \nthe Swiss quality regulations and the catalogue of a large German tree nursery. \n\nBIM2Field \nDigital Construction is based on digital data. In addition to the existing GIS data, one often \nneeds to collect his\/her own data. Point clouds from drone flights are a possibility. However, \nlandscape architects need height information below the tree and shrub layer. Drones cannot \nbe used here. The mobile laser scanner BLK2GO (LEICA Geosystems) is the latest generation of mobile laser scanning. A stationary device no longer has to be set up as before as the \nscanner can be used while walking through the site. This flexibility is extremely important in \nLandscape Architecture. The created point cloud is then post-processed and prepared for the \nBIM construction. The OST students use laser scanning as a regular tool in their projects. \n\nSince the fall semester of 2021, the surveying course has been renamed “BIM2Field”. The \napplication  of  one  person  tachymeter  stations  is  taught.  The  six  robotic  stations  use  the \ngeoreferenced  model loaded on the Construction  Cloud directly for stake out  without any \nintermediate steps. GNSS machine control systems in addition are presented in class by commercial companies. \n\nBIM2Cost \nThe central topics of the Landscape Architecture training in Rapperswil are cost calculations \nand construction specifications (specs). The 5th semester course, described under 2.3. is also \npioneering in this area. In Switzerland, the basis for efficient construction costs is the element-based  classification  eBKP-H,  which  was  developed  by  the  Swiss  Central  Office  for \nConstruction Rationalization CRB. The cost-relevant quantities are collected directly in the \ndigital twin. Automatically calculated dimensions and volumes result in more precise cost \nestimates. Due to clash detection, errors in the construction are detected early and thus in-\ncrease cost certainty. In the future, construction schedules will be displayed as 4D simulations \nusing the Common Data Environment ACC. In this way, the planned construction process \ncan be checked visually by the students and any contradictions can be solved. \n\nYouTube Channel \nThe YouTube Channel “Landscape Architecture Rapperswil” is the medium for the presentation of student independent study projects, thesis work and lectures on the topic. The following posts on building digital can be found on the constantly updated YouTube Channel: \nBIM2Field, laser scanner and robotic tachymeter: https:\/\/youtu.be\/icjaM3AoRa0 \nBIM2Field, 3D GNSS digger: https:\/\/youtu.be\/2MTnb7rV58o \nBIM Student independent study project: https:\/\/youtu.be\/q2SMcJ2knjg \nBIM Bachelor thesis: https:\/\/youtu.be\/0X8VUk-FyUM \n\nConclusion and Outlook \nDigitization in the Swiss construction industry is taking big steps forward; Landscape Architecture is not unaffected by this. Education in design, planting design, ecology and construction are the foundation of future Landscape Architects. In addition, skills in Digital Construction  must  be  integrated  today.  The  next  step  is  the  switch  to  Bring  Your  Own  Device \n(BYOD). From fall semester 2023, students are required to use their own devices in all Rapperswil courses. Computer labs will be no longer in use. It is also planned to integrate the \ndigital twin topic in the GIS teaching. In conclusion, OST Ostschweizer Fachhochschule in \nRapperswil acknowledges the challenges of Digital Construction and is riding along the digital wave.  

Digital Landscape Architecture Education – Where Do We Stand and Where Should We Go?  \n\nAbstract: Landscape architecture has a crucial role in designing landscapes to influence how they per-\nform in a desired manner and provide more resilient and adaptive environments. Sophisticated analyti-\ncal and design tools and techniques exist along with data from a range of allied disciplines that have \nthe capacity to inform and transform the way landscape design approaches are conceived. However, \nthese are not widely embraced across landscape architectural design schools as a status quo. This paper \naims to provide a prompt to initiate a critical theoretical discussion on the future pedagogical foci for \ndigital landscape architecture education discourse at the forthcoming DLA  conference in 2023. The \ncritical question is, what is the future direction of digital landscape architecture education to address \nthe pressing and complex challenges of the climate crisis? For the purpose of this paper and the confer-\nence, we have limited the focus to three streams of digital landscape architecture: approaches, tools, \nand techniques. These critical streams of the discipline are framed through a brief synopsis capturing \nlineages from the 1960s to identify their influence on landscape architecture design education. Patterns \nand processes that lead to shortcomings in implementing the approaches are discussed. This is con-\ncluded with a set of questions derived from identified gaps to stimulate a targeted discussion on the \nfuture trajectories of digital landscape architecture education. \n\nIntroduction \nIn the face of climate change, we are confronted with accelerated urbanization and environmental  degradation.  \nThe  transformation  of  our  landscape  and  urban  systems  toward  more \nequitable, resilient and adaptive environments is urgently required, imbuing the capacity to \nrepair  and  respond  to  future  crises  and  to  adapt  to  unpredictable  futures  (ELMQVIST et  al. \n2019, SHEARER et al. 2021, FRICKER 2022a). \n\nDigital design education in landscape architecture that considers scales of action from the \nplanetary to the regional and microbial, has a crucial role in equipping students with the design  capabilities to  generate  alternative  typologies  of  aesthetics  and  performance  (MEYER 2008, FRICKER et al. 2020). This includes landscape transformations that perform in a desired \nmanner  (STEINITZ 2012, URECH et  al. 2020, GRÊT-REGAMEY et  al. 2021). To  this  end,  the \ndiscipline has developed sophisticated design and analytical tools, such as 3D point clouds, \nas a basis for urban design and algorithmic analysis of energy absorption, wind flow, and \nshadow  provision  (URECH  et  al.  2020,  2022).  They  support  an  integrated  analysis  across \nscales and feedback loops, as evidenced in several recent projects, like the example of a river \nrehabilitation project at the local scale that consequently explores the larger catchment area \n(VOLLMER  et  al.  2015,  GRÊT-REGAMEY  2017).  An  illustrative  overview  of  tools  and  approaches for responsive \nlandscape design is provided by  WALLIS & RAHMANN (2016), as \nwell as by CANTRELL & HOLZMANN (2016). The publications provide a comprehensive overview of design projects and \nresponsive technologies that frame performance as a generative \ndesign approach. Furthermore, heterogeneous data on environmental and socio-economic as-\npects are available with unprecedented detail to inform design approaches. For example, ur-\nban sustainability transformation projects that use passively sensed geospatial data of land \nuse, service networks etc., may be augmented by active sensing of stakeholder perceptions \nand behaviour  with participatory  methods and technologies (GRÊT-REGAMEY et al. 2021). \nHowever, although increasingly more tools and datasets are developed, and the agency of \ntheir application is demonstrated in prototypes (CANTRELL & HOLZMAN 2015), they are not \nwidely used across landscape architectural design schools. We postulate that a critical dis-\ncussion on digital design education is lacking in the discipline of digital landscape architec-\nture  (FRICKER 2022b).  Therefore,  we  propose  to  investigate  this  in  the  forthcoming  2023 \nDigital Landscape Architecture conference. \n\nThis paper acts as a precursor for the future conference dialogue to interrogate where Digital \nDesign Education is positioned and how to advance the pedagogical approaches of digital \nlandscape architecture. For this,  we  want to highlight some of the  existing theories of the \ndiscipline in the discussion, describe the status quo, and point out recent  “streams of con-\nsciousness”. This demonstrates that the landscape architecture discipline has constantly been \ninfluenced by and porous to other disciplines, thinking, tools and techniques that have con-\nstructed amorphous streams and trajectories continually being made and positioned (FRICKER \n2021). A complete review of the theoretical streams in digital landscape architecture is be-\nyond the scope of this paper. However, we reflect on specific critical theories and associated \ntools and processes from the 1960s to today that have significantly influenced current educa-\ntional practice in digital landscape architecture. The intention is not to give a comprehensive \nhistory of digital landscape architecture, but a framing of various digital design approaches \nin landscape architecture as a departure point for a discussion on future tools and techniques. \nWe use this review to discuss recurring patterns and processes in how new approaches and \ntools are used and how the gap in implementation manifests (ERVIN 2018). This leads us to \nformulate concrete questions to specify further: How do students need to be taught digital \napproaches? Where should we focus on enhancing our students' teaching? Furthermore, what \nneeds to be taught in digital landscape architecture education? \n\nA Synopsis of Computational Lineages \nFrom System Thinking to Artificial Intelligence \nThis chapter aims to reflect on a selection of relevant concepts and  workflows developed \nmainly during the 1960s and 1970s, which strongly influenced a pedagogy for the computa-\ntional realm and demonstrated a radical approach to creatively interact with diverse data sets \nacross scales. The purpose of this brief historical reflection is to unveil concepts to be revisited within the current discussion on defining possible avenues for adjusting the present trajectory of the digital pedagogy in the field of landscape architecture. Due to the richness of \nhistorical  references,  the  discussion  is  focused  on  key  examples,  inviting  for  an  extended \ndiscussion towards the future of the digital landscape architecture education and implementation within practise. Note: the selected examples in this paper address only male scientists. \nWe want to acknowledge that in these known lineages, female leaders in the field often need \nto be discussed as being more instrumental to the development. We aim to capture and slowly \nrectify this familiar narrative in future discussions. \n\nAlready towards the end of the 60s, computational design thinking pioneers recognized the \npotential of machine-human interaction to sound out new potentials within architecture and \nlandscape architecture. Almost 25 years later, the integration of “computation” in teaching \nushered a fundamental pedagogic change in direction for design teaching, research, and a \nform making language. In addition, a parallel stream “Digital Design Education” established \nitself with a focus primarily on the visualization applications of digital tools and the teaching \nof new software (ERVIN & HASBROUCK 2001, FRICKER 2021). The presented historical overview allows for a discussion in order to shift the focus from a merely tool-based approach \ntowards holistic computational design thinking. \n\nThe history of computation goes far beyond the development of computing technology and \nrelates to the “interaction between internal rules and (morphogenetic) pressures that, themselves,  originate  in  other  adjacent  forms  (ecology)”  (MENGES & AHLQUIST 2011, 8). This \ncomplex theory and framework of relationships is based upon theories from disciplines like \nmathematics, computer science, cybernetics, biology and philosophy. The integration of in-\nformation technological developments into the landscape architectural curriculum accelerated especially during the 1960s and 1970s through an intensive exchange between cybernetics and its influence on architecture (MENGES & AHLQUIST 2011). This first manifestation \nwas driven by a deep theoretical discourse and led to the first integration of Artificial Intelligence (AI) in design methodology. The fusion of these two areas lay in new questions related \nto the rise of global ecological challenges, which also changed our relationship to data and \nour interaction with the information it contained (FULLER 1969, MEADOWS et al. 1972). The \ntheories developed in the area of cybernetics allowed a new computational design method to \nbe established mainly within architecture, which describes this complex network of relationships  through  the  integration  of  System  Theory  and  Patterns  (FRAZER  1993).  In  the  late \n1960s, Jay Forrester, a computer engineer and system  scientist by education, strongly engaged in describing the “systemic structure responsible for the dynamics of urban development and decay”, founded the Urban System Group at MIT (FORRESTER 1973). \n\nThe themes discussed between cybernetics and architecture influenced simultaneous developments in landscape architecture with respect to the domain of system thinking in the field \nof spatial data handling. This is because both the fields of architecture and landscape architecture  were called to address issues of rapid urbanization.  Though the field of landscape \narchitecture recognized the necessity of developing new approaches for handling data, it did \nnot develop meaningful questions or further research with AI. \n\nEmerging Pedagogical Principles \nOne of the pioneering academic institutions, introducing a new form of design education with \nspecial focus on computational design thinking, was the Ulm School of Design (Hochschule \nfür Gestaltung in Ulm, HfG), active from 1953 until 1968. Already 17 years before the foundation of the Architecture Machine Group by Nicholas Negroponte and Leon Groisser, at a time “computers were only available at a few research centres, (…) their capabilities were \nwidely recognised and the subject of much broader theorisation and influence, opening up \nthe field of logic and computer science to the social sciences and arts” (NEVES et al. 2013, \n292). The new pedagogical approach of the HfG is understood as a research-based activity, \nstrongly engaged in theoretical discourse focusing on a new understanding of design, which \nis based on thinking in connections and networks. \n\nThe “pioneered heuristic procedures that were related to the power of the new computational \nmethods” (NEVES et al. 2013, 299) developed at the HfG can be seen in strong relation to the \ncomputational  education  developed  by  Negroponte.  Negroponte  recognized  that  problem-based learning concepts and the opportunity to work together with the computer for direct \nfeedback  significantly  increased  students’  motivation.  Programming  was  understood  as  a \nnew way of thinking! Negroponte experimented with the potential of formal descriptions of \narchitectural solutions, implemented through a program and deployed as Computer  Aided \nParticipatory Design. Thus, he laid the foundation for current methods in the field of AI and \nemphasized, “However, remember that these systems assume the driver to be an architect” \n(NEGROPONTE 1975, 365). The influence of the early computational design development in \nthe education of architecture has had very little impact on the area of landscape architecture \neducation. The only traces of limited integration of computational design can be observed at \nthe newly founded Laboratory of Computer Graphics at Harvard Graduate School of Design \nin 1965. Contrary to the radical development and interaction with data for generative purposes at the Arch MAC Group at MIT, landscape architecture education at Harvard’s GSD \nconcentrated mainly on layered data-mapping methods. \n\nThe Evolution of Spatial Thinking: From GIS-based Layering towards Mapping \nThe field of landscape architecture focused its emerging computational possibilities on re-\nsearch and application in teaching during the 60s and 70s on problems related to Big Spatial \nData. The pressure to develop new methods to process the complex relations of nature-based \nprocesses was strengthened by the new arising “Ecological Awareness”. The idea of layering \nspatial information and its use for evaluating designs was presented 1971 in the book “Design \nwith Nature” by Ian McHarg (LEE et al. 2014). Based on this idea, Geographic Information \nSystems (GIS) originated largely at Harvard GSD enabled to geographically allocate digital \ndata and create maps (FOSTER 2016). Further developments in the 1970s and 1980s focused \non  spatially  analysing  the  system  from  different  aspects.  In  the  following  period  the  user \ninterfaces, data processing capabilities and data interoperability were enhanced, and with this, \nits  applicability  for  many  user  groups  (LEE at  al. 2014). This  development  enabled  easier \naccess to digital geodata and simulations for assisting in a design process, and in 2012, Carl \nSteinitz published a “Framework for Geodesign”, which presents an iterative process of integrating stakeholders’ knowledge, needs and desires, geospatial modelling, impact simulation and rapid feedback on the degree of achieving a desired goal to facilitate an informed, \nresponsive design (FOSTER 2016, STEINITZ 2012). \n\nCurrent “Streams of Consciousness” \nStreams of consciousness describe time infused recursively in the material reality of the landscape through states of formation, from those that signify stability to sequences that are predictable and observable processes of change to those that are uncertain and instantaneous. \nMASSUMI (2002) suggests that our own “human” sensing of the world experienced through \nsensation involves a “backward referral in time”. Therefore, a sensation is organised recursively prior to being part of our conscious chain or actions and reactions. In this process, the \nsmoothing over of the anomaly is made to fit our conscious requirements of continuity and \nlinear causality. \n\nThe act of measuring and making the landscape is not a neutral activity; therefore, the process,  techniques,  and  tools  of  representing  form  are  rooted  in  a  specific  understanding  of \necosystems and their processes. “Actant is a term from semiotics covering both humans and \nnonhumans; an actor is any entity that modifies another entity in a trial; of actors it can only \nbe said that they act; their competence is deduced from their performances; the action in turn \nis always recorded in the course of a trial and by an experimental protocol, elementary or \nnot” (LATOUR 2004). Tools for measuring the landscapes, and the techniques by which we \ndeploy them, have their own constraints that translate and transform information. The representations we make are constructed from a set of instruments, codes, techniques, and a lineage  of  conventions.  Consequently,  the  worlds  they  describe,  and  project  are derived  only \nfrom those aspects of reality susceptible to those techniques. These acts of measuring, anaysing and making the landscape can formulate a view of what already exists and set conditions for new worlds to emerge. Below are three examples of what we refer to as porous, \nconstantly evolving “streams of consciousness”. \n\nEntangled Knowledge Systems: STEINITZ’S (2012) framework provides a clear structure on \nhow to design a design process for multi-disciplinary collaboration to better address the complexity of environmental problems across scales (FOSTER 2016). Along with the emergence \nof the new field of Geodesign, geodesign education programs were launched (WILSON 2014) \nand today, universities worldwide participate in the International Geodesign Collaboration \n(https:\/\/www-igcollab.hub.arcgis.com).  A  major  challenge  in  the  education  of  Geodesign, \nhowever, are the strict disciplinary silos at the universities that hinder cross-disciplinary collaboration (WILSON 2014). Further, recent evaluation of geodesign processes reveal that not \nall projects implement the full structure and particularly the analysis of spatial relationships \nand impact analysis across scales are often not well performed (GU et al. 2020). \n\nThe emergence of geodesign and other GIS-based methodologies coincided with the critical \ndiscourse on big data and the development of open-source systems that enabled collective \ncontribution and alternative algorithms to reveal bias in large data sets. In landscape architecture education, students were educated on the ethical and responsible use of big data to \ncritically address the inherent power that data has had historically in producing unjust actions \nand policies on the oppressed. The emergence of “hacking” data approaches and the creation \nof alternative data sets as a public good and public service consequently emerged (GABRYS 2016, WILLIAMS & PROQUEST 2020). \n\nEmergent Patterns: In another stream, contemporary research in landscape architecture ad-\ndresses, in particular, the technical challenge of best-representing geo-data and environmen-\ntal factors to foster an understanding of information and making sense of it (URECH et al. \n2020). For example, a collection of drawing types, such as diagrams, axonometry and map-\npings, has been assembled (AMOROSO 2015). In response to more complex landscape rela-\ntionships and organizational patterns in landscape architecture education, digital syntax, such \nas codes and patterns, is used to establish quantitative correlations between the landscape and \ndata processing. These approaches are utilized as a generative component for design produc-\ntion (M’CLOSKEY & VANDERSYS 2017, CANTRELL & MEKIES 2018). \n\nInternational Fields: With the environment globally changing more rapidly than ever be-\nfore, in the first two decades of the 21st century the awareness of urgency for an immediate \nresponse for solving problems increased. This gave rise to the use of point clouds recorded \nin the field with a  terrestrial laser scanner to rapidly replicate the physical landscape with \nhigh resolution and fidelity and as basis for analysis and design (GIROT 2019, URECH et al.2020). The point cloud models provide a common ground between architects, engineers, and \nscientists to develop informed landscape design (GRÊT-REGAMEY et al. 2021, GRÊT-REGA-\nMEY 2017, VOLLMER et al. 2015). By performing geospatial analyses using the geometry of \nthe point cloud  model, spatial configuration parameters can be investigated and enhanced \nemploying simulation models, e. g., for improving climate conditions through altering build-\ning  and  vegetation  patterns  (URECH et  al. 2020).  In  this  way,  the  point  cloud  models  and \nimmersive data interaction allow for more dynamic and versatile forms of landscape design \nthrough all scales involving aesthetic and performance considerations (GIROT 2019, URECH \net al. 2022). But the approach is still very experimental and has not yet found widespread use \nin digital design education in landscape architecture. \n\nDiscussion and Conclusion: How to Consolidate the Gap? \nWhen we look at the outlined examples, there are some recurring patterns that suggest a gap \nin the implementation of tools and approaches. A major concern is that often the full understanding is left out of what the process behind a generated solution is. In particular, this is \nevident in three crucial pitfalls of tool implementation, which we exemplarily point out as: \n(1) using “black box” digital tools, (2) improper calibration, and linear processes (3) focusing \non single aspects rather than interactions and processes across systems. \n\nConcerning  the  first  pitfall  with  the  tremendously  fast  development  of  cutting-edge  tools, \ndesigners become mere users without an understanding of the underlying processes and the \ninherent critical distance to the results. Looking back in the history of digital landscape architecture, the invention and use of digital tools in the design process (such as Grasshopper) led to concerns of employing a “black-box” optimization, taking the output as a goal in itself \nand lacking a more holistic systems thinking (FRICKER et al. 2020). \n\nSecond, not understanding the complex relationships of the defined parameters of a model \nand making uninformed choices of input data can also lead to wrong design decisions and \noptimization processes. For example, a data set is assembled only in the beginning of a design \nproject  and  often  neither  updated  nor  further  data  is  collected  according  to  the  generated \nsimulation  outputs  (FRICKER 2021).  Overall,  a  critical  engagement  with  the  collected  and \ngenerated data across scale and fields is missing. \n\nThird, there is a risk of justifying a design through simulation results on single aspects or on \none specific scale while the design solution actually is not solving the problem when examined  at  a  large  scale  because  of  mutual  interactions  of  single  aspects  on  various  scales \n(FRICKER et al. 2020). Disregarding aspects can lead to undesired developments, for example, \nfocusing only on the design site for river rehabilitation one might overlook effects of developments in the catchment area still leading to severe flooding (VOLLMER et al. 2015). An urban densification that helps minimize urban sprawl can increase the urban heat island effect \nand negatively affect a series of services provided by the urban ecosystem such as the provi-\nsion of recreational area, storm water infiltration and retention, or habitat for species (GRÊT-\nREGAMEY 2017, WISSEN HAYEK & GRÊT-REGAMEY 2021). There is a lack of understanding \nof system dynamics, spatial patterns and relationships (WOOD 2017). \n\nLandscape architecture as a discipline is evolving rapidly as it responds to both broadening \nand intensifying changes in environmental, social and political conditions. These changing \nconditions require development and innovation in the digital competencies of landscape architects. What approaches, digital skills and technologies are needed by landscape architects \nto equip them to deal with the complexities brought forth by the climate crisis? Then comes \na  critical  consequential  question:  how  can  we  design  the  education  of  future  practitioners \n(MONACELLA & KEANE 2023). \n\nThe transformation of the digital landscape architectural education must involve profound \nreconfiguration  of,  and  innovation  in,  discrete  knowledge  systems  within  the  pedagogical \nframework of the curriculum, including the course’s techniques, approach and nature of the \nway students are taught and learn. In conclusion we posit the following questions for discussion: \nWhat is the current status of  pedagogical approaches to digital landscape architecture \ntechniques, tools and approaches? What are the former “streams of consciousness”? We \nargue that streams of consciousness are porous lineages and trajectories historically influenced by broader contextual innovations and pursuits. \n\nWhat are the recent critical developments in digital landscape architecture and related \napproaches? What are the current “streams of consciousness and potential challenges in \nrelation to emerging fields like Artificial Intelligence and Machine Learning”? \n\nWhat are the gaps in the technological-based technique developments in digital landcape architecture utilized to address the climate change related issues and their translation in advancing pedagogical approaches? \n

Enhancing Technical Grading Education: Finding the Right Tools for the Job \n\nIntroduction \nStated simply, grading design involves the alteration of the surface of a site to direct water \nflow. The reality of technical grading design is much more complex and involves the consideration of above- and below-ground, three-dimensional spatial relationships between people, \nvehicles, buildings, walls, pavements, utilities, plantings, and more. Considerable time and \nexpertise are required to develop the complex cognition necessary to produce complete and \ncorrect technical grading designs. Teaching technical grading skills to diverse groups of novice landscape architecture students is a difficult task for this reason – expertise and time are \nboth in short supply within four- or five-year undergraduate, and especially so within two-or three- year graduate academic programs.  \n\nIt  is  well-documented  that  learning  can  be  enhanced  by  using  both  static  and  dynamic \ngraphics (i. e., visualization tools) in instruction, see SCHNOTZ 2002 for a review. Learning \nenhancement can occur with the use of visualization tools both in external instruction and \ninternal learning processes. Teaching and especially learning of design skills heavily depends \non the use of visualization tools. In external instruction, design concepts (including technical \ngrading) are introduced and discussed, then modelled and practiced within a lecture\/studio \nenvironment using problems with varying degrees of application to “authentic” or real-world \nsites and problems. Students are expected to expand their learning of those concepts internally, by exploring design options and relationships with various graphic tools, such as drawing, modelling, rendering and\/or animation of views which they employ to visualize and understand the spatial relationships and tactile qualities of their design proposals.  \n\nGrading instruction necessarily relies on the use of two-dimensional visualization tools to \nboth explore design relationships and document grading changes. Sections and plans are two \ncommon visualization tools relied upon in grading instruction to foster design and documentation of grading plan proposals. Design grading, as a more conceptual, early-process activity, lends itself to design study via three-dimensional modelling. These models may be successfully used to rapidly understand initial conceptual or roughly detailed grading relationships and surface morphology. Technical grading cannot be easily or quickly explored with precision via complex three-dimensional modelling as the grading must first be completed before it can be modelled. This study hypothesizes that despite the use of complex external \nvisualization tools being less appropriate for use in technical grading design and communi-\ncation tasks, complex internal visualization operations must be utilized by learners to under-\nstand and manipulate the complex relationships among site design components in an efficient \nand confident manner. How, then, does an instructor best facilitate students’ internal visual-\nization skills without relying on the computer to visualize for them? Additionally, how does \nthe student avoid spending time developing complex digital models at the expense of devel-\noping the complex cognition required for technical grading competence? \n\nCognitive Load Theory \nThis study was initiated and informed by a literature review of Cognitive Load Theory (CLT) \nand through analysis of instructor responses to perceived student needs and direct student \nquestions during a semester-long design studio course. CLT was used as a lens through which \nto examine potential barriers to student development of the complex cognition required for \nindependent technical grading competence in the studio environment, and to inform instruc-\ntional changes to help promote higher degrees of learning. CLT assumes that two major goals \nof instruction are to facilitate the construction of internal schemas (models) and to automate \ntheir use to mitigate the significant impacts the limitations of human cognitive architecture \nhave on our ability to cognitively process complex learning (KALYUGA et al. 2003). In this \nauthor’s  opinion,  this  is  the  primary  goal  of  instruction  in  technical  grading  realms  –  the \ndevelopment of internal models, as opposed to external models (frequently manifesting as \nvisualizations and\/or displays), by which to process the complexity inherent in technical site \ndesign thinking quickly and efficiently. The limitations of human cognitive architecture may \nbe described in terms of three types of cognitive loads (for a more complete description, refer \nto RENKL & ATKINSON 2003):  \n\nIntrinsic Load refers to the complexity of the learning material itself. Technical grading \ninstruction may carry a high intrinsic load due to the complexity inherent in the interactivity  between  many  different  site  design  relationships.  Intrinsic  load  is  related  to  a \nlearner’s prior knowledge and should be expected to be at its highest levels with novice \nlearners. Intrinsic load may manifest as an information and\/or decision-making overload \n(too many requirements or too many relationships to attend to simultaneously). This load \ncan  be  decreased  with  experience  as  learners  develop  more  meaningful  information \nchunks which can be stored in long-term as opposed to working memory. \n“Germane Load refers to demands placed on  working  memory capacity that are imposed  by  mental  activities  that  contribute  directly  to  learning”  (RENKL  &  ATKINSON \n2003). This is the load learners should focus their cognitive resources on to facilitate \nlearning most successfully. \n“Extraneous Load is caused by mental activities during learning that do not contribute \ndirectly to learning” (RENKL & ATKINSON 2003). Due to the high intrinsic load inherent \nin technical grading tasks, it becomes very important for instruction to be designed spe-\ncifically to reduce extraneous loads. Considerable extraneous load is related to low levels \nof expertise. Low expertise can contribute to a simple lack of understanding of how to use available information and tools, and\/or inefficient use of the available information \nand tools. \n\nTo limit the impact of intrinsic and extraneous loads, an understanding of the technical grading expertise held by learners is critical to determine what information is relevant and how \nto present it to maximize the learner’s ability to attend to it. Novice learners generally learn \nbetter when given higher degrees of instructional guidance as they still need to develop their \navailable  schemas.  However,  more  knowledgeable  learners  (those  with  more  and\/or  more \ndetailed schemas) may require a different instructional approach that limits redundant information, otherwise they may experience cognitive overload and poor learning, despite their \nhigher level of expertise. This difference is termed the expertise reversal effect (KALYUGA et \nal 2003). It is theorized that this expertise reversal effect plays a role in the cognitive processes used by students to process technical design instruction activities and in the visualization tools and processes they use to supplement their learning. Therefore, it is important to develop instruction that recognizes and responds to the variable levels of expertise among \nthe students in a course, both related to the processes and visualization tools utilized to complete technical grading design. This study was undertaken to begin to understand the range \nof expertise variability and to theorize diagnostic tools which can be used in targeting instruction activities maximizing independence and development of expertise in the technical \ngrading design realm. \n\nInstructor Response Analysis \nThe experience of teaching this course over the past three years has been that the most educational impact (i. e. attention to germane load) comes from direct and personalized individual instructor interaction with students during their problem-solving process instead of group classroom interactions. Currently, the instructor must invest considerable time into individual \ninstruction to achieve this impact, so an analysis of instructor responses during these individual interactions was undertaken to balance effort and maximize learning. The analysis seeks \nto identify patterns among the actions or visualization tools recommended, the frequency of \nrecommendations and how those recommendations were accepted and implemented by the \nstudents. Additionally, the analysis sought to determine if there was any discernible impact \non the levels of independent thinking and use of visualization tools to enhance internal visualization processes. \n\nThe analysis focused on the following interactions due to their potential capacity to directly \nreduce intrinsic and extraneous loads. Intrinsic loads can be reduced by personalized discussion regarding  how to  use the information available, and  how to produce any lacking  but necessary information. Extraneous loads can be reduced by introducing and directly modelling the use of simple, abstract visualization tools to think and produce more complex internal \nvisualizations supporting technical grading design efforts. \n\nResponses were analyzed from interactions over three semesters of a studio course focused \non  the  technical  grading  design  of  a  complex  real-world  development  site.  The  fall  2020 studio (enrollment=18) considered a nearly 5-acre office development, fall 2021 studio (enrollment=23) focused on a 7-acre multi-family development and the fall 2022 studio (enrollment=19) designed a 5-acre office development. The course consists of a studio component with  460  minutes  (7.7  hours)  of  weekly  contact  time  (which  includes  lecture  time)  and  a separate  110-minute  (1.8  hours)  per  week  CAD  lab  component.  All  students  in  the  target course were novices with a negligible degree of variation among their prior grading expertise. All students were introduced to grading activities in an earlier course with a focused grading component where they were presented the grading process, techniques for visualizing landform and interpolating elevations, and developed grading skills via a grading design project. \n\nVariability of Student Expertise \nResults of the study identified potential barriers to learning situated within four pathways: \nLived Experience Variability: This pathway is defined as individual differences in recall\/codification (chunking) of actual human experiences, such as walking across surfaces,transitioning  grade-change  devices, noticing materials\/textures\/connections\/joints, etc. and linking or chunking those experiences together with technical grading skills in meaningful ways. \nCausal Chain Recognition: The skill to recognize critical relationships among existing \nand proposed component parts in the context of a technical grading design. \nInternal Animation: The ability to internally animate objects to transform them rotationally and\/or positionally  within the  site, or to animate and visualize  water  flowing across\/through elements of a site. \nDigital  Expertise  Variability:  This  pathway  defines  individual  differences  in  both \nknowledge of digital visualization tools (what they are and what they do) and how to \nmake them work to solve particular problems. This pathway refers both to simple inexperience\/lack of knowledge, and self-inflicted or self-limiting inexperience (such as refusal to spend the time needed to fully understand a software program). \n\nLearning activities in the first three of these pathways require the use of relatively simple external visualizations (such as diagrams, plans and sections) to support considerably more complex internal visualization and transformation operations such as flows and inferred motion – both of which represent aspects of mental animation (HEGARTY 1992).  \n\nCausal Chain Recognition supports the identification and documentation of critical relationships between site plan elements, and the visualization of responses to transformations \nof plan elements. CCR is the skill one would use to understand that as one corner of a flat \nrectangular pavement surface is depressed (lowered in elevation), the rest of the pavement \nsurface will tilt in that direction unless the surface is broken, creased, or otherwise deformed to accommodate multiple slopes. Thus, complex internal animation is required to mentally transform site objects and\/or surfaces with elevation differences efficiently without relying on an external visualizations to understand those transformations.  \n\nInternal  Animation  is  a  skill  utilized  when  considering  how  water  moves  across  and\/or through the system. Assuming water droplets remain intact from the moment they strike the surface until they leave the site at the outfall, technical graders should be able to trace a drop of water from the point it contacts the surface all the way to the site drainage outlet using internal animation. This skill requires the water to be mentally animated as it travels across surfaces and through conveyances. \n\nIt is hypothesized that Causal Chain Recognition, Internal Animation and, to an extent, Digital  Expertise  Variability  can  be  directly  influenced  via  instruction  emphasizing  germane loads, though this paper focuses only on addressing improvements to Causal Chain Recognition and Internal Animation. The study was conducted under the assumption that the creation  of  complex  external  visualizations,  such  as  detailed  3D  models,  would  contribute  to \nhigher  extraneous  loading  in  the  context  of  the  technical  grading  course,  so  instructor  responses were constrained to primarily 2D graphics, including static 3D views, but not models. Highly detailed 3D models of proposed grading solutions were not required or recommended by the instructor as a part of this course. However, TIN surfaces created from existing contours were required to be created using Civil 3D, and the use of the “Quick Profile” tool recommended for understanding existing topography and quickly testing proposed solutions and relationships. Additional solutions have been considered to address Lived Experience Variability, but those have yet to be implemented and tested in the course and will be addressed in a future paper. \n\nInstructional Methodologies \nFour  novel  technical  grading  instructional  methodologies  have  been  theorized  to  address learning improvements for each of the pathways mentioned in the previous section. Instructional methodologies were developed to both function as diagnostic tools to identify needed areas of focused instruction, and to facilitate the packaging or “chunking” of information to minimize  negative  cognitive  loads  and  enhance  development  of  technical  grading  design skills in novice learners.  \n\nSpot Skipping: a method of intentionally widespread, but very limited calculation of spot \nelevations early in the grading process which directly supports the recognition and calculation of critical grading relationships as a part of the Causal Chain Recognition pathway. \n\nFlow Branch Analysis: a method targeting spot elevations defining individual branches of \nthe site flow pattern to analyze flows and inform early technical grading design. This method of analysis is primarily concerned with flow lines and may be used independently or concurrently with Spot Skipping and primarily supports Internal Animation as water flows are visualized and defined across a site.  \n\nFlow Barrier Analysis: a method of analyzing site plan objects in terms of their impact on \nwater and\/or people flows. This method of analysis allows for the chunking of the site into \nflow barrier types and works to define and describe water flow patterns supporting Internal \nAnimation.  \n\nTransect Grading: a method of spot grading along discreet transects, usually drawn perpen-\ndicular to water, pedestrian and\/or vehicular flow paths, rather than at locations where spot \nelevations would be commonly calculated and included on a technical grading plan. Transect \nGrading  is  another  method  of  chunking  the  critical  relationships  between  elements  of  the grading plan into easy-to-understand sections, primarily supporting Causal Chain Recognition. Transect Grading may also be used in conjunction with Spot Skipping and Flow Branch Analysis. \n\nDiscussion and Conclusions \nEvidence from preliminary use of the four instructional methodologies described above in \nthe fall 2022 course seems to support the hypothesis that certain visualization tools may impose high extraneous loads on students. Their level of expertise in both interpreting critical grading design relationships and in the construction of suitably precise models is low enough that they don't  yet have the detailed schemas needed to develop efficient, low-extraneous-load processes. 3D model construction was deemphasized in the course and student outcomes seemed  to  improve.  Whether  the  improvement  was  related  to  the  lack  of  effort  spent  on model-building or simply more time developing grading skills has not yet been studied. However, CLT would support the notion that regardless of the reason, germane loads were prioritized and learning improved. \n\nResults suggest that, while digital drafting tools and Civil 3D can assist in drafting precision and in the process of working through the four methods, no complex visualization tools are required to achieve a high degree of expertise in technical grading (see Table 2). Even the 3D views may be sufficient if drawn inaccurately by hand or quickly and roughly modelled without any elevational precision in a program such as SketchUp (see Figure 3). Documentation and external communication of the grading solution may be best completed with sophisticated visualization tools, however this communication is secondary to the grading itself. The development of the grading design, to a high degree of detail, should be easily achieved using simple drafting tools and hand graphics if care is taken to do so with the required precision. This hypothesis must still be tested to confirm whether the use of complex external visualization tools  would be helpful in developing internal animation skills among novice technical graders.  \n\nThe production of a construction document quality grading plan has been a requirement for \neach iteration of this course and, given the complexity of the Civil 3D platform used to document those solutions, it is possible that some of the negative observations within the study may stem from extraneous loads imposed by the required documentation rather than issues regarding grading skills. The opinion of this author is that construction documentation should be an integral part of any technical grading plan. The primary purpose of the technical grading plan is to facilitate site construction and, learning to communicate design intent to the appropriate audience with the appropriate visualization tools should be the goal of any design education. Perhaps there should be a different focus on the documentation aspect of the grading plan, either concurrently or in a different semester. More work needs to be done to determine where any differences might exist between grading design skill and grading documentation skill. \n\nIt is important to note that the methodologies examined in this paper were applied to fine \ngrading of a site surface. Detailed considerations of mass grading and site stormwater management, such as balancing cut and fill and sizing stormwater management facilities were not included  as  a  part  of  the  target  course.  Accordingly,  additional  research  must  be  done  to examine the relationships between successful fine (surface) and mass grading activities while utilizing the methodologies described in this paper. \n\nThis study raises the question of which approach is the most appropriate for the most efficient transfer of knowledge and development of technical grading skill, the project-level approach, focusing  on  direct,  real-world  application  (as  presented  in  this  paper),  or  the  vignette  approach, focusing on individual skill development and repetition. The course within this study primarily relies upon the former, project-level approach, but does incorporate aspects of the vignette approach within the workshop and demonstration interventions, and the opinion of this author is that a combination is ideal. More work is required to answer the questions of what that combination should look like and how much time and effort should be spent by instructors and students within each. The four methodologies developed through this study should be developed into an online rapid diagnostic tool to identify the levels of technical grading expertise in a student population over time, and to match more detailed instructional methodologies to those students to help them overcome challenges to cognition and development of the expected technical grading competence. This study also suggests that additional  exploration  is  required  to  more  fully  understand  the  relationship  between  cognitive \nloading and the use of digital design skills and visualization tools versus analog design skills and visualization tools in an educational environment, especially in the realms of technically complex design tasks. \n

Future Resilient Landscape [Architects] \n\nAbstract: Parametric and computational design processes will evolve and inform the field of landscape \narchitecture. This paper investigates a bottom-up teaching approach about parametric design to novice \nlandscape architecture students as a viable method in their design pursuits. Using a case study, students \nexplored a translation of an intuitive approach to design into a parametric script, taking them through \nconcept ideation, fabrication, and ultimately informing implementation. \n\nIntroduction \nResilient landscape architects will be those who can anticipate, analyze, and address complex \nlandscapes, including those challenges yet to emerge. All professional fields are developing \ncutting-edge technologies to facilitate the analysis of complex issues and the implementation \nof viable solutions. The emergence of new technologies in landscape architecture has been a \nsignificant factor in the development of this discipline and has facilitated relevant research \nand design processes. Landscape architecture, now maturing with its own digital design practice  including  computational  design  (popularly  described  as  parametric  design)  is  gaining \nmomentum. The use of digital tools and techniques in the field of landscape architecture will \ncontinue to grow and evolve in the coming years (WALLIS & RAHMAN 2016).  \n\nFascinating  examples  of  new  computational  approaches  and  applications  are  emerging  in \nlandscape architectural projects. However, Bradley Cantrell and Adam Mekies believe that \nthe mechanism through which these applications are implemented remains obscure. This is a \nmissed opportunity since the logic, the thought process and the utilization of parametric design, could have been more evident to launch the complex execution (CANTRELL & MEKIES 2018). \n\nSince 1967, the MIT Media Lab has successfully “civilized” or “tamed” design and computer \ncode through years of effort. In 2003, the team designed the “Scratch” programing language \nso which began to employ a graphic interface rather than the laborious coding string (NAGLE \n2014). \n\nMitch Resnick, a computer scientist at the MIT Media Lab, conducts the “Lifelong Kindergarten”, where children learn to code and create from a very young age (RESNICK 2014). As \nResnick indicates, “When you learn to read (code), you can then read (code) to learn.” \n\nCoding identified as the core to parametric design describes parametric design thinking as a \nmethod, and not a tool. Do design school curricula or instructors provide effective strategies \nto  increase  the  broader  adaption  of  recent  technologies,  specifically  parametric  design, to \nfuture students?  \n\nIn recent years, the potential of computational media and its syntactical interface has been \nwidely explored by young designers through the GUI (Graphical User Interface) syntax of \nscripting. “How can we leverage this newly acquired foothold and understand better what we \nare gaining from parametric modeling\/visual programming\/coding as a design process and \nconceptual generator?” (CANTRELL & MEKIES 2018). \n\nIn this study, the authors investigated a kinaesthetic learning approach to cultivate a bottom-up understanding of the computational design process and engage students with advanced \ndigital tools. The goal was to encourage novice students to learn implicit knowledge of the \ncomputational design process first, and then learn explicit knowledge, in the following semesters. The emphasis was to employ parametric design tools in the design process rather \nthan limiting, or devaluing, their use to mere digital representation efforts. \n\nThe research team supervised a group of six (6) second-year Bachelor of Landscape Architecture students, interested in a public art competition, to utilize computational design tools \nin concept and design development of a public art piece, and subsequently, its off-site digital \nfabrication and production, and on-site implementation. Prior to this project, five of the six \nstudents had not utilized commercial 3D computer graphics and computer-aided design application software, such as the Rhinoceros 3D, as used for this project. The use of computational design tools  facilitated, and elevated, conventional design process activities into an ambitious design and implementation proposal.  \n\nAnalogue (Kinesthesia) to Digital (Parametric)  \nThe group of six (6) students enrolled in an independent course (design studio), co-taught by \ntwo faculty members, to prepare a design proposal for a public art design competition. Structured into three (3) phases, the studio included analogue rule sets, parametric\/digital rule sets, and fabrication. \n\nThe South Park project, by Fletcher Studio, inspired the method so that the computational tools test the resilience of analogue rules for spatial partitioning within a small park in San Francisco, CA. That project’s research was prepared for the Acadia 2014 exhibition, an annual parametric design conference (CANTRELL & MEKIES 2018). When Fletcher Studio first began work on San Francisco’s South Park, the initial design was developed “through iterative analogue diagramming” with a focus on “an intuitive understanding of the site and embedded in an analogue rule set” (FLETCHER 2021). \n\nCase Study: South Park, Fletcher Studio, 2017 \nFletcher Studio is a landscape architecture and urban design collaborative practice based in San Francisco, California. Fletcher Studio frequently uses parametric design software programs such ash such as Rhinoceros 3D, Grasshopper and Rhino script to test complex forms, \nfunctions, and site layout (Amoroso 2012). The Studio sought to reimagine San Francisco’s oldest public space (Figure 2) with a contemporary interpretation of the “picturesque style” landscape (CANTRELL & MEKIES 2018). The award-winning design transformed the site from an  English  strolling  garden  into  an  integrated  multi-purpose  communal  space  (FLETCHER 2021). \n\nAnalogue to Digital: The design intention sought to retain a hierarchy of circulation patterns, access points, social nodes, and existing trees and structures (CANTRELL & MEKIES 2018). “In the initial design phase, these decisions were made through an intuitive understanding of the parameters of the site and embedded in an analogue rule set that guided design decisions” \n(FLETCHER 2017). \n\nAnalogue rule sets require a considerable amount of testing time. “The same “idiosyncratic moments” that allow for the emergence of novel and intriguing design moves can also lead designers to overlook inconsistencies or flaws in their logic.” (FLETCHER 2017). By using a parametric  modelling  tool  in  the  Rhino  3D  software  program,  the  system  of  organization \ndeveloped in analogue (on paper), was translated into a Grasshopper digital script. The system  evaluated  “the  design  resiliency”  of  the  diagrammed  “tectonic  and  spatial  systems”(FLETCHER 2021). This enabled the designers to re-evaluate any flaws in their logic while also rapidly iterating upon the design in detail, without violating the previously established constraints of their design concept. \n\nTeaching Design Studio \nThe authors included kinaesthetic learning approaches in the phase of developing analogue rules associated with this studio. Kinaesthetic learning is a learning style in which individuals effectively “learn through doing”. Landscape architecture curricula historically include kinaesthetic learning approaches. Students increase understanding and testing of the products and  outcomes  of  design  exploration  by  touching  and  manipulating  them;  hence,  practical information is usually preferred over theoretical concepts. A kinesthetic learning experience can  aid  the  teaching  of  parametric  design;  one  can  read  about  it,  listen  to  instructions,  or watch videos of how to design parametrically – but deep learning occurs when one is physically involved with it. For their course, the instructors employed learning approaches including hand-sketching and model making to engage in an intuitive approach to design. These were “hands on” ways of exploring, developing and testing design concepts, aligning with the theme for a public art competition. \n\nCompetition Overview: The Winter Stations design competition is an open, single-stage, international design competition held annually in Toronto, Ontario. Guided by a provided theme, participants submit design proposals of temporary winter art installations incorporating the existing lifeguard towers situated along Toronto’s Woodbine Beach, on Lake Ontario. \nThe OneCanada project, informed under the competition’s provided theme of “Resiliency”, and designed and installed by six studio-course students, represents one of several submissions from artists and designers, worldwide, and was the only representation from landscape architecture, let alone an undergraduate student cohort. \n\nAnalogue to Digital  \nThe following phases characterize the study undertaken: \n\nPHASE 1: Developing Analogue Rule Set [Concept] \nStudents  were  asked  to  develop  a  concept  based  on  the  competition  theme  of  Resiliency. Their concept sought to interpret, appreciate, and promote the inspirational example of resilience of the Indigenous peoples of Canada, who continue to withstand adversity and persevere through generations of oppressive colonial policies. The concept also sought to bridge a gap between Indigenous and non-Indigenous peoples through an opportunity of “gathering” among the layering of the seven grandfather teachings (wisdom, love, respect, bravery, honesty, humility, and truth). The students envisioned the teachings to represent seven  white, \nand stacked circular forms, with a situational siting around a Woodbine Beach lifeguard station, representing the collective responsibility in the “guarding of life.” As an obvious beacon along the waterfront, art patrons, guests, and peoples from all backgrounds, could gather at \nthe  installation.  The  seven  teachings,  originating  with  the  Anishinaabeg,  and  have  been passed down from generation to generation ensuring the survival for all Indigenous peoples. \n\nPHASE 1: Developing Analogue Rule Set [Hand Sketching and Model Making]  \n\nBased on the initial concept, students generated ideas and imagined the form of the installation. Due to the lack of experience with 3D modeling software programs, students explored multiple design iterations through hand sketching and physical model making. As a result, the team developed analogue rule sets or design principles that guided design decisions: \n\n1.  Using circle as the prime form of installation. Circle is a sacred symbol of the interdependence of all forms of life in Indigenous culture (Stevenson 1999)  \n\n2.  Represent the seven grandfather teachings in minimum seven independent layers: wisdom, love, respect, bravery, honesty, humility, and truth. \n\n3.  Demonstrate unity in a sequence to symbolize bridging the gap between indigenous and non-indigenous people \n\n4.  Using a pattern to attach the separate layer which represents strengthening of relationships, and the protection of culture through the gathering and unity between people \n\nThese analogue rule sets, developed on paper and through model  making,  were translated into a parametric script using the Grasshopper plugin for the Rhino 3D software program.  \n\nPHASE 2: Developing Digital Ruleset [Parametric Script] \n\nWithout guiding the students through the complexity of learning algorithms or the coding behind the scripts, three algorithms or scripts developed in Grasshopper and were provided in a ready-to-use format to the students. The four (4) rule sets translated to ‘input parameters’ \nfollowed by multiple components in the Grasshopper plugin to rationalise the design process and to operationalise the principles. Using a ‘parametric lens’, the students could experiment, test, and generate design iterations and several design alternatives which allowed rule-based \nthree-dimensional platform to inform the decision making.  \n\nThe  final  iteration and  parametric  script,  for  the  competition  submission,  were determined from the various alternatives generated. \n\nPHASE 3: Digital Fabrication [Construction]  \nWith the rising presence of digital modeling in the field of landscape architecture, and accessibility to requisite equipment, digital fabrication has become a major facilitator in the development of research and design, in both professional practice and academia. As described by \nAndrew Madl “Professional design firms and universities providing use of digital fabrication in-house is becoming increasingly common. How to exploit such equipment is now taught in academic settings as skillset that is in line with traditional model making.” (MADL 2022). \n\nTeaching digital fabrication techniques requires significant amounts of time and resources. The intention of this phase was to develop a general introduction and awareness by providing a glimpse into the advantages of parametric design tools. \n\nFollowing the previous phases, students were required to prepare construction documentation or  “shop  drawings”  suitable  for  a  professional  fabricator.  Utilizing  computational  design tools developed in the previous phases, and through ongoing consultation with CNC fabrication  professionals,  students  learned  to  prepare  the  fabrication  files  and  facilitate  the  CNC cutting process. The goal was to encourage students to experiment with digital \nmodeling and file preparation, suitable to fabrication. \n\nFinally, the project’s implementation occurred through a team effort, ranging from detailed off-site work including material determination, metal welding, support strut wrapping, CNC-cut wood panel painting, transport, etc., to on-site assembly and construction. \n\nDiscussion \nIn the discourse of architectural fields, the term parametric design is associated with a particular attitude, aesthetic, and theory. Typically, one envisions the outcome as extraordinary and \nprovocative designs that inspire a set of  morphological principles (MADL 2022). The first perception of parametric design is limited to contorted formal expressions and the over-sophistication of geometry, which need to be deciphered. While this study emphasizes teaching \nthe  process  of  generating  complex  formal  expressions  for  a  public  art  installation,  the  research team addressed the potential of the method for future discoveries more specific to the field of landscape architecture. \n\nIn this course, some students gained the full understanding of the potential parametric design thinking offers at the end while a few had difficulties in developing a logic string of design steps that relate to the parametric approach, they preferred or felt back to intuitive or conventional  designing.  However,  the  later  group  was  interested  to  work  within  the  parametric framework if there is a team member managing the scripting part. This might inform an indication of the future of design so that the analogue embraces digital rather than introducing an absolute departure from analogue to digital. While parametric design offered a palette of \npossibilities, students got exposed to the realities of budget constraints and current limitations of digital fabrication, which eventually reduced the range of possibilities. \n\nRegardless of understanding the details behind the script, the “end product” and the process was well received by the students involved and has encouraged other students, privy to the course, to pursue “script” moving forward.  \n\nConclusion and Outlook \nImagined to provide interested students with an implicit understanding of the parametric process and to motivate them to create scripts unique to a project in future, this course enabled students to look outside the box, and even produce their own tool sets. Caroline Westort of Iowa State University explains that future landscape architects will not be only tool users but \nrather toolmakers. She indicates: “I actually think we do lose something by not training or teaching students the basic building blocks of what’s behind the black box, what’s behind the software . . . we are an information technology discipline, whether we like it or not.” (Bentley 2016). This indicates the need of training future resilient landscape architects, adept at creating script. \n\nParametric design can be difficult  for students  who  may  not have a strong background  in computer science or programming. It can also be time-consuming and challenging to learn and use these tools effectively, especially for those who are already comfortable with tradi-\ntional design approaches and intuition. Many designers will not engage at the high level of syntactical  knowledge  necessary  for  scripting  given  time  constraints  as  one  of  significant barriers. However, Grasshopper, Rhino, other GUI-based scripting allows designers to more readily connect the outcome of code with the formal representation without having to know how to write code (CANTRELL & MEKIES 2018). \n\nContemporary landscape architecture theory and practice necessitate the processing and design of data connected with complex systems in order to accurately reflect composite and emergent scenarios (MADL 2022). The field of landscape architecture, along with other design disciplines, are undoubtedly evolving through computational discovery.  \n\nLandscape architects and landscape architecture itself can respond to ever-evolving nature of practice and their resulting consequences. The outcomes of this design studio proved that parametric design permits a level of ambiguity, inquiry, discovery, confidence, and execution expected in creative and learning environments. \n\nBy training  future resilient landscape architects  with computational tools, universities and educational institutions can make a significant contribution in keeping pace with evolving principles. It is expected that these skilled professional practitioners and researchers will be \nintroduced to the community, adequately versed, and improve the model of practice-based research, which ultimately improves conventional and speculative design workflows. \n

Geodesign as Online Teaching Method – Lessons from a Multiple Case Study \n\nAbstract: This study analyses the geodesign workshop as a method for the online teaching of group \nwork methods in the context of geoinformation systems (GIS) in planning and design. In order to assess the learning outcome, four workshops with international landscape architecture students at master level were conducted over a period of four years (2018-2021) and compared in a qualitative multiple-case study. In times of Covid-19 and the need for remote workshop methods, the geodesign workshops seem well suited for online learning and teaching. The results show that the learning goals were achieved, that new ideas were created and stakeholder expectations reflected and challenged. In individual cases, the lack of on-site knowledge led to mistakes though, and online group work had different group dynamics  than  in-person  negotiations.  Vocal  and  well-organised  students  seem  to  engage  even  more whereas quiet students more easily disengage, as seen in a bimodal distribution of participation grades in the online class. In conclusion, geodesign workshops may be recommended as an online method for teaching GIS and group work methods such as brainstorming, consensus building and stakeholder-role play but a hybrid format or new virtual field trip techniques are preferable when familiarizing students with the case study site. The teaching of group work methods as part of planning and design may be transferred from geodesign to teaching building information models, which is also an information-based digitally facilitated collaboration process. \n\nIntroduction \nGeodesign has been included in many university curricula around the world. The International Geodesign Collaboration (IGC) introduced a standardized geodesign process, which \nhas been conducted by hundreds of universities around the world. WARREN-KRETZSCHMAR \net al. (2016) already demonstrated the benefits of geodesign as a teaching method in planning \nand design classes. Building on their insights, this paper further explores whether geodesign \nis also a suitable method for the teaching of group work methods, and how geodesign classes \nadapted to online teaching during the Covid-19 pandemic.  \n\nIn  short,  STEINITZ  (2012)  defines  geodesign  as  planning  geography  through  design.  In  a \nlonger  definition,  FLAXMAN  (2010)  defines  geodesign  as  “a  design  and  planning  method \nwhich tightly couples the creation of a design proposal with impact simulations informed by \ngeographic context and systems thinking normally supported by digital technology.” Among \nother methods, geodesign utilizes the scenario  method (BISHOP et al. 2007), which is also \npart of many university programs. \n\nHence, a common misconception is that geodesign is only about technology. Although ge-\nodesign is characterised by the integrated use of GIS tools and geodata as the basis for an \ninformed  design  and  decision-making  process  (CAMPAGNA  2014),  it  is  generally  a  group \nwork process. In this context, several group work methods correspond well with the geodesign process. These are brainstorming, stakeholder role-play, and collaborative negotiation \nmethods.  \n\nBrainstorming is a method for the quick generation of ideas (JONES 1992). In the first step, \nparticipants have to write down as many ideas as possible during a limited amount of time. \nSince this step is about the creation of ideas, no weighting, discussion or filtering takes place \nyet. In a second step, the ideas are discussed in the group, clustered thematically and redundant or unsuitable ideas are sorted out. DOMINGO et al. (2021) demonstrate how brainstorming can also be applied in remote settings to facilitate collaborative work.  \n\nAt the same time, geodesign addresses complex multi-stakeholder planning and negotiation \nprocesses. PETTIT et al. (2019) suggest collaborative negotiations and consensus-building as \npart of the geodesign process. Starting with an even number of stakeholder groups, e. g., eight \ngroups with one planning proposal each, these groups meet with the closest other group, e. g., \ngovernment and business, and negotiate a consensus between their two proposals. Then, the \nremaining four proposals are narrowed down to two and the two to a final one. Because the \nprocess is mediated through digital means, PETTIT et al. (2020) also call it digital negotiations. \nThey conclude that such digital negotiations are an effective planning method. \n\nSuch processes embody underlying roles and often hidden agendas and conflicts. LIGTENBERG et al. (2010) used a role-playing approach in which students took on the roles of local \ncitizens, farmers and nature conservationists together with an agent-based model for simulating a multi-actor spatial planning process. In the IGC process, the role-playing approach \nlends  itself  to  have  students  represent  different  stakeholder  groups.  Common  stakeholder \ngroups are local citizens, local businesses, local government, youth organisations or environmental NGOs. Research goals are to assess whether:  \nLearning goals were achieved; \nThe quality of the results changes between online and in-person geodesign workshops; \nGeodesign workshops as learning and teaching method for group work are transferable to other programs at Master level. \n\nMethods: Multiple Case Study Comparison  \nThe research design is based on the multiple case study method (see 2.2.) by YIN (2014). The \ncontext for the workshops is kept consistent and comparable by following the recommendations and templates of the International Geodesign Collaboration IGC (see 2.2.): scale, group \nsize, underlying global assumptions, time-frame, and range of scenarios do not change across \nworkshops. The workshops are informed by open data from the EU Copernicus programme, \nOpenStreetMap  and  local  environmental  agencies  (2.3.)  All  workshops  use  geodesignhub \n(www.geodesignhub.com) as online platform to facilitate the process (2.4.). In the comparison, quantitative data such as average  grades for participation and outcome are compared \ntogether with qualitative observations, i. e., data triangulation in the words of YIN (2014). \n\nInternational Geodesign Collaboration IGC Template  \nORLAND & STEINITZ (2019) describe the International Geodesign Collaboration IGC, a col-\nlaborative  project of  more  than  120  universities,  research  institutions  and  public  \/  private stakeholders across the world. In order to facilitate research into geodesign, the collaboration organizes annual workshops and provides a template to make the diverse geodesign projects comparable.  The  IGC  template  (https:\/\/www.igc-geodesign.org\/presentation-formats)  provides the following:  \nCommon geodesign systems (water, green infrastructure, energy, transport, agriculture, \nindustry and commerce, institutional, residential and two flexible systems) and a common colour palette to easier identify and compare land use patterns and alternative design scenarios.  \nGlobal assumptions and a library of geodesign innovations, such as new renewable energy solutions, transport innovations etc., which IGC participants are encouraged to apply in their individual projects.  \nCommon scenarios and timeframes at 2035 and 2050, and paths to achieve scenarios for \nthose: “Early Adopters” initiate design interventions in 2020; “Late Adopters” in 2035; \nand “Non-Adopters” continue with business-as-usual.  \nTemplates for common reporting formats as presentations and posters. \n\nThe geodesign projects compared here dropped the 5km and added a spatial extent at 40km \nbut adhered to the IGC systems, innovations, common timeframes and poster templates.  \n\nMultiple Case Study Design  \nThe basic concept of  this  multiple case-study is  to conduct the  workshops as similarly  as \npossible by referring to the IGC standard templates for participating projects. The four workshops (see Tab. 1) were embedded in a GIS module in the second year of an international \nMaster´s degree in landscape architecture. Student backgrounds were very diverse, with students from different Bachelor´s degrees and more than 20 different nationalities. Working \nlanguage  was English. Each  workshop had one day of preparation plus individual student \nhomework and three days of the actual workshop. Results were documented on two A2 posters per workshop. \n\nGeodata-Based Process \nFor each workshop, suitability analyses were run ahead of the workshop in ArcGIS Pro and \nsummarized  in  so-called  evaluation  maps.  The  suitability  analyses  were  mainly  based  on \nopen geodata from the Urban Atlas, which are derived from Copernicus satellite data (European Union, Copernicus Land Monitoring Service, European Environment Agency (EEA)),map data from OpenStreetMap and protected areas provided by the Bayerisches Landesamt \nfür Umwelt LfU and the Geoportal Baden-Württemberg.  \n\nOnline Platform \nAll workshops were conducted through the online platform geodesignhub, which uses maps \nand  diagrams  to  facilitate  the  negotiation  process.  FLINT & STEINLAUF-MILLO  (2021)  describe geodesignhub as “an interactive design method that uses stakeholder input, real-time \nfeedback, geospatial modelling and impact simulations to facilitate the development of an \neffective management strategy and smart decisions.” By presenting two maps with individual \ndiagrams of projects and policies, and adding functions for filtering and visual comparison \n(Fig. 2), geodesignhub provides the tools to reach an informed consensus. \n\nCase Descriptions \nAll four workshops correspond with local planning topics, i. e., Munich Parkmiles is elaborating the open space concept of the City of Munich; Regional Garden Festival Stuttgart is \ncontributing  to  the  International  Building  Exhibition  Stuttgart,  Heidelberg  Green  Belt  responded to the invitation by the City of Heidelberg to develop ideas for a green belt and the \nlast project is contributing to the forthcoming IBA Munich. The four geodesign workshops, \npresented here, share the same learning goals:  \nAddressing a planning question at city to regional scale  \nApplication of GIS skills and demonstration of geodata capacity  \nDeveloping group work skills  \n\nCase Study 2018\/19: Munich Parkmiles \nIn a competition of ideas, 30 international students drafted the 2035 and 2050 scenarios in \nparallel working teams. Nevertheless, the results are surprisingly consistent. The common \nidea is that green infrastructure innovations are concentrated in the „Park Mile” green corridors. Housing is mainly accommodated in mixed-used zoning. For example, the 2050 early \nadopters’ scenario is presented in Figure 1, which extends the high-density mixed-use areas \nalong the  major public transport lines towards the city´s edge. In this case, the colours  in \ngeodesignhub indicate different types of zoning policies. The green spaces in between, including urban forestry in the south and valuable farm land in the northwest, are put under \nprotection  protected  from  further  development.  The  large  inner-city  yellow  policy  zone \nmarks low-density laneway housing in the otherwise high-density neighborhoods.  \n\nCase Study 2019\/20: Regional Garden Festival Stuttgart  \n\nGerman garden festivals have become a powerful driver for sustainable urban development. \nThe focus of the student proposals is on a positive impact on the climate. The approach in \nthe 2035 and 2050 scenarios complement each other progressively to implement policies on \nrenewable energy combined with blue and green infrastructure. Land use is planned strategically to mitigate urban sprawl, reduce the urban heat island effect, and increase rainwater \ncollection.  Housing  is  addressed  through  high-rise  developments  by  converting  redundant \nindustrial areas into mixed land use with a focus on bringing in a large “breathing” space in \nthe form of a park that Nürtingen does not currently have (see Fig. 2). Renewable energy \nprojects introduce solar farms, solar surfaces on highways, and policies that require residential and industrial zones to contribute local solar energy.  \n\nCase Study 2020\/21: Heidelberg Green Belt \nThe City of Heidelberg and its neighbouring cities, most importantly the City of Mannheim \nnorthwest of Heidelberg, have already launched a number of landscape development projects \nfor ecological restoration. At the time of this workshop, the city council had asked the planning department to develop ideas for a multi-functional “green belt” between Heidelberg and \nMannheim. Please note that the term “green belt” has been discussed controversially in different contexts. In the context of this project, the “green belt” is supposed to integrate ecological and physical landscape characteristics with multiple land uses (protected natural areas, agriculture, infrastructure, recreation...) in a multifunctional landscape.  \n\nFigure 3 is showing the early adopters' scheme for 2050 with green and blue infrastructure \ncorridors visible west of Heidelberg, i. e. along the area adjacent to the Mannheim urban area. \nIn addition to introducing new blue infrastructure, the students suggested links to the strong \nmedical sector in Heidelberg by introducing therapeutic gardens and other forms of restorative landscapes. Interestingly, the seemingly novel idea of creating new blue infrastructure \ncorresponded with a local proposal for an artificial lake.  \n\nCase Study 2021\/22: Munich International Building Exhibition \nSimilar to regional garden shows, the regularly held Internationale Bauausstellung (IBA) is \na key driver of national building and planning culture in Germany. It has played an important \nrole in cooperation, innovation, participation, experimentation, and visualization of 10 years \nof planning and design. Since Munich was awarded the next IBA on the topic of mobility, \nstudents were encouraged to envision a regional IBA providing new sustainable approaches \nto mobility landscapes. The City of Munich IBA team supported the workshop.  \n\nThe first day mainly focused on learning about the area of Munich and analysing where pos-\nsible improvements could be made based on suggested systems such as: transport infrastruc-\nture, industry and commerce, mixed residential, tourism, blue and green infrastructure, en-\nergy infrastructure, climate, and agriculture. One key instrument was the further development \nof the “park miles”, seen in green Figure 4, which had already been addressed in the first \nworkshop by a different group of students.  \n\nCross-Case Comparison \nIn all four cases, the students achieved the learning goals. Comparing the four cases, there \nare commonalities but also differences between the in-person and the online settings:  \n\nStatistical Comparison \n\nThe students received grades for 1) participation in the workshops and 2) the quality of the \noutcome, i. e., the content of the resulting scenarios and their presentation on the posters. \nStudent numbers were supposed to be around 30, but actually varied between 23 and 36 depending on factors out of our control such as visa issues or Covid-19.  \n\nA simple descriptive analysis of the average mean grades across the four workshops is presented in Table 2. In general, grades are rather good (with 1.0 the best possible grade). The \nbest participation was recorded during the first in-person workshop in 2018\/19, whereas the \nsecond online workshop in 2021\/22 had the poorest participation. If you look closer at the \ngrades, participation in the last workshop shows a trend towards a bipolar distribution: quite \na few students participated very well in the online workshop, but in contrast, a large number \nof students participated poorly or dropped out. \n\nCross-Case Observations \nIn both the in-person and online settings, large numbers of diagrams were created, and both \nsettings led to comparable results in terms of quantity and diversity. With regard to the IGC \nframework, three scenarios were derived from the diagrams: early adopters, late adopters, \nand non-adopters. The geodesign process of narrowing down the scenarios to a smaller number of consensus scenarios also succeeded in both settings. For teaching purposes, the scenario process was combined with exercises in negotiation and students “role-played” different stakeholder groups, such as young people, government, business representatives or environmental NGOs. Some students fully embodied their roles and took on a new perspective, \nleading to interesting discussions, such as proposing affordable housing versus the provision \nof additional green space.  \n\nThe online platform geodesignhub facilitated the documentation of the process in both settings, online and in-person. Especially in a teaching environment, it is of great help for the \nteacher during assessment and grading that all ideas and the scenario building process are \narchived in geodesignhub.  \n\nDifferences between In-person and Online Settings  \nThe online setting can facilitate a broader geographical range of case study topics and locations, although it seemed to come at the costs of sometimes lacking understanding of the site. \nOne group was obviously not aware of local characteristics and depicted high-rises, which \nwere completely out of context.  \n\nIn  terms  of  organisation,  the  online  workshop  made  it  easier  for  international  students  to discuss with local stakeholders than organising such a session in person. Like an in-person setting, the online discussion inspired both groups, students and local stakeholders. \n\nHowever, the grading showed a lower grade in participation, particularly for the last work-\nshop. From observation, more vocal students tended to engage even more in the online set-\nting, whereas it was much harder than in-person to motivate quiet or disengaged students. \nNevertheless, the online setting was a suitable remote learning tool during Covid-19 times, \nand the geodesign workshop method proved to be well suited for online teaching.  \n\nConclusion and Outlook \n\nIn conclusion, the learning goals were achieved. Therefore, geodesign workshops are generally recommended for teaching group work methods such as brainstorming, consensus building, and stakeholder-roleplay in GIS-based planning and design. In times of Covid-19 and \nthe need for remote workshop methods, the geodesign workshops were also well-suited for \nonline learning and teaching although participation was slightly lower during the online sessions. These observations are consistent, though, with other classes that were taught online \nduring Covid-19 and could point to a certain online “fatigue”.  \n\nRegarding the quality of results, the online setting  might come at the cost of the students \nfamiliarizing themselves with the case study area. It is recommended to further develop hybrid  settings,  e. g.,  collaborations  with  local  experts  or  the  development  of  remote  or  VR enabled field trip techniques to facilitate a better understanding of the site (HASBROUK & \nSTEPNOSKI 2022).  \n\nFindings and group teaching methods from this multiple case study could be transferred to \nteaching Building Information Modeling BIM. Like geodesign, BIM is a collaborative process rather than a software. In a BIM class, the role-play could simulate the different stakeholders in a BIM process, from surveyor to architect and client, and the BIM model may be used to facilitate negotiations among these stakeholders. \n\nFor future geodesign research, it is suggested to focus further on the evaluation of scenarios. \nPeer review through the students themselves might contribute to the learning and teaching \nprocess. In addition, GIS-based or even artificial intelligence (AI) based methods might facilitate new learning and teaching methods by providing real-time quantitative and qualitative \nfeedback. It will have to be seen how the geodesign process is further developing and which \nrole, if any, AI will play in it.  \n

BIM Education in Landscape Architecture: The Rapperswil Model \nAbstract\nIn 2016, Switzerland decided to embrace digitization in every industry, in all sectors, and at \r\nall  levels.  Within  the  Landscape  Architecture  program  at  the  OST  University  of  Applied  Sciences, teaching and research with strong emphasis on Digital Construction had already been carried out for years. In view of the country’s vigorous drive towards digitization and being the only bachelor's pro-\r\ngram in the German-speaking part of Switzerland, Rapperswil Landscape Architecture is committed to \r\npartake and to aid in this digitization process. The paper gives an overview of Digital Construction in \r\nLandscape Architecture education in Rapperswil, status 2022.\n\nIntroduction \nIn the coming years, the construction industry worldwide will undergo digital transformation. \nBuilding Information Modeling (BIM) plays a central role with the digital twin and with a \nclearly defined process. The strategy of the Swiss Federal Council from 2016 for digital Swit-\nzerland (SCHWEIZERISCHE EIDGENOSSENSCHAFT 2016) and the establishment of the frame-\nwork “Bauen Digital Schweiz – Digital Construction  Switzerland”, the country chapter of \nbuildingSMART International, gave the Swiss construction industry a big boost in the direc-\ntion of digitization The Swiss government  even  went one step further and defined a clear \ntime frame. By 2025, Switzerland will implement the BIM process not only in architecture, \nbut in all infrastructure construction projects. The Swiss Federal railway company Schwei-\nzerische Bundesbahnen (SBB), a government-owned company, must apply it in all projects \nby then. SBB is also a big player in real estate architecture projects where greenspaces are \nalways required. Therefore, this is not only applicable to engineers, but also landscape archi-\ntects have to get ready for digital construction. As result, Rapperswil is consistently pursuing \nthe goal of implementing “Digital Construction” in Landscape Architecture education. \n\nDigital Construction \nDigital Construction is the general term for using different digital technologies to build more \nefficiently. The below methods and processes belong to Digital Construction and are taught \nat Rapperswil. \n\nBIM Construction \nRapperswil students work three-dimensionally from day one. In the first semester, the BIM \narchitecture construction software (Revit) and the principle of a digital twin are taught. The \nstudents  are  required  to  build  an  existing  structure  (pavilion,  pergola,  water  feature,  etc.). \nThis approach has the following advantages: \nFast learning of architecture representation principles (floor plans, elevations, sections, 3D views), \n“One single source of truth” – the 3D model is the basis for all (representations, schedules\/quantities, etc.),\nObject oriented construction only with 3D objects, which belong to building categories, \nVery structured modelling with parametric objects (standard families, external families and project families), The model has container function for fundamental construction data. It is used for structural engineering (formwork\/reinforcement) and for MEP (Mechanical, Electric, Plumbing) modelling. \n\nIn the 2nd semester, the students integrate a building into an existing site based on GIS data \nwith  civil  engineering  programs  (InfraWorks  \/  Civil  3D).  They  locate  driveways,  parking \nlots, retention\/infiltration basins in the project. Students get to know the 3D Global Naviga-\ntion Satellite System (GNSS) excavator control system and learn what to look out for when \npreparing data for it. In principal students learn to use the correct tools for different chal-\nlenges, as BIM construction in Landscape Architecture consists of tools for architecture pro-\njects and tools for civil engineering projects. The scheme BIM4RainWaterManament (PET-\nSCHEK 2019) gives a good overview of the Rapperswil teaching in Digital Construction. \n\nBefore the students model the terrain digitally, they learn the craft by hand for one semester. \nIn the construction course in the first semester, the students have to solve numerous small \ntasks on terrain modeling with elevation points\/contour lines and prove their skills in a 1 ½ \nhour exam at the end of the semester. The exercises are based on U.S. grading courses, which \nare common at every north American landscape architecture program. Richard Untermann, \nwith the book “Grade Easy” (UNTERMANN 1972), was an important pioneer for several text-\nbooks on the English-language book market on terrain modelling and serves as the basis for \nthe Rapperwil grading education. \n\nMore complex architectural constructions, which serve as the basis for structural calculations, \nare the subject of the “Constructive Design” module in the 4th semester. This is also where \nthe cooperation with the civil engineers of the newly formed School of Architecture, Civil \nEngineering, Landscape Architecture, Spatial Planning of the OST is practiced for the first \ntime. \n\nThe previously often time-consuming visualizations are becoming a sideshow thanks to the \nBIM construction training. Software like Enscape3D or Twinmotion enables the virtual real-\nity (VR) inspection of the model with a headset or on the screen. Individual images in differ-\nent resolutions can be created from any point. Students can check out headsets and VR capa-\nble computers from a central IT service at OST. \n\nCommon Data Environment (CDE) \nIn addition to the 3D model, the management of processes and information is central to Digital Construction. Internet-based platforms, called Common Data Environment (CDE), are \nresponsible for data exchange, collaboration on a model, costs, quantities, materials, delivery, \ntesting and acceptance processes in all life cycle phases of the structure. \n\nIn the spring semester of 2020, like the other Swiss universities, all of Rapperswil’s landscape \narchitecture courses took place online. Since the first wave of the pandemic, Teams \/ Zoom \nconferences had been part of everyday life for all students. However, Rapperswil went a step \nfurther. It was made mandatory for all students to submit their BIM construction projects via \nthe CDE Autodesk Construction Cloud (ACC). A project folder is dedicated to each course, with student subfolders. There the students store their models, photos, sketches and text. The \nlecturers evaluate the work digitally, with the 3D model being the starting point. \n\nThomas  Putscher,  lecturer  at  OST,  writes,  “I  consider  the  submission  to  the  Construction \nCloud to be a good thing. The students found their way around quickly and it went smoothly. \nMeetings took place directly via Teams with the open 3D cloud model. The corrections to \nthe  model  were very easy  for me because I could do them directly online. I uploaded my \nevaluation sheets and informed all students via email about their grades. If necessary, there \nwere debriefings online with the open 3D model. All in all, the construction cloud has saved \nme a lot of time.” For Rapperswil, the full integration of the Common Data Environment into \nlandscape architecture training was the next logical step towards Digital Construction. From \nfall semester 2021, all students in the construction courses worked with the CDE platform \nright from the first semester. The cloud solution is now used for submission and evaluation \nin all construction courses up to the bachelor thesis. Paper plans are no longer used as submissions. \n\nBIM4RainwaterManagement \n“Climate change is leading to heavier and more frequent precipitation. In urban areas, where \ndevelopment means the total impervious surface area is increasing, there is a growing risk of \nflooding from surface run-off after heavy rainfall. In climate-adapted and risk-based urban \ndevelopment,  there  is  an  increasing  need  to  manage  rainwater  resources  sustainably.  The \nconcept of 'sponge cities', which focus on increasing evaporation, infiltration, retention, controlled  temporary  flooding  and  providing  emergency  waterways,  is  a  planning  solution  to \nprevent damage from surface run-off and to reduce the effects of heat” (BAFU\/ARE 2022). \n\nLandscape Architects must take over a leading role in building 'sponge cities'. How can the \nprinciple of the sponge city be realized as part of Digital Construction? Although it belongs to  BIM  Construction,  this  very  important  topic  is  specifically  addressed  under  the  title \nBIM4RainWaterMangement. It is taught in the construction course in the 5th semester. The \nbasic goals of BIM4RainWaterManagement are: \nReturn of clean rainwater to the groundwater\nOptimization of a slow percolation\nUsage of a digital twin. \n\nWhen building the digital twin, the students apply the steps of the BIM4RainWaterManagement scheme (PETSCHEK 2019) and of course, they  use digital terrain  modelling. It is the \nbasis for quickly testing precise alternatives of retention and infiltration and thus find the \nideal solution for allowing rainwater percolation on site. The civil engineering software and \nthe  architecture  software,  combined  with  their  respective  strengths,  are  used  to  set  up  the \nBIM4RainWaterManagement project. \n\nThe 3D model has the following advantages: \nProof of retention areas in the event of heavy rain events. All water from roof tops and surfaces percolates on site, \nPrecise modeling of pavement surfaces and subsurface structures, \nParametric inlets, sludges collectors, manholes and pipes are included in the digital twin, \nClash detection between tree root balls and subsurface drainage elements, \nPrecise stakeout of all elements using BIM2Field, \nStudy of site grading alternatives with the help of automized grading (Grading Optimization in the civil engineering software). \n\nIn the next step, a link between the digital twin and numeric stormwater management soft-\nware will be established to validate the model and integrate it into a larger GIS analysis con-\ntext. Also extracting sustainability data from the digital twin will be an important topic. \n\nBIM4Trees \nStudents “plant” as part of the course described under point 2.3 BIMTrees. The trees were developed by Andreas Luka Consulting in close cooperation with Rapperswil, and the Swiss landscape contractor association Jardin Suisse, in which the Swiss tree nurseries are organized. \n\n3D trees are a major challenge for BIM in Landscape Architecture, as their geometry and \nproperties change significantly over the entire life cycle (LUKA & GUO 2021). Existing solutions did not adequately meet this challenge and could only insufficiently exploit the potential \nof BIM. Rapperswil therefore, supported the implementation of dynamic BIM trees with a \nresearch project. \n\nThe focus on the outer shells for crown, tree trunk and root, which are important for BIM, \nand their representation as solids allow both efficient clash detections and the extraction of \nvolumes  and  masses  for  further  analyses  (structural  engineering  calculations  and  shadow \ncasting). The tree size can be predicted interactively and quickly within the BIM software \nusing initial values for size when planting and growth functions for any point in time after \nplanting, complete with root in species, variety and age-specific shape and size. By simply \nlinking the very technical looking trees with Enscape3D, convincing visual representations \nof tree planting are created. \n\nThe feedback from independent study and bachelor projects flows directly into further development. There are currently 80 species\/varieties with the shapes and sizes according to \nthe Swiss quality regulations and the catalogue of a large German tree nursery. \n\nBIM2Field \nDigital Construction is based on digital data. In addition to the existing GIS data, one often \nneeds to collect his\/her own data. Point clouds from drone flights are a possibility. However, \nlandscape architects need height information below the tree and shrub layer. Drones cannot \nbe used here. The mobile laser scanner BLK2GO (LEICA Geosystems) is the latest generation of mobile laser scanning. A stationary device no longer has to be set up as before as the \nscanner can be used while walking through the site. This flexibility is extremely important in \nLandscape Architecture. The created point cloud is then post-processed and prepared for the \nBIM construction. The OST students use laser scanning as a regular tool in their projects. \n\nSince the fall semester of 2021, the surveying course has been renamed “BIM2Field”. The \napplication  of  one  person  tachymeter  stations  is  taught.  The  six  robotic  stations  use  the \ngeoreferenced  model loaded on the Construction  Cloud directly for stake out  without any \nintermediate steps. GNSS machine control systems in addition are presented in class by commercial companies. \n\nBIM2Cost \nThe central topics of the Landscape Architecture training in Rapperswil are cost calculations \nand construction specifications (specs). The 5th semester course, described under 2.3. is also \npioneering in this area. In Switzerland, the basis for efficient construction costs is the element-based  classification  eBKP-H,  which  was  developed  by  the  Swiss  Central  Office  for \nConstruction Rationalization CRB. The cost-relevant quantities are collected directly in the \ndigital twin. Automatically calculated dimensions and volumes result in more precise cost \nestimates. Due to clash detection, errors in the construction are detected early and thus in-\ncrease cost certainty. In the future, construction schedules will be displayed as 4D simulations \nusing the Common Data Environment ACC. In this way, the planned construction process \ncan be checked visually by the students and any contradictions can be solved. \n\nYouTube Channel \nThe YouTube Channel “Landscape Architecture Rapperswil” is the medium for the presentation of student independent study projects, thesis work and lectures on the topic. The following posts on building digital can be found on the constantly updated YouTube Channel: \nBIM2Field, laser scanner and robotic tachymeter: https:\/\/youtu.be\/icjaM3AoRa0 \nBIM2Field, 3D GNSS digger: https:\/\/youtu.be\/2MTnb7rV58o \nBIM Student independent study project: https:\/\/youtu.be\/q2SMcJ2knjg \nBIM Bachelor thesis: https:\/\/youtu.be\/0X8VUk-FyUM \n\nConclusion and Outlook \nDigitization in the Swiss construction industry is taking big steps forward; Landscape Architecture is not unaffected by this. Education in design, planting design, ecology and construction are the foundation of future Landscape Architects. In addition, skills in Digital Construction  must  be  integrated  today.  The  next  step  is  the  switch  to  Bring  Your  Own  Device \n(BYOD). From fall semester 2023, students are required to use their own devices in all Rapperswil courses. Computer labs will be no longer in use. It is also planned to integrate the \ndigital twin topic in the GIS teaching. In conclusion, OST Ostschweizer Fachhochschule in \nRapperswil acknowledges the challenges of Digital Construction and is riding along the digital wave.  \n\nDigital Landscape Architecture Education – Where Do We Stand and Where Should We Go?  \n\nAbstract: Landscape architecture has a crucial role in designing landscapes to influence how they per-\nform in a desired manner and provide more resilient and adaptive environments. Sophisticated analyti-\ncal and design tools and techniques exist along with data from a range of allied disciplines that have \nthe capacity to inform and transform the way landscape design approaches are conceived. However, \nthese are not widely embraced across landscape architectural design schools as a status quo. This paper \naims to provide a prompt to initiate a critical theoretical discussion on the future pedagogical foci for \ndigital landscape architecture education discourse at the forthcoming DLA  conference in 2023. The \ncritical question is, what is the future direction of digital landscape architecture education to address \nthe pressing and complex challenges of the climate crisis? For the purpose of this paper and the confer-\nence, we have limited the focus to three streams of digital landscape architecture: approaches, tools, \nand techniques. These critical streams of the discipline are framed through a brief synopsis capturing \nlineages from the 1960s to identify their influence on landscape architecture design education. Patterns \nand processes that lead to shortcomings in implementing the approaches are discussed. This is con-\ncluded with a set of questions derived from identified gaps to stimulate a targeted discussion on the \nfuture trajectories of digital landscape architecture education. \n\nIntroduction \nIn the face of climate change, we are confronted with accelerated urbanization and environmental  degradation.  \nThe  transformation  of  our  landscape  and  urban  systems  toward  more \nequitable, resilient and adaptive environments is urgently required, imbuing the capacity to \nrepair  and  respond  to  future  crises  and  to  adapt  to  unpredictable  futures  (ELMQVIST et  al. \n2019, SHEARER et al. 2021, FRICKER 2022a). \n\nDigital design education in landscape architecture that considers scales of action from the \nplanetary to the regional and microbial, has a crucial role in equipping students with the design  capabilities to  generate  alternative  typologies  of  aesthetics  and  performance  (MEYER 2008, FRICKER et al. 2020). This includes landscape transformations that perform in a desired \nmanner  (STEINITZ 2012, URECH et  al. 2020, GRÊT-REGAMEY et  al. 2021). To  this  end,  the \ndiscipline has developed sophisticated design and analytical tools, such as 3D point clouds, \nas a basis for urban design and algorithmic analysis of energy absorption, wind flow, and \nshadow  provision  (URECH  et  al.  2020,  2022).  They  support  an  integrated  analysis  across \nscales and feedback loops, as evidenced in several recent projects, like the example of a river \nrehabilitation project at the local scale that consequently explores the larger catchment area \n(VOLLMER  et  al.  2015,  GRÊT-REGAMEY  2017).  An  illustrative  overview  of  tools  and  approaches for responsive \nlandscape design is provided by  WALLIS & RAHMANN (2016), as \nwell as by CANTRELL & HOLZMANN (2016). The publications provide a comprehensive overview of design projects and \nresponsive technologies that frame performance as a generative \ndesign approach. Furthermore, heterogeneous data on environmental and socio-economic as-\npects are available with unprecedented detail to inform design approaches. For example, ur-\nban sustainability transformation projects that use passively sensed geospatial data of land \nuse, service networks etc., may be augmented by active sensing of stakeholder perceptions \nand behaviour  with participatory  methods and technologies (GRÊT-REGAMEY et al. 2021). \nHowever, although increasingly more tools and datasets are developed, and the agency of \ntheir application is demonstrated in prototypes (CANTRELL & HOLZMAN 2015), they are not \nwidely used across landscape architectural design schools. We postulate that a critical dis-\ncussion on digital design education is lacking in the discipline of digital landscape architec-\nture  (FRICKER 2022b).  Therefore,  we  propose  to  investigate  this  in  the  forthcoming  2023 \nDigital Landscape Architecture conference. \n\nThis paper acts as a precursor for the future conference dialogue to interrogate where Digital \nDesign Education is positioned and how to advance the pedagogical approaches of digital \nlandscape architecture. For this,  we  want to highlight some of the  existing theories of the \ndiscipline in the discussion, describe the status quo, and point out recent  “streams of con-\nsciousness”. This demonstrates that the landscape architecture discipline has constantly been \ninfluenced by and porous to other disciplines, thinking, tools and techniques that have con-\nstructed amorphous streams and trajectories continually being made and positioned (FRICKER \n2021). A complete review of the theoretical streams in digital landscape architecture is be-\nyond the scope of this paper. However, we reflect on specific critical theories and associated \ntools and processes from the 1960s to today that have significantly influenced current educa-\ntional practice in digital landscape architecture. The intention is not to give a comprehensive \nhistory of digital landscape architecture, but a framing of various digital design approaches \nin landscape architecture as a departure point for a discussion on future tools and techniques. \nWe use this review to discuss recurring patterns and processes in how new approaches and \ntools are used and how the gap in implementation manifests (ERVIN 2018). This leads us to \nformulate concrete questions to specify further: How do students need to be taught digital \napproaches? Where should we focus on enhancing our students' teaching? Furthermore, what \nneeds to be taught in digital landscape architecture education? \n\nA Synopsis of Computational Lineages \nFrom System Thinking to Artificial Intelligence \nThis chapter aims to reflect on a selection of relevant concepts and  workflows developed \nmainly during the 1960s and 1970s, which strongly influenced a pedagogy for the computa-\ntional realm and demonstrated a radical approach to creatively interact with diverse data sets \nacross scales. The purpose of this brief historical reflection is to unveil concepts to be revisited within the current discussion on defining possible avenues for adjusting the present trajectory of the digital pedagogy in the field of landscape architecture. Due to the richness of \nhistorical  references,  the  discussion  is  focused  on  key  examples,  inviting  for  an  extended \ndiscussion towards the future of the digital landscape architecture education and implementation within practise. Note: the selected examples in this paper address only male scientists. \nWe want to acknowledge that in these known lineages, female leaders in the field often need \nto be discussed as being more instrumental to the development. We aim to capture and slowly \nrectify this familiar narrative in future discussions. \n\nAlready towards the end of the 60s, computational design thinking pioneers recognized the \npotential of machine-human interaction to sound out new potentials within architecture and \nlandscape architecture. Almost 25 years later, the integration of “computation” in teaching \nushered a fundamental pedagogic change in direction for design teaching, research, and a \nform making language. In addition, a parallel stream “Digital Design Education” established \nitself with a focus primarily on the visualization applications of digital tools and the teaching \nof new software (ERVIN & HASBROUCK 2001, FRICKER 2021). The presented historical overview allows for a discussion in order to shift the focus from a merely tool-based approach \ntowards holistic computational design thinking. \n\nThe history of computation goes far beyond the development of computing technology and \nrelates to the “interaction between internal rules and (morphogenetic) pressures that, themselves,  originate  in  other  adjacent  forms  (ecology)”  (MENGES & AHLQUIST 2011, 8). This \ncomplex theory and framework of relationships is based upon theories from disciplines like \nmathematics, computer science, cybernetics, biology and philosophy. The integration of in-\nformation technological developments into the landscape architectural curriculum accelerated especially during the 1960s and 1970s through an intensive exchange between cybernetics and its influence on architecture (MENGES & AHLQUIST 2011). This first manifestation \nwas driven by a deep theoretical discourse and led to the first integration of Artificial Intelligence (AI) in design methodology. The fusion of these two areas lay in new questions related \nto the rise of global ecological challenges, which also changed our relationship to data and \nour interaction with the information it contained (FULLER 1969, MEADOWS et al. 1972). The \ntheories developed in the area of cybernetics allowed a new computational design method to \nbe established mainly within architecture, which describes this complex network of relationships  through  the  integration  of  System  Theory  and  Patterns  (FRAZER  1993).  In  the  late \n1960s, Jay Forrester, a computer engineer and system  scientist by education, strongly engaged in describing the “systemic structure responsible for the dynamics of urban development and decay”, founded the Urban System Group at MIT (FORRESTER 1973). \n\nThe themes discussed between cybernetics and architecture influenced simultaneous developments in landscape architecture with respect to the domain of system thinking in the field \nof spatial data handling. This is because both the fields of architecture and landscape architecture  were called to address issues of rapid urbanization.  Though the field of landscape \narchitecture recognized the necessity of developing new approaches for handling data, it did \nnot develop meaningful questions or further research with AI. \n\nEmerging Pedagogical Principles \nOne of the pioneering academic institutions, introducing a new form of design education with \nspecial focus on computational design thinking, was the Ulm School of Design (Hochschule \nfür Gestaltung in Ulm, HfG), active from 1953 until 1968. Already 17 years before the foundation of the Architecture Machine Group by Nicholas Negroponte and Leon Groisser, at a time “computers were only available at a few research centres, (…) their capabilities were \nwidely recognised and the subject of much broader theorisation and influence, opening up \nthe field of logic and computer science to the social sciences and arts” (NEVES et al. 2013, \n292). The new pedagogical approach of the HfG is understood as a research-based activity, \nstrongly engaged in theoretical discourse focusing on a new understanding of design, which \nis based on thinking in connections and networks. \n\nThe “pioneered heuristic procedures that were related to the power of the new computational \nmethods” (NEVES et al. 2013, 299) developed at the HfG can be seen in strong relation to the \ncomputational  education  developed  by  Negroponte.  Negroponte  recognized  that  problem-based learning concepts and the opportunity to work together with the computer for direct \nfeedback  significantly  increased  students’  motivation.  Programming  was  understood  as  a \nnew way of thinking! Negroponte experimented with the potential of formal descriptions of \narchitectural solutions, implemented through a program and deployed as Computer  Aided \nParticipatory Design. Thus, he laid the foundation for current methods in the field of AI and \nemphasized, “However, remember that these systems assume the driver to be an architect” \n(NEGROPONTE 1975, 365). The influence of the early computational design development in \nthe education of architecture has had very little impact on the area of landscape architecture \neducation. The only traces of limited integration of computational design can be observed at \nthe newly founded Laboratory of Computer Graphics at Harvard Graduate School of Design \nin 1965. Contrary to the radical development and interaction with data for generative purposes at the Arch MAC Group at MIT, landscape architecture education at Harvard’s GSD \nconcentrated mainly on layered data-mapping methods. \n\nThe Evolution of Spatial Thinking: From GIS-based Layering towards Mapping \nThe field of landscape architecture focused its emerging computational possibilities on re-\nsearch and application in teaching during the 60s and 70s on problems related to Big Spatial \nData. The pressure to develop new methods to process the complex relations of nature-based \nprocesses was strengthened by the new arising “Ecological Awareness”. The idea of layering \nspatial information and its use for evaluating designs was presented 1971 in the book “Design \nwith Nature” by Ian McHarg (LEE et al. 2014). Based on this idea, Geographic Information \nSystems (GIS) originated largely at Harvard GSD enabled to geographically allocate digital \ndata and create maps (FOSTER 2016). Further developments in the 1970s and 1980s focused \non  spatially  analysing  the  system  from  different  aspects.  In  the  following  period  the  user \ninterfaces, data processing capabilities and data interoperability were enhanced, and with this, \nits  applicability  for  many  user  groups  (LEE at  al. 2014). This  development  enabled  easier \naccess to digital geodata and simulations for assisting in a design process, and in 2012, Carl \nSteinitz published a “Framework for Geodesign”, which presents an iterative process of integrating stakeholders’ knowledge, needs and desires, geospatial modelling, impact simulation and rapid feedback on the degree of achieving a desired goal to facilitate an informed, \nresponsive design (FOSTER 2016, STEINITZ 2012). \n\nCurrent “Streams of Consciousness” \nStreams of consciousness describe time infused recursively in the material reality of the landscape through states of formation, from those that signify stability to sequences that are predictable and observable processes of change to those that are uncertain and instantaneous. \nMASSUMI (2002) suggests that our own “human” sensing of the world experienced through \nsensation involves a “backward referral in time”. Therefore, a sensation is organised recursively prior to being part of our conscious chain or actions and reactions. In this process, the \nsmoothing over of the anomaly is made to fit our conscious requirements of continuity and \nlinear causality. \n\nThe act of measuring and making the landscape is not a neutral activity; therefore, the process,  techniques,  and  tools  of  representing  form  are  rooted  in  a  specific  understanding  of \necosystems and their processes. “Actant is a term from semiotics covering both humans and \nnonhumans; an actor is any entity that modifies another entity in a trial; of actors it can only \nbe said that they act; their competence is deduced from their performances; the action in turn \nis always recorded in the course of a trial and by an experimental protocol, elementary or \nnot” (LATOUR 2004). Tools for measuring the landscapes, and the techniques by which we \ndeploy them, have their own constraints that translate and transform information. The representations we make are constructed from a set of instruments, codes, techniques, and a lineage  of  conventions.  Consequently,  the  worlds  they  describe,  and  project  are derived  only \nfrom those aspects of reality susceptible to those techniques. These acts of measuring, anaysing and making the landscape can formulate a view of what already exists and set conditions for new worlds to emerge. Below are three examples of what we refer to as porous, \nconstantly evolving “streams of consciousness”. \n\nEntangled Knowledge Systems: STEINITZ’S (2012) framework provides a clear structure on \nhow to design a design process for multi-disciplinary collaboration to better address the complexity of environmental problems across scales (FOSTER 2016). Along with the emergence \nof the new field of Geodesign, geodesign education programs were launched (WILSON 2014) \nand today, universities worldwide participate in the International Geodesign Collaboration \n(https:\/\/www-igcollab.hub.arcgis.com).  A  major  challenge  in  the  education  of  Geodesign, \nhowever, are the strict disciplinary silos at the universities that hinder cross-disciplinary collaboration (WILSON 2014). Further, recent evaluation of geodesign processes reveal that not \nall projects implement the full structure and particularly the analysis of spatial relationships \nand impact analysis across scales are often not well performed (GU et al. 2020). \n\nThe emergence of geodesign and other GIS-based methodologies coincided with the critical \ndiscourse on big data and the development of open-source systems that enabled collective \ncontribution and alternative algorithms to reveal bias in large data sets. In landscape architecture education, students were educated on the ethical and responsible use of big data to \ncritically address the inherent power that data has had historically in producing unjust actions \nand policies on the oppressed. The emergence of “hacking” data approaches and the creation \nof alternative data sets as a public good and public service consequently emerged (GABRYS 2016, WILLIAMS & PROQUEST 2020). \n\nEmergent Patterns: In another stream, contemporary research in landscape architecture ad-\ndresses, in particular, the technical challenge of best-representing geo-data and environmen-\ntal factors to foster an understanding of information and making sense of it (URECH et al. \n2020). For example, a collection of drawing types, such as diagrams, axonometry and map-\npings, has been assembled (AMOROSO 2015). In response to more complex landscape rela-\ntionships and organizational patterns in landscape architecture education, digital syntax, such \nas codes and patterns, is used to establish quantitative correlations between the landscape and \ndata processing. These approaches are utilized as a generative component for design produc-\ntion (M’CLOSKEY & VANDERSYS 2017, CANTRELL & MEKIES 2018). \n\nInternational Fields: With the environment globally changing more rapidly than ever be-\nfore, in the first two decades of the 21st century the awareness of urgency for an immediate \nresponse for solving problems increased. This gave rise to the use of point clouds recorded \nin the field with a  terrestrial laser scanner to rapidly replicate the physical landscape with \nhigh resolution and fidelity and as basis for analysis and design (GIROT 2019, URECH et al.2020). The point cloud models provide a common ground between architects, engineers, and \nscientists to develop informed landscape design (GRÊT-REGAMEY et al. 2021, GRÊT-REGA-\nMEY 2017, VOLLMER et al. 2015). By performing geospatial analyses using the geometry of \nthe point cloud  model, spatial configuration parameters can be investigated and enhanced \nemploying simulation models, e. g., for improving climate conditions through altering build-\ning  and  vegetation  patterns  (URECH et  al. 2020).  In  this  way,  the  point  cloud  models  and \nimmersive data interaction allow for more dynamic and versatile forms of landscape design \nthrough all scales involving aesthetic and performance considerations (GIROT 2019, URECH \net al. 2022). But the approach is still very experimental and has not yet found widespread use \nin digital design education in landscape architecture. \n\nDiscussion and Conclusion: How to Consolidate the Gap? \nWhen we look at the outlined examples, there are some recurring patterns that suggest a gap \nin the implementation of tools and approaches. A major concern is that often the full understanding is left out of what the process behind a generated solution is. In particular, this is \nevident in three crucial pitfalls of tool implementation, which we exemplarily point out as: \n(1) using “black box” digital tools, (2) improper calibration, and linear processes (3) focusing \non single aspects rather than interactions and processes across systems. \n\nConcerning  the  first  pitfall  with  the  tremendously  fast  development  of  cutting-edge  tools, \ndesigners become mere users without an understanding of the underlying processes and the \ninherent critical distance to the results. Looking back in the history of digital landscape architecture, the invention and use of digital tools in the design process (such as Grasshopper) led to concerns of employing a “black-box” optimization, taking the output as a goal in itself \nand lacking a more holistic systems thinking (FRICKER et al. 2020). \n\nSecond, not understanding the complex relationships of the defined parameters of a model \nand making uninformed choices of input data can also lead to wrong design decisions and \noptimization processes. For example, a data set is assembled only in the beginning of a design \nproject  and  often  neither  updated  nor  further  data  is  collected  according  to  the  generated \nsimulation  outputs  (FRICKER 2021).  Overall,  a  critical  engagement  with  the  collected  and \ngenerated data across scale and fields is missing. \n\nThird, there is a risk of justifying a design through simulation results on single aspects or on \none specific scale while the design solution actually is not solving the problem when examined  at  a  large  scale  because  of  mutual  interactions  of  single  aspects  on  various  scales \n(FRICKER et al. 2020). Disregarding aspects can lead to undesired developments, for example, \nfocusing only on the design site for river rehabilitation one might overlook effects of developments in the catchment area still leading to severe flooding (VOLLMER et al. 2015). An urban densification that helps minimize urban sprawl can increase the urban heat island effect \nand negatively affect a series of services provided by the urban ecosystem such as the provi-\nsion of recreational area, storm water infiltration and retention, or habitat for species (GRÊT-\nREGAMEY 2017, WISSEN HAYEK & GRÊT-REGAMEY 2021). There is a lack of understanding \nof system dynamics, spatial patterns and relationships (WOOD 2017). \n\nLandscape architecture as a discipline is evolving rapidly as it responds to both broadening \nand intensifying changes in environmental, social and political conditions. These changing \nconditions require development and innovation in the digital competencies of landscape architects. What approaches, digital skills and technologies are needed by landscape architects \nto equip them to deal with the complexities brought forth by the climate crisis? Then comes \na  critical  consequential  question:  how  can  we  design  the  education  of  future  practitioners \n(MONACELLA & KEANE 2023). \n\nThe transformation of the digital landscape architectural education must involve profound \nreconfiguration  of,  and  innovation  in,  discrete  knowledge  systems  within  the  pedagogical \nframework of the curriculum, including the course’s techniques, approach and nature of the \nway students are taught and learn. In conclusion we posit the following questions for discussion: \nWhat is the current status of  pedagogical approaches to digital landscape architecture \ntechniques, tools and approaches? What are the former “streams of consciousness”? We \nargue that streams of consciousness are porous lineages and trajectories historically influenced by broader contextual innovations and pursuits. \n\nWhat are the recent critical developments in digital landscape architecture and related \napproaches? What are the current “streams of consciousness and potential challenges in \nrelation to emerging fields like Artificial Intelligence and Machine Learning”? \n\nWhat are the gaps in the technological-based technique developments in digital landcape architecture utilized to address the climate change related issues and their translation in advancing pedagogical approaches? \n\nEnhancing Technical Grading Education: Finding the Right Tools for the Job \n\nIntroduction \nStated simply, grading design involves the alteration of the surface of a site to direct water \nflow. The reality of technical grading design is much more complex and involves the consideration of above- and below-ground, three-dimensional spatial relationships between people, \nvehicles, buildings, walls, pavements, utilities, plantings, and more. Considerable time and \nexpertise are required to develop the complex cognition necessary to produce complete and \ncorrect technical grading designs. Teaching technical grading skills to diverse groups of novice landscape architecture students is a difficult task for this reason – expertise and time are \nboth in short supply within four- or five-year undergraduate, and especially so within two-or three- year graduate academic programs.  \n\nIt  is  well-documented  that  learning  can  be  enhanced  by  using  both  static  and  dynamic \ngraphics (i. e., visualization tools) in instruction, see SCHNOTZ 2002 for a review. Learning \nenhancement can occur with the use of visualization tools both in external instruction and \ninternal learning processes. Teaching and especially learning of design skills heavily depends \non the use of visualization tools. In external instruction, design concepts (including technical \ngrading) are introduced and discussed, then modelled and practiced within a lecture\/studio \nenvironment using problems with varying degrees of application to “authentic” or real-world \nsites and problems. Students are expected to expand their learning of those concepts internally, by exploring design options and relationships with various graphic tools, such as drawing, modelling, rendering and\/or animation of views which they employ to visualize and understand the spatial relationships and tactile qualities of their design proposals.  \n\nGrading instruction necessarily relies on the use of two-dimensional visualization tools to \nboth explore design relationships and document grading changes. Sections and plans are two \ncommon visualization tools relied upon in grading instruction to foster design and documentation of grading plan proposals. Design grading, as a more conceptual, early-process activity, lends itself to design study via three-dimensional modelling. These models may be successfully used to rapidly understand initial conceptual or roughly detailed grading relationships and surface morphology. Technical grading cannot be easily or quickly explored with precision via complex three-dimensional modelling as the grading must first be completed before it can be modelled. This study hypothesizes that despite the use of complex external \nvisualization tools being less appropriate for use in technical grading design and communi-\ncation tasks, complex internal visualization operations must be utilized by learners to under-\nstand and manipulate the complex relationships among site design components in an efficient \nand confident manner. How, then, does an instructor best facilitate students’ internal visual-\nization skills without relying on the computer to visualize for them? Additionally, how does \nthe student avoid spending time developing complex digital models at the expense of devel-\noping the complex cognition required for technical grading competence? \n\nCognitive Load Theory \nThis study was initiated and informed by a literature review of Cognitive Load Theory (CLT) \nand through analysis of instructor responses to perceived student needs and direct student \nquestions during a semester-long design studio course. CLT was used as a lens through which \nto examine potential barriers to student development of the complex cognition required for \nindependent technical grading competence in the studio environment, and to inform instruc-\ntional changes to help promote higher degrees of learning. CLT assumes that two major goals \nof instruction are to facilitate the construction of internal schemas (models) and to automate \ntheir use to mitigate the significant impacts the limitations of human cognitive architecture \nhave on our ability to cognitively process complex learning (KALYUGA et al. 2003). In this \nauthor’s  opinion,  this  is  the  primary  goal  of  instruction  in  technical  grading  realms  –  the \ndevelopment of internal models, as opposed to external models (frequently manifesting as \nvisualizations and\/or displays), by which to process the complexity inherent in technical site \ndesign thinking quickly and efficiently. The limitations of human cognitive architecture may \nbe described in terms of three types of cognitive loads (for a more complete description, refer \nto RENKL & ATKINSON 2003):  \n\nIntrinsic Load refers to the complexity of the learning material itself. Technical grading \ninstruction may carry a high intrinsic load due to the complexity inherent in the interactivity  between  many  different  site  design  relationships.  Intrinsic  load  is  related  to  a \nlearner’s prior knowledge and should be expected to be at its highest levels with novice \nlearners. Intrinsic load may manifest as an information and\/or decision-making overload \n(too many requirements or too many relationships to attend to simultaneously). This load \ncan  be  decreased  with  experience  as  learners  develop  more  meaningful  information \nchunks which can be stored in long-term as opposed to working memory. \n“Germane Load refers to demands placed on  working  memory capacity that are imposed  by  mental  activities  that  contribute  directly  to  learning”  (RENKL  &  ATKINSON \n2003). This is the load learners should focus their cognitive resources on to facilitate \nlearning most successfully. \n“Extraneous Load is caused by mental activities during learning that do not contribute \ndirectly to learning” (RENKL & ATKINSON 2003). Due to the high intrinsic load inherent \nin technical grading tasks, it becomes very important for instruction to be designed spe-\ncifically to reduce extraneous loads. Considerable extraneous load is related to low levels \nof expertise. Low expertise can contribute to a simple lack of understanding of how to use available information and tools, and\/or inefficient use of the available information \nand tools. \n\nTo limit the impact of intrinsic and extraneous loads, an understanding of the technical grading expertise held by learners is critical to determine what information is relevant and how \nto present it to maximize the learner’s ability to attend to it. Novice learners generally learn \nbetter when given higher degrees of instructional guidance as they still need to develop their \navailable  schemas.  However,  more  knowledgeable  learners  (those  with  more  and\/or  more \ndetailed schemas) may require a different instructional approach that limits redundant information, otherwise they may experience cognitive overload and poor learning, despite their \nhigher level of expertise. This difference is termed the expertise reversal effect (KALYUGA et \nal 2003). It is theorized that this expertise reversal effect plays a role in the cognitive processes used by students to process technical design instruction activities and in the visualization tools and processes they use to supplement their learning. Therefore, it is important to develop instruction that recognizes and responds to the variable levels of expertise among \nthe students in a course, both related to the processes and visualization tools utilized to complete technical grading design. This study was undertaken to begin to understand the range \nof expertise variability and to theorize diagnostic tools which can be used in targeting instruction activities maximizing independence and development of expertise in the technical \ngrading design realm. \n\nInstructor Response Analysis \nThe experience of teaching this course over the past three years has been that the most educational impact (i. e. attention to germane load) comes from direct and personalized individual instructor interaction with students during their problem-solving process instead of group classroom interactions. Currently, the instructor must invest considerable time into individual \ninstruction to achieve this impact, so an analysis of instructor responses during these individual interactions was undertaken to balance effort and maximize learning. The analysis seeks \nto identify patterns among the actions or visualization tools recommended, the frequency of \nrecommendations and how those recommendations were accepted and implemented by the \nstudents. Additionally, the analysis sought to determine if there was any discernible impact \non the levels of independent thinking and use of visualization tools to enhance internal visualization processes. \n\nThe analysis focused on the following interactions due to their potential capacity to directly \nreduce intrinsic and extraneous loads. Intrinsic loads can be reduced by personalized discussion regarding  how to  use the information available, and  how to produce any lacking  but necessary information. Extraneous loads can be reduced by introducing and directly modelling the use of simple, abstract visualization tools to think and produce more complex internal \nvisualizations supporting technical grading design efforts. \n\nResponses were analyzed from interactions over three semesters of a studio course focused \non  the  technical  grading  design  of  a  complex  real-world  development  site.  The  fall  2020 studio (enrollment=18) considered a nearly 5-acre office development, fall 2021 studio (enrollment=23) focused on a 7-acre multi-family development and the fall 2022 studio (enrollment=19) designed a 5-acre office development. The course consists of a studio component with  460  minutes  (7.7  hours)  of  weekly  contact  time  (which  includes  lecture  time)  and  a separate  110-minute  (1.8  hours)  per  week  CAD  lab  component.  All  students  in  the  target course were novices with a negligible degree of variation among their prior grading expertise. All students were introduced to grading activities in an earlier course with a focused grading component where they were presented the grading process, techniques for visualizing landform and interpolating elevations, and developed grading skills via a grading design project. \n\nVariability of Student Expertise \nResults of the study identified potential barriers to learning situated within four pathways: \nLived Experience Variability: This pathway is defined as individual differences in recall\/codification (chunking) of actual human experiences, such as walking across surfaces,transitioning  grade-change  devices, noticing materials\/textures\/connections\/joints, etc. and linking or chunking those experiences together with technical grading skills in meaningful ways. \nCausal Chain Recognition: The skill to recognize critical relationships among existing \nand proposed component parts in the context of a technical grading design. \nInternal Animation: The ability to internally animate objects to transform them rotationally and\/or positionally  within the  site, or to animate and visualize  water  flowing across\/through elements of a site. \nDigital  Expertise  Variability:  This  pathway  defines  individual  differences  in  both \nknowledge of digital visualization tools (what they are and what they do) and how to \nmake them work to solve particular problems. This pathway refers both to simple inexperience\/lack of knowledge, and self-inflicted or self-limiting inexperience (such as refusal to spend the time needed to fully understand a software program). \n\nLearning activities in the first three of these pathways require the use of relatively simple external visualizations (such as diagrams, plans and sections) to support considerably more complex internal visualization and transformation operations such as flows and inferred motion – both of which represent aspects of mental animation (HEGARTY 1992).  \n\nCausal Chain Recognition supports the identification and documentation of critical relationships between site plan elements, and the visualization of responses to transformations \nof plan elements. CCR is the skill one would use to understand that as one corner of a flat \nrectangular pavement surface is depressed (lowered in elevation), the rest of the pavement \nsurface will tilt in that direction unless the surface is broken, creased, or otherwise deformed to accommodate multiple slopes. Thus, complex internal animation is required to mentally transform site objects and\/or surfaces with elevation differences efficiently without relying on an external visualizations to understand those transformations.  \n\nInternal  Animation  is  a  skill  utilized  when  considering  how  water  moves  across  and\/or through the system. Assuming water droplets remain intact from the moment they strike the surface until they leave the site at the outfall, technical graders should be able to trace a drop of water from the point it contacts the surface all the way to the site drainage outlet using internal animation. This skill requires the water to be mentally animated as it travels across surfaces and through conveyances. \n\nIt is hypothesized that Causal Chain Recognition, Internal Animation and, to an extent, Digital  Expertise  Variability  can  be  directly  influenced  via  instruction  emphasizing  germane loads, though this paper focuses only on addressing improvements to Causal Chain Recognition and Internal Animation. The study was conducted under the assumption that the creation  of  complex  external  visualizations,  such  as  detailed  3D  models,  would  contribute  to \nhigher  extraneous  loading  in  the  context  of  the  technical  grading  course,  so  instructor  responses were constrained to primarily 2D graphics, including static 3D views, but not models. Highly detailed 3D models of proposed grading solutions were not required or recommended by the instructor as a part of this course. However, TIN surfaces created from existing contours were required to be created using Civil 3D, and the use of the “Quick Profile” tool recommended for understanding existing topography and quickly testing proposed solutions and relationships. Additional solutions have been considered to address Lived Experience Variability, but those have yet to be implemented and tested in the course and will be addressed in a future paper. \n\nInstructional Methodologies \nFour  novel  technical  grading  instructional  methodologies  have  been  theorized  to  address learning improvements for each of the pathways mentioned in the previous section. Instructional methodologies were developed to both function as diagnostic tools to identify needed areas of focused instruction, and to facilitate the packaging or “chunking” of information to minimize  negative  cognitive  loads  and  enhance  development  of  technical  grading  design skills in novice learners.  \n\nSpot Skipping: a method of intentionally widespread, but very limited calculation of spot \nelevations early in the grading process which directly supports the recognition and calculation of critical grading relationships as a part of the Causal Chain Recognition pathway. \n\nFlow Branch Analysis: a method targeting spot elevations defining individual branches of \nthe site flow pattern to analyze flows and inform early technical grading design. This method of analysis is primarily concerned with flow lines and may be used independently or concurrently with Spot Skipping and primarily supports Internal Animation as water flows are visualized and defined across a site.  \n\nFlow Barrier Analysis: a method of analyzing site plan objects in terms of their impact on \nwater and\/or people flows. This method of analysis allows for the chunking of the site into \nflow barrier types and works to define and describe water flow patterns supporting Internal \nAnimation.  \n\nTransect Grading: a method of spot grading along discreet transects, usually drawn perpen-\ndicular to water, pedestrian and\/or vehicular flow paths, rather than at locations where spot \nelevations would be commonly calculated and included on a technical grading plan. Transect \nGrading  is  another  method  of  chunking  the  critical  relationships  between  elements  of  the grading plan into easy-to-understand sections, primarily supporting Causal Chain Recognition. Transect Grading may also be used in conjunction with Spot Skipping and Flow Branch Analysis. \n\nDiscussion and Conclusions \nEvidence from preliminary use of the four instructional methodologies described above in \nthe fall 2022 course seems to support the hypothesis that certain visualization tools may impose high extraneous loads on students. Their level of expertise in both interpreting critical grading design relationships and in the construction of suitably precise models is low enough that they don't  yet have the detailed schemas needed to develop efficient, low-extraneous-load processes. 3D model construction was deemphasized in the course and student outcomes seemed  to  improve.  Whether  the  improvement  was  related  to  the  lack  of  effort  spent  on model-building or simply more time developing grading skills has not yet been studied. However, CLT would support the notion that regardless of the reason, germane loads were prioritized and learning improved. \n\nResults suggest that, while digital drafting tools and Civil 3D can assist in drafting precision and in the process of working through the four methods, no complex visualization tools are required to achieve a high degree of expertise in technical grading (see Table 2). Even the 3D views may be sufficient if drawn inaccurately by hand or quickly and roughly modelled without any elevational precision in a program such as SketchUp (see Figure 3). Documentation and external communication of the grading solution may be best completed with sophisticated visualization tools, however this communication is secondary to the grading itself. The development of the grading design, to a high degree of detail, should be easily achieved using simple drafting tools and hand graphics if care is taken to do so with the required precision. This hypothesis must still be tested to confirm whether the use of complex external visualization tools  would be helpful in developing internal animation skills among novice technical graders.  \n\nThe production of a construction document quality grading plan has been a requirement for \neach iteration of this course and, given the complexity of the Civil 3D platform used to document those solutions, it is possible that some of the negative observations within the study may stem from extraneous loads imposed by the required documentation rather than issues regarding grading skills. The opinion of this author is that construction documentation should be an integral part of any technical grading plan. The primary purpose of the technical grading plan is to facilitate site construction and, learning to communicate design intent to the appropriate audience with the appropriate visualization tools should be the goal of any design education. Perhaps there should be a different focus on the documentation aspect of the grading plan, either concurrently or in a different semester. More work needs to be done to determine where any differences might exist between grading design skill and grading documentation skill. \n\nIt is important to note that the methodologies examined in this paper were applied to fine \ngrading of a site surface. Detailed considerations of mass grading and site stormwater management, such as balancing cut and fill and sizing stormwater management facilities were not included  as  a  part  of  the  target  course.  Accordingly,  additional  research  must  be  done  to examine the relationships between successful fine (surface) and mass grading activities while utilizing the methodologies described in this paper. \n\nThis study raises the question of which approach is the most appropriate for the most efficient transfer of knowledge and development of technical grading skill, the project-level approach, focusing  on  direct,  real-world  application  (as  presented  in  this  paper),  or  the  vignette  approach, focusing on individual skill development and repetition. The course within this study primarily relies upon the former, project-level approach, but does incorporate aspects of the vignette approach within the workshop and demonstration interventions, and the opinion of this author is that a combination is ideal. More work is required to answer the questions of what that combination should look like and how much time and effort should be spent by instructors and students within each. The four methodologies developed through this study should be developed into an online rapid diagnostic tool to identify the levels of technical grading expertise in a student population over time, and to match more detailed instructional methodologies to those students to help them overcome challenges to cognition and development of the expected technical grading competence. This study also suggests that additional  exploration  is  required  to  more  fully  understand  the  relationship  between  cognitive \nloading and the use of digital design skills and visualization tools versus analog design skills and visualization tools in an educational environment, especially in the realms of technically complex design tasks. \n\nFuture Resilient Landscape [Architects] \n\nAbstract: Parametric and computational design processes will evolve and inform the field of landscape \narchitecture. This paper investigates a bottom-up teaching approach about parametric design to novice \nlandscape architecture students as a viable method in their design pursuits. Using a case study, students \nexplored a translation of an intuitive approach to design into a parametric script, taking them through \nconcept ideation, fabrication, and ultimately informing implementation. \n\nIntroduction \nResilient landscape architects will be those who can anticipate, analyze, and address complex \nlandscapes, including those challenges yet to emerge. All professional fields are developing \ncutting-edge technologies to facilitate the analysis of complex issues and the implementation \nof viable solutions. The emergence of new technologies in landscape architecture has been a \nsignificant factor in the development of this discipline and has facilitated relevant research \nand design processes. Landscape architecture, now maturing with its own digital design practice  including  computational  design  (popularly  described  as  parametric  design)  is  gaining \nmomentum. The use of digital tools and techniques in the field of landscape architecture will \ncontinue to grow and evolve in the coming years (WALLIS & RAHMAN 2016).  \n\nFascinating  examples  of  new  computational  approaches  and  applications  are  emerging  in \nlandscape architectural projects. However, Bradley Cantrell and Adam Mekies believe that \nthe mechanism through which these applications are implemented remains obscure. This is a \nmissed opportunity since the logic, the thought process and the utilization of parametric design, could have been more evident to launch the complex execution (CANTRELL & MEKIES 2018). \n\nSince 1967, the MIT Media Lab has successfully “civilized” or “tamed” design and computer \ncode through years of effort. In 2003, the team designed the “Scratch” programing language \nso which began to employ a graphic interface rather than the laborious coding string (NAGLE \n2014). \n\nMitch Resnick, a computer scientist at the MIT Media Lab, conducts the “Lifelong Kindergarten”, where children learn to code and create from a very young age (RESNICK 2014). As \nResnick indicates, “When you learn to read (code), you can then read (code) to learn.” \n\nCoding identified as the core to parametric design describes parametric design thinking as a \nmethod, and not a tool. Do design school curricula or instructors provide effective strategies \nto  increase  the  broader  adaption  of  recent  technologies,  specifically  parametric  design, to \nfuture students?  \n\nIn recent years, the potential of computational media and its syntactical interface has been \nwidely explored by young designers through the GUI (Graphical User Interface) syntax of \nscripting. “How can we leverage this newly acquired foothold and understand better what we \nare gaining from parametric modeling\/visual programming\/coding as a design process and \nconceptual generator?” (CANTRELL & MEKIES 2018). \n\nIn this study, the authors investigated a kinaesthetic learning approach to cultivate a bottom-up understanding of the computational design process and engage students with advanced \ndigital tools. The goal was to encourage novice students to learn implicit knowledge of the \ncomputational design process first, and then learn explicit knowledge, in the following semesters. The emphasis was to employ parametric design tools in the design process rather \nthan limiting, or devaluing, their use to mere digital representation efforts. \n\nThe research team supervised a group of six (6) second-year Bachelor of Landscape Architecture students, interested in a public art competition, to utilize computational design tools \nin concept and design development of a public art piece, and subsequently, its off-site digital \nfabrication and production, and on-site implementation. Prior to this project, five of the six \nstudents had not utilized commercial 3D computer graphics and computer-aided design application software, such as the Rhinoceros 3D, as used for this project. The use of computational design tools  facilitated, and elevated, conventional design process activities into an ambitious design and implementation proposal.  \n\nAnalogue (Kinesthesia) to Digital (Parametric)  \nThe group of six (6) students enrolled in an independent course (design studio), co-taught by \ntwo faculty members, to prepare a design proposal for a public art design competition. Structured into three (3) phases, the studio included analogue rule sets, parametric\/digital rule sets, and fabrication. \n\nThe South Park project, by Fletcher Studio, inspired the method so that the computational tools test the resilience of analogue rules for spatial partitioning within a small park in San Francisco, CA. That project’s research was prepared for the Acadia 2014 exhibition, an annual parametric design conference (CANTRELL & MEKIES 2018). When Fletcher Studio first began work on San Francisco’s South Park, the initial design was developed “through iterative analogue diagramming” with a focus on “an intuitive understanding of the site and embedded in an analogue rule set” (FLETCHER 2021). \n\nCase Study: South Park, Fletcher Studio, 2017 \nFletcher Studio is a landscape architecture and urban design collaborative practice based in San Francisco, California. Fletcher Studio frequently uses parametric design software programs such ash such as Rhinoceros 3D, Grasshopper and Rhino script to test complex forms, \nfunctions, and site layout (Amoroso 2012). The Studio sought to reimagine San Francisco’s oldest public space (Figure 2) with a contemporary interpretation of the “picturesque style” landscape (CANTRELL & MEKIES 2018). The award-winning design transformed the site from an  English  strolling  garden  into  an  integrated  multi-purpose  communal  space  (FLETCHER 2021). \n\nAnalogue to Digital: The design intention sought to retain a hierarchy of circulation patterns, access points, social nodes, and existing trees and structures (CANTRELL & MEKIES 2018). “In the initial design phase, these decisions were made through an intuitive understanding of the parameters of the site and embedded in an analogue rule set that guided design decisions” \n(FLETCHER 2017). \n\nAnalogue rule sets require a considerable amount of testing time. “The same “idiosyncratic moments” that allow for the emergence of novel and intriguing design moves can also lead designers to overlook inconsistencies or flaws in their logic.” (FLETCHER 2017). By using a parametric  modelling  tool  in  the  Rhino  3D  software  program,  the  system  of  organization \ndeveloped in analogue (on paper), was translated into a Grasshopper digital script. The system  evaluated  “the  design  resiliency”  of  the  diagrammed  “tectonic  and  spatial  systems”(FLETCHER 2021). This enabled the designers to re-evaluate any flaws in their logic while also rapidly iterating upon the design in detail, without violating the previously established constraints of their design concept. \n\nTeaching Design Studio \nThe authors included kinaesthetic learning approaches in the phase of developing analogue rules associated with this studio. Kinaesthetic learning is a learning style in which individuals effectively “learn through doing”. Landscape architecture curricula historically include kinaesthetic learning approaches. Students increase understanding and testing of the products and  outcomes  of  design  exploration  by  touching  and  manipulating  them;  hence,  practical information is usually preferred over theoretical concepts. A kinesthetic learning experience can  aid  the  teaching  of  parametric  design;  one  can  read  about  it,  listen  to  instructions,  or watch videos of how to design parametrically – but deep learning occurs when one is physically involved with it. For their course, the instructors employed learning approaches including hand-sketching and model making to engage in an intuitive approach to design. These were “hands on” ways of exploring, developing and testing design concepts, aligning with the theme for a public art competition. \n\nCompetition Overview: The Winter Stations design competition is an open, single-stage, international design competition held annually in Toronto, Ontario. Guided by a provided theme, participants submit design proposals of temporary winter art installations incorporating the existing lifeguard towers situated along Toronto’s Woodbine Beach, on Lake Ontario. \nThe OneCanada project, informed under the competition’s provided theme of “Resiliency”, and designed and installed by six studio-course students, represents one of several submissions from artists and designers, worldwide, and was the only representation from landscape architecture, let alone an undergraduate student cohort. \n\nAnalogue to Digital  \nThe following phases characterize the study undertaken: \n\nPHASE 1: Developing Analogue Rule Set [Concept] \nStudents  were  asked  to  develop  a  concept  based  on  the  competition  theme  of  Resiliency. Their concept sought to interpret, appreciate, and promote the inspirational example of resilience of the Indigenous peoples of Canada, who continue to withstand adversity and persevere through generations of oppressive colonial policies. The concept also sought to bridge a gap between Indigenous and non-Indigenous peoples through an opportunity of “gathering” among the layering of the seven grandfather teachings (wisdom, love, respect, bravery, honesty, humility, and truth). The students envisioned the teachings to represent seven  white, \nand stacked circular forms, with a situational siting around a Woodbine Beach lifeguard station, representing the collective responsibility in the “guarding of life.” As an obvious beacon along the waterfront, art patrons, guests, and peoples from all backgrounds, could gather at \nthe  installation.  The  seven  teachings,  originating  with  the  Anishinaabeg,  and  have  been passed down from generation to generation ensuring the survival for all Indigenous peoples. \n\nPHASE 1: Developing Analogue Rule Set [Hand Sketching and Model Making]  \n\nBased on the initial concept, students generated ideas and imagined the form of the installation. Due to the lack of experience with 3D modeling software programs, students explored multiple design iterations through hand sketching and physical model making. As a result, the team developed analogue rule sets or design principles that guided design decisions: \n\n1.  Using circle as the prime form of installation. Circle is a sacred symbol of the interdependence of all forms of life in Indigenous culture (Stevenson 1999)  \n\n2.  Represent the seven grandfather teachings in minimum seven independent layers: wisdom, love, respect, bravery, honesty, humility, and truth. \n\n3.  Demonstrate unity in a sequence to symbolize bridging the gap between indigenous and non-indigenous people \n\n4.  Using a pattern to attach the separate layer which represents strengthening of relationships, and the protection of culture through the gathering and unity between people \n\nThese analogue rule sets, developed on paper and through model  making,  were translated into a parametric script using the Grasshopper plugin for the Rhino 3D software program.  \n\nPHASE 2: Developing Digital Ruleset [Parametric Script] \n\nWithout guiding the students through the complexity of learning algorithms or the coding behind the scripts, three algorithms or scripts developed in Grasshopper and were provided in a ready-to-use format to the students. The four (4) rule sets translated to ‘input parameters’ \nfollowed by multiple components in the Grasshopper plugin to rationalise the design process and to operationalise the principles. Using a ‘parametric lens’, the students could experiment, test, and generate design iterations and several design alternatives which allowed rule-based \nthree-dimensional platform to inform the decision making.  \n\nThe  final  iteration and  parametric  script,  for  the  competition  submission,  were determined from the various alternatives generated. \n\nPHASE 3: Digital Fabrication [Construction]  \nWith the rising presence of digital modeling in the field of landscape architecture, and accessibility to requisite equipment, digital fabrication has become a major facilitator in the development of research and design, in both professional practice and academia. As described by \nAndrew Madl “Professional design firms and universities providing use of digital fabrication in-house is becoming increasingly common. How to exploit such equipment is now taught in academic settings as skillset that is in line with traditional model making.” (MADL 2022). \n\nTeaching digital fabrication techniques requires significant amounts of time and resources. The intention of this phase was to develop a general introduction and awareness by providing a glimpse into the advantages of parametric design tools. \n\nFollowing the previous phases, students were required to prepare construction documentation or  “shop  drawings”  suitable  for  a  professional  fabricator.  Utilizing  computational  design tools developed in the previous phases, and through ongoing consultation with CNC fabrication  professionals,  students  learned  to  prepare  the  fabrication  files  and  facilitate  the  CNC cutting process. The goal was to encourage students to experiment with digital \nmodeling and file preparation, suitable to fabrication. \n\nFinally, the project’s implementation occurred through a team effort, ranging from detailed off-site work including material determination, metal welding, support strut wrapping, CNC-cut wood panel painting, transport, etc., to on-site assembly and construction. \n\nDiscussion \nIn the discourse of architectural fields, the term parametric design is associated with a particular attitude, aesthetic, and theory. Typically, one envisions the outcome as extraordinary and \nprovocative designs that inspire a set of  morphological principles (MADL 2022). The first perception of parametric design is limited to contorted formal expressions and the over-sophistication of geometry, which need to be deciphered. While this study emphasizes teaching \nthe  process  of  generating  complex  formal  expressions  for  a  public  art  installation,  the  research team addressed the potential of the method for future discoveries more specific to the field of landscape architecture. \n\nIn this course, some students gained the full understanding of the potential parametric design thinking offers at the end while a few had difficulties in developing a logic string of design steps that relate to the parametric approach, they preferred or felt back to intuitive or conventional  designing.  However,  the  later  group  was  interested  to  work  within  the  parametric framework if there is a team member managing the scripting part. This might inform an indication of the future of design so that the analogue embraces digital rather than introducing an absolute departure from analogue to digital. While parametric design offered a palette of \npossibilities, students got exposed to the realities of budget constraints and current limitations of digital fabrication, which eventually reduced the range of possibilities. \n\nRegardless of understanding the details behind the script, the “end product” and the process was well received by the students involved and has encouraged other students, privy to the course, to pursue “script” moving forward.  \n\nConclusion and Outlook \nImagined to provide interested students with an implicit understanding of the parametric process and to motivate them to create scripts unique to a project in future, this course enabled students to look outside the box, and even produce their own tool sets. Caroline Westort of Iowa State University explains that future landscape architects will not be only tool users but \nrather toolmakers. She indicates: “I actually think we do lose something by not training or teaching students the basic building blocks of what’s behind the black box, what’s behind the software . . . we are an information technology discipline, whether we like it or not.” (Bentley 2016). This indicates the need of training future resilient landscape architects, adept at creating script. \n\nParametric design can be difficult  for students  who  may  not have a strong background  in computer science or programming. It can also be time-consuming and challenging to learn and use these tools effectively, especially for those who are already comfortable with tradi-\ntional design approaches and intuition. Many designers will not engage at the high level of syntactical  knowledge  necessary  for  scripting  given  time  constraints  as  one  of  significant barriers. However, Grasshopper, Rhino, other GUI-based scripting allows designers to more readily connect the outcome of code with the formal representation without having to know how to write code (CANTRELL & MEKIES 2018). \n\nContemporary landscape architecture theory and practice necessitate the processing and design of data connected with complex systems in order to accurately reflect composite and emergent scenarios (MADL 2022). The field of landscape architecture, along with other design disciplines, are undoubtedly evolving through computational discovery.  \n\nLandscape architects and landscape architecture itself can respond to ever-evolving nature of practice and their resulting consequences. The outcomes of this design studio proved that parametric design permits a level of ambiguity, inquiry, discovery, confidence, and execution expected in creative and learning environments. \n\nBy training  future resilient landscape architects  with computational tools, universities and educational institutions can make a significant contribution in keeping pace with evolving principles. It is expected that these skilled professional practitioners and researchers will be \nintroduced to the community, adequately versed, and improve the model of practice-based research, which ultimately improves conventional and speculative design workflows. \n\nGeodesign as Online Teaching Method – Lessons from a Multiple Case Study \n\nAbstract: This study analyses the geodesign workshop as a method for the online teaching of group \nwork methods in the context of geoinformation systems (GIS) in planning and design. In order to assess the learning outcome, four workshops with international landscape architecture students at master level were conducted over a period of four years (2018-2021) and compared in a qualitative multiple-case study. In times of Covid-19 and the need for remote workshop methods, the geodesign workshops seem well suited for online learning and teaching. The results show that the learning goals were achieved, that new ideas were created and stakeholder expectations reflected and challenged. In individual cases, the lack of on-site knowledge led to mistakes though, and online group work had different group dynamics  than  in-person  negotiations.  Vocal  and  well-organised  students  seem  to  engage  even  more whereas quiet students more easily disengage, as seen in a bimodal distribution of participation grades in the online class. In conclusion, geodesign workshops may be recommended as an online method for teaching GIS and group work methods such as brainstorming, consensus building and stakeholder-role play but a hybrid format or new virtual field trip techniques are preferable when familiarizing students with the case study site. The teaching of group work methods as part of planning and design may be transferred from geodesign to teaching building information models, which is also an information-based digitally facilitated collaboration process. \n\nIntroduction \nGeodesign has been included in many university curricula around the world. The International Geodesign Collaboration (IGC) introduced a standardized geodesign process, which \nhas been conducted by hundreds of universities around the world. WARREN-KRETZSCHMAR \net al. (2016) already demonstrated the benefits of geodesign as a teaching method in planning \nand design classes. Building on their insights, this paper further explores whether geodesign \nis also a suitable method for the teaching of group work methods, and how geodesign classes \nadapted to online teaching during the Covid-19 pandemic.  \n\nIn  short,  STEINITZ  (2012)  defines  geodesign  as  planning  geography  through  design.  In  a \nlonger  definition,  FLAXMAN  (2010)  defines  geodesign  as  “a  design  and  planning  method \nwhich tightly couples the creation of a design proposal with impact simulations informed by \ngeographic context and systems thinking normally supported by digital technology.” Among \nother methods, geodesign utilizes the scenario  method (BISHOP et al. 2007), which is also \npart of many university programs. \n\nHence, a common misconception is that geodesign is only about technology. Although ge-\nodesign is characterised by the integrated use of GIS tools and geodata as the basis for an \ninformed  design  and  decision-making  process  (CAMPAGNA  2014),  it  is  generally  a  group \nwork process. In this context, several group work methods correspond well with the geodesign process. These are brainstorming, stakeholder role-play, and collaborative negotiation \nmethods.  \n\nBrainstorming is a method for the quick generation of ideas (JONES 1992). In the first step, \nparticipants have to write down as many ideas as possible during a limited amount of time. \nSince this step is about the creation of ideas, no weighting, discussion or filtering takes place \nyet. In a second step, the ideas are discussed in the group, clustered thematically and redundant or unsuitable ideas are sorted out. DOMINGO et al. (2021) demonstrate how brainstorming can also be applied in remote settings to facilitate collaborative work.  \n\nAt the same time, geodesign addresses complex multi-stakeholder planning and negotiation \nprocesses. PETTIT et al. (2019) suggest collaborative negotiations and consensus-building as \npart of the geodesign process. Starting with an even number of stakeholder groups, e. g., eight \ngroups with one planning proposal each, these groups meet with the closest other group, e. g., \ngovernment and business, and negotiate a consensus between their two proposals. Then, the \nremaining four proposals are narrowed down to two and the two to a final one. Because the \nprocess is mediated through digital means, PETTIT et al. (2020) also call it digital negotiations. \nThey conclude that such digital negotiations are an effective planning method. \n\nSuch processes embody underlying roles and often hidden agendas and conflicts. LIGTENBERG et al. (2010) used a role-playing approach in which students took on the roles of local \ncitizens, farmers and nature conservationists together with an agent-based model for simulating a multi-actor spatial planning process. In the IGC process, the role-playing approach \nlends  itself  to  have  students  represent  different  stakeholder  groups.  Common  stakeholder \ngroups are local citizens, local businesses, local government, youth organisations or environmental NGOs. Research goals are to assess whether:  \nLearning goals were achieved; \nThe quality of the results changes between online and in-person geodesign workshops; \nGeodesign workshops as learning and teaching method for group work are transferable to other programs at Master level. \n\nMethods: Multiple Case Study Comparison  \nThe research design is based on the multiple case study method (see 2.2.) by YIN (2014). The \ncontext for the workshops is kept consistent and comparable by following the recommendations and templates of the International Geodesign Collaboration IGC (see 2.2.): scale, group \nsize, underlying global assumptions, time-frame, and range of scenarios do not change across \nworkshops. The workshops are informed by open data from the EU Copernicus programme, \nOpenStreetMap  and  local  environmental  agencies  (2.3.)  All  workshops  use  geodesignhub \n(www.geodesignhub.com) as online platform to facilitate the process (2.4.). In the comparison, quantitative data such as average  grades for participation and outcome are compared \ntogether with qualitative observations, i. e., data triangulation in the words of YIN (2014). \n\nInternational Geodesign Collaboration IGC Template  \nORLAND & STEINITZ (2019) describe the International Geodesign Collaboration IGC, a col-\nlaborative  project of  more  than  120  universities,  research  institutions  and  public  \/  private stakeholders across the world. In order to facilitate research into geodesign, the collaboration organizes annual workshops and provides a template to make the diverse geodesign projects comparable.  The  IGC  template  (https:\/\/www.igc-geodesign.org\/presentation-formats)  provides the following:  \nCommon geodesign systems (water, green infrastructure, energy, transport, agriculture, \nindustry and commerce, institutional, residential and two flexible systems) and a common colour palette to easier identify and compare land use patterns and alternative design scenarios.  \nGlobal assumptions and a library of geodesign innovations, such as new renewable energy solutions, transport innovations etc., which IGC participants are encouraged to apply in their individual projects.  \nCommon scenarios and timeframes at 2035 and 2050, and paths to achieve scenarios for \nthose: “Early Adopters” initiate design interventions in 2020; “Late Adopters” in 2035; \nand “Non-Adopters” continue with business-as-usual.  \nTemplates for common reporting formats as presentations and posters. \n\nThe geodesign projects compared here dropped the 5km and added a spatial extent at 40km \nbut adhered to the IGC systems, innovations, common timeframes and poster templates.  \n\nMultiple Case Study Design  \nThe basic concept of  this  multiple case-study is  to conduct the  workshops as similarly  as \npossible by referring to the IGC standard templates for participating projects. The four workshops (see Tab. 1) were embedded in a GIS module in the second year of an international \nMaster´s degree in landscape architecture. Student backgrounds were very diverse, with students from different Bachelor´s degrees and more than 20 different nationalities. Working \nlanguage  was English. Each  workshop had one day of preparation plus individual student \nhomework and three days of the actual workshop. Results were documented on two A2 posters per workshop. \n\nGeodata-Based Process \nFor each workshop, suitability analyses were run ahead of the workshop in ArcGIS Pro and \nsummarized  in  so-called  evaluation  maps.  The  suitability  analyses  were  mainly  based  on \nopen geodata from the Urban Atlas, which are derived from Copernicus satellite data (European Union, Copernicus Land Monitoring Service, European Environment Agency (EEA)),map data from OpenStreetMap and protected areas provided by the Bayerisches Landesamt \nfür Umwelt LfU and the Geoportal Baden-Württemberg.  \n\nOnline Platform \nAll workshops were conducted through the online platform geodesignhub, which uses maps \nand  diagrams  to  facilitate  the  negotiation  process.  FLINT & STEINLAUF-MILLO  (2021)  describe geodesignhub as “an interactive design method that uses stakeholder input, real-time \nfeedback, geospatial modelling and impact simulations to facilitate the development of an \neffective management strategy and smart decisions.” By presenting two maps with individual \ndiagrams of projects and policies, and adding functions for filtering and visual comparison \n(Fig. 2), geodesignhub provides the tools to reach an informed consensus. \n\nCase Descriptions \nAll four workshops correspond with local planning topics, i. e., Munich Parkmiles is elaborating the open space concept of the City of Munich; Regional Garden Festival Stuttgart is \ncontributing  to  the  International  Building  Exhibition  Stuttgart,  Heidelberg  Green  Belt  responded to the invitation by the City of Heidelberg to develop ideas for a green belt and the \nlast project is contributing to the forthcoming IBA Munich. The four geodesign workshops, \npresented here, share the same learning goals:  \nAddressing a planning question at city to regional scale  \nApplication of GIS skills and demonstration of geodata capacity  \nDeveloping group work skills  \n\nCase Study 2018\/19: Munich Parkmiles \nIn a competition of ideas, 30 international students drafted the 2035 and 2050 scenarios in \nparallel working teams. Nevertheless, the results are surprisingly consistent. The common \nidea is that green infrastructure innovations are concentrated in the „Park Mile” green corridors. Housing is mainly accommodated in mixed-used zoning. For example, the 2050 early \nadopters’ scenario is presented in Figure 1, which extends the high-density mixed-use areas \nalong the  major public transport lines towards the city´s edge. In this case, the colours  in \ngeodesignhub indicate different types of zoning policies. The green spaces in between, including urban forestry in the south and valuable farm land in the northwest, are put under \nprotection  protected  from  further  development.  The  large  inner-city  yellow  policy  zone \nmarks low-density laneway housing in the otherwise high-density neighborhoods.  \n\nCase Study 2019\/20: Regional Garden Festival Stuttgart  \n\nGerman garden festivals have become a powerful driver for sustainable urban development. \nThe focus of the student proposals is on a positive impact on the climate. The approach in \nthe 2035 and 2050 scenarios complement each other progressively to implement policies on \nrenewable energy combined with blue and green infrastructure. Land use is planned strategically to mitigate urban sprawl, reduce the urban heat island effect, and increase rainwater \ncollection.  Housing  is  addressed  through  high-rise  developments  by  converting  redundant \nindustrial areas into mixed land use with a focus on bringing in a large “breathing” space in \nthe form of a park that Nürtingen does not currently have (see Fig. 2). Renewable energy \nprojects introduce solar farms, solar surfaces on highways, and policies that require residential and industrial zones to contribute local solar energy.  \n\nCase Study 2020\/21: Heidelberg Green Belt \nThe City of Heidelberg and its neighbouring cities, most importantly the City of Mannheim \nnorthwest of Heidelberg, have already launched a number of landscape development projects \nfor ecological restoration. At the time of this workshop, the city council had asked the planning department to develop ideas for a multi-functional “green belt” between Heidelberg and \nMannheim. Please note that the term “green belt” has been discussed controversially in different contexts. In the context of this project, the “green belt” is supposed to integrate ecological and physical landscape characteristics with multiple land uses (protected natural areas, agriculture, infrastructure, recreation...) in a multifunctional landscape.  \n\nFigure 3 is showing the early adopters' scheme for 2050 with green and blue infrastructure \ncorridors visible west of Heidelberg, i. e. along the area adjacent to the Mannheim urban area. \nIn addition to introducing new blue infrastructure, the students suggested links to the strong \nmedical sector in Heidelberg by introducing therapeutic gardens and other forms of restorative landscapes. Interestingly, the seemingly novel idea of creating new blue infrastructure \ncorresponded with a local proposal for an artificial lake.  \n\nCase Study 2021\/22: Munich International Building Exhibition \nSimilar to regional garden shows, the regularly held Internationale Bauausstellung (IBA) is \na key driver of national building and planning culture in Germany. It has played an important \nrole in cooperation, innovation, participation, experimentation, and visualization of 10 years \nof planning and design. Since Munich was awarded the next IBA on the topic of mobility, \nstudents were encouraged to envision a regional IBA providing new sustainable approaches \nto mobility landscapes. The City of Munich IBA team supported the workshop.  \n\nThe first day mainly focused on learning about the area of Munich and analysing where pos-\nsible improvements could be made based on suggested systems such as: transport infrastruc-\nture, industry and commerce, mixed residential, tourism, blue and green infrastructure, en-\nergy infrastructure, climate, and agriculture. One key instrument was the further development \nof the “park miles”, seen in green Figure 4, which had already been addressed in the first \nworkshop by a different group of students.  \n\nCross-Case Comparison \nIn all four cases, the students achieved the learning goals. Comparing the four cases, there \nare commonalities but also differences between the in-person and the online settings:  \n\nStatistical Comparison \n\nThe students received grades for 1) participation in the workshops and 2) the quality of the \noutcome, i. e., the content of the resulting scenarios and their presentation on the posters. \nStudent numbers were supposed to be around 30, but actually varied between 23 and 36 depending on factors out of our control such as visa issues or Covid-19.  \n\nA simple descriptive analysis of the average mean grades across the four workshops is presented in Table 2. In general, grades are rather good (with 1.0 the best possible grade). The \nbest participation was recorded during the first in-person workshop in 2018\/19, whereas the \nsecond online workshop in 2021\/22 had the poorest participation. If you look closer at the \ngrades, participation in the last workshop shows a trend towards a bipolar distribution: quite \na few students participated very well in the online workshop, but in contrast, a large number \nof students participated poorly or dropped out. \n\nCross-Case Observations \nIn both the in-person and online settings, large numbers of diagrams were created, and both \nsettings led to comparable results in terms of quantity and diversity. With regard to the IGC \nframework, three scenarios were derived from the diagrams: early adopters, late adopters, \nand non-adopters. The geodesign process of narrowing down the scenarios to a smaller number of consensus scenarios also succeeded in both settings. For teaching purposes, the scenario process was combined with exercises in negotiation and students “role-played” different stakeholder groups, such as young people, government, business representatives or environmental NGOs. Some students fully embodied their roles and took on a new perspective, \nleading to interesting discussions, such as proposing affordable housing versus the provision \nof additional green space.  \n\nThe online platform geodesignhub facilitated the documentation of the process in both settings, online and in-person. Especially in a teaching environment, it is of great help for the \nteacher during assessment and grading that all ideas and the scenario building process are \narchived in geodesignhub.  \n\nDifferences between In-person and Online Settings  \nThe online setting can facilitate a broader geographical range of case study topics and locations, although it seemed to come at the costs of sometimes lacking understanding of the site. \nOne group was obviously not aware of local characteristics and depicted high-rises, which \nwere completely out of context.  \n\nIn  terms  of  organisation,  the  online  workshop  made  it  easier  for  international  students  to discuss with local stakeholders than organising such a session in person. Like an in-person setting, the online discussion inspired both groups, students and local stakeholders. \n\nHowever, the grading showed a lower grade in participation, particularly for the last work-\nshop. From observation, more vocal students tended to engage even more in the online set-\nting, whereas it was much harder than in-person to motivate quiet or disengaged students. \nNevertheless, the online setting was a suitable remote learning tool during Covid-19 times, \nand the geodesign workshop method proved to be well suited for online teaching.  \n\nConclusion and Outlook \n\nIn conclusion, the learning goals were achieved. Therefore, geodesign workshops are generally recommended for teaching group work methods such as brainstorming, consensus building, and stakeholder-roleplay in GIS-based planning and design. In times of Covid-19 and \nthe need for remote workshop methods, the geodesign workshops were also well-suited for \nonline learning and teaching although participation was slightly lower during the online sessions. These observations are consistent, though, with other classes that were taught online \nduring Covid-19 and could point to a certain online “fatigue”.  \n\nRegarding the quality of results, the online setting  might come at the cost of the students \nfamiliarizing themselves with the case study area. It is recommended to further develop hybrid  settings,  e. g.,  collaborations  with  local  experts  or  the  development  of  remote  or  VR enabled field trip techniques to facilitate a better understanding of the site (HASBROUK & \nSTEPNOSKI 2022).  \n\nFindings and group teaching methods from this multiple case study could be transferred to \nteaching Building Information Modeling BIM. Like geodesign, BIM is a collaborative process rather than a software. In a BIM class, the role-play could simulate the different stakeholders in a BIM process, from surveyor to architect and client, and the BIM model may be used to facilitate negotiations among these stakeholders. \n\nFor future geodesign research, it is suggested to focus further on the evaluation of scenarios. \nPeer review through the students themselves might contribute to the learning and teaching \nprocess. In addition, GIS-based or even artificial intelligence (AI) based methods might facilitate new learning and teaching methods by providing real-time quantitative and qualitative \nfeedback. It will have to be seen how the geodesign process is further developing and which \nrole, if any, AI will play in it.  \n\n“Open Access” Climate Resilience Tools for Landscape Architects  \n\nAbstract: The devastating effects of climate change we are witnessing, such as the floods throughout Germany in 2022, are placing more pressure on designers to predict and mitigate such events while designing various sites. Greenskinslab’s researchers at the University of British Columbia, Canada have \nspent many years developing digital tools to predict, mitigate and provide design suggestions on how to manage increasing human risk to natural hazards, focusing particularly on flooding and more recently landslides. These globally “open access” and user-friendly digital tools support both students and professionals in the early site planning and design phase of projects to calculate the risks of landslides and flooding.  This  article  briefly  highlights  two  “open  access”  climate  resilience  tools.  These  are:  1)  a stormwater calculation (LID) application that dimensions the correct LID strategies (living roof, retention pond, swale) needed as a holistic system to manage rainwater on new and existing sites, published in  2021,  and  2)  a  landslide  susceptibility  toolkit  for  landscape  architects  comprised  of  a  GIS-based \nanalysis tutorial and multi-sensorial on-site analysis instructions, developed in 2022 by a multi-disciplinary UBC student team. These tools are based on computer software which is widely accessible and affordable and analog methods of investigation. Greenskinslab’s mandate is to combat climate change \nby supporting designers’ planning processes with digital and analog tools to assist them to better envision a more climate resilient future. \n\nIntroduction \nThe global Covid 19 pandemic in early 2020 initiated academia to learn digital communication and teaching tools at lightning speed to continue teaching at a satisfactory standard in landscape architectural programs around the world. Suddenly instructors had to depend on computers to deliver lectures and provide design reviews, demonstrating to academics and students the potential of online communication software. In my case it confirmed the teaching effectiveness to include instantly recorded ‘digital’ hand drawn three-dimensional visualizations in teaching. It confirmed how versatile and fast the tablet is in recording how environmental holistic systems functioned, in landscape architectural design. Additionally, it allowed \nfor animations of diagrams and audio explanations of the process shown. \n\nThis online teaching experience led to two areas of research focus: 1) to include the tablet \nand smart phone as observation and recoding tools when teaching my multisensory landscape \ndesign method, described in my second book: Multisensory Landscape Design: A Designer’s \nGuide for Seeing, (2022), and 2) in creating “open access” climate resilience tools in my lab \nto mitigate or predict global destructive climate change effects such as floods and landslides. \nThese tools are partially based on the research of my first co-authored book: Living Roofs in \nIntegrated Urban Water Systems (2015). The book describes the effective combination of \nLID tools (retention pond and swales) at grade with living roofs to manage urban flooding. \n\nIn order to manage the ongoing impacts of climate change, evidenced through the deadly \n2021 heat dome in British Columbia, the floods and landslides in British Columbia later the \nsame year and the ‘biblical’ Flood in Pakistan in 2022, I propose to increase applied on site \nquantitative and qualitative research to mitigate current climate change effects in landscape \narchitecture. Extensive research is currently being carried out to determine climate change \npredictions and how to manage the postulated effects. However, we are already witnessing \nan increasing magnitude and frequency of natural hazards due to climate change that is causing devastating impacts. Therefore, landscape architects and researchers should contribute \nmore to mitigate current climate change effects through their work. I propose “open access” \ntools for landscape architects to enhance their site analysis investigation and design process. \nThese tools help determine potential risk in the landscape (i. e. landslides) prior to development of a site, or measure the size and suggest LID interventions needed to mitigate future \nfloods in the preliminary design phase. These analysis and preliminary design calculations \nwill strengthen the initial design proposal and contribute to climate resilience in the planning \nprocess of landscape architectural design. Two “open access” tools created in Greenskinslab \nare presented here. It is anticipated that these tools increase professional analysis and design \npractice, and should also be incorporated into landscape architecture design education. “Open \naccess” tools enhance design teaching. \n\n“Open Access” Tools \nThe more tools are available online, the higher are the opportunities they are used. At Green-skinslab they are developed by a team comprised of both students and alumni that are inter-\nested in focused research opportunities against climate change. The inclusion of alumni provides them with an opportunity to integrate their professional office experience into the creation of the tools they are interested in to refine and use for their design process. The tools \nare created firstly for British Columbia’s local use and then expanded for global use (This \nwas also done because of the extensive data available online in BC on precipitation, soil types \netc.). For all the tools created, the premise was to be able to use affordable and easily available \nhard- and software, to allow access and use in regions with limited resources. The tools are \nuser-friendly and self-explanatory in layout and operation and accessible to students and professionals. An overarching goal is to encourage the next generation of landscape architects \nto co-lead ideas and research tools, to empower them to design for society and the environment in a climate resilient impactful way. In recent years digital visualization of designs in \nlandscape architecture schools has experienced a boost and focused often on learning sophisticated three-dimensional rendering tools. These renderings and video animations created a \nmore and more realistic spatial design narrative. The open access tools presented are enabling \nadditional  design  rigour  to  advance  the  technical  and  scientific  skills  to  combat  climate \nchange on site. They include digital and analog components. \n\nThe landslide tool uses user-friendly (free trial) ArcGIS software and access free data from \nCopernicus Access Open Hub, LiDAR BC, and Fresh Atlas Stream Network BC to locate \nmass movement areas at a map scale determined by the grid size. In a second field investigation phase, ‘multisensory’ applications (sight, touch, smell) are used to investigate the land \nform, geological processes, soil profile, texture, geological processes, hydrological characteristics and vegetation. The students and practitioners are instructed through a field guide \nposted on the blog how to examine the site. This tool allows landscape architects, architects \nand urban planners to develop a deeper understanding and awareness of mass movements \nphenomena, and eventually prevent these hazards on site and around it. It is an additional site \ninvestigation tool in the preliminary design phase to reduce the risks of destruction due to \nclimate change. \n\nThe stormwater calculation (Low Impact Development LID) application focuses on estimating the size and types of LID tools needed to manage stormwater on site of a new development. “Water drives design” was the strategy in mind when designing it (1). When designing a new site, the LID application uses the current local climate data (supplied by the airports close to future design project), to calculate the LID tool dimensions and types needed.  \n\nThis application has been created for the preliminary design phase of a project to enhance \nclimate resilience strategies such as stormwater management on site. This tool can also be \nused to design and dimension LID strategies in existing blocks of cities (2). \n\nBoth tools attempt to inspire and lay a foundation for other researchers to develop open access \ntools with their students and alumni for their area of expertise. Landscape architecture is a \nland-based profession, the design impact is “outside” in the environment, landscape architecture designs are exposed and constantly changing. More ‘applied research’ should be carried \nout in design school, to increase evidence-based designing with quantitative and qualitative \ndata. Landscape architects have a responsibility to design all projects with climate resiliency \nin mind today.  \n\nConclusion \nI believe that multidisciplinary applied research contributions such as the open access tool \nexamples above can support climate change resilience effectively. They can be adapted to \nlocal needs and expertise. Not only such tools are, in most cases, easily adaptable to local \nneeds and expertise, they can often be used immediately by the public, rather than depending \non time-consuming grant funding process. In the instances where funding is necessary, the \nstudents involved in the projects may subsequently support them as alumni.  \n\nI also suggest further that all landscape architecture students should learn basic ‘coding’, best \napplied directly in a research project or class exercises which involves the creation of a small \napplication which can be used in landscape architecture, for example a cut and fill calculator \nfor grading (3). This applied teaching will encourage and inspire the next generation (students \nand graduates) to co-lead ideas to implementation, and empower them to act for their future. \nStudents need to be provided a research platform to experiment and shape the environmental \nfuture.  Landscape  architecture  students  are  often  thinking  idealistic  in  “saving  the  world” \nthrough  designing.  This  positive  energy  should  be  channeled  into  the  “shaping”  or  land \nscapeing − the environment with more applied research driven projects and tools, helping \nothers. \n\nUncovering the Visibility of Blue Spaces: Design-oriented Methods for Analysing Water Elements and Maximizing Their Potential \n\nAbstract:  Existing  studies  indicate  that  a  direct  view  of  aquatic  elements  benefits  well-being,  and \nhouses  with  blue  views  are  often  associated  with  higher  prices.  Therefore,  developing  analysis  and \ndesign  methods  for  visibility  research  of  blue  spaces  are  crucial  to  advance  spatial  design  practice. \nEspecially  digital  methods  for  analysing  blue  visibility  and their potential  in design  still need  to be \nidentified and explored. This study explores the application potential of some powerful digital visibility \nanalysis methods for blue space design. Specifically, this research first provides an overview of poten-\ntial methods for analysing the visibility of water. Next, two practical design-oriented digital methods \nare briefly elaborated and illustrated by cases in Rotterdam (the Netherlands). Meanwhile, the study \nexplores how the analysis results support spatial design practice. Last, the study discusses the potential \nof integrating blue visibility analysis methods into the iterative design process and makes prospects for \nfuture research. \n\nIntroduction \nWith the prevalence of chronic lifestyle-related diseases and rapid urbanization, health issues \nhave received increasing attention. The health benefits of natural environments as a practical \nsolution to current health issues have been widely recognized, especially for green spaces. \nRecently, the health benefits of blue spaces have been identified, and many studies suggest that a direct view of aquatic elements (e. g. rivers, lakes, and oceans) benefits psychophysiological states and reduces stress (GRELLIER et al. 2017, HARTIG et al. 2014, ZHANG et al. 2022). Houses with blue views are associated with higher attractiveness and price (QIANG et al. 2019). Therefore, it is necessary to consider blue visibility in spatial planning or design process of urban environments, which could provide multiple benefits. On the other hand, current studies  mainly  focus  on  combining  blue  visibility  with  health  data  to  explore  the  relationship between them for developing health evidence. However, only limited studies focus on integrating this health evidence into practical spatial design or policymaking, especially in guiding the design processes, which could be regarded as a huge vacuum in the existing research \n(ZHANG et al. 2022). \n\nKnowledge\/evidence-based design approach and design research provide a solid foundation and  potential  for  filling  this  vacuum,  as  well  as  extend  scientific  understanding  of  design processes that were previously regarded as a black box (JONES 1992, OZTURK 2020). Specifically, spatial design is a core activity in landscape architecture and its related disciplines to provide solutions for urban or rural areas to achieve desired social, cultural, and ecological outcomes (NIJHUIS & DE VRIES 2019). According to the ASE paradigm, design could be regarded as an integrative practice consisting of three interrelated phases: analysis, synthesis, and evaluation. In these three phases, blue visibility analysis methods could be applied to the analysis and evaluation phases to identify site limitations and potentials of design proposals, as well as provide solid support for assisting designers in the synthesis phase. For designers, the integration and utilizing of these methods, especially innovative digital ones, have greatly aided the spatial design practice (LIU & NIJHUIS 2020a). In other words, these methods can \nhelp translate and apply research evidence in the design practice. However, only a limited \nnumber of studies have offered some available methods or tools to analyse the visibility of \nblue spaces and explore their design potential (LIU & NIJHUIS 2020b, NIJHUIS, 2011). To sum \nup, there is a need to identify practical digital visibility analysis methods that support design \nresearch and design for the development of effective blue spaces in the urban environment. \n\nThis paper aims first to provide an overview of potential methods for analysing the visibility \nof water from a design perspective. Also, two practical design-oriented digital methods are \nbriefly elaborated and illustrated by cases in Rotterdam (the Netherlands). The paper ends \nwith a discussion on the potential of integrating blue visibility analysis methods into the iterative design process and provides an outlook for future research. \n\nMethods \nDigital Methods for Analysing Blue Space Visibility \nReviewing the current research and practice on landscape visibility, there are six representative practical  methods suitable for analyzing blue  visibility, including the statistical  index \napproach, (Cumulative) viewshed analysis, (3D) Isovist analysis, segmentation analysis, eye-\ntracking analysis, and 3D landscape analysis (HELBICH et al. 2019, KIM et al. 2019, LIU & \nNIJHUIS 2020b, NIJHUIS 2015, PALMER 2022a and 2022b, PUSPITASARI & KWON 2020). Spe-\ncifically, the statistical index analysis uses alternative indicators, such as the number, total\/ \nmean area, or density, to measure blue visibility. It is mainly applied in spatial design projects \nat the regional scale and contains the advantage of simple and rapid calculation. (Cumulative) \nviewshed analysis adopts the continuous digital landscape model to calculate and visualize \nthe surfaces that are visible to specific observer features. Since it is integrated into GIS software, it could be applied in regional\/intermediate projects where computing power is sufficient. Due to the input data and used analysing tools, the way in which the above two methods \nare  integrated into the design process to assist design decisions is to allow comparison  of \nchanges pre\/post spatial design interventions. On the other hand, (3D) isovist analysis shares \na similar calculation logic with viewshed analysis, while the precision and ease of modelling \nallow it to be applied to projects at the individual scale by simulating blue visibility changes \nduring people’s movements. Segmentation analysis and eye-tracking analysis both borrow \nthe theory or techniques from computer vision to describe the visibility of blue space at the \nindividual scale through the analysis of characteristics in specific scenes quantitatively. The \neye-tracking analysis attempts to describe people’s perception of blue  spaces  more objectively by measuring observers’ eye movements and associating them with spatial characteristics. Last, 3D landscape analysis is widely used in designers’ daily practice to identify the characteristics  of  specific  spatial  arrangements  using  2\/3D  visualisations  (LIU  &  NIJHUIS 2020b). Unlike the two blue visibility analysis methods at the regional scale, the four methods at intermediate\/individual scales, when integrated into the design process, allow rapid simu-\nlation of the post-intervention scenario to help designers test and visualize different design intentions. Table 1 lists the existing blue visibility analysis methods with detailed information.  \n\nAfter reviewing the methods, it is important to assess their potential for integration into the \ndesign process and to select the representative and novel ones to demonstrate their application. There are four criteria to identify their potential in the design process. Specifically, the \nmethods should first allow implementation in designer-friendly software. Most of the methods listed could be integrated and run in existing and easy-learning software environments, \nincluding GIS, Rhino, SketchUp, Photoshop, and Excel. Only the segmentation analysis needs \nto  be  run  in  Python  directly  via  existing  deep-learning  packages  and  pre-training  models, \nwhich  may  be  unfamiliar  to  designers.  Therefore,  it  is  worth  showing  its  application  and \ndiscussing its design potential in this study. Second, the methods should be adaptive to the \ninput data with multiple precision and sources. Since eye-tracking analysis relies heavily on \nuser participation, the requirements for input data are relatively strict. In other words, it describes blue visibility subjectively from a non-designer’s point of view and therefore is not \nincluded in this study. Third, the method should understand the eye-level visibility of blue \nspaces since it is closely related to spatial design rather than planning and is currently receiving growing attention. Accordingly, the intermediate\/individual-level methods are selected, \nas the regional scale methods are closely related to landscape planning. Last, the methods \nneed to be integrated into design iterations, allowing quick changes to represent and test designers’ new ideas. Thus, the scenario-based methods indicated in the last column of Table 1 will  be  chosen.  Based  on  the  above  criteria  and  the  novelty  of  the  methods,  (3D)  Isovist analysis  and  segmentation  analysis  are  chosen  in  this  research  for  investigation,  and  their \ndesign possibilities are examined. \n\nStudy Area and Materials \nTo show the application of the two selected methods, a part of the Rotte River in Rotterdam \nis used as the study site. There are three reasons for choosing the Rotte River as the \ncase. First, the Rote River is located in Rotterdam, the second largest city in the Netherlands, \nwhich is rich in blue space resources. Rotterdam’s urban living is closely related to the water, \nand blue visibility plays an important role in the spatial design of the urban environments. \nSecond, the Rotte River used to be a major transport artery of the city in the past, and industrial products and vegetables were taken to the markets and auctions via it. Nowadays, the \nRotte River has been transformed into a public space which is closely related to the daily life \nof the public and provides multiple benefits. Last, as an essential urban river in Rotterdam, \ndata availability and physical accessibility (i. e. field survey) helped to evaluate and refine \nthe results of method applications. \n\nAs mentioned above, two methods for analyzing blue visibility are used to show their appli-\ncations and explore their design potential. Considering that each method has its unique char-\nacteristics, Table 2 lists the details of the tools and data required for the two methods. \n\nResults \nThree situations are presented to illustrate the application of the two methods and the design \npotential of their analysis results. Specifically, route-based visibility analysis adopts the 3D \nIsovist analysis method to calculate the visibility of water bodies under people’s movements. \nBuilding-based visibility analysis also adopts the 3D Isovist analysis method, taking the building as the analysis object to calculate the visible proportion of blue space in different areas of \nthe building surface. Moreover, the segmentation analysis method is incorporated into scene-\nbased visibility analysis to obtain the visual features of typical scenes in selected area. \n\nSituation 1: Route-based Visibility Analysis \n3D Isovist analysis is used in route-based visibility analysis via Rhino-Grasshopper environ-\nment to calculate the water visibility of people alongside the specific route (Fig. 2). The process consists of the following steps: (a) divide the selected route into several parts; (b) generate original view sphere based on horizontal and vertical FOV (field of view); (c) construct \nand compute the Isovist Rays by setting the obstacles and analyzing radius; (d) calculate the \nproportion of Isovist Rays contacting the water surfaces for each part of the route; € visualize \nthe  analyzing  results  into  chart  diagrams.  Three  transportation  modes,  including  walking, \njogging, and cycling, and two directions, including North-South and South-North, are conducted in the calculation, and detailed analysis parameters are shown in Table 3.  On the other hand, during the movements on the selected route, the overall patterns of blue visibility among the three transportation modes are similar, especially for jogging and cycling. This could be due to the lack of clear route classification and design between the different transportation modes in the study area. Accordingly, the analysis can be applied to the planning and design of different routes by simulating the people’s blue visibility of different traffic behaviours, such as the blue visibility of cycling routes at intersections could be lower to prevent distraction. \n\nSituation 2: Building-based Visibility Analysis \nBuilding-based  visibility  analysis  also  conducts  3D  Isovist  analysis  to  calculate  the  water \nvisibility of building surfaces located near the Rotte River (Fig. 4). The analysis process consists of the following steps: (a) divide the building surfaces into the small matrix; (b) set the \ncenter point of each small cell as the input for 3D Isovist analysis; (c) conduct the 3D Isovist \nanalysis as mentioned above; (d) calculate the proportion of visible water bodies in each cell \nof building surfaces; € visualize the results on buildings in an interactive way; (f) calculate \nmean blue visibility of each building and visualize it on map (optional). Detailed parameters \nfor analysis are shown in Table 4. \n\nThe left graph of Figure 4 presents the analysis results directly on building surfaces, where \nthe surfaces with blue-side colours indicate higher blue visibility. In comparison, the surface \nwith red-side colours shows lower blue visibility. Even for the same building, the outcome \ninteractively  represents  the  varying  degrees  of  blue  visibility  at  different  points.  The blue \nvisibility of two adjacent rooms can be varied, which may lead to completely different con-\nsequences, as ULRICH’s (1984) famous experiment demonstrated that a ward with a natural \nview could have a positive influence on the recovery of the patients living in it. The result \ncan help designers in architecture or vegetation design, in adjusting the layout of buildings \nor vegetation and repositioning the building windows or vegetation to increase blue visibility. \n\nOn the other hand, the analysis results could provide evidence for planning and policy-mak-\ning on the intermediate scale. Specifically, the study further calculated the average blue visibility of each building surface cell and visualized the results on the map. The results show \nthat the middle three buildings have higher blue visibility, which provides the potential for \nevaluating  the  differences  in  blue  visibility  among  several  buildings  (Fig.  4  [right]).  This \nresearch merely uses a few riverside buildings for testing. In the future, the analysis can be \nextended to the buildings within a specific area to support the planning of neighbourhood \nspatial layout and strategies for pricing housing units. \n\nSituation 3: Scene-based Visibility Analysis \nThe machine learning-based segmentation analysis method is used in scene-based visibility \nanalysis. Following the procedures in  HELBICH et al.’s (2019) research, the fully convolutional neural network for semantic segmentation (FCN-8s) model is trained by the ADE20K \nscene parsing and segmentation databases. A total of 208 photos taken based on the on-site \ninvestigation were used as input images for segmentation (Fig. 5). Next, the number of elements in scenes and the ratio of different element groups to total pixels are calculated. \n\nOn the other hand, the statistical analysis of the segmentation results can quantitatively describe the landscape element characteristics in scenes. The three graphs in the first row of \nFigure 7 demonstrate the distribution of the number of elements in each scene and the average \nproportion of area occupied by the main landscape element groups in all scenes. The number \nof elements in the current scenes is concentrated in 20-30, and the number of elements whose \nfield of view accounts for more than 5% is concentrated in 6-9. Vegetation, water, and sky \nare the main landscape element groups, while facilities only occupy a limited proportion. The \nthree graphs in the second row of Figure 7 compare the element characteristics of typical \nscenes among three sections of the river. From the results on the number of elements, the \nvalue in section 3 is higher than the other two sections, showing that it has relatively high \nvisual complexity. The value of section 1 is more concentrated and less distributed in large \nvalues, demonstrating that its visual complexity is stable and relatively easy to understand. \nHowever, the value of section 2 presents more scattered patterns than the others, indicating \nthat people’s visual perception changes in this section are more significant. In addition, according to the results on the proportion of main landscape element groups, it is obvious that \nthe proportion of vegetation in scenes of section 2 is less than in the other two sections, and \nthe proportion of buildings is higher. More buildings in scenes could also prove that the visual \nperception of this section is dynamic and varied. Accordingly, segmentation analysis could \nbe regarded as a powerful tool for designers to describe the landscape elements and the spatial-visual characteristics in design. Visual complexity, openness, naturalness and other indicators calculated by segmentation results can provide quantitative evidence for designers to \ncompare different design schemes or intentions and then make final decisions. \n\nDiscussion and Conclusion \n\nAs exemplified by the presented applications, three situations of using two methods not only \nhelp to get a grip on the different characteristics of blue visibility of Rotter River, but also \nexplore the possibilities in assisting multi-scale spatial design to improve it. In other words, \ntwo practical and design-oriented digital methods, including (3D) Isovist analysis and seg-\nmentation analysis, could facilitate a comprehensive understanding of the site’s restrictions, \nlimitations, and potentials on blue visibility, and provide clues for future design interventions. \nOn the other hand, these methods are complementary and can be combined together in some \ncircumstances.  For  instance,  segmentation  analysis  could  also  be  applied  in  situation  1  to \noffer an understanding of various landscape elements and their organizations during movements beyond the Isovist analysis only focusing on water bodies. \n\nThis  study  does  not  provide  the  complete  process  of  applying  the  two  methods  to  spatial \ndesign, but only shows the possibility of their design potential in a highly simplified form \nthrough three application situations. However, based on the ASE paradigm of design pro-\ncesses mentioned in section 1, it is evident that they could be parts of the design iterations, \nincluding analysis, synthesis (design), and evaluation (Figure 8). Here, designers could use \nthem to analyze and understand the existing situation, test and evaluate ideas or intentions, \nand compare different options to make final decisions. \n\nNowadays, with the advancement of technology, digital methods and tools are being widely \ninvestigated and introduced into spatial design practices, greatly expanding the toolbox of \ndesigners. The two methods presented in this study could serve as an inspiration to encourage \nexploration of the potential of multi-disciplinary methods to be incorporated into design processes. However, the traditional means (e. g. sketches or models) cannot be overlooked and \nshould be combined with the digital methods to achieve design objectives better and improve \nthe development of the knowledge\/evidence-based design approach. In addition, this study \nhas several limitations. First, this study cannot include all possible methods and only provides \nlimited applications to inspire future research. Second, the landscape is dynamic and greatly \naffected  by  time  or  season,  especially  for  water  bodies  and  vegetation.  People  may  have \ngreater blue visibility in winter since the leaves are gone. Last, the data acquisition or precisions, quantity of site photos, processing power or time, etc, may influence the results. Future \nresearch  can  optimize  these  factors  to  improve  the  effectiveness  and  practicability  of  the \nmethods. \n

“Open Access” Climate Resilience Tools for Landscape Architects  \n\nAbstract: The devastating effects of climate change we are witnessing, such as the floods throughout Germany in 2022, are placing more pressure on designers to predict and mitigate such events while designing various sites. Greenskinslab’s researchers at the University of British Columbia, Canada have \nspent many years developing digital tools to predict, mitigate and provide design suggestions on how to manage increasing human risk to natural hazards, focusing particularly on flooding and more recently landslides. These globally “open access” and user-friendly digital tools support both students and professionals in the early site planning and design phase of projects to calculate the risks of landslides and flooding.  This  article  briefly  highlights  two  “open  access”  climate  resilience  tools.  These  are:  1)  a stormwater calculation (LID) application that dimensions the correct LID strategies (living roof, retention pond, swale) needed as a holistic system to manage rainwater on new and existing sites, published in  2021,  and  2)  a  landslide  susceptibility  toolkit  for  landscape  architects  comprised  of  a  GIS-based \nanalysis tutorial and multi-sensorial on-site analysis instructions, developed in 2022 by a multi-disciplinary UBC student team. These tools are based on computer software which is widely accessible and affordable and analog methods of investigation. Greenskinslab’s mandate is to combat climate change \nby supporting designers’ planning processes with digital and analog tools to assist them to better envision a more climate resilient future. \n\nIntroduction \nThe global Covid 19 pandemic in early 2020 initiated academia to learn digital communication and teaching tools at lightning speed to continue teaching at a satisfactory standard in landscape architectural programs around the world. Suddenly instructors had to depend on computers to deliver lectures and provide design reviews, demonstrating to academics and students the potential of online communication software. In my case it confirmed the teaching effectiveness to include instantly recorded ‘digital’ hand drawn three-dimensional visualizations in teaching. It confirmed how versatile and fast the tablet is in recording how environmental holistic systems functioned, in landscape architectural design. Additionally, it allowed \nfor animations of diagrams and audio explanations of the process shown. \n\nThis online teaching experience led to two areas of research focus: 1) to include the tablet \nand smart phone as observation and recoding tools when teaching my multisensory landscape \ndesign method, described in my second book: Multisensory Landscape Design: A Designer’s \nGuide for Seeing, (2022), and 2) in creating “open access” climate resilience tools in my lab \nto mitigate or predict global destructive climate change effects such as floods and landslides. \nThese tools are partially based on the research of my first co-authored book: Living Roofs in \nIntegrated Urban Water Systems (2015). The book describes the effective combination of \nLID tools (retention pond and swales) at grade with living roofs to manage urban flooding. \n\nIn order to manage the ongoing impacts of climate change, evidenced through the deadly \n2021 heat dome in British Columbia, the floods and landslides in British Columbia later the \nsame year and the ‘biblical’ Flood in Pakistan in 2022, I propose to increase applied on site \nquantitative and qualitative research to mitigate current climate change effects in landscape \narchitecture. Extensive research is currently being carried out to determine climate change \npredictions and how to manage the postulated effects. However, we are already witnessing \nan increasing magnitude and frequency of natural hazards due to climate change that is causing devastating impacts. Therefore, landscape architects and researchers should contribute \nmore to mitigate current climate change effects through their work. I propose “open access” \ntools for landscape architects to enhance their site analysis investigation and design process. \nThese tools help determine potential risk in the landscape (i. e. landslides) prior to development of a site, or measure the size and suggest LID interventions needed to mitigate future \nfloods in the preliminary design phase. These analysis and preliminary design calculations \nwill strengthen the initial design proposal and contribute to climate resilience in the planning \nprocess of landscape architectural design. Two “open access” tools created in Greenskinslab \nare presented here. It is anticipated that these tools increase professional analysis and design \npractice, and should also be incorporated into landscape architecture design education. “Open \naccess” tools enhance design teaching. \n\n“Open Access” Tools \nThe more tools are available online, the higher are the opportunities they are used. At Green-skinslab they are developed by a team comprised of both students and alumni that are inter-\nested in focused research opportunities against climate change. The inclusion of alumni provides them with an opportunity to integrate their professional office experience into the creation of the tools they are interested in to refine and use for their design process. The tools \nare created firstly for British Columbia’s local use and then expanded for global use (This \nwas also done because of the extensive data available online in BC on precipitation, soil types \netc.). For all the tools created, the premise was to be able to use affordable and easily available \nhard- and software, to allow access and use in regions with limited resources. The tools are \nuser-friendly and self-explanatory in layout and operation and accessible to students and professionals. An overarching goal is to encourage the next generation of landscape architects \nto co-lead ideas and research tools, to empower them to design for society and the environment in a climate resilient impactful way. In recent years digital visualization of designs in \nlandscape architecture schools has experienced a boost and focused often on learning sophisticated three-dimensional rendering tools. These renderings and video animations created a \nmore and more realistic spatial design narrative. The open access tools presented are enabling \nadditional  design  rigour  to  advance  the  technical  and  scientific  skills  to  combat  climate \nchange on site. They include digital and analog components. \n\nThe landslide tool uses user-friendly (free trial) ArcGIS software and access free data from \nCopernicus Access Open Hub, LiDAR BC, and Fresh Atlas Stream Network BC to locate \nmass movement areas at a map scale determined by the grid size. In a second field investigation phase, ‘multisensory’ applications (sight, touch, smell) are used to investigate the land \nform, geological processes, soil profile, texture, geological processes, hydrological characteristics and vegetation. The students and practitioners are instructed through a field guide \nposted on the blog how to examine the site. This tool allows landscape architects, architects \nand urban planners to develop a deeper understanding and awareness of mass movements \nphenomena, and eventually prevent these hazards on site and around it. It is an additional site \ninvestigation tool in the preliminary design phase to reduce the risks of destruction due to \nclimate change. \n\nThe stormwater calculation (Low Impact Development LID) application focuses on estimating the size and types of LID tools needed to manage stormwater on site of a new development. “Water drives design” was the strategy in mind when designing it (1). When designing a new site, the LID application uses the current local climate data (supplied by the airports close to future design project), to calculate the LID tool dimensions and types needed.  \n\nThis application has been created for the preliminary design phase of a project to enhance \nclimate resilience strategies such as stormwater management on site. This tool can also be \nused to design and dimension LID strategies in existing blocks of cities (2). \n\nBoth tools attempt to inspire and lay a foundation for other researchers to develop open access \ntools with their students and alumni for their area of expertise. Landscape architecture is a \nland-based profession, the design impact is “outside” in the environment, landscape architecture designs are exposed and constantly changing. More ‘applied research’ should be carried \nout in design school, to increase evidence-based designing with quantitative and qualitative \ndata. Landscape architects have a responsibility to design all projects with climate resiliency \nin mind today.  \n\nConclusion \nI believe that multidisciplinary applied research contributions such as the open access tool \nexamples above can support climate change resilience effectively. They can be adapted to \nlocal needs and expertise. Not only such tools are, in most cases, easily adaptable to local \nneeds and expertise, they can often be used immediately by the public, rather than depending \non time-consuming grant funding process. In the instances where funding is necessary, the \nstudents involved in the projects may subsequently support them as alumni.  \n\nI also suggest further that all landscape architecture students should learn basic ‘coding’, best \napplied directly in a research project or class exercises which involves the creation of a small \napplication which can be used in landscape architecture, for example a cut and fill calculator \nfor grading (3). This applied teaching will encourage and inspire the next generation (students \nand graduates) to co-lead ideas to implementation, and empower them to act for their future. \nStudents need to be provided a research platform to experiment and shape the environmental \nfuture.  Landscape  architecture  students  are  often  thinking  idealistic  in  “saving  the  world” \nthrough  designing.  This  positive  energy  should  be  channeled  into  the  “shaping”  or  land \nscapeing − the environment with more applied research driven projects and tools, helping \nothers. \n

Uncovering the Visibility of Blue Spaces: Design-oriented Methods for Analysing Water Elements and Maximizing Their Potential \n\nAbstract:  Existing  studies  indicate  that  a  direct  view  of  aquatic  elements  benefits  well-being,  and \nhouses  with  blue  views  are  often  associated  with  higher  prices.  Therefore,  developing  analysis  and \ndesign  methods  for  visibility  research  of  blue  spaces  are  crucial  to  advance  spatial  design  practice. \nEspecially  digital  methods  for  analysing  blue  visibility  and their potential  in design  still need  to be \nidentified and explored. This study explores the application potential of some powerful digital visibility \nanalysis methods for blue space design. Specifically, this research first provides an overview of poten-\ntial methods for analysing the visibility of water. Next, two practical design-oriented digital methods \nare briefly elaborated and illustrated by cases in Rotterdam (the Netherlands). Meanwhile, the study \nexplores how the analysis results support spatial design practice. Last, the study discusses the potential \nof integrating blue visibility analysis methods into the iterative design process and makes prospects for \nfuture research. \n\nIntroduction \nWith the prevalence of chronic lifestyle-related diseases and rapid urbanization, health issues \nhave received increasing attention. The health benefits of natural environments as a practical \nsolution to current health issues have been widely recognized, especially for green spaces. \nRecently, the health benefits of blue spaces have been identified, and many studies suggest that a direct view of aquatic elements (e. g. rivers, lakes, and oceans) benefits psychophysiological states and reduces stress (GRELLIER et al. 2017, HARTIG et al. 2014, ZHANG et al. 2022). Houses with blue views are associated with higher attractiveness and price (QIANG et al. 2019). Therefore, it is necessary to consider blue visibility in spatial planning or design process of urban environments, which could provide multiple benefits. On the other hand, current studies  mainly  focus  on  combining  blue  visibility  with  health  data  to  explore  the  relationship between them for developing health evidence. However, only limited studies focus on integrating this health evidence into practical spatial design or policymaking, especially in guiding the design processes, which could be regarded as a huge vacuum in the existing research \n(ZHANG et al. 2022). \n\nKnowledge\/evidence-based design approach and design research provide a solid foundation and  potential  for  filling  this  vacuum,  as  well  as  extend  scientific  understanding  of  design processes that were previously regarded as a black box (JONES 1992, OZTURK 2020). Specifically, spatial design is a core activity in landscape architecture and its related disciplines to provide solutions for urban or rural areas to achieve desired social, cultural, and ecological outcomes (NIJHUIS & DE VRIES 2019). According to the ASE paradigm, design could be regarded as an integrative practice consisting of three interrelated phases: analysis, synthesis, and evaluation. In these three phases, blue visibility analysis methods could be applied to the analysis and evaluation phases to identify site limitations and potentials of design proposals, as well as provide solid support for assisting designers in the synthesis phase. For designers, the integration and utilizing of these methods, especially innovative digital ones, have greatly aided the spatial design practice (LIU & NIJHUIS 2020a). In other words, these methods can \nhelp translate and apply research evidence in the design practice. However, only a limited \nnumber of studies have offered some available methods or tools to analyse the visibility of \nblue spaces and explore their design potential (LIU & NIJHUIS 2020b, NIJHUIS, 2011). To sum \nup, there is a need to identify practical digital visibility analysis methods that support design \nresearch and design for the development of effective blue spaces in the urban environment. \n\nThis paper aims first to provide an overview of potential methods for analysing the visibility \nof water from a design perspective. Also, two practical design-oriented digital methods are \nbriefly elaborated and illustrated by cases in Rotterdam (the Netherlands). The paper ends \nwith a discussion on the potential of integrating blue visibility analysis methods into the iterative design process and provides an outlook for future research. \n\nMethods \nDigital Methods for Analysing Blue Space Visibility \nReviewing the current research and practice on landscape visibility, there are six representative practical  methods suitable for analyzing blue  visibility, including the statistical  index \napproach, (Cumulative) viewshed analysis, (3D) Isovist analysis, segmentation analysis, eye-\ntracking analysis, and 3D landscape analysis (HELBICH et al. 2019, KIM et al. 2019, LIU & \nNIJHUIS 2020b, NIJHUIS 2015, PALMER 2022a and 2022b, PUSPITASARI & KWON 2020). Spe-\ncifically, the statistical index analysis uses alternative indicators, such as the number, total\/ \nmean area, or density, to measure blue visibility. It is mainly applied in spatial design projects \nat the regional scale and contains the advantage of simple and rapid calculation. (Cumulative) \nviewshed analysis adopts the continuous digital landscape model to calculate and visualize \nthe surfaces that are visible to specific observer features. Since it is integrated into GIS software, it could be applied in regional\/intermediate projects where computing power is sufficient. Due to the input data and used analysing tools, the way in which the above two methods \nare  integrated into the design process to assist design decisions is to allow comparison  of \nchanges pre\/post spatial design interventions. On the other hand, (3D) isovist analysis shares \na similar calculation logic with viewshed analysis, while the precision and ease of modelling \nallow it to be applied to projects at the individual scale by simulating blue visibility changes \nduring people’s movements. Segmentation analysis and eye-tracking analysis both borrow \nthe theory or techniques from computer vision to describe the visibility of blue space at the \nindividual scale through the analysis of characteristics in specific scenes quantitatively. The \neye-tracking analysis attempts to describe people’s perception of blue  spaces  more objectively by measuring observers’ eye movements and associating them with spatial characteristics. Last, 3D landscape analysis is widely used in designers’ daily practice to identify the characteristics  of  specific  spatial  arrangements  using  2\/3D  visualisations  (LIU  &  NIJHUIS 2020b). Unlike the two blue visibility analysis methods at the regional scale, the four methods at intermediate\/individual scales, when integrated into the design process, allow rapid simu-\nlation of the post-intervention scenario to help designers test and visualize different design intentions. Table 1 lists the existing blue visibility analysis methods with detailed information.  \n\nAfter reviewing the methods, it is important to assess their potential for integration into the \ndesign process and to select the representative and novel ones to demonstrate their application. There are four criteria to identify their potential in the design process. Specifically, the \nmethods should first allow implementation in designer-friendly software. Most of the methods listed could be integrated and run in existing and easy-learning software environments, \nincluding GIS, Rhino, SketchUp, Photoshop, and Excel. Only the segmentation analysis needs \nto  be  run  in  Python  directly  via  existing  deep-learning  packages  and  pre-training  models, \nwhich  may  be  unfamiliar  to  designers.  Therefore,  it  is  worth  showing  its  application  and \ndiscussing its design potential in this study. Second, the methods should be adaptive to the \ninput data with multiple precision and sources. Since eye-tracking analysis relies heavily on \nuser participation, the requirements for input data are relatively strict. In other words, it describes blue visibility subjectively from a non-designer’s point of view and therefore is not \nincluded in this study. Third, the method should understand the eye-level visibility of blue \nspaces since it is closely related to spatial design rather than planning and is currently receiving growing attention. Accordingly, the intermediate\/individual-level methods are selected, \nas the regional scale methods are closely related to landscape planning. Last, the methods \nneed to be integrated into design iterations, allowing quick changes to represent and test designers’ new ideas. Thus, the scenario-based methods indicated in the last column of Table 1 will  be  chosen.  Based  on  the  above  criteria  and  the  novelty  of  the  methods,  (3D)  Isovist analysis  and  segmentation  analysis  are  chosen  in  this  research  for  investigation,  and  their \ndesign possibilities are examined. \n\nStudy Area and Materials \nTo show the application of the two selected methods, a part of the Rotte River in Rotterdam \nis used as the study site. There are three reasons for choosing the Rotte River as the \ncase. First, the Rote River is located in Rotterdam, the second largest city in the Netherlands, \nwhich is rich in blue space resources. Rotterdam’s urban living is closely related to the water, \nand blue visibility plays an important role in the spatial design of the urban environments. \nSecond, the Rotte River used to be a major transport artery of the city in the past, and industrial products and vegetables were taken to the markets and auctions via it. Nowadays, the \nRotte River has been transformed into a public space which is closely related to the daily life \nof the public and provides multiple benefits. Last, as an essential urban river in Rotterdam, \ndata availability and physical accessibility (i. e. field survey) helped to evaluate and refine \nthe results of method applications. \n\nAs mentioned above, two methods for analyzing blue visibility are used to show their appli-\ncations and explore their design potential. Considering that each method has its unique char-\nacteristics, Table 2 lists the details of the tools and data required for the two methods. \n\nResults \nThree situations are presented to illustrate the application of the two methods and the design \npotential of their analysis results. Specifically, route-based visibility analysis adopts the 3D \nIsovist analysis method to calculate the visibility of water bodies under people’s movements. \nBuilding-based visibility analysis also adopts the 3D Isovist analysis method, taking the building as the analysis object to calculate the visible proportion of blue space in different areas of \nthe building surface. Moreover, the segmentation analysis method is incorporated into scene-\nbased visibility analysis to obtain the visual features of typical scenes in selected area. \n\nSituation 1: Route-based Visibility Analysis \n3D Isovist analysis is used in route-based visibility analysis via Rhino-Grasshopper environ-\nment to calculate the water visibility of people alongside the specific route (Fig. 2). The process consists of the following steps: (a) divide the selected route into several parts; (b) generate original view sphere based on horizontal and vertical FOV (field of view); (c) construct \nand compute the Isovist Rays by setting the obstacles and analyzing radius; (d) calculate the \nproportion of Isovist Rays contacting the water surfaces for each part of the route; € visualize \nthe  analyzing  results  into  chart  diagrams.  Three  transportation  modes,  including  walking, \njogging, and cycling, and two directions, including North-South and South-North, are conducted in the calculation, and detailed analysis parameters are shown in Table 3.  On the other hand, during the movements on the selected route, the overall patterns of blue visibility among the three transportation modes are similar, especially for jogging and cycling. This could be due to the lack of clear route classification and design between the different transportation modes in the study area. Accordingly, the analysis can be applied to the planning and design of different routes by simulating the people’s blue visibility of different traffic behaviours, such as the blue visibility of cycling routes at intersections could be lower to prevent distraction. \n\nSituation 2: Building-based Visibility Analysis \nBuilding-based  visibility  analysis  also  conducts  3D  Isovist  analysis  to  calculate  the  water \nvisibility of building surfaces located near the Rotte River (Fig. 4). The analysis process consists of the following steps: (a) divide the building surfaces into the small matrix; (b) set the \ncenter point of each small cell as the input for 3D Isovist analysis; (c) conduct the 3D Isovist \nanalysis as mentioned above; (d) calculate the proportion of visible water bodies in each cell \nof building surfaces; € visualize the results on buildings in an interactive way; (f) calculate \nmean blue visibility of each building and visualize it on map (optional). Detailed parameters \nfor analysis are shown in Table 4. \n\nThe left graph of Figure 4 presents the analysis results directly on building surfaces, where \nthe surfaces with blue-side colours indicate higher blue visibility. In comparison, the surface \nwith red-side colours shows lower blue visibility. Even for the same building, the outcome \ninteractively  represents  the  varying  degrees  of  blue  visibility  at  different  points.  The blue \nvisibility of two adjacent rooms can be varied, which may lead to completely different con-\nsequences, as ULRICH’s (1984) famous experiment demonstrated that a ward with a natural \nview could have a positive influence on the recovery of the patients living in it. The result \ncan help designers in architecture or vegetation design, in adjusting the layout of buildings \nor vegetation and repositioning the building windows or vegetation to increase blue visibility. \n\nOn the other hand, the analysis results could provide evidence for planning and policy-mak-\ning on the intermediate scale. Specifically, the study further calculated the average blue visibility of each building surface cell and visualized the results on the map. The results show \nthat the middle three buildings have higher blue visibility, which provides the potential for \nevaluating  the  differences  in  blue  visibility  among  several  buildings  (Fig.  4  [right]).  This \nresearch merely uses a few riverside buildings for testing. In the future, the analysis can be \nextended to the buildings within a specific area to support the planning of neighbourhood \nspatial layout and strategies for pricing housing units. \n\nSituation 3: Scene-based Visibility Analysis \nThe machine learning-based segmentation analysis method is used in scene-based visibility \nanalysis. Following the procedures in  HELBICH et al.’s (2019) research, the fully convolutional neural network for semantic segmentation (FCN-8s) model is trained by the ADE20K \nscene parsing and segmentation databases. A total of 208 photos taken based on the on-site \ninvestigation were used as input images for segmentation (Fig. 5). Next, the number of elements in scenes and the ratio of different element groups to total pixels are calculated. \n\nOn the other hand, the statistical analysis of the segmentation results can quantitatively describe the landscape element characteristics in scenes. The three graphs in the first row of \nFigure 7 demonstrate the distribution of the number of elements in each scene and the average \nproportion of area occupied by the main landscape element groups in all scenes. The number \nof elements in the current scenes is concentrated in 20-30, and the number of elements whose \nfield of view accounts for more than 5% is concentrated in 6-9. Vegetation, water, and sky \nare the main landscape element groups, while facilities only occupy a limited proportion. The \nthree graphs in the second row of Figure 7 compare the element characteristics of typical \nscenes among three sections of the river. From the results on the number of elements, the \nvalue in section 3 is higher than the other two sections, showing that it has relatively high \nvisual complexity. The value of section 1 is more concentrated and less distributed in large \nvalues, demonstrating that its visual complexity is stable and relatively easy to understand. \nHowever, the value of section 2 presents more scattered patterns than the others, indicating \nthat people’s visual perception changes in this section are more significant. In addition, according to the results on the proportion of main landscape element groups, it is obvious that \nthe proportion of vegetation in scenes of section 2 is less than in the other two sections, and \nthe proportion of buildings is higher. More buildings in scenes could also prove that the visual \nperception of this section is dynamic and varied. Accordingly, segmentation analysis could \nbe regarded as a powerful tool for designers to describe the landscape elements and the spatial-visual characteristics in design. Visual complexity, openness, naturalness and other indicators calculated by segmentation results can provide quantitative evidence for designers to \ncompare different design schemes or intentions and then make final decisions. \n\nDiscussion and Conclusion \n\nAs exemplified by the presented applications, three situations of using two methods not only \nhelp to get a grip on the different characteristics of blue visibility of Rotter River, but also \nexplore the possibilities in assisting multi-scale spatial design to improve it. In other words, \ntwo practical and design-oriented digital methods, including (3D) Isovist analysis and seg-\nmentation analysis, could facilitate a comprehensive understanding of the site’s restrictions, \nlimitations, and potentials on blue visibility, and provide clues for future design interventions. \nOn the other hand, these methods are complementary and can be combined together in some \ncircumstances.  For  instance,  segmentation  analysis  could  also  be  applied  in  situation  1  to \noffer an understanding of various landscape elements and their organizations during movements beyond the Isovist analysis only focusing on water bodies. \n\nThis  study  does  not  provide  the  complete  process  of  applying  the  two  methods  to  spatial \ndesign, but only shows the possibility of their design potential in a highly simplified form \nthrough three application situations. However, based on the ASE paradigm of design pro-\ncesses mentioned in section 1, it is evident that they could be parts of the design iterations, \nincluding analysis, synthesis (design), and evaluation (Figure 8). Here, designers could use \nthem to analyze and understand the existing situation, test and evaluate ideas or intentions, \nand compare different options to make final decisions. \n\nNowadays, with the advancement of technology, digital methods and tools are being widely \ninvestigated and introduced into spatial design practices, greatly expanding the toolbox of \ndesigners. The two methods presented in this study could serve as an inspiration to encourage \nexploration of the potential of multi-disciplinary methods to be incorporated into design processes. However, the traditional means (e. g. sketches or models) cannot be overlooked and \nshould be combined with the digital methods to achieve design objectives better and improve \nthe development of the knowledge\/evidence-based design approach. In addition, this study \nhas several limitations. First, this study cannot include all possible methods and only provides \nlimited applications to inspire future research. Second, the landscape is dynamic and greatly \naffected  by  time  or  season,  especially  for  water  bodies  and  vegetation.  People  may  have \ngreater blue visibility in winter since the leaves are gone. Last, the data acquisition or precisions, quantity of site photos, processing power or time, etc, may influence the results. Future \nresearch  can  optimize  these  factors  to  improve  the  effectiveness  and  practicability  of  the \nmethods. \n

A Mixed Reality Experience: Advancing Design \nDecision-Making with Performance Metrics \nThrough Augmented Reality and Physical Media \n\nAbstract: Augmented reality and virtual reality have effectively served as an immersive environment overlayed within the real world. These experiences are, however, often static and limited in function, primarily, as a full-scale walk-through. This study tests the capabilities of including real-time interactivity and engagement as a tool for dynamic design decision-making. The idea is further explored by integrating landscape performance metrics and project goals to determine how this supplemental data may influence the participants design decision-making. The synchronization of qualitative and quantitative information through a mixed reality experience has major implications to design development as the stakeholder groups have the opportunity of experiencing real-time responses to their engagement of a design project. \n\nIntroduction \n\nThrough the emergence of landscape performance and the integration of quantitative metrics \ninto  outdoor  spaces,  technology  and  innovate  methods  can  begin  to  communicate  naturebased benefits as tangible outcomes to comprehend the complex ecological, social, and economic relationships of our complex environments (BECK 2015). Due to the fact that many of \nthese ecosystem services are intangible and abstract, new methods must be explored to effectively  communicate  these  invisible  landscape  performance  outcomes  into  a  perceptual \nrealm for a comprehensive understanding of design decisions (ZHANG et al. 2021). The viability of this process can be explored at both full and reduced scales as a real-time feedback \nmodel using a hybridization of augmented reality and physical media; catalysed as a mixed \nreality (MR) experienced in the p[AR]k.  \n\nThrough this comprehensive qualitative and quantitative mixed reality process, divergent decision-making, predicated on the information presented in an augmented reality (AR) interface, can be made for multiple responsive and sensible outcomes (LAHAIE 2016). This ultimately generates an immersive, engaging, and interactive design process for a more universal \naudience to participate in for specific needs from a landscape architecture project, shown in \nFigure 1. \n\nThe modeling of dynamic landscape benefits within a mixed reality experience of both physical demonstration pieces and augmented reality interfaces creates an accessible means to \nparticipate in the design development of any project. Augmented reality is not only gaining \ntraction as an innovative representation tool but with the integration of parametric modeling \nand performance metrics it can also serve as a decision-making tool (DUENSER et al. 2008) \nto the design process for specific goals and outcomes. With the incorporation of performance \nmetrics into the augmented interface, data becomes responsive to real-time change, performance parameters, and user decision-making.  \n\nA Mixed Reality Experience and Workflow \n\nThe p[AR]k attempts to model these dynamic landscape benefits within a mixed reality experience of both physical demonstration pieces and augmented reality interfaces to reach a \nuniversal audience within a dedicated workstation comprised of a physical sandbox and com-\nputational hardware. Augmented reality sandboxes are becoming a common tool to understand topography; however, they are often limited to specific outcomes visualized as coloured \nheightfields projected on sand media. Although it is beneficial to understand these funda-\nmentals, the computational rigor of these models can be advanced further using parametric \nsoftware  to  measure  additional  terrain  characteristics  and  ecosystem  services  that  include stormwater management, carbon sequestration, and energy savings, shown in Figure 2.  \n\nProviding visualizations, interactive properties, and tangible media in a mixed reality experience gives the user ownership and agency behind their decisions for outdoor spaces. Professionals,  community  members,  and  stakeholder  groups  can  come  together  and  learn  the \nimpact of their decisions as their actions are stored in a database for further analysis of communal interests.  \n\nSandbox Station Capacity \n\nThe feedback collected from the point cloud of the sandscape is processed through the parametric modeling software to analyse and generate visuals with charted information based on \nvarious  parameters.  In  addition  to  sand  media  serving  as  the  terrain  model,  mobile  smart \ndevices, oculus quest controllers, or aruco markers can reference and embed various amenities  that  include  trees,  benches,  water  features,  paths  and  other  elements  onto  the  terrain \nmodel for a conceptual landscape performance rendition. There are several benefits from this \nprocess that include workflow and design development, design-thinking, scenario modeling, \ntrade-offs assessments, landscape performance, and co-creation and collaboration.  \n\nIn this study, users were put into a hybrid mixed reality experience of physical and augmented \nreality to address design opportunities within the scope of landscape performance. They were \nintroduced to environment, social, and economic scenario objectives that could be mitigated \nthrough the manipulation of a physical sandscape and augmented placement of trees within \na hypothetical neighbourhood pop-up park. With this experience, users manipulated the sand \ntopography and moved pieces around, perceiving the impacts different designs had in achieving stormwater management, carbon dioxide (CO2) sequestration, and energy savings. Simultaneously, data readouts from the topography, amenities, and surrounding neighbourhood \ncontext communicated quantitative data on the design iterations to assess the trade-offs and \nperformance of different scenario models related to ecosystem services. With each change or \naddition to this AR model, the data readouts instantly updated to inform next moves within \nthe design decision-making process. \n\nThe metrics and formulas for these parametric performance models were configured from \nmetrics and calculators commonly used by allied professions that include iTree’s tree benefit \ncalculator, NRCS stormwater calculator, and the US Department of Energy typical household \nenergy consumption. The iTree benefit values used in the different performance scenarios \nwere based on a thirty-year-old healthy honey mesquite tree.  \n\nPerformance Objectives \n\nWith the influx of real-time quantitative data that updates during this process, there is a profound opportunity to fundamentally shift design thinking and intent from these augmented \noutcomes. By embedding measurables and metrics to this workflow, this new design process \nand methodology of a MR experience can potentially emerge that enables the respective parties to generate robust design strategies for evaluation on specific goals and objectives. As \npart  of  this  performative  MR  experience,  the  interface  can  be  configured  to  display  data readouts communicating quantitative information throughout the design process to assess the \ntrade-offs of different scenarios as a divergent process (CIRULIS & BRIGMANIS 2013).  \n\nThere are many different AR programs available for users to immerse themselves within a \ndesigned space, however, the values most programs don’t provide is the ability to synchronize it to an industry standard modeling software such as 3D Rhinoceros to further refine a \ndesign concept. This is made possible with the plugin additions of Grasshopper (parametric \nmodeling) and Fologram (AR platform) to create a real-time feedback between the perceptual \nAR interface and the cognitive modeling software. Grasshopper manages and integrates the \ntangible and intangible analytics for performative landscapes while establishing a dialogue \nwith the Fologram application on a smart device. Fologram is then used to engage with the \ndesign  model  using  either  finger  gestures  on  a  touch  screen  or  by  scanning  printed  aruco \n\n\nmarkers, referencing different design elements such as trees, pavers, or benches. The programs in tandem create a responsive workflow of reciprocating outcomes based on the decision-making process of the user. The perceptual experience created can be viewed simultaneously on the computer and in the physical world, blending the two in a hybridized environment.  \n\nWithin the p[AR]k project experience, users were evaluated on two different scenarios to \ndetermine if performance objective and metrics impacted their decision making. In the first \nscenario, users were only required to manipulate the p[AR]k space through a perceptual lense \nof  only  seeing  the  site  as  qualitative  and  figural  with  landforms,  water  features,  and  tree \nplantings. In the second scenario the users were provided different landscape performance \nobjectives that integrated and displayed quantified metrics to determine if and how this may \nimpact their design decision-making any differently. Performance scenarios included managing stormwater runoff, sequestering CO2, and reducing energy consumption from buildings.  \n\nPerformance Methods \n\nFor the stormwater runoff scenario, the surrounding impervious conditions of streets, sidewalks and buildings coupled with the park’s landforms contributed to a calculated stormwater \nrunoff volume from a typical 0.5” rain event. This volume served as a baseline objective to \nmanage through a catchment system (landscape depression) and trees. The runoff from those \nlandform conditions were conveyed through drainage lines, suggesting the most optimal location to place trees to intercept the runoff. In the carbon sequestration scenario, the EPA’s \nmetric for an average typical passenger vehicle emitting about 4.6 metric tons of carbon dioxide per year, or 10,141 pounds of CO2, was used to establish a site measurement for the \nsurrounding neighbourhood context. And according to the U.S. Department of Transportation, the average number of cars owned per household is about 1.88. Based on these statistics, \nthe surrounding residential households contributed a total of 247,853 pounds of carbon emissions annually. Lastly, the energy savings scenario used the U.S. Energy Information Administration’s average household consumption metric of 10,715 kilowatt hours annually. \n\nUsers  were  first  expected  to  manipulate  the  topography  to  create  dynamic  landforms  and \nwater features within the neighbourhood pop-up park, see Figure 3. This would ultimately \nimpact  stormwater runoff potential  while  simultaneously creating drainage  and  catchment \nsystems in the landscape. The participant quickly realized that either one large or multiple \nintermediate  catchment  systems  within  the  topography  could  not  contain  the  site’s  runoff alone and would also require the strategic placement of trees to help with the mitigation process through interception.  \n\nTrees can intercept runoff through both their tree canopy and root structure. For the modeling \nand  calculation process,  the tree’s  ability  to  intercept  runoff from  their root  structure was \nused and would be assessed on its performance based on its proximity to the generated drain-\nage lines in the topography. As the user realized this in their tree placement, they began to \nstrategically place trees within close proximity to the visualized drainage lines in order to \nmaximize their interception potential leading to a higher percentage of managed runoff. \n\nThe design of the neighbourhood pop-up park for CO2 sequestration was more ambiguous \nleading to less consistent tree placement strategy since this is a non-point source pollution. \nIn these types of situations, it was found during the study that when there are unclear demar-\ncations for tree placement users would often place trees with less reason or logic into the \nlandscape, shown in Figure 4. This did, however, lead to users manipulating the sandscape \ntopography to have less area dedicated to catchment basins so that more trees could be placed. \n\nTree placement within the energy savings scenario had similar results to the stormwater management one in that as users realized the relationship of a tree’s location to a building would \nreduce its energy use, their strategic planting gravitated towards those specific parameters. \nWith the energy savings, tree distance and orientation to the building would result in different \nkilowatt hours saved. This can be seen in the tree and building values in Figure 5. \nFor example, a tree in near proximity to the north facing façade of a building would be less \nbeneficial to a marginally further distanced tree from the south facing façade since that sun \nfacing façade would be absorbing the most, requiring more tree shade protection to reduce \nthe building HVAC use.  \nThe combination of the responsive interface, the ability to co-create and engage with different \nstakeholder groups, and the real-time integration of data can fundamentally reroute the design \nthinking methods. Abstract and formal ideas can merge into one process, helping the user \nengage, innovate, increase options, and see the implications of those decisions simultane-\nously with their collaborators as a cyclical feedback loop. \n\nResults \n\nAfter users participated in this study through the p[AR]k, they were provided a survey to \nassess if and how the integration of landscape performance metrics (quantitative information) \nimpacted  their  design  decision-making  (qualitative  information).  Out  of  this  beta  testing \ngroup of roughly thirty undergraduate landscape architecture students, one hundred percent \nagreed that the inclusion of landscape performance values impacted their design decision-\nmaking, as asked in the first survey question. This broad overview of landscape performance \nmetrics were followed-up with more specific performance questions to determine which ones \ndid  or  did  not  have  a  significant  impact  on  their  design  decision-making,  provided  in  the \nbelow figures.  \n\nDesign Decision-Making Outcomes \n\nFollowing  the  first  survey  question,  the  first  landscape  performance  question  in  Figure  6 asked the user to rate how significant the inclusion of performance metrics was to their design \ndecision-making.  A  scale  of  1  to  5  (x-axis)  was  used  where  1  meant  no  significance  and 5 meant major significance. The number of results (y-axis) shows the majority found it within \nthe high significance range. \n\nThe next performance question in Figure 7 asked which landscape performance scenario was \nof value to the user. The scenarios were specifically diversified to reflect environmental, social, and economic values. The participant could select multiple options so many users found \nmultiple performance scenarios to be important to them. \n\nThe last performance question in Figure 8 asks for the significance in designing towards a \nspecific goal. This question also used a scale ranking of 1 to 5 was used where 1 meant no \nsignificance and 5 meant major significance. This clearly indicated that establishing performance goals for individuals drove their design decision-making.  \n\nConclusion and Outlook \n\nThis  mixed  reality  provides  an  advanced  learning  experience  for  the  user  to  engage  with innovative technology, create robust analytical modeling and assessments, and create a participatory decision-making experience to advocate for healthy sustainable cities (WANG et al. \n2013). The hybridization between malleable model making with augmented data reveals a \nsymbiotic connection of instrumental tools in design thinking. It correlates with many STEM \nrelated principles of evidenced-based strategies where design can act as a strategic response \nto problematic issues of flooding, public health, and equitable resources within the built environment.  \n\nThis mixed reality design experience has tremendous potential to increase the capacity for \nusers, designers, and the community to work together and advocate for healthy living environments, make design more efficient by linking tools that help speed up the process, and \nhelps  everyone  involved  see  the  benefits  of  proposed  designs  (LIU & WANG  2019).  Augmented Reality has shown to serve as an innovative and versatile tool to visually communicate information as part of an analytical and design narrative for more informed design decision-making. This MR experience and workflow can further separate itself from other methods in the immersion and perception of space as it overlays qualitative and quantitative information impacting the composition and performance of a design concept. Rarely is it possible to experience the data that binds ecologies together. AR still contains limitations to this \nprogressive design approach, however, it can begin to give a glimpse of these interactions \nand  relationships  between  systems  in  real-time  as  designers  augment  space  into  a  mixed-reality. \n

A Parametric Design Methodology for a Novel \nEcosystem \n\nAbstract: A literature recognizes the ecological value of biodiverse “novel ecosystems” that arise from an interplay of anthropogenic and independent ecosystem processes. Several factors influence the creation of these novel ecosystems and there is no shortage of data acquisition methods for, or complexity of descriptive data about them. However, there is a need to articulate and contextualize tools and meth-ods for formal design iteration that can make sense of this complexity and facilitate its translation into functional and meaningful landscape form. This paper describes a method for abstracting ecosystem processes into a simple parametric model, presented in the context of a riverbed mining operation in southern Georgia. The results of this initial methodology reveal the potential of parametric models to mediate between digital models of formal proposals and abstract models of ecosystem process. Separating and recombining parametric site models, abstract ecosystem models, and landscape photomontages reveals possibilities for a more facile methodology for iteration of design interventions. A parametric model also serves as a site for negotiation between project goals and constraints. Finally, the generation of a digital parametric site model allows for subsequent visualisation and editing of more specific digital models for design development and construction. \n\nIntroduction \n\nAggregate Mining Landscapes as “Novel Ecosystems” \n\nAggregate mining landscapes are widely understood as sites of chronic ecosystem degradation and destruction. However, there is a growing consensus that recognizes the ecological \nvalue of chronically degraded but “novel” ecosystems in supporting biodiversity and providing ecosystem services. A “novel ecosystem” is understood here broadly as a “new species \ncombination that arises spontaneously and irreversibly in response to anthropogenic land-use \nchanges” (MURCIA 2011). While significant debate surrounds the “irreversibility” of these \nchanges, the “restoration” of sites with a high degree of anthropogenic disturbance can improve biodiversity and ecosystem services. Rather than allow descriptions of “novel ecosystems” to simplify the project of ecological restoration, designers’ engagement with these sites \nshould focus on relationships between anthropogenic disturbance, ecosystem services, and \ncomplexity (MURCIA 2011). \n\nRestoration research in riverbed aggregate mining contexts suggests that disturbed mining \nsites provide new ecological niches that support overall site biodiversity, and that restoration \nefforts could benefit from a focus on “near-natural” restoration rather than more resource-\nintensive planting approaches (ŘEHOUNKOVÁ 2011). Similar approaches advocate for designing ecological processes and functions to allow for plants to “develop a measure of control \n[themselves]” (DEL TREDICI 2007) or “intervene and leave room” (GROSSE-BACHLE 2005). \n\nThis paper expands and applies this approach to ecosystem restoration to explore the role of \ngeodiversity in parametric modeling and redesign of aggregate mining operations. An active \nopen riverbed mining site in the south of Georgia provides the case study for the project, \npresenting opportunities to explore a design methodology that attempts to engage and alter \nan existing process of anthropogenic geomorphological change. Restoration is here under-\nstood as amplification and diversification of remaining landscape qualities. \n\nFrom Geodiversity to Geomorphological Diversity \n\nWhere geodiversity is a result of a broader collection of factors including mineral, paleontological, and structural\/ tectonic factors, geomorphological diversity describes that subset of \nfactors encompassed by surface processes. While variations in surface processes like differential weathering lead to increased geomorphic diversity, these same processes can also lead \nto reduced geomorphic diversity (THOMAS 2011). In the shifting landscape of an active aggregate mining operation, geodiversity can be understood not only as geo-determinism but \nalso as an effect of the existence of multiple states of succession within a given ecosystem. \nAltering the mining process can lead to coordination and amplification of a wider range of \necosystem successional states.  \n\nParametric Modeling and Scenario-based Design \n\nAshari et al. note that “Key academics and practitioners of landscape architecture are implementing parametric design as part of their design process in generating various design scenarios” (ASHARI et al. 2022). The goal of the methodology is incorporating the context of the \ndesign  work  into  a  formal  parametric  model  from  which  to  generate  proposed  scenarios. \nThus, the project is an attempt to situate parametric modeling within a broader definition of \ndesign scenarios (SHEARER 2022). The method should allow the model to facilitate a formal \ndialectic between given and proposed formal scenarios that is open to interpretation by designers. Additional parameters can facilitate landscape designers, planners, quarry operators, \nscientists documenting biodiversity, and the interests of laypersons living near and visiting \nthe site. The methodology elaborated in this article proposes modifications to a traditional \nAnalysis\/Concept\/Design\/Evaluation workflow. The aim was to use parametric modeling to \nencode simple formal models of observed ecological processes and begin to visualize formal \ninterventions in these processes. \n\nMethodology + Case Study \n\nSite-based Research + Analysis \n\nInterview \n\nThe design team kicked off the project with a semi-formal site visit and workshop to identify \nformal parameters and key system concepts of the mining operation. We invited architecture \nstudents from a local university to work with the design team to interview on-site excavators, \noperations managers, and ecologists.  \n\nThe interviews revealed that excavators control the shifting landscape by creating dams, which allow them to direct the river and its powerful floods. \n\nAccording to the on-site operations manager, dams are built for a variety of reasons, including protecting machinery from periodic flooding, reducing sediment flow, and allowing \nfor truck access from extraction points to processing sites. After a certain area is excavated, \ndams are eventually removed to allow larger floods to “replenish excavated material”, a \nprocess which can take as few as five years.  \n\nAfter collecting the qualitative description of the mining process and ecological diversity, the \nteam compiled satellite imagery, reviewed drone footage, and composed diagrams describing \nthe mining process and its extent. While the drone footage from the February visit shows a \nsparse winter landscape, satellite imagery shows a river in constant flux, coming to life each \nspring as a new layer of vegetation spreads over the bed of silt and rocks freshly deposited \nby winter flooding. The imagery serves as a partial record of the last decade and reveals the \nimpact of mining activities on the riverbed over time. Aggregate mining disturbs the riverbed \non a timeline of weeks to months, but this impact is superseded by spring floods that occur \nyearly. \n\nIn response to these conditions, we hypothesized that the mining and dam-building processes could be harnessed to increase the geomorphological diversity of the mining site. Processes of excavation, dam building, and periodic flooding have the potential to create a \nwide range of microhabitats that support increased species biodiversity. \n\nDefinition of Ecosystem \n\nGrasshopper Definition of Formal Ecosystem Drivers \n\nA secondary step of analysis involved the translation of the narrative context of the site into \nformal parameters that respond to existing ecological conditions and planned works on the \nsite. To create a tool for quick formal iteration with respect to site-scale decisions (“Planning \nDecisions” in Fig. 1), we needed to identify and abstract the appropriate metrics and formal \nparameters of the mining operation and larger landscape ecological dynamics and forms. \n\nIdentifying Macro- and Micro-ecosystems \n\nThe abstraction of landscape ecological features was accomplished by comparing areas of \nsimilar  vegetation  cover  to  the  topographical  surface  to  define  boundary  conditions  on  a \nbroad site-scale. As observed on site and through sectional analysis in Google Earth, the edge \nof the “Intact Bosque” follows the scoured edge of the riverbed, providing a sharp boundary \nat the site scale. A lighter shade of vegetation composes an intermediary edge zone that has \nallowed for the development of an early successional bosque, which exists at 1-2m above the \napparent limit of flooding. The floodplain is defined by a zone of high-frequency scouring \nevidenced by the limited presence of shrubs. Scattered across the floodplain are former borrow pits, which host littoral micro-ecosystems similar to the early successional bosque. \n\nBy  applying  Richard  Forman’s  principles  of  landscape  ecology,  the  early  successional \nbosque can be understood as the cellular or nuclear membrane of the intact bosque, facilitating the migration and succession of species (DRAMSTAD et al. 1996). A key assumption for \nthe model is the role of the intact bosque as a nucleus for expansion into the shifting floodplain and active mine zones (CORBIN et al. 2016). This allowed us to argue for the conservation of the bosque by shifting tipping grounds from the floor of the intact bosque to the more \nfrequently  disturbed  early  successional  bosque.  The  tipping  piles  currently  interspersed \nthroughout the intact bosque should be redistributed as a way of amplifying the seed bank of \nthe more frequently disturbed landscape.  \n\nConcept Development \n\nThe goal of the concept development phase was to create a simple iterative model of masterplan forms. At the site scale, the ecological system is represented as a “boundary model” \n(HILL 2005). The boundaries represented in the model (the edges of the intact forest and the \nedge of the main channel of the river) define the limits of the proposed berm system. These \nedges are subdivided to generate start and end points for the berm and distances between all \npossible start and end points are calculated. Each of these possible berms is then recalculated \nas a set of bi-arcs, or curves composed of two simple arcs. The definition incorporates In-\nCurve interior culling to exclude iterations that exceed the limits of the project.  \n\nFollowing initial iteration, a selected curve or set of curves is then used as a basis for the \ngeneration  of  a  surface  that  can  be  intersected  and  compared  with  models  of  the  existing \nsurface. In this project, the lack of a high-definition surface model forced us to rely on abstracted orthoimagery to infer elevation information. In future iterations of the methodology, \na context model with greater definition can add greater analytical definition to this stage. \n\nThe vertical difference in the model is developed relative to a “system zero” that can shift to \nany average or given waterline elevation. This allowed us to reduce the complexity of varying \nwater levels while also retaining the ability to incorporate more complex water elevation data. \n\nDesign Development \n\nThe iterative outcomes of the concept development model form the base of a further-refined \n“design development” model. The development model is broken into smaller models of individual zones to generate a greater diversity of micro-ecosystems. \n\nManual and Parametric Cutting Operations \n\nBerms can be cut with individual curve parameters or sets at specific intervals to introduce \nfurther porosity in the system. Varying the elevation of these cuts can generate check-dams \nor culverts with a range of crestline elevations. \n\nDistribution of Varying Nutrient Levels and Soil Types \n\nParameters  for  specific  soil  types  can  be displayed by  developing  a key-color  legend and \ncustom material previews. \n\nIncorporation of Environmental Simulation on Smaller Sites \n\nThe generation of a physical model allows for a range of existing Grasshopper plugins to \nanalyze  environmental  factors  such  as  hydrological  saturation  (Groundhog)  and  temperature\/energy modeling (e. g. Ladybug, Honeybee). \n\nPhotomontage and Further Descriptive Visualisation \n\nPhotomontage was used not as a way of determining the fixed or final condition but as a \nmethod of representing combinations of potential project conditions to create an image open \nto interpretation (M’CLOSKEY 2014). The definition generates measured landscape forms that \ncan be montaged over existing landscape conditions and textures to produce projective photomontages (Fig. 5). Such hybrid images can be used to establish metric visual parameters \nthat facilitate systemic and systemic design decisions. \n\nDiscussion \n\nDeveloping Facile Definitions \n\nThere is a need to balance specificity and complexity with respect to digital data inputs and \noutcomes when generating formal iterations. Nested, simplified models allow design teams \nto move efficiently between decision-making scales without carrying unnecessary complexity to the model.  \n\nInitial investigations in the design development phase revealed that the use of environmental \nsimulation plugins such as Ladybug, Honeybee, and Groundhog (saturation) work best on \nsmaller sub-sites, where more subtle forces of surface geology become legible. Further case \nstudies are needed to elaborate possible applications to large-scale sites. \n\nDesigners working with parametric software must always take caution when integrating specific datasets. Beginning with an abstracted master plan model with simple geometry allows \nfor fast and loose overlays with existing context data (orthoimagery, topography, observed \nvegetation, etc.). \n\nRules-based Experimentation \n\nSome rules were fast and loose, aimed at experimentation and later refinement (i. e., each \nberm should be double-curved). Others were highly specific, (i. e., the length of the berm, \nthe radius of the berm enclosing an area). The definitional simplicity of the model facilitates \ngeometric iteration and pattern generation. \n\nRepresentation through Hybrid Drawings \n\nThe hybrid drawings produced through this workflow operate like traditional landscape rep-\nresentational method of “landscape overlays” and yet add another layer of specificity and \ncomplexity.  User-displayed  text  and  annotation  can  further  integrate  metric  and  technical \nparameters of the represented form. \n\nConclusion and Outlook \n\nThis project and paper focus on the outcomes of an initial draft of a parametric methodology \nfor engagement with novel ecosystems through geodiversity. The goal of the case study project was to bring the mining operation parameters into a model that can facilitate rapid visualisation of site-scale forms, followed by an initial investigation of smaller parameters. Creating a protocol for nested models with feedback was crucial to moving between scales and \ndiscerning between “hard” and “loose” formal design decisions. \n\nThe methodology allows the abstracted parametric model to become a site of physical and \ndiscursive negotiation and coordination between the various actors involved in the project. \nThe next step in refining this methodology is to test the utility of this parametric model facilitating interdisciplinary data and feedback in the design process. The case study used to develop this tool aimed to establish a more intentional role for formal decision-making by facilitating decisions that incorporate aesthetic and metric-based approaches. \n\nFurther research could unfold along two strategies for further defining the surface: a more \nspecific integration of soil types and the use of 3D scanning to incorporate temporal change \nof landscape form.  \n\nEventual applications of the methodology could inform design protocols for novel and ex-\npanded riparian ecosystems. The workflow can also be applied to smaller sites and the gen-\neration of formal iterations.  \n

Comparing Transportation Metrics to Measure\nAccessibility to Community Amenities \n\nAbstract: Landscape architects and urban planners play an important role in helping create inclusive, \naccessible communities. To do this, it is helpful to understand how the built environment and access to \nvarious amenities and places affect activities of daily community living. These activities can influence social engagement with others and satisfaction with one’s social life. Thus, understanding how the built environment  influences  their  choice  of  living  can  shed  light  on  the  ramifications  to  an  individual’s overall social satisfaction. As previous studies show, there are various methods for measuring accessibility. But to what extent are these metrics different or provide similar results? In the current study, we generate various geospatial models to measure distance, density, and accessibility. The metrics produced are then compared to identify how similar they are in measuring access to several different places. Results show that all metrics are statistically significant and similar, however, similarities range from poor  correlation  to  very  high  correlation.  The  most  consistent  can  then  be  used  in  future  studies  to identify how well they correlate to stated access, actual access, and the influence on social satisfaction. \n\nIntroduction \n\nA tenant of landscape architects and urban planners is to improve the quality of life in communities. An essential variable in this equation is improving the satisfaction of social life as \na result of community integration (SEEMAN 1996, YEN & SYME 1999) and development patterns.  To  accomplish  this,  landscape  architects,  policymakers,  and urban planners need  to \nknow how social and environmental factors impact community integration and, ultimately, \nsatisfaction with social life. Prior work in the discipline has shown that if we create improved \nintegration of people within their community, people experience a higher level of social satisfaction (CHRISTENSEN et al. 2010). \n\nSeveral factors affect the level of satisfaction with social life among people, including factors \ninfluenced by the surrounding neighborhood. For example, social factors such as place attachment (CAO et al. 2020, HUR & MORROW-JONES 2008) and neighborhood cohesion (Liu, \n2017), make higher rates of satisfaction (MITCHELL et al. 2013, OKTAY et al. 2021). Additionally, environmental factors including landscape and green space (BOTTICELLO et al. 2014, \nYOUSSOUFI et al. 2020), mixed land use (BEARD et al. 2009, CAO et al. 2020) destinations \nand urban amenities (ALLEN 2015, SATARIANO et al. 2010), and street connectivity (GÓMEZ \net al. 2010) also play a role in facilitating social satisfaction. However, there is no standardized rule used to calculate how built environment factors and the proximity to urban amenities are associated with the level of social satisfaction. Further, there are limited studies that \nsystematically compare accessibility measures (KAPATSILA et al. 2023). Not only can it be \nimportant to compare the results from a range of accessibility metrics, but equally important \nis  the  level  of  technical  difficulty  required  to  accomplish  each.  In  this  paper,  we  explore \naccessibility to common community places in which individuals take part in a range of different  activities  (JONES  1981).  These  places  include  areas  of  outdoor  recreation,  grocery \nstores, retail stores, restaurants, and others. Throughout this paper, we refer to these places \nas “place types”. Since accurate metrics are necessary for effective policy development and \nimplementation, we are keen to ask: to what extent do different accessibility metrics agree \nwith each other”? Specifically, how different are geospatial techniques for measuring and \nquantifying access to place types by neighborhood?  \n\nThere is a diverse knowledge and broad understanding of the linkages between the level of \nsatisfaction among individuals and environmental factors including neighborhood and place \ntypes (SAMMER et al. 2012, WHITE & SUMMERS 2017). There is, however, uncertainty about \nhow  accessibility  to  each  place  type  affects  social  engagement.  Given  the  potential  importance of these places to facilitate social satisfaction, it will be helpful to identify a robust \nspatial metric that could explain the impact of place types on the level of satisfaction with \nsocial life. We could further use the spatial models to assess and promote policies that integrate people into their surrounding environments and communities. Understanding the relationship between place types and the level of satisfaction can inform how landscape architects \nand planners design inclusive communities. However, before we can understand this relationship, it is also important to understand how we assess accessibility. \n\nOne of the more basic measures of accessibility is to determine the degree of access from one \nlocation to another. Certainly, there are differences in the kinds of reliance on different modes \nof transportation across demographics (PARK et al. 2022). There are several ways for measuring accessibility. However, this paper is not focused on attempting to identify the range of \nmulti-modal transportation accessibility techniques. Instead, we focus on understanding the \nconsistency of different measures of accessibility, mainly because some models may be biased (GIANNOTTI et al. 2022). In this paper, we identify the degree to which six common \nspatial accessibility models are related. We generate a systematic comparison of these models \nby measuring the spatial pattern and accessibility to several place types. By identifying similarities, we can determine trade-offs between model complexity and consistency of the metrics. The easiest and most consistent models can then be used in future studies where empirical data about demographics, disability status, and travel behaviors can be used to identify \nthe relationship between access to place types and social satisfaction. \n\nMethods  \n\nDetermining accessibility is a rather complex process. Transportation-related studies provide \nseveral  means  to  produce  an  accessibility  measure,  with  the  most  precise  being  produced \nusing empirical data collected about individual travel patterns. However, these data can be \nparticularly expensive to obtain and may be logistically prohibitive to obtain in some international contexts. In this study, we pursued comparing six different geospatial models that use aggregate data at the US Census block group level (heretofore: “block group”). To conduct our analyses, we used the Safegraph Point of Interest data points (POI) and aggregated \nthese data into eight categories. Each category is then referred to as a place type. Six metrics \nwere defined to represent the spatial pattern of place types. including, accessibility to different destinations (CAO et al. 2020), proximity to various destinations (TSEMBERIS et al. 2003), \nthe density of place types (YANG 2008), and spatial models (GIANNOTTI et al. 2022, LUO & \nQI  2009).  More  specifically  our  metrics  are:  1)  Average  proximity  to  place  types,  2)  frequency of place types (count of place types within the block group), 3) Density of place types \n(using kernel density), 4) The number of block groups (in USA, referred to as block groups) \nwithin the service area of place types, 5) the gravity model, and 6) two-step floating catchment  area  (2SFCA).  Each  of  these  metrics  measures  accessibility  to  place  types,  but  it  is \nimportant to note that the level of technical difficulty varies widely. Furthermore, variables \nused to calculate each metric can differ. For instance, 2SFCA and gravity model considers \npopulation,  while  frequency  metrics  and  the  kernel  density  focus  on  the  number  of  place \ntypes in a unit (e. g. block group). As a study area, we analyzed data from across the Greater \nSalt Lake City, UT region in the USA. Some items, including accessibility and distribution \nof place types, were selected from neighborhood satisfaction scales developed by (GUTTING \net al. 2021, OKTAY et al. 2021). This technique provides a simple proxy for determining the \nextent  of  access  to  place  types  KWEON  et  al.  (2010)  produced  similar  measures  based  on \ndifferent place types by limiting the distance of an accessible amenity to a respondent’s home. \nConducting a correlation between the metrics results can provide insight into the similar results that we can get. \n\nTwo density measures were produced, frequency of place types (count of urban place types \nwithin the block groups) and density of place types (using kernel density), Metric #1 and #2, \nrespectively. Metric #1 counts the total number of the selected place types intersecting with \na block group, using spatial selection. Metric #2 uses kernel density to calculate the density \nof features and place types in a block group (YANG 2008). Kernel density was performed by \nthe planar option, which is appropriate for the analysis on the local scale.  \n\nFor average proximity to place types (Metric #3), we generated a set of origin locations that \nwould provide a meaningful context for trips from within each block group. Every intersection of the road network within each block group became an origin node. Intersections within \nthe block groups balance the trade-off between identifying every building from which people \ntravel to and from, but also provide more precision than the centroid of the block group. We \ncalculated the median distance of travel using Safegraph datapoints and set as a travel distance to get to different destinations from the intersection nodes. Driving distance was used \nbecause there is limited information about alternative use of public transit. Then, the average \nproximity from all origin nodes to all place types within the travel distance was calculated. \nFigure 1 shows the average proximity, by block groups, from origins to each place type.  \n\nThe number of block groups within the service area of place types (Metric #4) flips the density calculation by starting from the place instead of the block group. To produce this metric, \na service area was generated for each place type using a network analysis. The route distance \nwas calculated based on the median distance that individuals reported as the travel distance \nto  get  to  different  destinations  (similar  to  Metric  #1).  Then  the  number  of  place  types  by \nservice area within each block group was calculated by spatial joining the service area with \nany intersecting block groups.  \n\nThen, the more established 2SFCA metric (Metric #5) was used for measuring accessibility \nto each place type. 2SFCA defines a catchment area around each location and computes the \nsupply-to-demand ratio for the catchment area. The boundary of the catchment area can be \ndelineated with the radius, travel distance, and travel time (LUO & QI 2009).  \n\nThe other commonly used transportation metric we employed was the gravity model (Metric \n6). The gravity model has been used to model human mobility and accessibility. The model \nconsiders the distance between two nodes and the population within the place type’s service \n\nResults  \n\nResults of this systematic comparison are provided first as a correlation matrix (Table 1), \nthen as a series of visual representations below. To demonstrate the statistical differences \nbetween  each  metric,  we  produced  correlation  tables  for  each  of  the  eight  different  place \ntypes, by each of the different Metrics (or geospatial models). Here we show a summary table \nof the average correlation values across all eight place types. The purpose of this table is to \nhighlight  (in)consistencies  between  metrics  and  to  identify  potential  sensitivities  of  these \nmodels  across  place  types.  Note,  Metric  #3  is  inversely  correlated  with  all  other  metrics. \nThus, these were this inverted to positive values so the average correlation would maintain \nits accuracy of strength, regardless of signing. Consistently high correlation values with a \nrelatively low standard deviation suggest that these Metrics are likely to produce similar results. This table highlights that Metrics #4, #5, and #6 maintain very high correlation and low \nvariance across all place types. Metric #2 also seems to be highly correlated with Metrics #4, \n#5, and #6.  \n\nTo depict these data visually, several maps were generated (Figures 1-6, for Metric #1 – #6, \nrespectively). Given the diversity of values for each of the different Metrics, we created Table \n2, which identifies the grouping of values for each of the different legends. Using these visuals one can see differences in the relative distribution of high to low access to various place \ntypes. For instance, Metric #1 shows a clear visual difference from the other Metrics (which \nis also clear in Table 1). Also, the overall consistency of Metric #2 with Metrics #3 – #6 is \nalso fairly apparent in these figures. Note, the groupings used in these figures were not the \nvalues used to conduct the correlations (correlations were not run by groups). Instead, correlations were conducted on the raw values produced from each metric for each of the block \ngroups. The figures are providing only a visual reference with the groupings of data produced \nusing Natural Jenks, these groupings were not used in the correlation analysis. The figures \nonly symbolize the results of the metrics for access to outdoor recreation. \n\nA summary description of these figures is provided here. Figures 1 and 2 illustrate the results \nof the count of place types within each block group (Metric #1) and the density of place types \nusing kernel density (Metric #2). The darker color indicates that a block group has a greater \nnumber and density of urban place types. Alternatively, as the color gets lighter, there are \nfewer place types and a lower density of place types. Figure 3 illustrates the results of the \naverage proximity to place types. The darker color shows a shorter distance to the outdoor \nrecreation  and  as  the  color  becomes  lighter,  it  shows  a  longer  distance  to  the  destination. \n\nDiscussion and Conclusion \n\nLandscape architects are regularly involved in community design and transportation planning. Finding models that provide reliable metrics and are easy to perform can help facilitate \nrapid  iterations  of  design  and  planning  recommendations.  In  this  study,  we  compared  six \nmetrics that measure accessibility to different place types and showed differences between \nthem. Results indicate a high correlation between metrics #5 of the 2-SFCA method, #6 of \nthe gravity model, #4 of the number of block groups within each place type service area, and \n#2 of the kernel density. Furthermore, we provide a stark warning about the dangers of using \na  single  geospatial  metric  –  especially  if  the  metric  needs  further  empirical  evaluation  to compare  its  reliability  and  effectiveness.  The  2FSCA  (Metric  #5)  and  gravity  (Metric  #6) \nmodels have been well published in the literature (KAPATSILA et al. 2023, LIU et al. 2022, \nLUO & QI 2009) but can be more complex to run than other metrics (Metric #2), though these \nare highly correlated. This comparison highlights the potential trade-off between model complexity and the outcomes. It will be important for future studies to ascertain the value of the \nmore complex models. For instance, correlations between models might be high, but do they \nmaintain the same level of consistency when other variables are included (e. g. demographics \nor disability status)? If reliability is maintained, then simpler methods should be used first, \nwith  the  more  complex  methods  becoming  necessary  only  if  there  is  a  good  empirically-sound reason. \n\nOur analysis has shown that some models of accessibility differ quite substantially. At the \nsame time, some of these models share a high statistical similarity. One of the challenges \nthese models provide is that they can serve to validate actual travel times and provide data to \ninform planning policy. However, there are limited studies that attempt to connect these models  to  social  satisfaction.  Our  results  are  aligned  with  other  studies  that  compared  simple cumulative opportunity measures and the measures produced by the gravity model to under-\nstand if there is a significant correlation or not between them. The results showed that cumulative  opportunity  measures  can  substitute  complex  measures  like  the  gravity  model \n(KAPATSILA et al. 2023). It can be argued that social satisfaction is an important indicator of \nthe quality of life, perhaps more so than just assessing how long it takes someone to get from \npoint A to B. Thus, to validate these models, a future study should study the statistical relationship between each model and how they relate to social satisfaction. Further, we also anticipate gathering empirical data on the time to travel and modes of travel for people living \nwith disabilities. This information can then be used to compare differences between the general public and those with disabilities – not only for functional access to place types but more \nimportantly for how the spatial relationship to these place types influences social satisfaction. \nThe study can contribute to a wide range of fields, including landscape architecture, urban \ndesign,  urban  planning,  and  transportation  planning.  Yet,  landscape  architects  do  play  an \nimportant role in helping design access to a range of different place types, particularly greenspace and open space. With this work, we have established the importance of testing different \nmodels  to  determine  community  access  to  place  types,  including  outdoor  recreation.  This \nwork provides a means to connect accessibility and the design of urban spaces, to create more \nsuitable and equitable access to different place types for all citizens. \n

Eroding Terrains: Developing Computational Design \nTools for Interactive Site Erosion \n\nAbstract: Landscape erosion processes can be problematic and are universal in their effect on all forms \nof landscape contexts and conditions. Hydrological erosion processes are important features of ecologies, yet are often extremely problematic, and can be exacerbated by climate extremes, weather events, \nanimal and human activities, and especially transformations through agricultural processes. This research documents and proposes computational design tools and methods for erosion simulation in real-world scenarios. While there are many examples of soil erosion modelling in the life sciences and engineering fields, they are rarely applied at the detailed scale of the landscape-, architecture- and design \ndisciplines. The work  attempts  to  leverage erosion  processes  for  design  by creating  new  workflows \ninside familiar design and modelling programs. Applications may vary between agricultural land and \nareas of accelerated climate change, however, the test case for this application is in a fire-affected landscape particularly prone to erosion. This research seeks to unite site investigation and survey techniques with interactive erosion modelling within AEC design software. By introducing intuitive ways to model erosion processes mitigation becomes possible within the landscape analysis and design process, creating opportunities to avoid erosion before it occurs. \n\nIntroduction \n\nErosion is a fundamental landscape process that underlies all landform generation. In combination with transport and sedimentation, it is integral to all landscape processes and their \ninhabited ecologies (KONDOLF 1994). Despite many advancements in the various ways in \nwhich humans have formed and manipulated the earth, erosion remains process we still struggle to work with. How we counter the degenerative effects of erosion have barely changed \nover  the  last  century  (BATES  &  ZEASMAN  1930).  This  research  is  particularly  focused  on \nhydrological erosion as one of the key forms of erosion affecting landscapes worldwide at an \naccelerating rate due to climate change and land-use practices. The perceived demand for \nsuch techniques comes from both an observed lack of such analyses executed in the AEC \nindustries,  and  the direct demand for  such  methods  from  within  parallel  research projects \n(MELSOM 2022).  The  parallel  research  projects  inform  this  case  study  and  initial  practice \nmodel for this technique, namely the specific and heightened erosion issues faced by post-fire landscapes, although the techniques are equally applicable to a wide range of other landscape and built environment circumstances, such as disused agricultural and cleared landscape plots, de-vegetated drought-affected areas, and building construction sites. The work \ndocuments the current progress in developing specific tools for common erosion models at a \nlocal site scale, leveraging high-resolution user-generated site models. \n\nRecent shifts in weather patterns, storm event frequency and magnitude, connected with on-going climate change exacerbates the loss of soils as a key societal issue that already dates back  centuries  (MONTGOMERY  2012).  It  commonly  results  in  the  loss  of  arable  land,  increased landscape disturbance, the destruction of ecological systems, as well as negative effects on the built environment. Erosion simulation is an answer in the field of civil engineering, providing insight to where and how it might occur. Large to medium scale modelling is widespread, and often leverage GIS or proprietary and specialised modelling software solutions (MAY et al. 2005, ARGENTIERO et al. 2021). Furthermore, there is also some scepticism in the relevance and accuracy of such simulations at territorial scales (MONDA et al. 2017). Nevertheless, these models tend to focus on the territorial or catchment scale, distinctly abstract from the detailed site scale and restricted to the realm of specialised engineering applications and computation intensive instrumentation (KANITO & FEYISSA 2021). \n\nAt the design scales there are clear and compelling examples of methods that propose to work \nwith avalanches and sedimentation events instead of against it. Here materials are redistributed as they erode or arrive on site (HURKXKENS 2021). The potential to combine detailed site data with predictive or preventative erosion modelling provides many compelling avenues for landscape management, generative possibilities, effective hybridisation of erosion, \nsedimentation, and design. \n\nIterative Erosion Modelling \n\nHydrological erosion types follow several generally established patterns and types, each often the precursor for the next: splash, sheet, rill, gully, bank and stream, ordered by increasing \nscale. Rill and gully erosion have been isolated as the most useful for this research, to generate a simplified simulation tool. Existing specialised applications in earth engineering have \nformed  both  the  basis  for  this  selection  and  a  model  for  confirmation  of  applicability \n(HANCOCK et al. 2008). As can be seen in such precedents, high-resolution site data is required \nin order to generate relevant results. Detailed, recent models are a necessary starting point, \nwith medium to high-resolution laser scanning or photogrammetry a base requirement. Due \nto the nature of rilling scale erosion sites, photogrammetry is particularly interesting as it excels \nin open ground, un-vegetated areas, allowing a consistent accuracy of 10cm down to 2cm. \nOpen agricultural fields, mining landscapes, and post-industrial sites are suitable examples \nof high-resolution photogrammetry subjects, or in the case of this research, post-fire affected \nsites, cleared of foliage and vegetation canopy. The site of Rosedale, NSW, Australia was \nchosen as an ideal candidate for such a fire-affected erosion modelling scenario (Figure 1). \n\nA simplified model closely mimics the established model (KANITO et al. 2021, HANCOCK et \nal. 2008) yet allows for a close to real-time feedback loop, and the integration of iterative \nworkflows. To this end, Rhinoceros was chosen as a base software, with the integration of \nscripting and plugin development to provide a seamless connection with three-dimensional \ndesign  software  that  is  both  intuitive  and  an  industry  standard.  Rather  than  analysing  the \nmodel  outside  the  design  software,  it  is  compatible  with  other  AEC  design  software  and \nmethods, without interrupting the design process. It is also compatible with GIS software and \nvarious spatial data types. This simplified process can integrate other landscaper factors such \nas soil characteristics, barriers, and vegetation to improve the accuracy of the simulation further. \n\nThe concept of iteration is also considered an important simulation criterion within the design \nspace, in this case, differentiated into two iterative models, integral and event iteration: \n\nIntegral  Iteration  describes  the  case  in  which  the  eroded  model  accounts  for  erosion  and \ndeposition  within  the  same  continuous  modelling  cycle,  with  eroded  areas  having  a  compound effect on their continued erosive processes, rather than acting on existing site characteristics alone, and allowing for changes to the site to be made during simulation.  \n\nEvent Iteration modelling allows for separate events, with a shift in intervening characteristics (vegetation, topography, physical intervention). This model allows for multiple hydrological events to take place separately, with shifts in the intervening period. This is an im-\nportant factor in many erosion-prone landscapes, as long-term erosion effects are often generated through multiple or repeated erosion events that are incremental, rather than occurring \nin one discrete event. \n\nTerrain Characteristics also play a key role in understanding the processes of erosion. Many \nfactors affect its course over the surface in landscape terrain models and have therefore been \nincluded in the erosion model, with varying levels of integration (Figure 2). \n\nSoil Texture is a key characteristic in erosion models. Different soil types erode and deposit \nat varied rates, and the standard simulation technique of a reference raster image has been \napplied. \n\nTerrain Slope and rate of slope change are fundamental erosion characteristics, regardless of \nsoil type, as they affect the speed and acceleration\/deceleration of water movement and there-\nfore energy of the water, and its propensity to either erode or deposit material. To this point, \nonly slope angle has been implemented.  \n\nTerrain Roughness at this site scale. Here individual terrain details such as vehicle tyre tracks, \nanimal marks and soil or rock texture can have a huge influence on erosion patterns. Not to \nbe  confused  with  soil  texture,  additional  roughness or  triggers  to  erosion  can be  included \neither as proxy mesh, with noise, or as a raster image. \n\nErosion Resistance is the demarcation of areas that physically cannot erode or are otherwise \nresistant to erosion due to solid barriers, vegetation, root systems, or other physical hindrance. \nThese areas can be either physically modelled or marked with images, allowing a graduated \neffect. \n\nEach of these terrain characteristics has been integrated into the workflow to form a working \nmodel for a limited range of case studies (Table 1). Surface water flow calculation is based \non detailed DTM data and simplified erosion simulation using reference parameters implemented with the computational terrain modelling plugin. Preliminary simulations assume a homogenous substrate, although the overall resistance characteristics to erosion can be manipulated. Additional testing and verification would be required for broader applications with accurate and repeatable results. \n\nSurveying and Erosion Modelling Tool \n\nThe implementation and testing of the erosion simulations have centred around specific post-fire landscapes. These make appropriate sites, as they are often immediately susceptible to \nreal erosion following an intense fire event. Erosion in these landscapes presents a massive \nissue for many authorities and communities. Analysis conducted in such landscapes has determined that up to 50 times more erosion can take place in an extremely fire-damaged landscape than in one marginally affected by fire, as well as many other mitigating factors (TULAU et al. 2015). To survey the affected terrain, there are precedents for UAV imagery and its use in general erosion mapping and simulation (MISTHOS et al. 2019). This survey method functions well in circumstances where erosion risk is high, due to lack of ground cover and exposed terrain. It also enables successive landscape surveys and allows for the mapping of landscape change and subsequent model adaptation. In addition, reference layers for erosion \nresistance can be generated from this data. In this case study, industry-standard photogrammetric  software  –  Agisoft  Metashape  –  was  used  for  this  stage  of  research,  however,  the \ndelineation of the scanned site and predetermined route may aid in generating more accurate \nresults with repeated scans of the same site, documenting its evolution and supporting iterative model generation. The required resolution and detail of the test site required a relatively \nlow altitude (40m) scan height with a high overlap of 80% in both directions, with an angled \ncamera (75°) flying in a gridded pattern around tree-bases (a common area of photogrammetric error). \n\nThe  simulations  are  built  on  top  of  Docofossor,  a  terrain  modelling  plugin  for  the  visual \nscripting environment Grasshopper of Rhino 3D (HURKXKENS et al. 2019). It uses regular \nraster grids as underlying data-model for its terrain representation. This enables simple modelling operations in cut and fill using distance functions. Like the plugin, the erosion simulations make use of build-in class methods from RhinoCommon or via the Rhinoscript python \nimplementation to compute the grid values.  \n\nThe implication allows for animation and simulation of any part or any duration timeline. For \ndetailed areas of the case study site (Figures 3) of around 50 x 50 m, animation frames can \nbe calculated in less than 5 seconds on a standard workstation set-up, facilitating an optimal \nsimulation-to-design workflow. When compared with large-scale surface water flow, the results  demonstrate  that  a  laminar  surface  concept  of  layered,  continuous  water  supply  and movement works well to imitate site-observed erosion phenomena in scale, extent, and pattern (Figure 4). Within the chosen case-study typology of fire-affected areas, there are ample examples and areas for verification and refinement of the erosion models in various soil and \nsurface conditions. The varied performance in differing substrates is an area for further research and study. \n\nDiscussion \n\nThe resulting model combines a cleaned, photogrammetric terrain model collected on site, \nwith a scalable yet detailed simulation of rill-model erosion. This can form the basis for site \nselection, risk analysis, or developing of erosion mitigation strategies in high-risk areas.  \n\nThe various data elements produced during the described process consists of base data and \nsite analysis, as well as simulation data, such as the resulting flow-paths, erosion \/ deposition \npatterns and debris transport lines. The results reinforced the individual and combined roles \nthat terrain roughness, converging slope details, and obstacles such as trees, fallen tree trunks, \nand rocks play in the resulting site-specific example. Site observation reflected the general \ntrends of the erosion model; however, additional calibration can be applied to further refine \nthe exact volume of material transported. The nature of the site material, being a mixture of \nfine ash, partially fire-consumed debris and dry topsoil that are non-uniformly distributed on \nthe terrain lead to a model in which exact depths and volumes of material erosion and depo-\nsition are variable. Therefore, the erosion paths and patterns can be shown to have a higher \nfidelity than the depth of erosion. As referred to in the conclusion section, additional material-specific experimentation or specialist data would help to refine these models.  \n\nThe potential for iteration in this process, both as a computational tool in simulation, and a \nworkflow for gradually improving and refining the model worked well, especially given the \nresponsiveness  of  the  algorithm.  The  proposed  additional  applications  for  iterative  design \nproposals  using  the  same  process  are  feasible  in  both  implementation  and  viable  design \nmethod. \n\nConclusion and Outlook \n\nThe  relevance  and  importance  of  erosion  simulation,  and  its  challenges  within  the  design \ndisciplines are established, and the case is made for its viability and implementation. The \nlack of a designer-level toolset to deal with these issues has been addressed in this research, \nas well as the potential of these techniques for the AEC professions. The range of further \napplications and possible sites is only increasing with the growing impacts of climate change. \nSuch simulation techniques can be deployed to predict erosion issues that may occur in the \nfuture  and  allow  for  pre-emptive  interventions  that  may  avoid  or  transform  such  erosion \nevents into more positive outcomes. Within the space of fire events alone, there is a huge \nscope for adjusting landscape management practices to better recover from fire events, especially their implications for soils, sediment, and the fostering of their landscape biodiversity \n(TULAU et al. 2015, ATKINSON et al. 2020). \n\nThe imitation of real-world site conditions and recorded erosion sites are of key importance \nto further refine the plugin settings based on local conditions and the predictive accuracy and \nusefulness of the technique. Similar optimisation tools such as RAMMS have also been carried out both during and after the release of landslide and rockfall computational simulation \n(CHRISTEN et al. 2012).  \n\nFurther development is required to optimize and better simulate existing high-resolution simulation  models,  as  well  as  the  integration  of  these  techniques  into  teaching,  research,  and \npractice (IGWE et al. 2017). There is a strong potential for generating data layers for other \napplications and GIS systems and facilitating new forms of engagement and multidisciplinary \ncollaboration in landscape engineering, remediation, and management projects. Where erosion can be predicted and mitigated, the potential for design with erosion processes emerges. \nThe possibilities of hybrid design processes that work hand in hand with the environment, \n(GIROT & HURKXKENS 2018) open areas of potential design endeavour, in which the landscape can be shaped over time with minimal intervention and resources. \n

Exploring Less Geometric Landfill Slopes through \nParametric Digital Modelling \n\n\n\nAbstract: Massive and visually disruptive landfills in urban areas can potentially be seen by hundreds \nof thousands of people daily. Even after landfill closure, constructed slopes and ridgelines can contrast \nwith the surrounding terrain because of their signature geometric form. This paper uses three landfills \nin Southern California to demonstrate the need for better visual mitigation, test the sculpting of landfill \nslopes through parametric digital modelling, and then discuss how the process can be enhanced for real-world application that improves visual quality while meeting engineering requirements. This is an area \nwhere landscape architects can make greater contributions in mitigating the visual impacts of landfills. \n\nIntroduction \n\nLandfills are the primary way that non-recyclable municipal waste is managed. Incineration \nis eschewed due to the introduction of carbon particulates and a harmful airborne brew of \npotentially carcinogenic chemicals associated with plastics and other modern manufactured \nmaterials. The ever-growing volume of waste is also a major concern and landfills can be \nmassive. Besides the large physical dimensions of landfills, the process of land filling requires a substantial investment in time, expense, and effort to locate suitable sites, meet stringent permit requirements, prepare the site for liquid containment and methane gas extraction, manage daily fill operations, and mitigate a full range of impacts. For these reasons, the trend is towards fewer, but larger landfills (EPA 2014, 2-11). \n\nMany landfills are geometric in shape and the planar sides and mesa-like top can be recognized from miles away. In urban areas, the number of viewers can be numbered in the hundreds-of-thousands, and unsightly views or the presence of landfills can negatively impact property  values  ranging  from  3-7%  (REICHERT  et  al.  1991,  BOUVIER  et  al.  2000,  READY \n2005). Moreover, the scale of urban landfills can be dominating. For example, Puente Hills \nlandfill in Southern California, which closed in 2013, has a footprint of 283 ha (700 ac) and \nis 150 m (490 ft) in height. Counting buffer land, the facility consumes 526 ha (1,300 ac).  \n\nObjective \n\nLandfill design and operations generally fall within the realm of engineering and scientific \nconsultants.  Landscape  architects  become  involved  when  considering  landfill  aesthetics. \nTypically, these activities are related to landcover planting and the preparation of visual analyses and simulations when preparing environmental impact documents prior to landfill permitting. As “shapers of land”, the objective of this paper is to explore how landscape architects might become more involved in the earlier stages of landfill design through enhanced \ndigital modelling so the landfill shape upon closure can better blend with the terrain context. \n\nExtending the Role of Landscape Architects? \n\nThe origin of this paper derives from the primary author’s environmental consulting work \npreparing visual assessments for two landfills in southern California: Elsmere Canyon Landfill (early 1990s) and Simi Valley Landfill Expansion (mid 2000s). In both cases, the landfills were  of  the  canyon\/valley  type.  Extensive  3D  computer  modelling,  GIS-based  viewshed mapping, and before\/after photo simulations (Fig. 1) were conducted to determine visual exposure and estimate visual impacts as viewed from key observation points (KOPs). These points were public gathering areas like parks, major travel ways, and nearby residential and commercial areas at distance ranges from 0.3 to 8.5 km (0.2 to 5.3 mi). \n\nIn the case of Elsmere Canyon Landfill, the permit was denied after much public opposition. \nSimi Valley Landfill was already an operational landfill in mid-life, and the expansion was \napproved after a multi-year environmental review process which addressed public concerns. \nEven though the expanded landfill conformed to conventional engineering design, the pri-\nmary author wondered if landfill slopes could be made to appear less geometric and better \nblend with contextual terrain. Instead of involving landscape architects to assess or mitigate \nvisual impacts after landfill design is nearly complete, the role of landscape architects could \nbe  expanded  to  perform  landform  sculptural  studies  earlier  in  the  design  process.  Closely \ncoordinating  with  engineers,  slope  sculpting  would  still  need  to  meet  fill  volume  requirements, access road routing, methane gas piping, and comply with efficient daily operations. \nThis paper only explores landform, and does not address vegetation, atmospheric conditions, \nor other factors affecting visual quality. \n\nConcept Overview \n\nEnhanced Slope Sculpting to Reduce Visual Impacts \n\nThe goal of the “sculpting” process is to introduce more slope undulation into uniform slopes \nto replicate convex finger ridges and concave drainages found in contextual terrain, but to a \nlesser  degree  to  still  support  engineering  requirements.  This  will  increase  tonal  variation \n(shade\/shadow)  patterns  which  will  better  blend  with  contextual,  undisturbed  terrain.  As \nviewing distance increases and atmospheric factors become more pronounced, tonal contrasts \nare more important to visual mitigation compared to texture or hue variations. \n\nSlope Sculpting Procedures \n\nLandfill form emerges as systematic lifts (layers) approximately 8-20 feet thick. Each lift is \ncomposed of cells where daily to weekly accumulation of refuse is compacted and covered \nwith 6” of soil. Once cell placement reaches the perimeter of the lift, the outer slope is shaped \nat a not to exceed 1.5:1 ratio. A series of 15’ wide benches are also added per EPA regulations \n(EPA 1988, 62). Under the sculpting concept, none of these standard filling and grading operations would be appreciably altered until the lift edge nears. At this point, GPS-enabled earth moving equipment would grade an undulating edge, that over years, would emerge as finger ridges or drainages on the slope much like 3D printing. The precision of GPS is essential to accurately locate and place cover material along the undulating lift perimeter where \nthe eventual slope form is not immediately apparent. \n\nTowards this goal, several additional steps are needed beyond traditional engineering design: \n\nNumerically determine the slope gradient and vertical\/horizontal convexity of the surrounding  topography  (usually  applicable  to  canyon\/valley  landfill  types)  for  use  as  a \ncontextual referent. \n\nIteratively sculpt a 3D landfill computer model where exposed sides more closely replicate contextual slopes and topographic features, and then shape a rounded cap or ridgeline profile that undulates as opposed to a flat mesa. The model footprint may have to be slightly expanded to offset anticipated volume losses compared to conventional geometric forms, or the overall height increased (LAW et al. 2008). \n\nTransfer the preliminary sculpted model into Civil 3D or other engineering software for detailed design and implementation documentation. \n\nPrepare a grading plan that can be uploaded into GPS-enabled refuse\/earthmoving equipment to guide landfill slope shaping over decades. \n\nMethods \n\nThere are multiple  methods to analyze  undulations in topographic surfaces to set numeric \nbase conditions for slope modelling: slope aspect and gradient, planform curvature, profile \ncurvature,  topographic  openness,  and  landscape  roughness.  Some  numeric  techniques  include fractal dimension indexing (FRAC) (CUSHMAN et al. 2005, 103-104; MCGARIGAL & MARKS 1995), standard deviations of contour line segments, and topographic position indexing (TPI) (JASIEWICZ & STEPINSKI 2013, MOKARRAM & HOJATI 2016).  \n\nTo identify a landfill as a test case for parametric digital sculpting, Zhong (2020) inventoried \n43 landfills including 14 active and 29 closed landfills larger than 100 acres in Los Angeles \nCounty. As part of the review, FRAC indices were calculated for the landfills to assess how \ngeometric the slopes appear and identify candidate landfills for further analysis. \n\nAfter candidate landfills  were reviewed, attention  turned towards  which digital  modelling \nsoftware might be most useful. WESTORT (2015, 225-226) discusses the need for improved \nlandform design tools which are 3D, provide geometric control, are easy to handle, provide \nquick response time, and are quantitatively accurate. Furthermore, the ability to iterate before \nand during the construction [or design] phase is desirable. From our experience, Autodesk \nCivil 3D meets most of the criteria for Digital Elevation Modeling (DEM) but is deficient in \ninteractive surface sculpting. A spline modeler like Rhino is better suited for this purpose and \noffers parametric automation through Grasshopper terrain plug-ins like Docofossor, Bison, \nand TOPO kit. Upon initial review, it appears that these plug-ins do not offer the sculpting \nfeatures\/control as envisioned without additional customization. \n\nTo test how inclined finger ridges can be introduced to geometric landfill slopes while still \nmaintaining landfill capacity, ZHONG (2020) used the closed Puente Hills Landfill to proto-type  a  hybrid  manual-parametric  landform  sculpting  process  using  a  customized  Rhino\/ \nGrasshopper script using 3D control framework (Fig. 2). Preparatory work consisted of man-\nually digitizing major ridgelines, finger ridges, and intervening drainage flow lines from the \nundisturbed  1950  topography  as  a  fully  detailed  referent  of  pre-landfill  conditions.  Points \nfrom this skeletal landform structure were then filtered through a Grasshopper script to interactively reduce ridge and valley point detail as a percentage. Two highpoint locations from \nthe 2018 landfill top deck (or intended height of a planned landfill) served as landfill closure \n(2013) height parameters. Once parameters were set, a simplified surface was interpolated \nthrough the points. Using various combinations of 22%, 66%, and 88% remaining ridgeline \nand  valley  points,  seven  surfaces  (P1-P7)  were  generated  for  comparison.  Processing  per \niteration took about 4-6 hours. \n\nResults \n\nAfter the manual-parametric methods were established to generate landform alternatives, further modelling exploration was undertaken. For comparison against standard landfill design, \ntwo planar geometric surfaces (G1-G2) and three advanced contoured surfaces (A1-3) were \nmanually defined through contours and generated through Rhino. The G1 and G2 surfaces \ntypified landfills of low visual quality and the A1-3 surfaces represented enhanced landfills \nhaving some amount of slope undulation (Fig. 3). \n\nFig. 3:  Results  were  compared  for  manual  geometric,  manual  advanced  contouring,  and \nparametric  modelling  of  landfill  configurations  against  1950  existing  topography \nand the 2018 shape configuration of the Puente Hills landfill (ZHONG 2020) \n\nOnce the 12 alternative landform  surfaces  were  modelled in Rhino, the surfaces  were exported into Civil3D for volume calculations. Two sets of volumetrics were compared: 1) the \n1950 pre-landfill referent surface compared to the 12 Rhino alternatives (2020) for total volume capacity; and 2) the 2018 DEM dataset (2013 closed landfill conditions) compared to \nthe same 12 Rhino alternatives. The latter comparison is intended to directly reveal net volume  gain\/loss by introducing  more  undulations to constructed landfill  surfaces.  Of the 12 \nalternatives, three showed capacity gains:  A3 (+5%), G2 (+24%), and G1 (40%). Surface \nundulations for the closed landfill and among the 12 alternatives were also compared through \nslope mapping and aspect mapping which quickly made differences visually evident. \n\nDiscussion and Conclusions \n\nModelling results reveal that although simplified, the P1-P7 parametric surfaces too closely \nresemble the complexity of the 1950 topographic referent. P1-P7 volume capacity was not \nsufficient, slope angles were not constrained to the 1.5:1 standard (not part of script), and \nexcessive slope undulations\/aspect variation would likely make implementation difficult. As \nexpected, introducing more surface undulations decreased landfill volume capacity for most \nalternatives compared to the more geometric landfill forms. \n\nThrough this exploratory test, however, advancements were made in parametric surface modelling that enabled rapid iteration, testing, and comparison of landfill alternatives. The A3 \nalternative demonstrates that more slope undulations can be introduced to improve landfill \naesthetics while still maintaining capacity volume. Rapid iteration, as demonstrated through \n12 alternatives, is essential in finding the right balance of improved aesthetics, capacity volume, and other engineering factors to be tested in future work. \n\nResults demonstrate the potential of applying digital sculpting tools to enhance landfill slopes \nto make them appear less geometric and planar. Artistic manipulation must be coupled with \nengineering requirements to maintain slope stability, constructability, and volume capacity. \nGPS enabled refuse\/earth moving equipment can provide the precision and locational accu-\nracy across large lift expanses to achieve more naturally appearing “outer shell” forms. \n\nMaking progress to sculpt landfill surfaces iteratively and more freely for aesthetic purposes \npartially fulfilled the initial objective of this paper. Full realization of the objective requires \nlandscape architects to better understand the complexity, timing, and workflow commensu-\nrate with landfill design and operations if they are to be involved. Based on past professional \nexperience  with  landfills,  many  engineers,  technical  consultants,  regulatory  agencies,  and \nenvironmental assessment  specialists are involved, and the  design and permitting requirements are substantial. Reducing visual impacts is a worthwhile goal but knowing when and \nhow optimized landform modelling with a greater emphasis on aesthetics can be introduced \ninto the design process is challenging. To be cost- and time-efficient, it needs to be used early \nin the process, offer rapid iteration, meet engineering requirements, and then be passed off to \nothers for technical refinement. At the landfill operational stage, extra edge\/slope requirements must also be safe and compatible with the myriad of choreographed activities taking \nplace on the working deck. \n\nSeveral limitations are evident: more documented research is needed regarding the long-term \naesthetic  impacts  of  landfills  upon  closure  after  vegetation  has  matured;  more  parametric \ncontrols are needed for slope shaping, the Rhino to Civil 3D transference needs to be more \nstreamlined; and most importantly, a test case involving engineers needs to be identified. \n\nFuture Work \n\nFuture improvements are needed to incorporate more parametric control of slope undulation \nand shaping. Additional ridgeline control is also needed for shaping the landfill cap to avoid \na mesa-like appearance. A hybrid between the P and A models is envisioned. \n\nIn addition to ridgeline controls, select parametric contours (splines) could be added as control  features  to  parameterize  surface  undulation.  Parameters  would  be  based  on  sinuosity \nwhich is simply calculated by dividing the sinuous contour length by the straight distance \nbetween the contour line endpoints. Calculating sinuosity is typically associated with stream \nsystems but can also be applied to characterizing landfill slopes where FRAC and TPI indices \nare too general to control shaping. Referencing the sinuosity index of contextual landform, a \nfew parametric contours placed at strategic locations at the edge of landfill lifts could seed \nthe formation of finger ridges. The finger ridges would become more apparent as the landfill \nheight grows with each successive lift much like 3D printing. \n\nDifferences in slope sinuosity can be illustrated using existing portions of the Lopez Canyon \nlandfill in Sylmar, California (Fig. 4). In this 3D view of the 2016 DEM surface, a contour \nline (L1) traces undulating slopes of the contextual foreground slopes, whereas the more linear contour line (L2) traces the constructed geometric slope of the landfill face rising above \nthe foreground ridge. The calculated sinuosity indices (SI) are 1.44 and 1.19, respectively. In \na revised parametric model, the SI of L2 could be adjusted to resemble the undulations of L1 \nmore closely while still allowing for proper slope benching, access road construction, and \nmethane gas piping. This will be tested as the Rhino\/Grasshopper script is improved. \n\nThese future improvements should enhance modelling capabilities, increase ease-of-use, and \nprovide better integration with software used to prepare construction documents. Discussions \nare also needed with landfill designers with regards to landfill operations, sequencing, and \noverall feasibility of this envisioned approach to landfill aesthetics and visual mitigation. \n

A Mixed Reality Experience: Advancing Design \nDecision-Making with Performance Metrics \nThrough Augmented Reality and Physical Media \n\nAbstract: Augmented reality and virtual reality have effectively served as an immersive environment overlayed within the real world. These experiences are, however, often static and limited in function, primarily, as a full-scale walk-through. This study tests the capabilities of including real-time interactivity and engagement as a tool for dynamic design decision-making. The idea is further explored by integrating landscape performance metrics and project goals to determine how this supplemental data may influence the participants design decision-making. The synchronization of qualitative and quantitative information through a mixed reality experience has major implications to design development as the stakeholder groups have the opportunity of experiencing real-time responses to their engagement of a design project. \n\nIntroduction \n\nThrough the emergence of landscape performance and the integration of quantitative metrics \ninto  outdoor  spaces,  technology  and  innovate  methods  can  begin  to  communicate  naturebased benefits as tangible outcomes to comprehend the complex ecological, social, and economic relationships of our complex environments (BECK 2015). Due to the fact that many of \nthese ecosystem services are intangible and abstract, new methods must be explored to effectively  communicate  these  invisible  landscape  performance  outcomes  into  a  perceptual \nrealm for a comprehensive understanding of design decisions (ZHANG et al. 2021). The viability of this process can be explored at both full and reduced scales as a real-time feedback \nmodel using a hybridization of augmented reality and physical media; catalysed as a mixed \nreality (MR) experienced in the p[AR]k.  \n\nThrough this comprehensive qualitative and quantitative mixed reality process, divergent decision-making, predicated on the information presented in an augmented reality (AR) interface, can be made for multiple responsive and sensible outcomes (LAHAIE 2016). This ultimately generates an immersive, engaging, and interactive design process for a more universal \naudience to participate in for specific needs from a landscape architecture project, shown in \nFigure 1. \n\nThe modeling of dynamic landscape benefits within a mixed reality experience of both physical demonstration pieces and augmented reality interfaces creates an accessible means to \nparticipate in the design development of any project. Augmented reality is not only gaining \ntraction as an innovative representation tool but with the integration of parametric modeling \nand performance metrics it can also serve as a decision-making tool (DUENSER et al. 2008) \nto the design process for specific goals and outcomes. With the incorporation of performance \nmetrics into the augmented interface, data becomes responsive to real-time change, performance parameters, and user decision-making.  \n\nA Mixed Reality Experience and Workflow \n\nThe p[AR]k attempts to model these dynamic landscape benefits within a mixed reality experience of both physical demonstration pieces and augmented reality interfaces to reach a \nuniversal audience within a dedicated workstation comprised of a physical sandbox and com-\nputational hardware. Augmented reality sandboxes are becoming a common tool to understand topography; however, they are often limited to specific outcomes visualized as coloured \nheightfields projected on sand media. Although it is beneficial to understand these funda-\nmentals, the computational rigor of these models can be advanced further using parametric \nsoftware  to  measure  additional  terrain  characteristics  and  ecosystem  services  that  include stormwater management, carbon sequestration, and energy savings, shown in Figure 2.  \n\nProviding visualizations, interactive properties, and tangible media in a mixed reality experience gives the user ownership and agency behind their decisions for outdoor spaces. Professionals,  community  members,  and  stakeholder  groups  can  come  together  and  learn  the \nimpact of their decisions as their actions are stored in a database for further analysis of communal interests.  \n\nSandbox Station Capacity \n\nThe feedback collected from the point cloud of the sandscape is processed through the parametric modeling software to analyse and generate visuals with charted information based on \nvarious  parameters.  In  addition  to  sand  media  serving  as  the  terrain  model,  mobile  smart \ndevices, oculus quest controllers, or aruco markers can reference and embed various amenities  that  include  trees,  benches,  water  features,  paths  and  other  elements  onto  the  terrain \nmodel for a conceptual landscape performance rendition. There are several benefits from this \nprocess that include workflow and design development, design-thinking, scenario modeling, \ntrade-offs assessments, landscape performance, and co-creation and collaboration.  \n\nIn this study, users were put into a hybrid mixed reality experience of physical and augmented \nreality to address design opportunities within the scope of landscape performance. They were \nintroduced to environment, social, and economic scenario objectives that could be mitigated \nthrough the manipulation of a physical sandscape and augmented placement of trees within \na hypothetical neighbourhood pop-up park. With this experience, users manipulated the sand \ntopography and moved pieces around, perceiving the impacts different designs had in achieving stormwater management, carbon dioxide (CO2) sequestration, and energy savings. Simultaneously, data readouts from the topography, amenities, and surrounding neighbourhood \ncontext communicated quantitative data on the design iterations to assess the trade-offs and \nperformance of different scenario models related to ecosystem services. With each change or \naddition to this AR model, the data readouts instantly updated to inform next moves within \nthe design decision-making process. \n\nThe metrics and formulas for these parametric performance models were configured from \nmetrics and calculators commonly used by allied professions that include iTree’s tree benefit \ncalculator, NRCS stormwater calculator, and the US Department of Energy typical household \nenergy consumption. The iTree benefit values used in the different performance scenarios \nwere based on a thirty-year-old healthy honey mesquite tree.  \n\nPerformance Objectives \n\nWith the influx of real-time quantitative data that updates during this process, there is a profound opportunity to fundamentally shift design thinking and intent from these augmented \noutcomes. By embedding measurables and metrics to this workflow, this new design process \nand methodology of a MR experience can potentially emerge that enables the respective parties to generate robust design strategies for evaluation on specific goals and objectives. As \npart  of  this  performative  MR  experience,  the  interface  can  be  configured  to  display  data readouts communicating quantitative information throughout the design process to assess the \ntrade-offs of different scenarios as a divergent process (CIRULIS & BRIGMANIS 2013).  \n\nThere are many different AR programs available for users to immerse themselves within a \ndesigned space, however, the values most programs don’t provide is the ability to synchronize it to an industry standard modeling software such as 3D Rhinoceros to further refine a \ndesign concept. This is made possible with the plugin additions of Grasshopper (parametric \nmodeling) and Fologram (AR platform) to create a real-time feedback between the perceptual \nAR interface and the cognitive modeling software. Grasshopper manages and integrates the \ntangible and intangible analytics for performative landscapes while establishing a dialogue \nwith the Fologram application on a smart device. Fologram is then used to engage with the \ndesign  model  using  either  finger  gestures  on  a  touch  screen  or  by  scanning  printed  aruco \n\n\nmarkers, referencing different design elements such as trees, pavers, or benches. The programs in tandem create a responsive workflow of reciprocating outcomes based on the decision-making process of the user. The perceptual experience created can be viewed simultaneously on the computer and in the physical world, blending the two in a hybridized environment.  \n\nWithin the p[AR]k project experience, users were evaluated on two different scenarios to \ndetermine if performance objective and metrics impacted their decision making. In the first \nscenario, users were only required to manipulate the p[AR]k space through a perceptual lense \nof  only  seeing  the  site  as  qualitative  and  figural  with  landforms,  water  features,  and  tree \nplantings. In the second scenario the users were provided different landscape performance \nobjectives that integrated and displayed quantified metrics to determine if and how this may \nimpact their design decision-making any differently. Performance scenarios included managing stormwater runoff, sequestering CO2, and reducing energy consumption from buildings.  \n\nPerformance Methods \n\nFor the stormwater runoff scenario, the surrounding impervious conditions of streets, sidewalks and buildings coupled with the park’s landforms contributed to a calculated stormwater \nrunoff volume from a typical 0.5” rain event. This volume served as a baseline objective to \nmanage through a catchment system (landscape depression) and trees. The runoff from those \nlandform conditions were conveyed through drainage lines, suggesting the most optimal location to place trees to intercept the runoff. In the carbon sequestration scenario, the EPA’s \nmetric for an average typical passenger vehicle emitting about 4.6 metric tons of carbon dioxide per year, or 10,141 pounds of CO2, was used to establish a site measurement for the \nsurrounding neighbourhood context. And according to the U.S. Department of Transportation, the average number of cars owned per household is about 1.88. Based on these statistics, \nthe surrounding residential households contributed a total of 247,853 pounds of carbon emissions annually. Lastly, the energy savings scenario used the U.S. Energy Information Administration’s average household consumption metric of 10,715 kilowatt hours annually. \n\nUsers  were  first  expected  to  manipulate  the  topography  to  create  dynamic  landforms  and \nwater features within the neighbourhood pop-up park, see Figure 3. This would ultimately \nimpact  stormwater runoff potential  while  simultaneously creating drainage  and  catchment \nsystems in the landscape. The participant quickly realized that either one large or multiple \nintermediate  catchment  systems  within  the  topography  could  not  contain  the  site’s  runoff alone and would also require the strategic placement of trees to help with the mitigation process through interception.  \n\nTrees can intercept runoff through both their tree canopy and root structure. For the modeling \nand  calculation process,  the tree’s  ability  to  intercept  runoff from  their root  structure was \nused and would be assessed on its performance based on its proximity to the generated drain-\nage lines in the topography. As the user realized this in their tree placement, they began to \nstrategically place trees within close proximity to the visualized drainage lines in order to \nmaximize their interception potential leading to a higher percentage of managed runoff. \n\nThe design of the neighbourhood pop-up park for CO2 sequestration was more ambiguous \nleading to less consistent tree placement strategy since this is a non-point source pollution. \nIn these types of situations, it was found during the study that when there are unclear demar-\ncations for tree placement users would often place trees with less reason or logic into the \nlandscape, shown in Figure 4. This did, however, lead to users manipulating the sandscape \ntopography to have less area dedicated to catchment basins so that more trees could be placed. \n\nTree placement within the energy savings scenario had similar results to the stormwater management one in that as users realized the relationship of a tree’s location to a building would \nreduce its energy use, their strategic planting gravitated towards those specific parameters. \nWith the energy savings, tree distance and orientation to the building would result in different \nkilowatt hours saved. This can be seen in the tree and building values in Figure 5. \nFor example, a tree in near proximity to the north facing façade of a building would be less \nbeneficial to a marginally further distanced tree from the south facing façade since that sun \nfacing façade would be absorbing the most, requiring more tree shade protection to reduce \nthe building HVAC use.  \nThe combination of the responsive interface, the ability to co-create and engage with different \nstakeholder groups, and the real-time integration of data can fundamentally reroute the design \nthinking methods. Abstract and formal ideas can merge into one process, helping the user \nengage, innovate, increase options, and see the implications of those decisions simultane-\nously with their collaborators as a cyclical feedback loop. \n\nResults \n\nAfter users participated in this study through the p[AR]k, they were provided a survey to \nassess if and how the integration of landscape performance metrics (quantitative information) \nimpacted  their  design  decision-making  (qualitative  information).  Out  of  this  beta  testing \ngroup of roughly thirty undergraduate landscape architecture students, one hundred percent \nagreed that the inclusion of landscape performance values impacted their design decision-\nmaking, as asked in the first survey question. This broad overview of landscape performance \nmetrics were followed-up with more specific performance questions to determine which ones \ndid  or  did  not  have  a  significant  impact  on  their  design  decision-making,  provided  in  the \nbelow figures.  \n\nDesign Decision-Making Outcomes \n\nFollowing  the  first  survey  question,  the  first  landscape  performance  question  in  Figure  6 asked the user to rate how significant the inclusion of performance metrics was to their design \ndecision-making.  A  scale  of  1  to  5  (x-axis)  was  used  where  1  meant  no  significance  and 5 meant major significance. The number of results (y-axis) shows the majority found it within \nthe high significance range. \n\nThe next performance question in Figure 7 asked which landscape performance scenario was \nof value to the user. The scenarios were specifically diversified to reflect environmental, social, and economic values. The participant could select multiple options so many users found \nmultiple performance scenarios to be important to them. \n\nThe last performance question in Figure 8 asks for the significance in designing towards a \nspecific goal. This question also used a scale ranking of 1 to 5 was used where 1 meant no \nsignificance and 5 meant major significance. This clearly indicated that establishing performance goals for individuals drove their design decision-making.  \n\nConclusion and Outlook \n\nThis  mixed  reality  provides  an  advanced  learning  experience  for  the  user  to  engage  with innovative technology, create robust analytical modeling and assessments, and create a participatory decision-making experience to advocate for healthy sustainable cities (WANG et al. \n2013). The hybridization between malleable model making with augmented data reveals a \nsymbiotic connection of instrumental tools in design thinking. It correlates with many STEM \nrelated principles of evidenced-based strategies where design can act as a strategic response \nto problematic issues of flooding, public health, and equitable resources within the built environment.  \n\nThis mixed reality design experience has tremendous potential to increase the capacity for \nusers, designers, and the community to work together and advocate for healthy living environments, make design more efficient by linking tools that help speed up the process, and \nhelps  everyone  involved  see  the  benefits  of  proposed  designs  (LIU & WANG  2019).  Augmented Reality has shown to serve as an innovative and versatile tool to visually communicate information as part of an analytical and design narrative for more informed design decision-making. This MR experience and workflow can further separate itself from other methods in the immersion and perception of space as it overlays qualitative and quantitative information impacting the composition and performance of a design concept. Rarely is it possible to experience the data that binds ecologies together. AR still contains limitations to this \nprogressive design approach, however, it can begin to give a glimpse of these interactions \nand  relationships  between  systems  in  real-time  as  designers  augment  space  into  a  mixed-reality. \n\nA Parametric Design Methodology for a Novel \nEcosystem \n\nAbstract: A literature recognizes the ecological value of biodiverse “novel ecosystems” that arise from an interplay of anthropogenic and independent ecosystem processes. Several factors influence the creation of these novel ecosystems and there is no shortage of data acquisition methods for, or complexity of descriptive data about them. However, there is a need to articulate and contextualize tools and meth-ods for formal design iteration that can make sense of this complexity and facilitate its translation into functional and meaningful landscape form. This paper describes a method for abstracting ecosystem processes into a simple parametric model, presented in the context of a riverbed mining operation in southern Georgia. The results of this initial methodology reveal the potential of parametric models to mediate between digital models of formal proposals and abstract models of ecosystem process. Separating and recombining parametric site models, abstract ecosystem models, and landscape photomontages reveals possibilities for a more facile methodology for iteration of design interventions. A parametric model also serves as a site for negotiation between project goals and constraints. Finally, the generation of a digital parametric site model allows for subsequent visualisation and editing of more specific digital models for design development and construction. \n\nIntroduction \n\nAggregate Mining Landscapes as “Novel Ecosystems” \n\nAggregate mining landscapes are widely understood as sites of chronic ecosystem degradation and destruction. However, there is a growing consensus that recognizes the ecological \nvalue of chronically degraded but “novel” ecosystems in supporting biodiversity and providing ecosystem services. A “novel ecosystem” is understood here broadly as a “new species \ncombination that arises spontaneously and irreversibly in response to anthropogenic land-use \nchanges” (MURCIA 2011). While significant debate surrounds the “irreversibility” of these \nchanges, the “restoration” of sites with a high degree of anthropogenic disturbance can improve biodiversity and ecosystem services. Rather than allow descriptions of “novel ecosystems” to simplify the project of ecological restoration, designers’ engagement with these sites \nshould focus on relationships between anthropogenic disturbance, ecosystem services, and \ncomplexity (MURCIA 2011). \n\nRestoration research in riverbed aggregate mining contexts suggests that disturbed mining \nsites provide new ecological niches that support overall site biodiversity, and that restoration \nefforts could benefit from a focus on “near-natural” restoration rather than more resource-\nintensive planting approaches (ŘEHOUNKOVÁ 2011). Similar approaches advocate for designing ecological processes and functions to allow for plants to “develop a measure of control \n[themselves]” (DEL TREDICI 2007) or “intervene and leave room” (GROSSE-BACHLE 2005). \n\nThis paper expands and applies this approach to ecosystem restoration to explore the role of \ngeodiversity in parametric modeling and redesign of aggregate mining operations. An active \nopen riverbed mining site in the south of Georgia provides the case study for the project, \npresenting opportunities to explore a design methodology that attempts to engage and alter \nan existing process of anthropogenic geomorphological change. Restoration is here under-\nstood as amplification and diversification of remaining landscape qualities. \n\nFrom Geodiversity to Geomorphological Diversity \n\nWhere geodiversity is a result of a broader collection of factors including mineral, paleontological, and structural\/ tectonic factors, geomorphological diversity describes that subset of \nfactors encompassed by surface processes. While variations in surface processes like differential weathering lead to increased geomorphic diversity, these same processes can also lead \nto reduced geomorphic diversity (THOMAS 2011). In the shifting landscape of an active aggregate mining operation, geodiversity can be understood not only as geo-determinism but \nalso as an effect of the existence of multiple states of succession within a given ecosystem. \nAltering the mining process can lead to coordination and amplification of a wider range of \necosystem successional states.  \n\nParametric Modeling and Scenario-based Design \n\nAshari et al. note that “Key academics and practitioners of landscape architecture are implementing parametric design as part of their design process in generating various design scenarios” (ASHARI et al. 2022). The goal of the methodology is incorporating the context of the \ndesign  work  into  a  formal  parametric  model  from  which  to  generate  proposed  scenarios. \nThus, the project is an attempt to situate parametric modeling within a broader definition of \ndesign scenarios (SHEARER 2022). The method should allow the model to facilitate a formal \ndialectic between given and proposed formal scenarios that is open to interpretation by designers. Additional parameters can facilitate landscape designers, planners, quarry operators, \nscientists documenting biodiversity, and the interests of laypersons living near and visiting \nthe site. The methodology elaborated in this article proposes modifications to a traditional \nAnalysis\/Concept\/Design\/Evaluation workflow. The aim was to use parametric modeling to \nencode simple formal models of observed ecological processes and begin to visualize formal \ninterventions in these processes. \n\nMethodology + Case Study \n\nSite-based Research + Analysis \n\nInterview \n\nThe design team kicked off the project with a semi-formal site visit and workshop to identify \nformal parameters and key system concepts of the mining operation. We invited architecture \nstudents from a local university to work with the design team to interview on-site excavators, \noperations managers, and ecologists.  \n\nThe interviews revealed that excavators control the shifting landscape by creating dams, which allow them to direct the river and its powerful floods. \n\nAccording to the on-site operations manager, dams are built for a variety of reasons, including protecting machinery from periodic flooding, reducing sediment flow, and allowing \nfor truck access from extraction points to processing sites. After a certain area is excavated, \ndams are eventually removed to allow larger floods to “replenish excavated material”, a \nprocess which can take as few as five years.  \n\nAfter collecting the qualitative description of the mining process and ecological diversity, the \nteam compiled satellite imagery, reviewed drone footage, and composed diagrams describing \nthe mining process and its extent. While the drone footage from the February visit shows a \nsparse winter landscape, satellite imagery shows a river in constant flux, coming to life each \nspring as a new layer of vegetation spreads over the bed of silt and rocks freshly deposited \nby winter flooding. The imagery serves as a partial record of the last decade and reveals the \nimpact of mining activities on the riverbed over time. Aggregate mining disturbs the riverbed \non a timeline of weeks to months, but this impact is superseded by spring floods that occur \nyearly. \n\nIn response to these conditions, we hypothesized that the mining and dam-building processes could be harnessed to increase the geomorphological diversity of the mining site. Processes of excavation, dam building, and periodic flooding have the potential to create a \nwide range of microhabitats that support increased species biodiversity. \n\nDefinition of Ecosystem \n\nGrasshopper Definition of Formal Ecosystem Drivers \n\nA secondary step of analysis involved the translation of the narrative context of the site into \nformal parameters that respond to existing ecological conditions and planned works on the \nsite. To create a tool for quick formal iteration with respect to site-scale decisions (“Planning \nDecisions” in Fig. 1), we needed to identify and abstract the appropriate metrics and formal \nparameters of the mining operation and larger landscape ecological dynamics and forms. \n\nIdentifying Macro- and Micro-ecosystems \n\nThe abstraction of landscape ecological features was accomplished by comparing areas of \nsimilar  vegetation  cover  to  the  topographical  surface  to  define  boundary  conditions  on  a \nbroad site-scale. As observed on site and through sectional analysis in Google Earth, the edge \nof the “Intact Bosque” follows the scoured edge of the riverbed, providing a sharp boundary \nat the site scale. A lighter shade of vegetation composes an intermediary edge zone that has \nallowed for the development of an early successional bosque, which exists at 1-2m above the \napparent limit of flooding. The floodplain is defined by a zone of high-frequency scouring \nevidenced by the limited presence of shrubs. Scattered across the floodplain are former borrow pits, which host littoral micro-ecosystems similar to the early successional bosque. \n\nBy  applying  Richard  Forman’s  principles  of  landscape  ecology,  the  early  successional \nbosque can be understood as the cellular or nuclear membrane of the intact bosque, facilitating the migration and succession of species (DRAMSTAD et al. 1996). A key assumption for \nthe model is the role of the intact bosque as a nucleus for expansion into the shifting floodplain and active mine zones (CORBIN et al. 2016). This allowed us to argue for the conservation of the bosque by shifting tipping grounds from the floor of the intact bosque to the more \nfrequently  disturbed  early  successional  bosque.  The  tipping  piles  currently  interspersed \nthroughout the intact bosque should be redistributed as a way of amplifying the seed bank of \nthe more frequently disturbed landscape.  \n\nConcept Development \n\nThe goal of the concept development phase was to create a simple iterative model of masterplan forms. At the site scale, the ecological system is represented as a “boundary model” \n(HILL 2005). The boundaries represented in the model (the edges of the intact forest and the \nedge of the main channel of the river) define the limits of the proposed berm system. These \nedges are subdivided to generate start and end points for the berm and distances between all \npossible start and end points are calculated. Each of these possible berms is then recalculated \nas a set of bi-arcs, or curves composed of two simple arcs. The definition incorporates In-\nCurve interior culling to exclude iterations that exceed the limits of the project.  \n\nFollowing initial iteration, a selected curve or set of curves is then used as a basis for the \ngeneration  of  a  surface  that  can  be  intersected  and  compared  with  models  of  the  existing \nsurface. In this project, the lack of a high-definition surface model forced us to rely on abstracted orthoimagery to infer elevation information. In future iterations of the methodology, \na context model with greater definition can add greater analytical definition to this stage. \n\nThe vertical difference in the model is developed relative to a “system zero” that can shift to \nany average or given waterline elevation. This allowed us to reduce the complexity of varying \nwater levels while also retaining the ability to incorporate more complex water elevation data. \n\nDesign Development \n\nThe iterative outcomes of the concept development model form the base of a further-refined \n“design development” model. The development model is broken into smaller models of individual zones to generate a greater diversity of micro-ecosystems. \n\nManual and Parametric Cutting Operations \n\nBerms can be cut with individual curve parameters or sets at specific intervals to introduce \nfurther porosity in the system. Varying the elevation of these cuts can generate check-dams \nor culverts with a range of crestline elevations. \n\nDistribution of Varying Nutrient Levels and Soil Types \n\nParameters  for  specific  soil  types  can  be displayed by  developing  a key-color  legend and \ncustom material previews. \n\nIncorporation of Environmental Simulation on Smaller Sites \n\nThe generation of a physical model allows for a range of existing Grasshopper plugins to \nanalyze  environmental  factors  such  as  hydrological  saturation  (Groundhog)  and  temperature\/energy modeling (e. g. Ladybug, Honeybee). \n\nPhotomontage and Further Descriptive Visualisation \n\nPhotomontage was used not as a way of determining the fixed or final condition but as a \nmethod of representing combinations of potential project conditions to create an image open \nto interpretation (M’CLOSKEY 2014). The definition generates measured landscape forms that \ncan be montaged over existing landscape conditions and textures to produce projective photomontages (Fig. 5). Such hybrid images can be used to establish metric visual parameters \nthat facilitate systemic and systemic design decisions. \n\nDiscussion \n\nDeveloping Facile Definitions \n\nThere is a need to balance specificity and complexity with respect to digital data inputs and \noutcomes when generating formal iterations. Nested, simplified models allow design teams \nto move efficiently between decision-making scales without carrying unnecessary complexity to the model.  \n\nInitial investigations in the design development phase revealed that the use of environmental \nsimulation plugins such as Ladybug, Honeybee, and Groundhog (saturation) work best on \nsmaller sub-sites, where more subtle forces of surface geology become legible. Further case \nstudies are needed to elaborate possible applications to large-scale sites. \n\nDesigners working with parametric software must always take caution when integrating specific datasets. Beginning with an abstracted master plan model with simple geometry allows \nfor fast and loose overlays with existing context data (orthoimagery, topography, observed \nvegetation, etc.). \n\nRules-based Experimentation \n\nSome rules were fast and loose, aimed at experimentation and later refinement (i. e., each \nberm should be double-curved). Others were highly specific, (i. e., the length of the berm, \nthe radius of the berm enclosing an area). The definitional simplicity of the model facilitates \ngeometric iteration and pattern generation. \n\nRepresentation through Hybrid Drawings \n\nThe hybrid drawings produced through this workflow operate like traditional landscape rep-\nresentational method of “landscape overlays” and yet add another layer of specificity and \ncomplexity.  User-displayed  text  and  annotation  can  further  integrate  metric  and  technical \nparameters of the represented form. \n\nConclusion and Outlook \n\nThis project and paper focus on the outcomes of an initial draft of a parametric methodology \nfor engagement with novel ecosystems through geodiversity. The goal of the case study project was to bring the mining operation parameters into a model that can facilitate rapid visualisation of site-scale forms, followed by an initial investigation of smaller parameters. Creating a protocol for nested models with feedback was crucial to moving between scales and \ndiscerning between “hard” and “loose” formal design decisions. \n\nThe methodology allows the abstracted parametric model to become a site of physical and \ndiscursive negotiation and coordination between the various actors involved in the project. \nThe next step in refining this methodology is to test the utility of this parametric model facilitating interdisciplinary data and feedback in the design process. The case study used to develop this tool aimed to establish a more intentional role for formal decision-making by facilitating decisions that incorporate aesthetic and metric-based approaches. \n\nFurther research could unfold along two strategies for further defining the surface: a more \nspecific integration of soil types and the use of 3D scanning to incorporate temporal change \nof landscape form.  \n\nEventual applications of the methodology could inform design protocols for novel and ex-\npanded riparian ecosystems. The workflow can also be applied to smaller sites and the gen-\neration of formal iterations.  \n\nComparing Transportation Metrics to Measure\nAccessibility to Community Amenities \n\nAbstract: Landscape architects and urban planners play an important role in helping create inclusive, \naccessible communities. To do this, it is helpful to understand how the built environment and access to \nvarious amenities and places affect activities of daily community living. These activities can influence social engagement with others and satisfaction with one’s social life. Thus, understanding how the built environment  influences  their  choice  of  living  can  shed  light  on  the  ramifications  to  an  individual’s overall social satisfaction. As previous studies show, there are various methods for measuring accessibility. But to what extent are these metrics different or provide similar results? In the current study, we generate various geospatial models to measure distance, density, and accessibility. The metrics produced are then compared to identify how similar they are in measuring access to several different places. Results show that all metrics are statistically significant and similar, however, similarities range from poor  correlation  to  very  high  correlation.  The  most  consistent  can  then  be  used  in  future  studies  to identify how well they correlate to stated access, actual access, and the influence on social satisfaction. \n\nIntroduction \n\nA tenant of landscape architects and urban planners is to improve the quality of life in communities. An essential variable in this equation is improving the satisfaction of social life as \na result of community integration (SEEMAN 1996, YEN & SYME 1999) and development patterns.  To  accomplish  this,  landscape  architects,  policymakers,  and urban planners need  to \nknow how social and environmental factors impact community integration and, ultimately, \nsatisfaction with social life. Prior work in the discipline has shown that if we create improved \nintegration of people within their community, people experience a higher level of social satisfaction (CHRISTENSEN et al. 2010). \n\nSeveral factors affect the level of satisfaction with social life among people, including factors \ninfluenced by the surrounding neighborhood. For example, social factors such as place attachment (CAO et al. 2020, HUR & MORROW-JONES 2008) and neighborhood cohesion (Liu, \n2017), make higher rates of satisfaction (MITCHELL et al. 2013, OKTAY et al. 2021). Additionally, environmental factors including landscape and green space (BOTTICELLO et al. 2014, \nYOUSSOUFI et al. 2020), mixed land use (BEARD et al. 2009, CAO et al. 2020) destinations \nand urban amenities (ALLEN 2015, SATARIANO et al. 2010), and street connectivity (GÓMEZ \net al. 2010) also play a role in facilitating social satisfaction. However, there is no standardized rule used to calculate how built environment factors and the proximity to urban amenities are associated with the level of social satisfaction. Further, there are limited studies that \nsystematically compare accessibility measures (KAPATSILA et al. 2023). Not only can it be \nimportant to compare the results from a range of accessibility metrics, but equally important \nis  the  level  of  technical  difficulty  required  to  accomplish  each.  In  this  paper,  we  explore \naccessibility to common community places in which individuals take part in a range of different  activities  (JONES  1981).  These  places  include  areas  of  outdoor  recreation,  grocery \nstores, retail stores, restaurants, and others. Throughout this paper, we refer to these places \nas “place types”. Since accurate metrics are necessary for effective policy development and \nimplementation, we are keen to ask: to what extent do different accessibility metrics agree \nwith each other”? Specifically, how different are geospatial techniques for measuring and \nquantifying access to place types by neighborhood?  \n\nThere is a diverse knowledge and broad understanding of the linkages between the level of \nsatisfaction among individuals and environmental factors including neighborhood and place \ntypes (SAMMER et al. 2012, WHITE & SUMMERS 2017). There is, however, uncertainty about \nhow  accessibility  to  each  place  type  affects  social  engagement.  Given  the  potential  importance of these places to facilitate social satisfaction, it will be helpful to identify a robust \nspatial metric that could explain the impact of place types on the level of satisfaction with \nsocial life. We could further use the spatial models to assess and promote policies that integrate people into their surrounding environments and communities. Understanding the relationship between place types and the level of satisfaction can inform how landscape architects \nand planners design inclusive communities. However, before we can understand this relationship, it is also important to understand how we assess accessibility. \n\nOne of the more basic measures of accessibility is to determine the degree of access from one \nlocation to another. Certainly, there are differences in the kinds of reliance on different modes \nof transportation across demographics (PARK et al. 2022). There are several ways for measuring accessibility. However, this paper is not focused on attempting to identify the range of \nmulti-modal transportation accessibility techniques. Instead, we focus on understanding the \nconsistency of different measures of accessibility, mainly because some models may be biased (GIANNOTTI et al. 2022). In this paper, we identify the degree to which six common \nspatial accessibility models are related. We generate a systematic comparison of these models \nby measuring the spatial pattern and accessibility to several place types. By identifying similarities, we can determine trade-offs between model complexity and consistency of the metrics. The easiest and most consistent models can then be used in future studies where empirical data about demographics, disability status, and travel behaviors can be used to identify \nthe relationship between access to place types and social satisfaction. \n\nMethods  \n\nDetermining accessibility is a rather complex process. Transportation-related studies provide \nseveral  means  to  produce  an  accessibility  measure,  with  the  most  precise  being  produced \nusing empirical data collected about individual travel patterns. However, these data can be \nparticularly expensive to obtain and may be logistically prohibitive to obtain in some international contexts. In this study, we pursued comparing six different geospatial models that use aggregate data at the US Census block group level (heretofore: “block group”). To conduct our analyses, we used the Safegraph Point of Interest data points (POI) and aggregated \nthese data into eight categories. Each category is then referred to as a place type. Six metrics \nwere defined to represent the spatial pattern of place types. including, accessibility to different destinations (CAO et al. 2020), proximity to various destinations (TSEMBERIS et al. 2003), \nthe density of place types (YANG 2008), and spatial models (GIANNOTTI et al. 2022, LUO & \nQI  2009).  More  specifically  our  metrics  are:  1)  Average  proximity  to  place  types,  2)  frequency of place types (count of place types within the block group), 3) Density of place types \n(using kernel density), 4) The number of block groups (in USA, referred to as block groups) \nwithin the service area of place types, 5) the gravity model, and 6) two-step floating catchment  area  (2SFCA).  Each  of  these  metrics  measures  accessibility  to  place  types,  but  it  is \nimportant to note that the level of technical difficulty varies widely. Furthermore, variables \nused to calculate each metric can differ. For instance, 2SFCA and gravity model considers \npopulation,  while  frequency  metrics  and  the  kernel  density  focus  on  the  number  of  place \ntypes in a unit (e. g. block group). As a study area, we analyzed data from across the Greater \nSalt Lake City, UT region in the USA. Some items, including accessibility and distribution \nof place types, were selected from neighborhood satisfaction scales developed by (GUTTING \net al. 2021, OKTAY et al. 2021). This technique provides a simple proxy for determining the \nextent  of  access  to  place  types  KWEON  et  al.  (2010)  produced  similar  measures  based  on \ndifferent place types by limiting the distance of an accessible amenity to a respondent’s home. \nConducting a correlation between the metrics results can provide insight into the similar results that we can get. \n\nTwo density measures were produced, frequency of place types (count of urban place types \nwithin the block groups) and density of place types (using kernel density), Metric #1 and #2, \nrespectively. Metric #1 counts the total number of the selected place types intersecting with \na block group, using spatial selection. Metric #2 uses kernel density to calculate the density \nof features and place types in a block group (YANG 2008). Kernel density was performed by \nthe planar option, which is appropriate for the analysis on the local scale.  \n\nFor average proximity to place types (Metric #3), we generated a set of origin locations that \nwould provide a meaningful context for trips from within each block group. Every intersection of the road network within each block group became an origin node. Intersections within \nthe block groups balance the trade-off between identifying every building from which people \ntravel to and from, but also provide more precision than the centroid of the block group. We \ncalculated the median distance of travel using Safegraph datapoints and set as a travel distance to get to different destinations from the intersection nodes. Driving distance was used \nbecause there is limited information about alternative use of public transit. Then, the average \nproximity from all origin nodes to all place types within the travel distance was calculated. \nFigure 1 shows the average proximity, by block groups, from origins to each place type.  \n\nThe number of block groups within the service area of place types (Metric #4) flips the density calculation by starting from the place instead of the block group. To produce this metric, \na service area was generated for each place type using a network analysis. The route distance \nwas calculated based on the median distance that individuals reported as the travel distance \nto  get  to  different  destinations  (similar  to  Metric  #1).  Then  the  number  of  place  types  by \nservice area within each block group was calculated by spatial joining the service area with \nany intersecting block groups.  \n\nThen, the more established 2SFCA metric (Metric #5) was used for measuring accessibility \nto each place type. 2SFCA defines a catchment area around each location and computes the \nsupply-to-demand ratio for the catchment area. The boundary of the catchment area can be \ndelineated with the radius, travel distance, and travel time (LUO & QI 2009).  \n\nThe other commonly used transportation metric we employed was the gravity model (Metric \n6). The gravity model has been used to model human mobility and accessibility. The model \nconsiders the distance between two nodes and the population within the place type’s service \n\nResults  \n\nResults of this systematic comparison are provided first as a correlation matrix (Table 1), \nthen as a series of visual representations below. To demonstrate the statistical differences \nbetween  each  metric,  we  produced  correlation  tables  for  each  of  the  eight  different  place \ntypes, by each of the different Metrics (or geospatial models). Here we show a summary table \nof the average correlation values across all eight place types. The purpose of this table is to \nhighlight  (in)consistencies  between  metrics  and  to  identify  potential  sensitivities  of  these \nmodels  across  place  types.  Note,  Metric  #3  is  inversely  correlated  with  all  other  metrics. \nThus, these were this inverted to positive values so the average correlation would maintain \nits accuracy of strength, regardless of signing. Consistently high correlation values with a \nrelatively low standard deviation suggest that these Metrics are likely to produce similar results. This table highlights that Metrics #4, #5, and #6 maintain very high correlation and low \nvariance across all place types. Metric #2 also seems to be highly correlated with Metrics #4, \n#5, and #6.  \n\nTo depict these data visually, several maps were generated (Figures 1-6, for Metric #1 – #6, \nrespectively). Given the diversity of values for each of the different Metrics, we created Table \n2, which identifies the grouping of values for each of the different legends. Using these visuals one can see differences in the relative distribution of high to low access to various place \ntypes. For instance, Metric #1 shows a clear visual difference from the other Metrics (which \nis also clear in Table 1). Also, the overall consistency of Metric #2 with Metrics #3 – #6 is \nalso fairly apparent in these figures. Note, the groupings used in these figures were not the \nvalues used to conduct the correlations (correlations were not run by groups). Instead, correlations were conducted on the raw values produced from each metric for each of the block \ngroups. The figures are providing only a visual reference with the groupings of data produced \nusing Natural Jenks, these groupings were not used in the correlation analysis. The figures \nonly symbolize the results of the metrics for access to outdoor recreation. \n\nA summary description of these figures is provided here. Figures 1 and 2 illustrate the results \nof the count of place types within each block group (Metric #1) and the density of place types \nusing kernel density (Metric #2). The darker color indicates that a block group has a greater \nnumber and density of urban place types. Alternatively, as the color gets lighter, there are \nfewer place types and a lower density of place types. Figure 3 illustrates the results of the \naverage proximity to place types. The darker color shows a shorter distance to the outdoor \nrecreation  and  as  the  color  becomes  lighter,  it  shows  a  longer  distance  to  the  destination. \n\nDiscussion and Conclusion \n\nLandscape architects are regularly involved in community design and transportation planning. Finding models that provide reliable metrics and are easy to perform can help facilitate \nrapid  iterations  of  design  and  planning  recommendations.  In  this  study,  we  compared  six \nmetrics that measure accessibility to different place types and showed differences between \nthem. Results indicate a high correlation between metrics #5 of the 2-SFCA method, #6 of \nthe gravity model, #4 of the number of block groups within each place type service area, and \n#2 of the kernel density. Furthermore, we provide a stark warning about the dangers of using \na  single  geospatial  metric  –  especially  if  the  metric  needs  further  empirical  evaluation  to compare  its  reliability  and  effectiveness.  The  2FSCA  (Metric  #5)  and  gravity  (Metric  #6) \nmodels have been well published in the literature (KAPATSILA et al. 2023, LIU et al. 2022, \nLUO & QI 2009) but can be more complex to run than other metrics (Metric #2), though these \nare highly correlated. This comparison highlights the potential trade-off between model complexity and the outcomes. It will be important for future studies to ascertain the value of the \nmore complex models. For instance, correlations between models might be high, but do they \nmaintain the same level of consistency when other variables are included (e. g. demographics \nor disability status)? If reliability is maintained, then simpler methods should be used first, \nwith  the  more  complex  methods  becoming  necessary  only  if  there  is  a  good  empirically-sound reason. \n\nOur analysis has shown that some models of accessibility differ quite substantially. At the \nsame time, some of these models share a high statistical similarity. One of the challenges \nthese models provide is that they can serve to validate actual travel times and provide data to \ninform planning policy. However, there are limited studies that attempt to connect these models  to  social  satisfaction.  Our  results  are  aligned  with  other  studies  that  compared  simple cumulative opportunity measures and the measures produced by the gravity model to under-\nstand if there is a significant correlation or not between them. The results showed that cumulative  opportunity  measures  can  substitute  complex  measures  like  the  gravity  model \n(KAPATSILA et al. 2023). It can be argued that social satisfaction is an important indicator of \nthe quality of life, perhaps more so than just assessing how long it takes someone to get from \npoint A to B. Thus, to validate these models, a future study should study the statistical relationship between each model and how they relate to social satisfaction. Further, we also anticipate gathering empirical data on the time to travel and modes of travel for people living \nwith disabilities. This information can then be used to compare differences between the general public and those with disabilities – not only for functional access to place types but more \nimportantly for how the spatial relationship to these place types influences social satisfaction. \nThe study can contribute to a wide range of fields, including landscape architecture, urban \ndesign,  urban  planning,  and  transportation  planning.  Yet,  landscape  architects  do  play  an \nimportant role in helping design access to a range of different place types, particularly greenspace and open space. With this work, we have established the importance of testing different \nmodels  to  determine  community  access  to  place  types,  including  outdoor  recreation.  This \nwork provides a means to connect accessibility and the design of urban spaces, to create more \nsuitable and equitable access to different place types for all citizens. \n\nEroding Terrains: Developing Computational Design \nTools for Interactive Site Erosion \n\nAbstract: Landscape erosion processes can be problematic and are universal in their effect on all forms \nof landscape contexts and conditions. Hydrological erosion processes are important features of ecologies, yet are often extremely problematic, and can be exacerbated by climate extremes, weather events, \nanimal and human activities, and especially transformations through agricultural processes. This research documents and proposes computational design tools and methods for erosion simulation in real-world scenarios. While there are many examples of soil erosion modelling in the life sciences and engineering fields, they are rarely applied at the detailed scale of the landscape-, architecture- and design \ndisciplines. The work  attempts  to  leverage erosion  processes  for  design  by creating  new  workflows \ninside familiar design and modelling programs. Applications may vary between agricultural land and \nareas of accelerated climate change, however, the test case for this application is in a fire-affected landscape particularly prone to erosion. This research seeks to unite site investigation and survey techniques with interactive erosion modelling within AEC design software. By introducing intuitive ways to model erosion processes mitigation becomes possible within the landscape analysis and design process, creating opportunities to avoid erosion before it occurs. \n\nIntroduction \n\nErosion is a fundamental landscape process that underlies all landform generation. In combination with transport and sedimentation, it is integral to all landscape processes and their \ninhabited ecologies (KONDOLF 1994). Despite many advancements in the various ways in \nwhich humans have formed and manipulated the earth, erosion remains process we still struggle to work with. How we counter the degenerative effects of erosion have barely changed \nover  the  last  century  (BATES  &  ZEASMAN  1930).  This  research  is  particularly  focused  on \nhydrological erosion as one of the key forms of erosion affecting landscapes worldwide at an \naccelerating rate due to climate change and land-use practices. The perceived demand for \nsuch techniques comes from both an observed lack of such analyses executed in the AEC \nindustries,  and  the direct demand for  such  methods  from  within  parallel  research projects \n(MELSOM 2022).  The  parallel  research  projects  inform  this  case  study  and  initial  practice \nmodel for this technique, namely the specific and heightened erosion issues faced by post-fire landscapes, although the techniques are equally applicable to a wide range of other landscape and built environment circumstances, such as disused agricultural and cleared landscape plots, de-vegetated drought-affected areas, and building construction sites. The work \ndocuments the current progress in developing specific tools for common erosion models at a \nlocal site scale, leveraging high-resolution user-generated site models. \n\nRecent shifts in weather patterns, storm event frequency and magnitude, connected with on-going climate change exacerbates the loss of soils as a key societal issue that already dates back  centuries  (MONTGOMERY  2012).  It  commonly  results  in  the  loss  of  arable  land,  increased landscape disturbance, the destruction of ecological systems, as well as negative effects on the built environment. Erosion simulation is an answer in the field of civil engineering, providing insight to where and how it might occur. Large to medium scale modelling is widespread, and often leverage GIS or proprietary and specialised modelling software solutions (MAY et al. 2005, ARGENTIERO et al. 2021). Furthermore, there is also some scepticism in the relevance and accuracy of such simulations at territorial scales (MONDA et al. 2017). Nevertheless, these models tend to focus on the territorial or catchment scale, distinctly abstract from the detailed site scale and restricted to the realm of specialised engineering applications and computation intensive instrumentation (KANITO & FEYISSA 2021). \n\nAt the design scales there are clear and compelling examples of methods that propose to work \nwith avalanches and sedimentation events instead of against it. Here materials are redistributed as they erode or arrive on site (HURKXKENS 2021). The potential to combine detailed site data with predictive or preventative erosion modelling provides many compelling avenues for landscape management, generative possibilities, effective hybridisation of erosion, \nsedimentation, and design. \n\nIterative Erosion Modelling \n\nHydrological erosion types follow several generally established patterns and types, each often the precursor for the next: splash, sheet, rill, gully, bank and stream, ordered by increasing \nscale. Rill and gully erosion have been isolated as the most useful for this research, to generate a simplified simulation tool. Existing specialised applications in earth engineering have \nformed  both  the  basis  for  this  selection  and  a  model  for  confirmation  of  applicability \n(HANCOCK et al. 2008). As can be seen in such precedents, high-resolution site data is required \nin order to generate relevant results. Detailed, recent models are a necessary starting point, \nwith medium to high-resolution laser scanning or photogrammetry a base requirement. Due \nto the nature of rilling scale erosion sites, photogrammetry is particularly interesting as it excels \nin open ground, un-vegetated areas, allowing a consistent accuracy of 10cm down to 2cm. \nOpen agricultural fields, mining landscapes, and post-industrial sites are suitable examples \nof high-resolution photogrammetry subjects, or in the case of this research, post-fire affected \nsites, cleared of foliage and vegetation canopy. The site of Rosedale, NSW, Australia was \nchosen as an ideal candidate for such a fire-affected erosion modelling scenario (Figure 1). \n\nA simplified model closely mimics the established model (KANITO et al. 2021, HANCOCK et \nal. 2008) yet allows for a close to real-time feedback loop, and the integration of iterative \nworkflows. To this end, Rhinoceros was chosen as a base software, with the integration of \nscripting and plugin development to provide a seamless connection with three-dimensional \ndesign  software  that  is  both  intuitive  and  an  industry  standard.  Rather  than  analysing  the \nmodel  outside  the  design  software,  it  is  compatible  with  other  AEC  design  software  and \nmethods, without interrupting the design process. It is also compatible with GIS software and \nvarious spatial data types. This simplified process can integrate other landscaper factors such \nas soil characteristics, barriers, and vegetation to improve the accuracy of the simulation further. \n\nThe concept of iteration is also considered an important simulation criterion within the design \nspace, in this case, differentiated into two iterative models, integral and event iteration: \n\nIntegral  Iteration  describes  the  case  in  which  the  eroded  model  accounts  for  erosion  and \ndeposition  within  the  same  continuous  modelling  cycle,  with  eroded  areas  having  a  compound effect on their continued erosive processes, rather than acting on existing site characteristics alone, and allowing for changes to the site to be made during simulation.  \n\nEvent Iteration modelling allows for separate events, with a shift in intervening characteristics (vegetation, topography, physical intervention). This model allows for multiple hydrological events to take place separately, with shifts in the intervening period. This is an im-\nportant factor in many erosion-prone landscapes, as long-term erosion effects are often generated through multiple or repeated erosion events that are incremental, rather than occurring \nin one discrete event. \n\nTerrain Characteristics also play a key role in understanding the processes of erosion. Many \nfactors affect its course over the surface in landscape terrain models and have therefore been \nincluded in the erosion model, with varying levels of integration (Figure 2). \n\nSoil Texture is a key characteristic in erosion models. Different soil types erode and deposit \nat varied rates, and the standard simulation technique of a reference raster image has been \napplied. \n\nTerrain Slope and rate of slope change are fundamental erosion characteristics, regardless of \nsoil type, as they affect the speed and acceleration\/deceleration of water movement and there-\nfore energy of the water, and its propensity to either erode or deposit material. To this point, \nonly slope angle has been implemented.  \n\nTerrain Roughness at this site scale. Here individual terrain details such as vehicle tyre tracks, \nanimal marks and soil or rock texture can have a huge influence on erosion patterns. Not to \nbe  confused  with  soil  texture,  additional  roughness or  triggers  to  erosion  can be  included \neither as proxy mesh, with noise, or as a raster image. \n\nErosion Resistance is the demarcation of areas that physically cannot erode or are otherwise \nresistant to erosion due to solid barriers, vegetation, root systems, or other physical hindrance. \nThese areas can be either physically modelled or marked with images, allowing a graduated \neffect. \n\nEach of these terrain characteristics has been integrated into the workflow to form a working \nmodel for a limited range of case studies (Table 1). Surface water flow calculation is based \non detailed DTM data and simplified erosion simulation using reference parameters implemented with the computational terrain modelling plugin. Preliminary simulations assume a homogenous substrate, although the overall resistance characteristics to erosion can be manipulated. Additional testing and verification would be required for broader applications with accurate and repeatable results. \n\nSurveying and Erosion Modelling Tool \n\nThe implementation and testing of the erosion simulations have centred around specific post-fire landscapes. These make appropriate sites, as they are often immediately susceptible to \nreal erosion following an intense fire event. Erosion in these landscapes presents a massive \nissue for many authorities and communities. Analysis conducted in such landscapes has determined that up to 50 times more erosion can take place in an extremely fire-damaged landscape than in one marginally affected by fire, as well as many other mitigating factors (TULAU et al. 2015). To survey the affected terrain, there are precedents for UAV imagery and its use in general erosion mapping and simulation (MISTHOS et al. 2019). This survey method functions well in circumstances where erosion risk is high, due to lack of ground cover and exposed terrain. It also enables successive landscape surveys and allows for the mapping of landscape change and subsequent model adaptation. In addition, reference layers for erosion \nresistance can be generated from this data. In this case study, industry-standard photogrammetric  software  –  Agisoft  Metashape  –  was  used  for  this  stage  of  research,  however,  the \ndelineation of the scanned site and predetermined route may aid in generating more accurate \nresults with repeated scans of the same site, documenting its evolution and supporting iterative model generation. The required resolution and detail of the test site required a relatively \nlow altitude (40m) scan height with a high overlap of 80% in both directions, with an angled \ncamera (75°) flying in a gridded pattern around tree-bases (a common area of photogrammetric error). \n\nThe  simulations  are  built  on  top  of  Docofossor,  a  terrain  modelling  plugin  for  the  visual \nscripting environment Grasshopper of Rhino 3D (HURKXKENS et al. 2019). It uses regular \nraster grids as underlying data-model for its terrain representation. This enables simple modelling operations in cut and fill using distance functions. Like the plugin, the erosion simulations make use of build-in class methods from RhinoCommon or via the Rhinoscript python \nimplementation to compute the grid values.  \n\nThe implication allows for animation and simulation of any part or any duration timeline. For \ndetailed areas of the case study site (Figures 3) of around 50 x 50 m, animation frames can \nbe calculated in less than 5 seconds on a standard workstation set-up, facilitating an optimal \nsimulation-to-design workflow. When compared with large-scale surface water flow, the results  demonstrate  that  a  laminar  surface  concept  of  layered,  continuous  water  supply  and movement works well to imitate site-observed erosion phenomena in scale, extent, and pattern (Figure 4). Within the chosen case-study typology of fire-affected areas, there are ample examples and areas for verification and refinement of the erosion models in various soil and \nsurface conditions. The varied performance in differing substrates is an area for further research and study. \n\nDiscussion \n\nThe resulting model combines a cleaned, photogrammetric terrain model collected on site, \nwith a scalable yet detailed simulation of rill-model erosion. This can form the basis for site \nselection, risk analysis, or developing of erosion mitigation strategies in high-risk areas.  \n\nThe various data elements produced during the described process consists of base data and \nsite analysis, as well as simulation data, such as the resulting flow-paths, erosion \/ deposition \npatterns and debris transport lines. The results reinforced the individual and combined roles \nthat terrain roughness, converging slope details, and obstacles such as trees, fallen tree trunks, \nand rocks play in the resulting site-specific example. Site observation reflected the general \ntrends of the erosion model; however, additional calibration can be applied to further refine \nthe exact volume of material transported. The nature of the site material, being a mixture of \nfine ash, partially fire-consumed debris and dry topsoil that are non-uniformly distributed on \nthe terrain lead to a model in which exact depths and volumes of material erosion and depo-\nsition are variable. Therefore, the erosion paths and patterns can be shown to have a higher \nfidelity than the depth of erosion. As referred to in the conclusion section, additional material-specific experimentation or specialist data would help to refine these models.  \n\nThe potential for iteration in this process, both as a computational tool in simulation, and a \nworkflow for gradually improving and refining the model worked well, especially given the \nresponsiveness  of  the  algorithm.  The  proposed  additional  applications  for  iterative  design \nproposals  using  the  same  process  are  feasible  in  both  implementation  and  viable  design \nmethod. \n\nConclusion and Outlook \n\nThe  relevance  and  importance  of  erosion  simulation,  and  its  challenges  within  the  design \ndisciplines are established, and the case is made for its viability and implementation. The \nlack of a designer-level toolset to deal with these issues has been addressed in this research, \nas well as the potential of these techniques for the AEC professions. The range of further \napplications and possible sites is only increasing with the growing impacts of climate change. \nSuch simulation techniques can be deployed to predict erosion issues that may occur in the \nfuture  and  allow  for  pre-emptive  interventions  that  may  avoid  or  transform  such  erosion \nevents into more positive outcomes. Within the space of fire events alone, there is a huge \nscope for adjusting landscape management practices to better recover from fire events, especially their implications for soils, sediment, and the fostering of their landscape biodiversity \n(TULAU et al. 2015, ATKINSON et al. 2020). \n\nThe imitation of real-world site conditions and recorded erosion sites are of key importance \nto further refine the plugin settings based on local conditions and the predictive accuracy and \nusefulness of the technique. Similar optimisation tools such as RAMMS have also been carried out both during and after the release of landslide and rockfall computational simulation \n(CHRISTEN et al. 2012).  \n\nFurther development is required to optimize and better simulate existing high-resolution simulation  models,  as  well  as  the  integration  of  these  techniques  into  teaching,  research,  and \npractice (IGWE et al. 2017). There is a strong potential for generating data layers for other \napplications and GIS systems and facilitating new forms of engagement and multidisciplinary \ncollaboration in landscape engineering, remediation, and management projects. Where erosion can be predicted and mitigated, the potential for design with erosion processes emerges. \nThe possibilities of hybrid design processes that work hand in hand with the environment, \n(GIROT & HURKXKENS 2018) open areas of potential design endeavour, in which the landscape can be shaped over time with minimal intervention and resources. \n\nExploring Less Geometric Landfill Slopes through \nParametric Digital Modelling \n\n\n\nAbstract: Massive and visually disruptive landfills in urban areas can potentially be seen by hundreds \nof thousands of people daily. Even after landfill closure, constructed slopes and ridgelines can contrast \nwith the surrounding terrain because of their signature geometric form. This paper uses three landfills \nin Southern California to demonstrate the need for better visual mitigation, test the sculpting of landfill \nslopes through parametric digital modelling, and then discuss how the process can be enhanced for real-world application that improves visual quality while meeting engineering requirements. This is an area \nwhere landscape architects can make greater contributions in mitigating the visual impacts of landfills. \n\nIntroduction \n\nLandfills are the primary way that non-recyclable municipal waste is managed. Incineration \nis eschewed due to the introduction of carbon particulates and a harmful airborne brew of \npotentially carcinogenic chemicals associated with plastics and other modern manufactured \nmaterials. The ever-growing volume of waste is also a major concern and landfills can be \nmassive. Besides the large physical dimensions of landfills, the process of land filling requires a substantial investment in time, expense, and effort to locate suitable sites, meet stringent permit requirements, prepare the site for liquid containment and methane gas extraction, manage daily fill operations, and mitigate a full range of impacts. For these reasons, the trend is towards fewer, but larger landfills (EPA 2014, 2-11). \n\nMany landfills are geometric in shape and the planar sides and mesa-like top can be recognized from miles away. In urban areas, the number of viewers can be numbered in the hundreds-of-thousands, and unsightly views or the presence of landfills can negatively impact property  values  ranging  from  3-7%  (REICHERT  et  al.  1991,  BOUVIER  et  al.  2000,  READY \n2005). Moreover, the scale of urban landfills can be dominating. For example, Puente Hills \nlandfill in Southern California, which closed in 2013, has a footprint of 283 ha (700 ac) and \nis 150 m (490 ft) in height. Counting buffer land, the facility consumes 526 ha (1,300 ac).  \n\nObjective \n\nLandfill design and operations generally fall within the realm of engineering and scientific \nconsultants.  Landscape  architects  become  involved  when  considering  landfill  aesthetics. \nTypically, these activities are related to landcover planting and the preparation of visual analyses and simulations when preparing environmental impact documents prior to landfill permitting. As “shapers of land”, the objective of this paper is to explore how landscape architects might become more involved in the earlier stages of landfill design through enhanced \ndigital modelling so the landfill shape upon closure can better blend with the terrain context. \n\nExtending the Role of Landscape Architects? \n\nThe origin of this paper derives from the primary author’s environmental consulting work \npreparing visual assessments for two landfills in southern California: Elsmere Canyon Landfill (early 1990s) and Simi Valley Landfill Expansion (mid 2000s). In both cases, the landfills were  of  the  canyon\/valley  type.  Extensive  3D  computer  modelling,  GIS-based  viewshed mapping, and before\/after photo simulations (Fig. 1) were conducted to determine visual exposure and estimate visual impacts as viewed from key observation points (KOPs). These points were public gathering areas like parks, major travel ways, and nearby residential and commercial areas at distance ranges from 0.3 to 8.5 km (0.2 to 5.3 mi). \n\nIn the case of Elsmere Canyon Landfill, the permit was denied after much public opposition. \nSimi Valley Landfill was already an operational landfill in mid-life, and the expansion was \napproved after a multi-year environmental review process which addressed public concerns. \nEven though the expanded landfill conformed to conventional engineering design, the pri-\nmary author wondered if landfill slopes could be made to appear less geometric and better \nblend with contextual terrain. Instead of involving landscape architects to assess or mitigate \nvisual impacts after landfill design is nearly complete, the role of landscape architects could \nbe  expanded  to  perform  landform  sculptural  studies  earlier  in  the  design  process.  Closely \ncoordinating  with  engineers,  slope  sculpting  would  still  need  to  meet  fill  volume  requirements, access road routing, methane gas piping, and comply with efficient daily operations. \nThis paper only explores landform, and does not address vegetation, atmospheric conditions, \nor other factors affecting visual quality. \n\nConcept Overview \n\nEnhanced Slope Sculpting to Reduce Visual Impacts \n\nThe goal of the “sculpting” process is to introduce more slope undulation into uniform slopes \nto replicate convex finger ridges and concave drainages found in contextual terrain, but to a \nlesser  degree  to  still  support  engineering  requirements.  This  will  increase  tonal  variation \n(shade\/shadow)  patterns  which  will  better  blend  with  contextual,  undisturbed  terrain.  As \nviewing distance increases and atmospheric factors become more pronounced, tonal contrasts \nare more important to visual mitigation compared to texture or hue variations. \n\nSlope Sculpting Procedures \n\nLandfill form emerges as systematic lifts (layers) approximately 8-20 feet thick. Each lift is \ncomposed of cells where daily to weekly accumulation of refuse is compacted and covered \nwith 6” of soil. Once cell placement reaches the perimeter of the lift, the outer slope is shaped \nat a not to exceed 1.5:1 ratio. A series of 15’ wide benches are also added per EPA regulations \n(EPA 1988, 62). Under the sculpting concept, none of these standard filling and grading operations would be appreciably altered until the lift edge nears. At this point, GPS-enabled earth moving equipment would grade an undulating edge, that over years, would emerge as finger ridges or drainages on the slope much like 3D printing. The precision of GPS is essential to accurately locate and place cover material along the undulating lift perimeter where \nthe eventual slope form is not immediately apparent. \n\nTowards this goal, several additional steps are needed beyond traditional engineering design: \n\nNumerically determine the slope gradient and vertical\/horizontal convexity of the surrounding  topography  (usually  applicable  to  canyon\/valley  landfill  types)  for  use  as  a \ncontextual referent. \n\nIteratively sculpt a 3D landfill computer model where exposed sides more closely replicate contextual slopes and topographic features, and then shape a rounded cap or ridgeline profile that undulates as opposed to a flat mesa. The model footprint may have to be slightly expanded to offset anticipated volume losses compared to conventional geometric forms, or the overall height increased (LAW et al. 2008). \n\nTransfer the preliminary sculpted model into Civil 3D or other engineering software for detailed design and implementation documentation. \n\nPrepare a grading plan that can be uploaded into GPS-enabled refuse\/earthmoving equipment to guide landfill slope shaping over decades. \n\nMethods \n\nThere are multiple  methods to analyze  undulations in topographic surfaces to set numeric \nbase conditions for slope modelling: slope aspect and gradient, planform curvature, profile \ncurvature,  topographic  openness,  and  landscape  roughness.  Some  numeric  techniques  include fractal dimension indexing (FRAC) (CUSHMAN et al. 2005, 103-104; MCGARIGAL & MARKS 1995), standard deviations of contour line segments, and topographic position indexing (TPI) (JASIEWICZ & STEPINSKI 2013, MOKARRAM & HOJATI 2016).  \n\nTo identify a landfill as a test case for parametric digital sculpting, Zhong (2020) inventoried \n43 landfills including 14 active and 29 closed landfills larger than 100 acres in Los Angeles \nCounty. As part of the review, FRAC indices were calculated for the landfills to assess how \ngeometric the slopes appear and identify candidate landfills for further analysis. \n\nAfter candidate landfills  were reviewed, attention  turned towards  which digital  modelling \nsoftware might be most useful. WESTORT (2015, 225-226) discusses the need for improved \nlandform design tools which are 3D, provide geometric control, are easy to handle, provide \nquick response time, and are quantitatively accurate. Furthermore, the ability to iterate before \nand during the construction [or design] phase is desirable. From our experience, Autodesk \nCivil 3D meets most of the criteria for Digital Elevation Modeling (DEM) but is deficient in \ninteractive surface sculpting. A spline modeler like Rhino is better suited for this purpose and \noffers parametric automation through Grasshopper terrain plug-ins like Docofossor, Bison, \nand TOPO kit. Upon initial review, it appears that these plug-ins do not offer the sculpting \nfeatures\/control as envisioned without additional customization. \n\nTo test how inclined finger ridges can be introduced to geometric landfill slopes while still \nmaintaining landfill capacity, ZHONG (2020) used the closed Puente Hills Landfill to proto-type  a  hybrid  manual-parametric  landform  sculpting  process  using  a  customized  Rhino\/ \nGrasshopper script using 3D control framework (Fig. 2). Preparatory work consisted of man-\nually digitizing major ridgelines, finger ridges, and intervening drainage flow lines from the \nundisturbed  1950  topography  as  a  fully  detailed  referent  of  pre-landfill  conditions.  Points \nfrom this skeletal landform structure were then filtered through a Grasshopper script to interactively reduce ridge and valley point detail as a percentage. Two highpoint locations from \nthe 2018 landfill top deck (or intended height of a planned landfill) served as landfill closure \n(2013) height parameters. Once parameters were set, a simplified surface was interpolated \nthrough the points. Using various combinations of 22%, 66%, and 88% remaining ridgeline \nand  valley  points,  seven  surfaces  (P1-P7)  were  generated  for  comparison.  Processing  per \niteration took about 4-6 hours. \n\nResults \n\nAfter the manual-parametric methods were established to generate landform alternatives, further modelling exploration was undertaken. For comparison against standard landfill design, \ntwo planar geometric surfaces (G1-G2) and three advanced contoured surfaces (A1-3) were \nmanually defined through contours and generated through Rhino. The G1 and G2 surfaces \ntypified landfills of low visual quality and the A1-3 surfaces represented enhanced landfills \nhaving some amount of slope undulation (Fig. 3). \n\nFig. 3:  Results  were  compared  for  manual  geometric,  manual  advanced  contouring,  and \nparametric  modelling  of  landfill  configurations  against  1950  existing  topography \nand the 2018 shape configuration of the Puente Hills landfill (ZHONG 2020) \n\nOnce the 12 alternative landform  surfaces  were  modelled in Rhino, the surfaces  were exported into Civil3D for volume calculations. Two sets of volumetrics were compared: 1) the \n1950 pre-landfill referent surface compared to the 12 Rhino alternatives (2020) for total volume capacity; and 2) the 2018 DEM dataset (2013 closed landfill conditions) compared to \nthe same 12 Rhino alternatives. The latter comparison is intended to directly reveal net volume  gain\/loss by introducing  more  undulations to constructed landfill  surfaces.  Of the 12 \nalternatives, three showed capacity gains:  A3 (+5%), G2 (+24%), and G1 (40%). Surface \nundulations for the closed landfill and among the 12 alternatives were also compared through \nslope mapping and aspect mapping which quickly made differences visually evident. \n\nDiscussion and Conclusions \n\nModelling results reveal that although simplified, the P1-P7 parametric surfaces too closely \nresemble the complexity of the 1950 topographic referent. P1-P7 volume capacity was not \nsufficient, slope angles were not constrained to the 1.5:1 standard (not part of script), and \nexcessive slope undulations\/aspect variation would likely make implementation difficult. As \nexpected, introducing more surface undulations decreased landfill volume capacity for most \nalternatives compared to the more geometric landfill forms. \n\nThrough this exploratory test, however, advancements were made in parametric surface modelling that enabled rapid iteration, testing, and comparison of landfill alternatives. The A3 \nalternative demonstrates that more slope undulations can be introduced to improve landfill \naesthetics while still maintaining capacity volume. Rapid iteration, as demonstrated through \n12 alternatives, is essential in finding the right balance of improved aesthetics, capacity volume, and other engineering factors to be tested in future work. \n\nResults demonstrate the potential of applying digital sculpting tools to enhance landfill slopes \nto make them appear less geometric and planar. Artistic manipulation must be coupled with \nengineering requirements to maintain slope stability, constructability, and volume capacity. \nGPS enabled refuse\/earth moving equipment can provide the precision and locational accu-\nracy across large lift expanses to achieve more naturally appearing “outer shell” forms. \n\nMaking progress to sculpt landfill surfaces iteratively and more freely for aesthetic purposes \npartially fulfilled the initial objective of this paper. Full realization of the objective requires \nlandscape architects to better understand the complexity, timing, and workflow commensu-\nrate with landfill design and operations if they are to be involved. Based on past professional \nexperience  with  landfills,  many  engineers,  technical  consultants,  regulatory  agencies,  and \nenvironmental assessment  specialists are involved, and the  design and permitting requirements are substantial. Reducing visual impacts is a worthwhile goal but knowing when and \nhow optimized landform modelling with a greater emphasis on aesthetics can be introduced \ninto the design process is challenging. To be cost- and time-efficient, it needs to be used early \nin the process, offer rapid iteration, meet engineering requirements, and then be passed off to \nothers for technical refinement. At the landfill operational stage, extra edge\/slope requirements must also be safe and compatible with the myriad of choreographed activities taking \nplace on the working deck. \n\nSeveral limitations are evident: more documented research is needed regarding the long-term \naesthetic  impacts  of  landfills  upon  closure  after  vegetation  has  matured;  more  parametric \ncontrols are needed for slope shaping, the Rhino to Civil 3D transference needs to be more \nstreamlined; and most importantly, a test case involving engineers needs to be identified. \n\nFuture Work \n\nFuture improvements are needed to incorporate more parametric control of slope undulation \nand shaping. Additional ridgeline control is also needed for shaping the landfill cap to avoid \na mesa-like appearance. A hybrid between the P and A models is envisioned. \n\nIn addition to ridgeline controls, select parametric contours (splines) could be added as control  features  to  parameterize  surface  undulation.  Parameters  would  be  based  on  sinuosity \nwhich is simply calculated by dividing the sinuous contour length by the straight distance \nbetween the contour line endpoints. Calculating sinuosity is typically associated with stream \nsystems but can also be applied to characterizing landfill slopes where FRAC and TPI indices \nare too general to control shaping. Referencing the sinuosity index of contextual landform, a \nfew parametric contours placed at strategic locations at the edge of landfill lifts could seed \nthe formation of finger ridges. The finger ridges would become more apparent as the landfill \nheight grows with each successive lift much like 3D printing. \n\nDifferences in slope sinuosity can be illustrated using existing portions of the Lopez Canyon \nlandfill in Sylmar, California (Fig. 4). In this 3D view of the 2016 DEM surface, a contour \nline (L1) traces undulating slopes of the contextual foreground slopes, whereas the more linear contour line (L2) traces the constructed geometric slope of the landfill face rising above \nthe foreground ridge. The calculated sinuosity indices (SI) are 1.44 and 1.19, respectively. In \na revised parametric model, the SI of L2 could be adjusted to resemble the undulations of L1 \nmore closely while still allowing for proper slope benching, access road construction, and \nmethane gas piping. This will be tested as the Rhino\/Grasshopper script is improved. \n\nThese future improvements should enhance modelling capabilities, increase ease-of-use, and \nprovide better integration with software used to prepare construction documents. Discussions \nare also needed with landfill designers with regards to landfill operations, sequencing, and \noverall feasibility of this envisioned approach to landfill aesthetics and visual mitigation. \n\nParametric Planting Design: Algorithmic Methods \nfor Resilient Communities \n\nAbstract: Parametric applications in landscape architecture are gaining traction as designers realize the full potential of script-based analysis in various stages of design. Planting design is one realm of parametric landscape architecture that is traditionally done manually with books, websites, or other research on hand, thereby keeping its application within the grasp of landscape designers. This discussion proposes a method of using algorithmic design to analyze and specify plant species based on four different measures. Further, it is possible to expand this method in the form of a browser-based program for non-designers to take part in resilient landscape planting. \n\nIntroduction \n\nThe origin of computation-driven landscape analysis and design is often attributed to Carl \nSteinitz’s 1966 land evaluation and the SYMAP print of the Delmarva Peninsula (STEINITZ \n2014). Subsequent decades brought advancements in processing power and user interface, \nallowing a variety of software to gain traction in the design field including Photoshop, AutoCAD, and specifically for landscape architects, LANDCADD (ERVIN 2020, MACDOUGALL\n1984). ERVIN (2020) also describes the rise of optimization software and use of algorithmically-generated landscapes in response to the formation of the internet (ERVIN & HASBROUCK\n2001). As such, the success of spatial design computer applications has led to at least a partial \nreliance on digital workflows in the design process, if not a large portion of the work. \n\nIn an effort to explore emerging landscape architectural frontiers, designers echoed Steinitz’ \nexperimentation  and  began  programming  new  tools  for  greater  flexibility  in  their  work \n(CANTRELL &  MEKIES 2018). Development of programs continued with some tools being \ncodified  as  permanent  sub-tools,  such  as  Grasshopper  within  the  3D  modelling  software \nRhino. \n\nGeneral investigations of parametric landscape design problems can exist in the form of blog \nposts  (GENERATIVE LANDSCAPES),  online  videos,  and  academic  papers  (SERDAR & KAYA\n2019). Commercial tools have also been created and added to aid in the digital landscape \ndesign process (LANDKIT). Notable built projects include Eda U. Gerstacker Grove by Stoss \nLandscape Urbanism at University of Michigan, where student desire lines, drainage, and \nparametric bench profiles were incorporated into an algorithm to generate a site model that \nresponds to and supports the pedestrian experience (REED 2018). \n\nExisting Research \n\nWhile parametricism in landscape architecture is an ever-expanding area of study, limited \nresearch has been conducted on the use of algorithmic workflows regarding ecological factors including species selection and planting location. \n\nA thesis by Roasliina Luminiitty (2021) from Aalto University examines parametric planting \ndesign’s integration in the landscape design progress. LUMINIITTY (2021) analyzes prior software used for landscape design, as well as more modern investigations into algorithmic landscape architecture. The paper offers a detailed explanation into the components of parametric \nplanting design and offers a framework for digital planting design workflows, which can be \nexpanded further with the inclusion of measurements and real-world data to inform the final \nresult. Parametric patterns can be tailored for design continuity and be ecologically developed \nby an algorithmic plant selection process to create a holistic planting concept. \n\nOLIN Lab’s Tech- and Eco Labs have also developed research into digital workflows for \nplanting design. The process utilizes AutoCAD, Rhino, Grasshopper, Python, LandFX, and \nAdobe Suite products to derive functional planting plans for use in a landscape architecture \noffice setting (AREVALO 2020). The process is broken down into four parts: (1) Investigating \nplants as living material, (2) Speculating and experimenting with parametric components, (3) \n3D Spatial and aesthetic analysis, and (4) Seasons and time. AREVALO (2020) states that this \nworkflow was successful for the office-side design process, but documentation was still prepared manually, which requires work by a landscape architect. \n\nLandKit is a Grasshopper plug-in developed by LANDAU Design+Technology that creates \ncustom  components  in Grasshopper  for  users  to  more  effectively  design  fundamentals  including with specific components called TopoKit, PavingKit, and PlantKit (LANDKIT). What \nsets PlantKit apart from other parametric planting tools is that it does not automatically assign \nspecies to the plants it generates, but rather establishes certain plant typologies to be determined later by the user. The plant typologies refer to similar sizes, environmental considerations, and biodiversity, which gives the end user more agency when specifying species and \naccounts for regional climate variations. \n\nThe creation of these products is testament to years of dedicated research into accurate and \nefficient planting algorithms, for use in a landscape architecture office. The development of \ndifferent algorithms and tools has taken off regarding landscape design and will continue to \nbring new and improved iterations into the field. However, this computation-driven analysis \nis mostly locked behind software with intense learning curves and high price points. With the \ngoal of simplifying communication between designer and computer, parametric application \ndevelopers should also strive to lower the barrier of entry for the use of these tools. \n\nDigital Equivalents to Planting Design Concepts \n\nBefore attempting to develop any parametric tools, it is critical to understand the concepts \nbehind existing ecologically sound landscapes. Traditional landscape design practice includes \nresearching native or naturalized plants in a specific region, while placing them in a pattern \ncorresponding  to  a  design  intent.  This  varies  from  project  to  project,  but  is  generally  the \nformat of the planting design strategy. Many different categories of planting information exist \nfor each plant, and can be addressed through different ways to fit the project’s goals. \n\nQualitative information exists as a subjective rationale in landscape design. This may include \naesthetic considerations, natural plant communities (dependent on location and ecosystems), \nand features included on a site and how the site functions as a whole. In the digital realm \nthese  cannot  be  quantified,  though  studies  have  investigated  various  methods  to  evaluate \nlandscape perception (KARMANOV 2009). Use of qualitative data or input is still important, \nhowever, as it establishes “the meaning individuals or groups ascribe to a social or human \nproblem (CRESWELL 2014). Use of this type of information can be presented in the setup of \na project, or change with user preferences. \n\nQuantitative information is a numerical type of data representation where all possible results \nare accounted for, and often presented numerically. In landscape architecture, common uses \nof quantitative data are found in climate data, topography, and geotechnical properties. Data \ncan also be extracted from the site itself through analysis tools and simulations, unearthing \nunderlying layers of data otherwise hidden. \n\nPlanting for a Post-Wild World (RAINER AND WEST 2015) describes the structure of a designed landscape through a series of layers: a structural layer, groundcover layer, seasonal \nfiller layer, and dynamic filler. The clear distinction between individual plants work together \nto  form  the  identity  of  a  garden  which  can  establish  character  even before  more  complex \ndesign motifs take place. Additionally, thinking of plants in a binary manner lends itself to \ndigital applications where a machine must be programmed to receive input and output information in a highly controlled manner. \n\nRAINER AND WEST (2015)  also  signify  the  importance  of  employing  ecological  strategies \nwithin plantings to promote resilience in plant communities. “Resiliency” is a commonly used \nterm referring to the ecological health of a landscape and its ability to recover after periods \nof distress (RAINER & WEST 2015), but it also can be interpreted as a human community’s \nability to recover from a disturbance (FLINT 2010). Given the difficulty of evaluating a multifaceted  concept  such  as  resilience,  we  can  instead  look  at  establishing  environmentally \nsound  starting  points  (namely  regionally  accurate  plant  spreadsheets)  in  order  to  generate \nresilient planting communities. \n\nSoftware for this algorithm process utilizes the Grasshopper tool in Rhinoceros 3D. Because \nthis software allows for algorithmic design approaches, a single input can trigger a variety of \nsubsequent processes and analyses, culminating in an algorithm-derived final product. \n\nPreparation for this tool included development of a plant list extracted from eastern Nebraska \nnursery stock listings to ensure success in the Nebraska landscape. In total, 198 unique species \nand 80 cultivars or varieties were identified and constructed in a spreadsheet with important \ninformation such as mature height and width, shade tolerance, salt tolerance, drought tolerance, bloom timings and color, nativity to Nebraska, and hardiness zone range. All plants \nwere parsed based on their landscape function: overstory conifer, overstory deciduous, understory conifer, understory deciduous, perennial, tall grass, groundcover, and annual. \n\nAlgorithm \n\nGrid \n\nA  site’s  surface  can  be  divided  into  a  grid  of  any  size,  though  standard  1-,  2,  and  5-foot \nsquares work best. The grid allows for easy analysis of site features such as elevation, edge \nproximities, and sun exposure. The ideal scale for this depends on the overall scale of the \nsite, with smaller sites requiring a higher level of detail. The ideal resolution for a 7,000 sq. \nft.  (650  m2)  site  can  be  2  feet,  while  larger  sites  may  need  5-foot  resolution  to  minimize \ncomputing time. \n\nPlant and Environment Scoring \n\nA surface in Rhino is referenced in Grasshopper where four analyses take place: Elevation, \nShade, Salt intensity, and Structure proximity (Fig. 1). The interplay between envi-\nronmental factors plays a large role in identifying a “best fit” species for a particular location \non a site (CZAJA et al. 2020). Elevation analysis takes note of local high and low points on \nthe  site,  and  corresponds  with  low  points  requiring  less  drought  tolerant  plants  and  high \npoints requiring more drought tolerant plants. Shade analysis looks at sun exposure on the \nsite from existing buildings and trees to accurately place plants with regards to sunlight hours. \nSalt tolerance measures look at a point’s distance from paving surfaces or curves to account \nfor  road  salt  accumulation  in  winter  months  near  the  planting  surface  peripheries.  Lastly, \nstructure proximity  refers  to the distance between  a plant  and  a  structure, preferring  slow \ngrowth nearer to the structure to reduce risk of root damage to the foundation. \n\nThe  “environmental  scores”  (elevation,  sun  exposure,  and  salt  intensity) of each  potential \nplanting spot on the site are compared to each “plant score” of a particular plant typology \n(Fig. 2). The difference between the “environmental score” and “plant score” determines the \nresiliency of the proposed plant, with a lesser difference resulting in higher probability of \nresilience. It is possible for the algorithm to compare all 278 plants for every potential location, though it would leave too many options available for the user and result in an unclear \ndirection. Parsing plants into specific landscape structures provides opportunity for a better \nstructured landscape (RAINER & WEST 2015). \n\nFlexibility \n\nThe  uniform  analysis  and  subsequent  visualization  of  a  landscape  allows  for  a  variety  of \nplanting  regimes  to  occur.  Planting  locations  can  be  derived  from  the  algorithm  itself,  or \ndecided by a user making informed decisions from the data. \n\nAlgorithm-derived  planting  plans  can  be  applied  in  a  few  ways,  provided  a  distinction  is \nmade for the specific plant typology being used in each spot (overstory, understory, tall grass, \netc.).  Grids,  attractor  curves,  and  random  points  are  options  when  considering  automatic \nplanting proposals (Fig. 3). Further exploration in parametric design tools such as Grasshop-\nper may offer informed layouts with emphasis on user comfort and more complex designs. \n\nThe user  may  defer  to  stylistic  choices based on  a  desired  theme  (naturalistic  landscapes, \nEnglish gardens, etc.). For manual placement of plants, collections of points can be projected \nonto the site surface and analysis be drawn for those, where inputted plant patterns and sizes \nare matched to the “best fit” plant for that location. The variability of planting styles allows \nfor highly unique landscapes designed by the end user. To aid in this process, planting pattern \ndiagrams were developed to demonstrate the core principles of various landscape styles. A \nfew selected styles being presented to the user show successful patterns easily replicable from \nan amateur designer’s perspective and increase the appearance of legible design intent. \n\nAlgorithm to Browser-based Tool \n\nThis algorithm can be interpreted as a backend process for a planting design tool intended \nfor people inexperienced with landscape design. Given that the tool is able to suggest planting \nstrategies from topographic information, it is possible to derive this information from other \nsources using real-world data and leave the user with freedom to focus on designing their \nspace. Further, incorporating heterogeneous (containing structure and hierarchy, biodiverse) \nlandscape  design  in  homeowner-designed  landscapes  improves  landscape  perception \n(KHACHATRYAN et al. 2020). These positive attributes prompted the idea of a browser-based \ntool that can aid in the creation of a homeowner’s landscape design. \n\nTranslating a Grasshopper script to a programming language is a fairly straightforward task, \ngiven  that  Grasshopper  is  in  essence  a  visual  coding  language.  Python  and  Javascript  are \npopular programming languages capable of creating interactive maps, ones which users could \nuse to select site boundaries in their respective regions. An application programming interface (API) allows for communication between two or more computer applications, such as \nan individual computer requesting data from a large online database. Integration of Open-StreetMap (OSM) and United States Geological Survey (USGS) API allows for up-to-date \ninterpretation of landscapes with OSM providing location data, and USGS providing elevation data. \n\nOpenStreetMap is a free, open-source online mapping service that uses volunteer-provided \ninformation to gather location data, along with deriving maps from Bing aerial imagery and \nother mapping techniques (OPENSTREETMAP). Overpass API is a resource for an application \nto request read-only data in a variety of formats for any particular use due to the open-source \nnature of the service. To obtain specific site data, a bounding box is drawn over a map and \ncoordinate boundaries are established. The software requests any road and building geometry \nintersecting or within the boundaries from Overpass, which can be interpreted and shown in \nthe map view. \n\nThe National Map (TNM) is a project by the USGS’s National Geospatial Program to con-\nsolidate  downloadable  products  into  one  location  for  all  public  and  private  use  (UNITED \nSTATES GEOLOGICAL SURVEY). The TNMAccess API allows developers access to multiple \ndatasets, and will use the highest resolution dataset for the desired location request. To obtain \naccurate elevation data, the Elevation Point Query Service returns the elevation in requested \nunits at a specified latitude and longitude. Coordinates from the initial OSM bounding box \ncan be used to create a rectangular array of coordinate points and sent as a request to TNMAccess, where surface analysis can begin. Once a site is selected, the user can demarcate structures, roads, and potential barriers to plants that are present but not recorded to cull any areas \nincapable of supporting plant life. \n\nThe development of a user interface or user experience (UI\/UX) can lower the barrier of entry \nto individual landscape design. User interface is considered the format in which users see and \noperate the software, which should contain simple and concise language to explain the concepts at play in landscape design such as elements of analysis, plant environment descriptions, and list of results provided by the algorithm. This is considered front end development: \nthe side that the user is allowed to see. All the user’s inputs are relayed to the back end of a \nsoftware to be analyzed before a response is sent back to the front with a clear visual result \n(Fig.4). \n\nUser experience is the act of using the software and experiencing it through various steps to \nproduce a valid result. The flow of this process includes clear language to describe what each \ncomponent is and how it changes the result. This includes the language and response of any \nanalysis performed by the application. The goal of this experience is to provide the user with \na simplified approach to landscape design, performing site-specific landscape analysis in the \nback end to produce a tailored result for further consideration. \n\nDiscussion \n\nThe creation of a digital planting tool geared towards the general public provides an accessible platform that can elevate the ecological diversity and architectural quality of typically \nunderutilized landscapes. The use of this proposed process does not necessarily end with the \nindividual user in a single-family home setting, but can be applied in a broader application \nfor use in community organizations and people interested in improving the landscape of their \ncommunity spaces. \n\nWhile the project does accurately complete the task of planting design, it performs mainly as \nbackend development with complex inputs. Further exploration into user interface and user \nexperience front end could improve the clarity of language and process of the tool, and ultimately may be able to compile a custom document regarding maintenance and further resources for the end user for future planning. Further back-end analysis of landscape can increase the accuracy of the results regarding unique species preferences, or the variable shade from vertical layers of vegetation. \n\nCertain limitations apply to the accuracy and scope of an accessible planting tool, with datasets  needing  to  be  researched  and  formatted  to  a  uniform  spreadsheet.  One  approach  to \nexpanding this process into a United States-wide resource would be the use of the Federal \nHighway Administration’s Ecoregional Revegetation Application (ERA). This resource is a \ncompiled list of plant species and related information found in ecoregions across the entire \nUnited States, including Alaska and Hawaii (STEINFELD et al. 2007). \n\nThrough  this  framework,  it  is  possible  to  begin  the  development  of  a  tool  that  brings  informed, site-specific planting information to the general public. \n\nRobots in the Garden: Artificial Intelligence and \nAdaptive Landscapes  \n\nAbstract: This paper introduces ELUA, the Ecological Laboratory for Urban Agriculture, a \ncollaboration among landscape architects, architects and computer scientists who specialize \nin  artificial  intelligence,  robotics  and  computer  vision.  ELUA  has  two  gantry  robots,  one \nindoors and the other outside on the rooftop of a 6-story campus building. Each robot can \nseed, water, weed, and prune in its garden. To support responsive landscape research, ELUA \nalso includes sensor arrays, an AI-powered camera, and an extensive network infrastructure. \nThis  project  demonstrates  a  way  to  integrate  artificial  intelligence  into  an  evolving  urban \necosystem, and encourages landscape architects to develop an adaptive design framework \nwhere design becomes a long-term engagement with the environment.  \n\nIntroduction \n\nIn the discipline of landscape architecture, a major epistemological framework has assumed \nthat the environment is a closed and static system that can be measured, predicted, and conceivably controlled by technology (LYSTRA 2014). The reality of severe climate change challenges this view. Although advancements in industrial technology have given humans some control over their environment, carbon continues to be released into the atmosphere at unprecedented rates. While science rigorously measures and predicts the increasingly grim im-\npact of human decisions, quantities of computer-processed data and complex control policies \nhave yet to provide straightforward solutions to climate change. This paper argues for a research paradigm where artificial intelligence (AI) helps adapt landscapes to a changing environment rather than control them. It considers the role AI can play within this new focus on adaptivity, and how they can contribute to an adaptive design framework that requires a long-term engagement with the environment. \n\nELUA, the Ecological Laboratory for Urban Agriculture, offers a case study for an evolving \necosystem, embedded with AI, that responds to the uncertainties of a changing climate. In a \ncollaborative endeavor among computer scientists, landscape architects and architects, two \ncommercial gantry robots and an extensive infrastructure support cultivation in two polyculture  gardens (Figure  1).  Our  work  includes  construction  and  customization  of  the  robots, \nincorporation of sensor arrays, an AI camera, and network infrastructure, as well as the design \nand construction of the garden beds.  \n\nIn the past two decades, many landscape programs have built laboratories with machinery \nand a “lab culture” as both research and education infrastructure. Examples include Alexander Robinson’s work at the Landscape Morphologies Lab of the University of Southern California(ROBINSON  &  DAVIS  2018);  Bradley Cantrell  and Xun  Liu’s work  with hydromorphology tables at the University of Virginia and Harvard University (LIU 2020); Matthew Seibert’s work at Milton Land Lab;1 and Ilmar Hurkxken and Christophe Girot’s Robotic \nLandscape work at ETH Zurich (HURKXKENS 2020). Each of these develops landscape laboratories that integrate physical spaces with customized tools and machinery. \n\nThis phenomenon mirrors the 21st-century development in landscape theory that prioritizes \ndynamic landscape processes and ecological evolution over static forms. Landscapes are imagined to evolve with recursive and process-based strategies over time, instead of as a one-time construction. Projects such as the Fresh Kills and Downsview Park competitions in the early 2000s weres examples of this design paradigm (CZERNIAK 2001, REED & LISTER 2014). Since then, many scholars have incorporated a broad range of ideas and concepts from both sciences and humanities to diversify and develop that paradigm. They include multispecies \nco-production,  novel  ecology,  feral  ecology,  and  cyborg  landscapes  (HOUSTON,  HILLIER, \nMACCALLUM, STEELE & BYRNE 2018, KLOSTERWILL 2019, PROMINSKI 2014). In addition, new tools have been imagined for integration into landscape systems that would execute process-based strategies and co-evolve with other landscape actors (CANTRELL & HOLTZMAN \n2015). \n\nOur research contributes to this body of work in theory and practice. We view an intelligent \nsystem, like its human counterparts, as an imperfect agent, rather than an omniscient, omnipotent black box. The perspective  of  collaborative  intelligence (EPSTEIN  2015) provides  an emergent, constructive view of artificially intelligent agents that participate in and support a collaborative  design  process.  We  envision  an  alternative  future  where  technology  plays  a more integral role in adaptation to rapidly changing environments. \n\nThis paper documents the design, construction, and preliminary testing of ELUA, and provides practical recommendations for such landscape laboratories. It also reflects on the ramifications of ELUA for landscape design and argues for a new research paradigm where AI \nis  an  integral  part  of  evolving  ecosystems.  From  our  perspective,  landscape  design  is  no \nlonger a finished product, but a long-term engagement and collaboration with an assemblage \nof actors, including AI systems, that co-creates an evolving ecosystem.  \n\nTechnologies and Design-build  \n\nELUA has two sites within the Spitzer School of Architecture at City College of New York, \nan outdoor garden on a rooftop and an indoor garden in a communal area near the landscape \nstudios. The outdoor garden (shown in Figures 1 and 2) focuses on growing food; the indoor \ngarden (shown in Figures 1 and 3) is used for education, prototype research, and experimental \ndevelopment.  \n\nInitial Hardware and Software \n\nBoth ELUA robots are from FarmBot,2 a California-based firm that designs and markets open-\nsource commercial gardening robots, and develops web applications for users to interface with \nthose robots. These are gantry robots that operate in three dimensions and employ interchange-\nable tool heads to rake soil, plant seeds, water plants, and weed. FarmBots are highly customi-\nzable; users can design and replace most parts to suit their individual needs. For ELUA, we have \ndesigned and 3D-printed our own watering nozzles, seeders, seed troughs, and camera mounts. \n\nFarmBot’s supporting code for farm design and robot control is also open source; users can \ncustomize  it  through  an  online  web  app.  This  allows  us  to  revise  or  replace  the  provided \nexecutable programs and to introduce new functionality into ELUA. The basic FarmBot code \nvisualizes garden designs before planting, photographs the garden, and provides primitive \nsensing and behaviors. \n\nCustomization for Robot-assisted Gardening  \n\nWe customized each of ELUA’s robots in several ways for our indoor and outdoor gardens, \nand both systems function as intended. ELUA’s rooftop robot has 2-meter tracks from a third-party vendor tailored to the spatial parameters of its site; they replace FarmBot’s original (x-axis) 1.5-meter robot tracks. To install the tracks on the I-beams on the rooftop, we designed \nand built our own joints. We developed planters made with standard milk crates lined with a \nlayer of geo-fabric. This modular approach provides flexibility to the entire rooftop design \nand  installation.  The  5'x5'  structural  frames  are  custom-built  with  10-foot,  16-gauge  steel \ndrywall studs and tracks, cut to size and assembled on-site with L-shaped corner clips. The \nstructural frames fit between two I-beams and support milk crate planters or wooden planting \nboxes (Figures 1, 2 and 4). As shown in Figures 1 and 3, the indoor system consists of a \nblack-pipe armature for the robot and mobile garden beds, instead of gantry tracks fixed directly onto the garden beds as suggested by FarmBot Inc.. This armature design allows us to \nremove and replace mobile garden beds if needed without deconstructing the entire gantry.  \n\nThe rooftop garden will eventually be used by a student group to produce food. In contrast, \nthe indoor garden is intended for more advanced experiments, where we will prototype and \ndevelop new algorithms, tool heads, and operations to be used for both robots.  \n\nEach FarmBot includes a camera that photographs the garden, with software to roughly stitch \nthe images together. We developed algorithms to improve image mosaicing (DICKSON et al. \n2002, MOLINA & ZHU 2014). Meanwhile, we installed a second, more powerful AI camera \nOAK-D camera3 to perform more advanced computer vision tasks, such as depth detection, \nweed detection and plant identification. We have used this AI camera and developed seamless image stitching for two-dimensional aerial views in ELUA that are more accurate and \nmore visually appealing.  \n\nInitially,  the  user  describes  the  garden’s  contents  to  a  FarmBot  as  a  simple  placement  of \nplants from the provided “plant dictionary” on a garden map, a two-dimensional grid visualized by the web app. FarmBot stores the location of each plant as a datapoint (x, y) on that \nmap. Other emerging plants, if detected by the camera, are treated uniformly as “weeds” that \nshould be managed by the robot. FarmBot’s software has no plant identification algorithm to \ndifferentiate between different weed species. Some “weeds,” however, such as dandelion and \npurslane, are edible, while others, such as red clover, can fix nitrogen and support soil health. \nWe expect that these species could play important roles in an urban polyculture garden and \nincrease urban biodiversity and resilience. Thus, we intend to process images from the AI \ncamera with deep learning models to detect such opportunistic species, record their locations \nin the garden map, and have the robot cultivate all welcome but unanticipated plants.  \n\nMultimodal Sensing and the New Database \n\nIn  addition  to  the  FarmBot  armature,  our  gardens  are  designed  to  benefit  from  additional \nsensors. We have incorporated an array of capacitive soil-moisture sensors connected to a \nmicrocontroller with a WiFi module. Our outdoor garden also includes a personal weather \nstation  connected  to Weather  Underground. With  their  application programming  interface \n(API) service, we can access real-time and historical weather data, as well as a seven-day \nweather forecast from the rooftop. Additional sensors could be similarly installed to measure \nother environmental factors, such as solar radiation, CO2, and air pollution.   \n\nTo incorporate this sensor data into ELUA, we have created a virtual server that hosts our \nown database as well as any API services. This greatly expands ELUA’s capability because \nit connects each FarmBot to other types of open data and services. For example, with weather \nforecast data, ELUA could modify scheduled watering regimens for precipitation and drought.  \n\nMachine Learning and AI \n\nAI is pervasive in this research. Non-experts. including many landscape researchers, often \nthink of an AI system as a general artificial intelligence that addresses multiple goals simultaneously. In ELUA, however, AI algorithms are individually built for specific tasks.  \n\nIn ELUA, the AI camera we added processes images with OpenCV, an open-source computer \nvision  and  machine  learning  software  library.  This  provides  machine  learning  algorithms, \nincluding pre-trained deep neural network modules that can be modified and used for specific \ntasks, such as measuring plant canopy coverage and plant height. Machine learning and AI \nplanning can also be used with the multimodal sensory data described in Section 2.4 to provide data-driven guidance to improve garden management. \n\nAn AI system that relies on reinforcement learning (RL) develops a policy for its behavior \nwhen it is rewarded or punished for the outcome of its actions. Such systems have devised \nunexpected behaviors in Go, chess, and some video game that expanded human players’ understanding of these games and provided new insights (SCHRITTWIESER et al. 2020). Landscape architects and ecologists now also imagine how RL systems might manage the environment and construct wild landscapes (CANTRELL et al. 2017, ZHANG & CANTRELL 2021). One research team conducted RL experiments to prune a polyculture garden with a FarmBot \nto increase biodiversity (PRESTEN et al. 2022). We will perform RL experiments in a simulated environment with a virtual robot and later test the learned policies with a physical robot. \nWe envision a version of ELUA that will evolve and propose novel methods, such as combinations of different watering nozzles and watering paths in different scenarios. We hope some \nunexpected  combinations  will  surprise,  intrigue,  and  inspire  us  with  their  successful  outcomes that help the garden adapt to a changing climate.  \n\nResults and Discussion \n\nIn the fall of 2022, we planted herbs on the rooftop and lettuce and herbs in the indoor garden \nto learn and test the basic functions of the robots. (The indoor ELUA robot in action appears \nin  a  brief  video  at  https:\/\/youtu.be\/VTec_SXO5Lk.)  Both  ELUA  robots  captured  garden \nmaps and carried out watering events as expected, although maintenance and troubleshooting \nare needed from time to time. This is a design-build project and every aspect of ELUA was \nconstructed  by  faculty  and  students.  Our  team  has  gained  hands-on  knowledge  as  it  constructed ELUA. Here, we offer three recommendations.  \n\nFirst, in cities like New York, public and free resources are available for academic research \nand well worth the time to track down the networks of organizations and groups. We received \n3 cubic meters of free, clean mineral soil from the NYC Clean Soil Bank hosted by the NYC \nMayor's Office of Environmental Remediation. The soil was delivered by the crews from the \nNew York Restoration Project. We received 40 bags (1 cubic meter) of compost from the \nNYC Composting Project hosted by Big Reuse. In return, we took the Big Reuse crews to \ntour ELUA, and hope to maintain this relationship. We learned about our current soil mix (⅓ \nmineral  soil,  ⅓  compost,  and  ⅓  perlite)  during  a  free  guided  tour  of  a  green-roof  facility \nhosted by the NYC Department of Parks & Recreation. ELUA would have been far more \ncostly and difficult to construct without these public resources. We encourage prospective \ndevelopers to seek out similar assistance. \n\nSecond, “open-source” offers adaptivity to its users but also forewarns the necessity of substantial troubleshooting. Although FarmBots appears to be user-friendly, some knowledge of \ncomputer science and electrical engineering is required to set up such a system. Researchers \nand research assistants from our Departments of Computer Science successfully overcame \nmany issues during our installation. Moreover, a licensed architect on the team successfully \ndesigned and constructed the garden beds and installed a FarmBot onto the existing load-bearing rooftop structure. A multidisciplinary team of computer scientists and designers is \nhighly recommended to replicate ELUA.  \n\nFinally, institutional knowledge is important in academic research. Institutional structures in \nuniversities, especially public schools, can support or hinder academic research visions. Researchers need to be nimble and adaptive in pursuing their goals. For example, thanks to the \nArchitecture School, the Colleges, and the University, we received multiple seed funds for \nELUA.  The  Architecture  School  also  provided  space  and  infrastructure  to  house  ELUA. \nSome rules, however, could not be bent. Because the University’s security regulations blocked \nnetwork ports used by a FarmBot, we have had to set up alternative 5G Wi-Fi hotspots untill \nthe University can provide a research-only network. We recommend an ample buffer in the \nresearch schedule to account for unexpected circumstances, as well as a good rapport with \nuniversity offices.  \n\nConclusion  \n\nDespite ELUA’s practical focus on urban food production, it is also a thought experiment \nthat challenges landscape architects’ conventional views on agency and intelligence. With \nELUA, we want to formulate a theory that questions how the environment is conceived and \nconstructed.  To  some  extent,  ELUA  offers  a  glimpse  into  an  ecosystem  of  computerized \necology where the relationship between humans and plants is deeply mediated and, at the \nsame time, enabled by sensors, controllers, computer hardware, layers of computer code, and \nonline servers.   \n\nA  problem  in  the  perspective  of  landscape  architecture  is  its  current  conception  of  AI  as \nintelligence embodied in a single agent (CANTRELL & ZHANG 2018). Ideas in posthumanism, \nhowever, such as assemblage and sympoiesis become new concepts to reframe agency and \nintelligence as distributive (BENNETT 2010, HARAWAY 2016, TSING 2015). From this posthu-\nmanist  perspective,  intelligence  should  be  viewed  as  an  emergent  epiphenomenon  arising \nfrom the interactions of an assemblage of actors – humans, AI agents, animals, and plants. \nThis framing sheds light on a new landscape design paradigm based on co-evolution among \nbiotic  and  abiotic  agents.  We  could  allow  AI  systems  and  plant  and  animal  agents  to  co-evolve to create novel ecosystems that inspire us with new methods to construct the land-\nscape.  \n\nIn  this  new  landscape  design  paradigm,  AI  agents  would  no  longer  simply  model  human \nbehavior under human control. Instead, they would become co-creators among human and \nnonhuman  actors.  They  could  offer  novel  approaches  and  long-term  cultivation  strategies \nthat humans can learn from and use to adapt to the changing climate. ELUA is a physical \ndemonstration of this new paradigm of landscape design. It provides empirical evidence that \ncollaboration among AI agents and other human and nonhuman actors is within reach.  \n\nToward Acoustic Landscapes: A Digital Design \nWorkflow for Embedding Noise Reduction in \nGround-forming  \n\nAbstract: Noise pollution is considered the number two environmental health risk in Europe, and there \nis increasing global awareness of the health risks associated with noise exposure. As urbanization expands, a growing number of people are exposed to urban noise, to which airports and large urban infrastructure are significant contributors. Unlike indoor noise, which is extensively addressed using digital \ntools in architecture, there are limited parallel efforts in landscape architecture. In this context, mitigating outdoor noise through ground forming can replace the standard use of sound barriers and offer noise \nreduction means together with recreational use. The paper presents and demonstrates a digital workflow \nfor designing acoustic grounds. The workflow links environmental noise data, parametric design, and \nacoustic simulation in a single design environment. A case study site adjacent to Munich Airport is \nused to demonstrate the workflow and comparatively examine the acoustic performance of different \ndesign patterns. The results indicate a possibility of reducing noise levels through ground forming.  \n\nIntroduction: In Search of a Vast, Horizontal Acoustic Tile \n\nIn  2011,  noise  pollution  was  named  the  number  two  environmental  health  risk  in  Europe \n(WORLD HEALTH ORGANIZATION 2011). Since then, the World Health Organization (WHO) \nhas  constantly  been  updating  the  health  risks  associated  with  noise  exposure  (WORLD \nHEALTH ORGANIZATION  2022).  As  urban  areas  expand,  the  number  of  people  exposed  to \nurban  noise  grows.  Airports  and  large  urban  infrastructure  significantly  contribute  to  this \nnoise (BOUCSEIN et al. 2017). In addition, the propagation of noise caused by transport infrastructure has been shown to increase through poor urban design (MORILLAS et al. 2018). Existing methods for mitigating outdoor noise typically consist of prefabricated, vertical indus-\ntrial acoustic walls. In contrast to acoustics in architecture, where digital tools are used to \ndesign and fabricate site-customized acoustic tiling, outdoor noise is not  met with similar \nmeans. However, preliminary examples indicate that formed grounds could mitigate sound. \nDespite this potential, there is a lack of dedicated methods for creating acoustic grounds in \nlandscape architecture. Mitigating noise through acoustic grounds could be beneficial for urban airports, which are typically bordered by buffering open spaces. In such areas, ground \nforming can provide noise mitigation as well as public recreational use. \n\nContext and Objective \n\nDigital tools enable the introduction of preciseness into landscape architecture design and \nmaking (CANTRELL & MEKIES 2018). Preciseness is defined here as the process of highly-articulate tailoring of a design for mitigating a natural phenomenon. In this context, the design of acoustic grounds requires linking environmental data relating to noise to a landscape \ndesign aiming to mitigate it. The research seeks to promote noise mitigation through what \ncan be viewed as the horizontal, ground-made, site-tailored version of an acoustic tile. To this \nend, the paper presents a digital design workflow for embedding noise reduction and simulating \nacoustic performance in landscape architecture. The workflow is based on a method for incorporating  acoustic  analysis  in  landscape  architecture  design  developed  by  the  authors  (BARSINAI et al. 2023). This is demonstrated through a case study site in Hallbergmoos, adjacent to \nthe  Munich  airport,  which  is  amongst  the  ten  busiest  airports  in  Europe  (BOUCSEIN et  al. \n2017). Currently, no physical noise reduction measures exist in the area (Figure 1). \n\nState-of-the-art: Design and Simulation of Acoustic Grounds \n\nThere is a growing awareness of the need to protect from noise in outdoor spaces (SORVIG & \nTHOMPSON 2018). In the context of airport noise, mitigation is addressed through three levels: a primary level, which targets the noise source and is applied during the production \nof aircraft; a secondary level, which adapts aircraft arrival and departure procedures; and \na tertiary level which includes measures by the local airport of aviation authority grounds \n(NETJASOV 2012).  \n\nUntil recently, tertiary-level noise mitigation measures around airports did not include the \nformation of acoustic grounds. This possibility is beginning to be explored in landscape architecture projects, demonstrating a capacity to mitigate noise and vibrations through targeted \nground forming. For example, Buitenschot Park demonstrates a reduction of the Schiphol airport noise through the construction of ground ridges and furrows. The park design distorts \nand disperses low-frequency noise waves, which have been reported to reduce the noise surrounding the airport by 10 dB (TASHAKKOR et al. 2020). A similar approach employed ground-\nforming for mitigating vibrations around a MAX Lab IV in Sweden  \n\n(WALLISS & RAHMANN 2016). These two examples challenge the standard practice of constructing absorbing sound barriers surrounding urban noise sources. However, there is still a \nlack of methods for performing noise mitigation through ground forming.  \n\nAcoustic simulation is often performed using stand-alone tools (SAKUMA et al. 2014), and as \nsuch, they do not readily support design iteration. While there are dedicated frameworks to \nintegrate acoustic simulation in architectural design (PETERS 2015), there is a lack of similar \nmethods for embedding noise reduction in landscape architecture design processes. Pachyderm, a recently developed open-source tool, performs ray tracing-based sound propagation \nsimulation and visualization embedded in 3D design environments (VAN DER HARTEN 2013). \nHowever, despite the availability of Pachyderm, there is still a need for methods for applying \nit toward noise reduction in the design of open spaces. The lack of such methods currently \nlimits the possibility of addressing noise in landscape architecture and urban design. \n\nAcoustic Landscape Design Workflow \n\nThe paper addresses these gaps by developing a digital workflow that links noise, design, and \nsimulation for embedding acoustic performance in landscapes. The workflow consolidates \nthe design and simulation in a single digital environment. It consists of noise data analysis, \nthe design of parametric ground formations, and acoustic simulation and evaluation. \n\nNoise Data Analysis: Combining Online and On-Site Measurements  \n\nAirport  noise  consists  of  ground-level  noise  as  well  as  noise  produced  by  aircraft  during \ntakeoff,  landing,  taxiing,  and  idling  stages.  The  Munich  Airport  tracks  noise  in  real-time \nthrough monitoring stations and provides publicly available data (MUNICH AIRPORT 2022). \nHowever, only one of the monitoring stations is positioned on the perimeter of the site. In \naddition, the monitoring stations are situated 4 m above ground. This height is determined by \nthe  Environmental  Noise  Directive  (END)  (EUROPEAN COMMISSION  2002),  as  it  is  where \nreflections from the ground stop playing a major role. Measurements performed above 4 m, \ntherefore, allow the official comparison of different contexts. This calculation method is also \nthe basis for all the noise abatement measures. However, measurements at the height of 4 m \nlimit the possibility of understanding the noise as it is perceived by a listener on-site.  \n\nTo  sense  the  noise  as  it  is  felt  on-site,  the  study  combined  online  data  with  on-site  noise sampling using mobile phones. The use of mobile devices has become an increasingly prevalent method for collecting environmental data (MURPHY & KING 2016). The noise sampling \nwas conducted using five different devices. The devices simultaneously recorded the noise \nlevels during takeoff and landing on five points on-site for 90 consecutive seconds (Fig. 2). \n\nDespite their limited accuracy, and while at this point of the research, no identifiable rela-\ntionship between the official and on-site sampling could be specified, the mobile phone meas-\nurements provided a picture of the felt noise levels on the ground. This noise, therefore, also \nincludes the ground effect which the official measurement stations exclude. While the official \naveraged noise contour (BAVARIAN MINISTRY OF ENVIRONMENT 2021) is limited to the runway areas (Figure 1), the on-site noise sampling recorded peaks of 75 dB and above beyond \nthe airport fence and within Hallbergmoos, underscoring the need for noise abatement in the \narea notwithstanding the averaged noise levels. \n\nDesign: Parametric Ground Formations \nThe design first proposed a basic layout for the park and coupled the desired noise mitigation \nstrategy with urban design considerations. The plan defined the movement paths, program, \nand areas dedicated to acoustic ground-forming (Figure 3). In these areas, the research tested \nformations consisting of mounds with public paths located between them. Four design patterns were tested at three heights: 2.5 m, 5 m, and 7.5 m. These included: high-to-low (HL), \nundulating mounds in gradually decreasing heights (7.5\/5\/2.5 m – 0.5 m); low-to-high (LH), \nundulating  mounds  in  gradually  increasing  heights  (0.5  m  -2.5\/5\/7.5  m);  constant  height \nmounds (CM), featuring uniformly sized undulating mounds (tested in 2.5\/5\/7.5 m); and constant heigh solid ground (CS), an elevated ground without any undulation mounds (tested in \n2.5\/5\/7.5 m). The width of the mounds was set to 21 m to provide an inclination and slope \nthat allow public accessibility even in the higher-mound instances (Figure 3).  \n\nAcoustic Simulation and Evaluation \n\nThe simulation process includes several aspects:  \n\n1)  A  base  model  –  the  base  model  aimed  to  reproduce  the on-site  conditions  and  noise \nlevels  as  measured  before  any  ground  modification.  In  construction,  base  models  are \noften referred to as 'digital twins,' a digital environment that simulates the existing condition in the physical environment (BOSCHERT & ROSEN 2016). The topography and the \nbuilt fabric were created using Blender with imported layers derived from Shuttle Radar \nTopography Mission (SRTM) data, a GIS data source with a 9-16 m accuracy. The model \nwas then placed in a bounding box to support the acoustic simulations.  \n\n2)  Noise source (emitter) – the noise source was introduced into the model and situated on \ntakeoff lane 08R\/26L at a level of 75 dB. The simulation works as a transfer path that \ndisperses the noise. The transfer path holds a noise spectrum with several frequencies. \nHowever, simulations, including the presented ones, rarely include all frequencies.  \n\n3)  Simulation tool – for simulating sound propagation, the research employed Pachyderm \nRC 26, an open-source tool that integrates acoustic simulation and visualization in a 3D \ndesign environment (VAN DER HARTEN 2013). Pachyderm provided a ray tracing-based \nmethod and was integrated into Grasshopper and then linked to the base model. This \nintegration allowed performing the design and the simulations in the same digital environment. The research employed an i7 processor, 32GB of RAM, and an Nvidia Quadro \n1000 graphics card which could not perform a full-site acoustic simulation due to insufficient processing power, memory, and graphic processing.  \n\n4)  Noise sampling grid and points (receivers) – to address the simulation challenge, the \nstudy  developed  a  method  for  sampling  the  acoustic  performance  using  a  grid  with \n20\/20m size cells. Within this grid, three listener points were positioned as noise receivers at a height of 1.65 m (Figure 4). Point 1 was closest to the airport runway, point 2 \nwas in the middle of the site, and point 3 was in the residential area of Hallbergmoos. \n\nEvaluation – while the simulation calculated noise levels, the evaluation focused on the relative reduction in the noise levels as measured in the model before and after the design intervention and reformed grounds. The focus on the reduction levels allowed us to analyze noise \nreduction as a trend and compare the acoustic performance of the different design scenarios. \n\nResults \n\nThe results of the simulations, summarized in the table (Table 1) and graph (Figure 5), can \nbe viewed through three aspects: listener points, design, and height. Each scenario is referred \nto according to the initials of the design pattern, followed by the height in m (i. e., CS2.5). \n\nPerformance by Listener Point \n\nPoint 1: The best noise mitigation was achieved here with HL5 (28.4 dB reduction) followed \nby CS2.5 (27.5 dB reduction), C7.5, and CS7.5 (27.4 dB reduction). Last ranked the LH 2.5 \n(1.3 dB reduction).  \n\nPoint 2: The best noise mitigation was achieved with C5 (26.4 dB), poorest performance \nwith LH2.5 (1.3 dB reduction).  \n\nPoint 3: The best mitigation is achieved with CS7.5 (21.9 dB reduction), followed by C5 \n(21.6 dB), then CS5 and HL7.5 with a similar noise reduction (20.5 dB reduction). The poorest performance was CS2.5 (16.2 dB).  \n\nPerformance by Design Pattern \n\nHigh-to-low (HL): This design showed the most effective noise reduction levels at point 1 \nfor all heights. The best performance was achieved with 5 m mounds which showed an average reduction of 23.9 dB across the three points.  \n\nLow-to-high (LH): In this design, there was a significant difference in effectivity between \nthe lower mound patterns (2.5 m) and the 5 and 7 m high ones. The best average reduction \nacross the three points was attained by the 5 m high mounds (23.1 dB).  \n\nConstant  mounds  (C):  This  design  demonstrated  highly  effective  noise  reduction  levels. \nThe highest effectivity was attained by the CM2.5 (24.7 dB), followed by CS7.5 (24.7 dB), \nand CS5 (24.1 dB).  \n\nConstant solid height (CS): This design demonstrated that CS5 performed better than CS2.5 \n(4.3 – 2.2 dB difference), CS 7.5 performed better than CS5 at point 1, but CS5 performed \nbetter than CS 7.5 m at points 2 and 3. The average reduction of CS5 and 7.5 m was equal.  \n\nPerformance by Height \n\n2.5  m:  Constant  mounds  perform  best  at  all  points,  followed  by  the  constant  solid.  LH \nshowed the poorest performance at points 1 and 2 and then a significant increase in performance once height was reached toward point 3. At point 3, HL and LH performed with a \nnegligible difference.  \n\n5 m: At this height, the HL pattern demonstrated the best performance at point 1. In average \nperformance, however, constant mounds outperformed HL with an average reduction of 24.1 \ndB compared to 23.8 dB by HL. CS followed with an average reduction of 20.67 dB.  \n\n7.5 m: At this height, HL, C, and CS all lost effectiveness with distance. The LH section \nperformed marginally better at point 2 than at point 1 (0.4 dB). All the designs at this height \nshowed the lowest noise reduction at point 3. \n\nDiscussion \n\nAcoustic Performance Trends \n\nIn line with existing works, the results indicate a possibility of reducing noise levels through \nground forming. A few noteworthy trends can be summarized based on the acoustic performance results:  \n\n1)  Higher does not equal better – while we would expect the highest mounds to perform \nbetter,  the  simulations  showed  that  performance  is  impacted  more  by  design  than  by \nheight. The  best-performed mitigation,  HL5,  was not  achieved  by  the  highest design. \nThe lowest height, C2.5, outperformed other designs and showed an equal performance \nto C7.5. In addition, in comparing the same design at different heights, such as in the HL \npattern, the best performance was shown at 5m rather than 7.5 m.  \n\n2)  The benefit of patterns – the results indicate the benefit of the constant mounds (CM) \npattern in relation to constant solid ground (CS) for mitigating noise. This trend can be \nclearly seen by comparing the CS2.5 m to the other patterns, as well as in the high average performance of CM in relation to the CS 5 pattern.  \n\n3)  High-to-low  outperforms  low-to-high  mound  patterns–  overall,  the  patterns  that \nranged from high to low performed better than the designs ranging from low to high. A \nsignificant  difference  was  seen  with  the  LH2.5  pattern,  which  had  the  lowest  performance (1.3 dB reduction) compared to the other results. An exception is LH 7.5, with \nslightly better performance than HL7.5 at point 2. This trend may indicate that patterns \nincreasing in height in the direction of the noise dispersion provide less effective mitigation unless they are high enough to form a barrier adjacent to the noise source, which \nexplains the improved performance of LH5 and LH7.5.  \n\n4)  Constant height outperforms inclined, high-to-low slopes – constant mounds (CM) \nshow great effectivity regardless of their height, and mounds as low as 2.5 m can yield \nsignificant noise reduction. This result aligns with the effectivity demonstrated around \nthe Schiphol airport, where constant-height ridges and furrows were used and reportedly \nreduced the noise levels by 10 dB (TASHAKKOR et al. 2020). However, in comparison to \nridges, mounds can offer more accessible and versatile public use throughout the park, \nnot limiting public activity to the furrows. \n\nContribution and Current Limitations  \n\nThe paper contributed a novel digital workflow for designing acoustic grounds. The work-flow provides and demonstrates a step-by-step guide for parametrically designing, prototyping, simulating, and evaluating noise mitigation through ground forming. The digital work-\nflow includes several steps – noise analysis, parametric design, and acoustic simulations, and \nallows for comparatively evaluating ground formations to understand their noise mitigation \neffectivity.  \n\nThe  workflow  was  demonstrated  in  a  site  adjacent  to  the  Munich  airport.  The  parametric \ndesigns featured three patterns of undulating mounds and one constant-height solid ground \nat  three different  heights.  The  acoustic  evaluation  was  performed  by  comparing  the  noise \nlevels  before  and  after  the  ground  modification  in  selected  points  within  the  simulation \nmodel. The results indicate the benefit of using constant height, undulating mounds rather \nthan inclined patterns of higher flat elevated grounds as a noise mitigation strategy.  \n\nDue to the complexity of the task at hand, the study faced several limitations, which will be \naddressed in future work. First, the complexity of acoustic simulations increases significantly \nwith the size of the tested area and, along with them, the computing time. This complexity is \na  current  obstacle  in  using  such  tools  in  landscape  architecture.  In  the  future,  this  can  be addressed with improved processing power, memory, and graphics processing capabilities. \nAlternatively, a different simulation method with a lower sampling rate could be used. The site  could  also  be  divided  into  smaller  parts,  simulated  separately  before  their  results  are \ncombined. Second, the research focused on noise propagation and did not look at: the frequency aspect; the effects of weather, a factor known to influence noise; and the effect of \nnoise absorption on the ground surface and the way different ground covers may contribute \nto reducing noise transmission. Finally, future work will also look at additional design patterns, varying heights, and regularity vs. irregularity and consider these factors in relation to \nmultiple noise emitters in various locations. \n\nConclusion and Outlook \n\nThe presented workflow allowed us to comparatively assess the acoustic performance of different landscape designs and discern trends associated with their design features. This contributes to the capacity to embed acoustic performance in landscape architecture and mitigate \nnoise pollution through ground forming. The digital workflow can be used for addressing \nnoise around other airports as well as around other sources of urban noise or large transportation infrastructure. As such, the workflow also promotes a broader endeavor – the development  of  dedicated  methods  which  link  environmental  data  and  parametric  design  toward \nforming performative grounds in response to environmental challenges. \n\nUrban Agriculture:  \nClimate-Responsive Design Strategies for \nBlue Infrastructure in the Context of Singapore \n\nAbstract: Global climate change poses various threats to densely developed cities and their urban infrastructures. As an archipelago and product of many significant land reclamation efforts over the centuries, Singapore is most vulnerable to rising seawater levels and faces increasing climate change-related challenges, including intense rainstorms, water scarcity, food shortage, and extreme heat. This \npaper proposes a Synergy System, or climate-responsive computational design method, to create a new \ntypology of urban agriculture integrated with the existing infrastructure. This study employs performance-based generative design to generate typologies and optimize them in terms of driving factors \nsuch  as  food  production  and  microclimate  adjustment.  The  performance  evaluation  model  as  a  tool \nsupports the design of multifunctional and climate-responsive infrastructure typologies. \n\nIntroduction: Climate Challenges in Singapore \n\nAs a highly urbanized city, Singapore has suffered from complex challenges related to climate change, putting climate adaptation in the spotlight for future urban development. Singapore’s main climate crisis includes intense rainstorms, water scarcity, food shortage, and \nextreme heat (NATIONAL CLIMATE CHANGE SECRETARIAT 2022). For instance, frequent extreme rainstorms have put overwhelming stress on the stormwater infrastructure and have caused flash floods in urban areas (PUB 2022). Half of Singapore’s fresh water supply relies \non imports from Malaysia (ROMAN & CHEOK 2016), which will end in 2060. In addition, \n90% of Singapore's consumer food is imported, motivating local food production to achieve \n30% food self-sufficiency by 2030 (TENG & MONTESCLAROS 2019).  \n\nThe increasing challenges on urban infrastructure and residents’ daily lives have raised increased concerns about building a climate-resilient city for the future. The concept of urban climate  resilience  emphasizes  city  and  urban  infrastructure  as  a  complex  adaptive  system \nwith the ability to maintain basic functions in the face of climate disruptions (MEEROW et al. \n2016).  \n\nThe main climate challenges Singapore faces are closely related to three aspects: stormwater \ninfrastructure, urban agriculture, and microclimate. The synergies between them serve as the \nbasic  logic  for  building  climate-responsive  infrastructures.  Concrete  canals,  one  typical \nstormwater infrastructure, cannot accommodate peak flows during extreme rainstorms and \nare ineffectively utilized during the dry season. They are required to be transformed into a \nnew generation of infrastructure bolstering ecosystem services (LIQUETE et al. 2015). This \ncan be achieved by integrating a new typology of urban agriculture with the concrete canals, \nwhich promotes “the well-being of urban ecosystems and resilience in a circular economy.”(DEKSISSA et al. 2021). \nMoreover, decentralized urban farming integrated with underutilized \nspaces is a sustainable way to enhance local food production (SINGAPORE FOOD AGENCY \n2021). Utilizing the rainfall collected in canals for food production contributes to water conservation,  which  is  highly  imperative  to  sustainable  vertical  farming  (ASSOCIATION  FOR \nVERTICAL FARMING et al. 2015). Furthermore, the food-growing structures provide shading \nfor  microclimate  mitigation,  as  vegetation  shading  can  effectively  reduce  solar  heat  gain \n(OLGYAY 1963).  \n\nIn terms of the identified research gap, the synergies among the above three aspects have not \nbeen fully investigated and implemented into a computationally driven design strategy. A \nnew approach is required to explore the material flow of this synergy system and design a \nclimate-responsive typology based on its performance on food production and microclimate.  \n\nCore Synergies \n\nThe core synergies are summarized as follows (Figure 1): The rainfall collected in the stormwater infrastructure can be supplied for food production, while the growing plants provide \nvegetation  shading  for  cooling  the  microclimate.  Stormwater  infrastructure  in  connection \nwith urban agriculture can increase drainage capacity, offer accessible community spaces, \nimprove green features, and create comfortable microclimates.  \n\nThe synergies set the fundamental logic for a new computational design approach to transform existing monofunctional concrete canals into an adaptable and multifunctional infrastructure  typology  for the future. They  not only serve  as a  drainage  facility,  but  also  as  a \nresponsive landscape that interacts with the water. They not only sustain an industrial food \nproduction factory, but also provide diverse spaces for community interaction. Establishing \nthis infrastructure typology requires a systematic approach that explores and evaluates the \nenvironmental interactions related to rainfall and sunlight. This complexity is viewed as a \npositive challenge, to create a new design language connecting the green, the blue, and the \ngrey elements of a city to produce new and vibrant patches of local identity. \n\nClimate-responsive Design Methodology  \n\nComputational Design & Performance-based Design \n\nThe proposed design method aims to generate and optimize a climate-responsive typology \nbuilt on the domains of computational design and performance-based design (CHAOWEN & \nFRICKER 2021). Computational design thinking understands architecture as a dynamic system \ncomposed  of  interactions  between  elements,  rather  than  as  a  static  entity  (MENGES  & \nAHLQUIST 2011). Working with systems rather than forms is the connecting thread between \nspace-making  and  environmental  challenges  of  design.  Performance-based  design  underscores the value of environmental performance that reflects how architecture continually interacts with the surrounding environment. Performance-based design method combined with \ncomputational design has potentials in generating a climate-responsive typology. First, as a \nsystematic approach, computational design enables designers to focus on the connections and \nmaterial flows between the three identified elements and exploit their synergies; second, this \nmethod takes environmental performance as a design driver that facilitates the generation and \noptimization of design solutions. The infrastructure can no longer be seen as static built environments, but as “’dynamic’ self-constructing, living, breathing, and even artificially intelligent (thinking) environments” (CANTRELL & MEKIES 2018, 28).  \n\nPerformance Evaluation Model \n\nThis study develops a new performance evaluation model for the generation and optimization \nof climate-responsive infrastructure by establishing the relationship between the parameters \nand performance related to the synergy system. Within the synergy system, three subsystems \nare defined and connected with the material and energy flow of water and sunlight: the water \nsystem, the food production system, and the microclimate system.  \n\nBased on the connection, the performance criteria and parameters are defined that inform \nhow to build the three subsystems (Figure 2). Firstly, the focus is set on the catchment area \nof the canal and the calculation of runoff variability and water storage capacity, informing \nthe amount of food that can be grown. Secondly, the food production system establishes a \ncomputational model and defines a set of performance evaluation criteria for the food production condition and capacity, including direct sun hours, yield, food type, and water consumption. This paper takes a planting tower from a previous study as the prototype and extracts its relevant parameters (SONG et al. 2022). Lastly, the microclimate system measures \nthe thermal comfort of public spaces, which refers to the radiation performance of the space \nshaded by planting geometries. The Grasshopper plug-ins, Ladybug and Honeybee, are used \nas tools for the simulation and visualization of direct sun hours and radiation. Therefore, the \nkey performance of the model is derived from three subsystems: direct sun hours, yield, water \nconsumption, and radiation of the shaded area. \n\nForm Optimization Experiment \nThe  performance  simulation  starts  with  the  adjusted  prototype  planting  tower  introduced \nabove to explore the impact of different geometric parameters on the performance of food \nproduction and microclimate (Figure 3). Four metrics of performance need to be evaluated: \nyield, direct sun hours, water consumption, and solar radiation of the shaded area. The simulation experiments explore the impact of the variation of three parameters of the planting geometry on these performances: slope, orientation, and curvature. The footprint of the planting tower was kept constant (12.5m2) in all experiments. \n\nThe results can be summarized below: \n\n1. When the slope is in the domain ranges from 60 to 75 degrees, the performance of direct \nsun  hours  and  yield  is  balanced  well.  The  area  with  radiation  lower  than  200W\/m²  is  the \nlargest when the shape of the footprint is similar to a square. \n\n2. The performance of direct sun hours and radiation performs best when the planting surface \nis facing east and west. It can be taken as the main direction of the design. \n\n3. The variation of vertical curvature creates planting areas with different direct sun hours, \nwhich is ideal for producing various types of vegetables. The yield and shaded area are pos-\nitively correlated with horizontal curvature(D), and performance can reach the optimal point \nwhen it is above 0.4m.  \n\nThese main findings inform the designer how to adjust and optimize the form to balance the \noverall performance and achieve the desired state. It ignites the thought that environmental \nperformance should also be considered as an essential design trigger, especially in climate-responsive infrastructure design. In addition to the described performance of food production and thermal comfort, the design needs to consider other aspects, such as function, structure, \nand spatial aesthetics. Nevertheless, the introduced computational workflow offers designers \nan iterative tool to measure the environmental performance of different design solutions and \nunderstand the relationship between parameters and performance (FRICKER 2022). This tool \nfacilitates design trade-offs between environmental performance and other aspects of design \nto enhance the overall quality of infrastructures.  \n\nDesign Speculation for New Urban Typology  \n\nWater Volume Calculation \n\nThe prerequisite for designing a food production factory is establishing a dynamic water system with associated flow processes, specifying the amount of water flowing into the canal \nand used for food production. The first step is identifying the catchment area and the population it serves; the second is calculating the dynamic water supply and consumption for the specific site to deduce the maximum volume of water storage. \n\nFirstly, the upper stream catchment of the Jurong canal can be roughly depicted based on \nland contours and simulation of water distribution using Kangaroo plug-in in Grasshopper. \nThe area is around 9,560,000m² (Figure 4). Among the eight districts in this catchment area, \nHong Kah  and Yuhua West are  the  two  adjacent  to  the  Jurong  Canal with  76,560 people \nliving there. The goal for food production is to reach 30% of the annual food consumption of \nthese populations, which implies an ideal yield of about 370 tons of leafy vegetables. Secondly, this system pumps water during heavy rainstorms to reduce peak flow in the canal and store them to use during the dry season. Taking the rainfall data in 2021 as a reference, the \nmaximum water volume of storage is roughly 4,000 m3, informing the generation of water \nstorage space.  \n\nWater flow and plant growth give this system a dynamic character, where the water storage \nlevel and the amount of food production adjust according to the rainfall patterns. This feedback  mechanism  enables  the  system  to  cope  with  extreme  rainfall  weather  by  effectively \nrelieving the pressure of heavy rainfall on the urban drainage system. This performance echoes the original purpose of developing climate-responsive water infrastructure. \n\nDesign Generation \n\nThe design generation begins with the selection of the optimized type of planting geometry \nby comparing the performance of different forms (Figure 5). The generated geometry of two \nparallel curves has the best overall performance, in line with the results of the simulation \nexperiments. The yield is higher due to the large horizontal curvature (D), which maximizes \nplanting area and allows sufficient sunlight to reach both sides. Moreover, the twisted surfaces  provide  enough  shaded  spaces  with  good  thermal  comfort.  The  key  to  this  iterative \nprocess lies in balancing the performance of yield, water consumption, and solar radiation, \nthus producing sufficient food and creating comfortable public spaces. \n\nThe  design  generation  includes  the  generation  of  planting  geometry,  building  details,  and \nlandscape. First, planting geometry is generated based on the selected type and optimized \naccording to the performance on yield, direct sun hours, and radiation, which is evaluated by \nthe computational model built in Grasshopper. The finalized geometry produces the targeted \nyield and provides several public spaces with solar radiation below 200 w\/m². Second is the \ngeneration of the water distribution system and functional spaces. The water storage space is \nset with a certain amount of flexibility added to the maximum demand to cope with the seasonal variation of future rainfall. The final step is to create a responsive landscape by reshaping the terrain according to water and pedestrian flow.\nGround-level planting areas for flood retention reduce the drainage pressure for the canal during peak periods. Overall, the generated example, the food factory on the lively riverfront offers communal space for people to \nmeet and allow new activities to emerge in the neighbourhood. \n\nConclusion and Discussion \n\nThe  design  of  climate-responsive  infrastructure  typology  driven  by  environmental  performance  is  of  vital  concern  for  creating  resilient  urban  habitats  in  the  future,  especially  for \nintensely developed cities like Singapore. The study fills in the existing gap in the computational design method for generating and optimizing an urban agricultural typology integrated with stormwater infrastructure that responds to climate change. The study explores the impacts of various geometric parameters on the performance for food production and thermal comfort, which serve as the guidelines for finding optimal design solutions for the typology. This design workflow demonstrates the iterative processes of design optimization according \nto the balance of the performance on yield, water consumption and solar radiation, which can \nbe applied to the renewal of other concrete canals. \n\nThis study presents a speculative design proposal combining multiple parameters \ninto a single design model instead of considering the economics of implementation \nand feasibility of the design. The discussion focuses on two aspects: Firstly, how much \neach performance factor contributes to the overall change in the design was not explicitly \nstated in the study, which might depend on the objectives of the design. Future research can \nhence set several design scenarios and discuss the contribution of multiple factors respectively. Secondly, social aspects, such as how to involve people in the food production process, deserve more careful consideration in order to ensure the long-term success of the endeavor. \nIn addition, project planning requires the collaboration of multiple stakeholders, which has \nbeen a constraint to large-scale government implementation of food production in the past. \n\nVisualizing and Clustering Eye Tracking within 3D \nVirtual Environments \n\nAbstract: Visual perception is one of the most important sensory processes for most of the population. \nThis process plays a key role in how we navigate and way find in urban environments. A wide range of \nliterature offers insight into the relationship between the structure of urban spaces and navigability, as \nwell as literature identifying how individual differences play a role in how well people can recall elements and navigate environments. Measurement techniques that reveal these differences are often captured as procedurally based evaluations after individuals have navigated through an environment. However, these valuations do not necessarily help us understand the process of how observations link to \nrecall and navigation. In this paper, we show a new technique for conducting eye tracking in 3D virtual \nenvironments to assess the process of observation in urban environments. Further, we demonstrate how \nclustering techniques can be used to improve eye tracking data generated in these 3D environments. \nThe techniques we provide can offer a new means to better understand how form, function, and design \nelements are observed. \n\nIntroduction \n\nIn navigation, the visual perception of an environment plays a significant role in decision-making, as well as informs knowledge about the properties of spaces and the relationships of \nobjects that form the collective environment. One of the ways researchers have attempted to \nunderstand the decisions we make during navigation is to create highly controlled experiments using virtual environments (BRUNS & CHAMBERLAIN 2019). However, many of the \nprevious experiments lack a comparable diversity of objects, routes, scales, and relationships \nbetween these elements in comparison to the real world. The benefit of virtual environments, \nparticularly with modern gaming engines, is that the designer can control all elements within \nthe environment. This offers a means to simplify a problem and employ a more deductive \nscientific process, but it may come at the cost of understanding how perception works holistically within complex spaces. Unfortunately, quantifying perception holistically would require new methods for analyzing the process of perception. Further, a method like this would need to be implemented in complex environments, which can be self-defeating if it requires an oversimplification of the environment itself to operate. Thus, finding a technique that enables both the employment of complex virtual environments and a seamless integration of \nanalyzing  perception  holistically  would  be  a  major  step  in  understanding  the  relationship \nbetween human and environmental interactions. \n\nDesigning spaces for intuitive navigation is an important process for urban designers, campus \n(e. g. business parks) planners, and outdoor recreation trail designers. There are many design \nproblems to undertake in these instances, with navigation and wayfinding within the set of \nissues. Both these processes require individuals to recognize spatial patterns, comprehend \nrelationships of elements, make determinations of how to focus their attention, and remember \nimportant objects or spaces. There have been many approaches to assessing these processes, \nsuch  as  measuring  response  times  and  accuracy  in  remembering  landmarks  or  locations \n(CHRASTIL & WARREN 2015, ERICSON & WARREN 2020, GAGNON et al. 2018, WEISBERG et \nal. 2014) and assessing map drawings of paths and spatial layout (BRUNS & CHAMBERLAIN \n2019, GARDONY et al. 2016, WANG & SCHWERING 2015). However, these measures are usually done post hoc rather than in real time. Further, the measures are usually procedurally \nbased (e. g., memory recall), rather than processes based (formation of memories). To improve our understanding of how process-based navigational activities unfold, we need to understand what drives perception and decision-making. A better understanding could help designers support meaningful relationships between objects to facilitate these perceptual processes. Fortunately, computational techniques can be created and then combined with cognitive science and urban and landscape design principles to better understand how individuals \nobserve and make inferences about those spaces. \n\nEye tracking is one technique that has been used by researchers to better understand the process of observation. It has been used for decades to understand how and why an individual \nfocuses on particular objects, areas, and elements of space. Implementations of eye tracking \nhave been primarily conducted in 2D environments (e. g. looking at a screen or flat image). \nThis includes architectural-related studies (XIANGMIN et al. 2021, ZHANG et al. 2019), with \nlandscape studies emphasizing 2D static images (DUPONT et al. 2016). In landscape and architectural studies, eye tracking is used to identify fixations within scenes, and in psychology \ncan help describe visual attention and arousal (KIM & LEE 2021). With many metrics that \ncan be analyzed from these data, broadly, one major advantage is to provide an objective \nmeasure of perception (DUPONT et al. 2014). Yet, relative to 2D eye tracking studies, there \nis little literature showcasing implementation in 3D dynamic environments. \n\nIn this study, we combine the generation of virtual environments and eye tracking to visualize \nindividual observational patterns in a virtual space. We extend previous work (FERNBERG et \nal. 2022) to showcase a new open-source software package we developed, as well as an analytical framework for representing these data. This paper deviates from the previous by showing specific visualizations and analyses of large datasets that have been analyzed, whereas the previous version was introducing the construct. The purpose of this study is to showcase how eye tracking data can be represented in a dynamic 3D environment and how those data can be analyzed using mathematical clustering mechanisms. We ask, to what extent can eye \ntracking be implemented in 3D gaming environments and analyzed post-hoc to determine \nfixations of objects within virtual urban spaces? \n\nData Collection from the Virtual Environment \n\nFor this work, we employ the Unity gaming engine and prefabricated 3D assets (from Kitbash) to create a procedurally generated urban environment which can be explored in VR. \nThe design of the environment was not intended to be complex or realistic world because this \nis not necessary for the primary purpose of implementing eye tracking and testing different \nclustering techniques. The environment consisted of 40 x 100-meter-long blocks, where each \nblock was one of five different architectural styles. The purpose of this setup was to observe \nif the pattern of clustering was different closer to transitions between different architectural \nstyles compared to areas where changes did not exist. However, in this study, we were merely \nattempting to identify how we might cluster data, the actual test of these transition zones is \nmeant for a later study. For now, all elements are static, the user can make observations freely, \nbut cannot change their location or speed of experience. Further, we have not included any \nother cues, such as sound, lights, or atmospheric changes. Each object was placed along a \ntwo-lane road in succession, with variable spacing between each building.  \n\n\nThe eye tracking software we developed, was created for implementation in the Unity gaming \nengine  only.  For  this  implementation,  the  Vive  Eye  Pro  virtual  reality  headset  was  used. \nWithin Unity, the headset was established as the user camera and the scripts were then associated with this camera. Movement through the environment was maintained at a consistent \nspeed, but the viewer can fully move their head around. The eye tracking software stores the \nlocation and rotation of the user’s eyes at every frame that gets rendered (about 60\/sec). The \ndata from each eye is averaged to create one point and one direction. This direction is, of \ncourse, where the user is looking. Using this information, we create an invisible virtual ray \nthat extends from the eye outwards (see Figure 2). Once this ray hits an object, the collision \npoint (location of the intersection of the eye tracking ray and the surface of the object) is \nrecorded along with the object’s name and position (recorded for accurate reproduction of \nthe data before participation). Other metrics are recorded and computed such as eye angle \n(looking left\/right\/etc.), distance from eye to collision, and whether they are blinking or not. \nThe figure below is a representation of rays produced along the route as a user looks at objects \non the buildings’ surfaces. \n\nThe data produced from a single experiment can result in a substantial number of data points \ncollected. With such a vast amount of data being produced, it is important to identify the most \nrelevant data that could provide researchers with meaningful interpretations of observational \npatterns. So, we needed to identify ways to reduce the amount of data by reducing noise and \njitter. Then, we needed to cluster the remaining data into meaningful groups, referred to as \nfixations. From this data, we can determine the total dwell time and total fixation count for \nspecific areas of interest. The areas of interest are regions in the environment that are important (HOLMQVIST et al. 2011) and could be identified as specific objects of general areas \nalong an object. \n\nTo  produce  clusters  from  denoised  data,  we  tested  a  clustering  method  called  DBSCAN. \nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. This technique uses an unsupervised machine-learning algorithm to identify clusters of observations. \nAs the name implies, it uses the spatial density of the data points (in any number of dimensions) to create clusters and eliminate noise. One important function of this algorithm is that \nyou can use more than the 3-dimensional distance to find spatial density. It can include factors \nsuch as eye rotation and time in its calculations. This can be useful because in a dynamic \nenvironment participants can first look at an object in the distance, then as they move forward \nthrough an environment, look back at the object again. This dynamic  facet of 3D gamine \nenvironments makes it critically important to ascertain what is a fixation across distances, \nversus random noise that could have been part of a rapid eye movement across an area and \nalong an object. \n\nImplementation and Outcomes \n\nIn this section we highlight the results from the clustering technique to show: 1) the volume \nof data produced by a single participant, 2) the observational patterns of the participant, and \n3) the effects of implementing the clustering algorithm on the previous two. In this section, \nwe provide context to the implementation (for each of these three), takeaways from our experience, and general statistics to highlight an overview of the outcomes. \n\nIn our experiment, a single individual produced 39404 observations over the entire 9 minutes \nand 35 seconds of the experience. This averaged about sixty-eight observations per second. \n\nThe rate of eye tracking data collection depends upon Unity’s internal update function, which \nis the same as the framerate. Framerate is affected by how many objects are in view and need \nto be processed by the GPU. Therefore, the framerate can vary throughout the experiment. \nWhile it can be helpful to maintain a high-frequency rate to minimize motion sickness and \nimprove realism, it is unknown the extent to which the rate of data collection would impact \nthe results, for whatever results are being sought. \n\nUsing these data, we developed a simple metric to highlight how often an individual may \nlook at objects versus other elements in the environment. In our implementation, the objects \nwere buildings, and the other elements included the ground (terrain), the street, and the sky. \nIn our implementation, the ground and street are objects because they have a surface with a \ncollider that enables the collection of eye tracking data points when the vector of the observation intersects with that surface. Unlike buildings, these two surfaces are continuous through-out  the  entire  environment,  whereas  buildings  are  separate  objects.  Our  eye  tracking  also indirectly collected observations of the sky (or distant void), in which there is a frame with no distinct object with a collision surface. Figure 3 shows global statistics of the proportion of observations made between the sky, terrain, road, and buildings. The figure also compares those data after removing saccades. Saccades and their removal are explained below. \n\nEye tracking data can be difficult to interpret. This is particularly true in 3D, where there are \nvery few studies that have attempted to validate how observational patterns in 3D are associated  with  meaningful outcomes  of  navigation (UGWITZ  et  al.  2022).  One  crucial  step  in \ngenerating interpretations of data is to remove irrelevant data (e. g., noise), such as saccades. \nA saccade is essentially a rapid view of an object, then a focus away from that object with \nanother quick return to the original object. In terms of perception, little to no information is \ngleaned during a saccade. Therefore, in addition to general noise (single random observation \nin space), eliminating saccades can also help streamline the data analysis. \n\nHowever, identifying these saccadic movements requires playing with the clustering parameters. This is because eye tracking data is generated based on a collision point for each frame, \nbut how the algorithm determines if a single data point belongs to a cluster or not is a little \ntricky. To reduce noise and eliminate saccades, we implemented DBSCAN. DBSCAN takes \nthe {X, Y, Z} vector position where it collided with the object, but also the time dimension \nof when it collided. Clusters are determined by locational and temporal similarities of vectors \nby turning two parameters. First, epsilon is the distance threshold from one observation to \nanother (in 4D). Second, minimum points is the number of minimum points that constitute a \ncluster. Our parameters were an epsilon of two meters and a minimum points of seven, which \nrepresents roughly a tenth of a second or the approximate minimum amount of time for a \nfixation. Again, Figure 3 highlights the global statistics before and after the removal of saccades, or points that did not belong to a cluster.  \n\nFigure 4 further depicts an example of different clusters using different colored dots. In this \nfigure, similar colored dots represent a single observational point. Find the set of green dots \nright below the purple dots. The large cluster of green dots shares a similar proximity with a \nsingle black dot right on the roof ridge. This black dot seems to be part of that green dot \ncluster. However, DBSCAN identified that the observation point represented as the black dot \nshould not belong to a cluster. This was because the observational point was created several \nseconds prior as part of an earlier saccade (singular rapid observation), whereas the other \nobservations were made in sequence (suggesting a focal point or area). \n\nDiscussion and Conclusion \n\n3D gaming platforms offer the ability to produce vast amounts of user-centric data. Having \ntools to analyze these data can help designers identify environmental cues or triggers that \ncould influence the perception of a design or plan. Eye-tracking offers a non-intrusive, process-based technique for collecting very precise observations within a space. However, finding a robust algorithm to cluster thousands of eye data points is essential to making meaningful interpretations of these perceptions. Using the software we produced, these patterns can be visualized and reduced for making assessments about areas or objects favored by users. DBSCAN is one of several techniques available but has been shown to produce good \nresults (ESTER et al. 1996). This paper was not intended to conduct a systematic comparison \nacross these techniques and variations, but instead to demonstrate the potential for eye tracking data in combination with a clustering technique to produce useful data. \n\nThe next major step in this research is to understand how these data can be related to meaningful observations to help form decision-making and recall. Understanding this link can help \ndesigners better associate the placement and patterns of objects, such as landmarks (BRUNS \n& CHAMBERLAIN 2019), within the environment to improve wayfinding and navigation. As \nThus,  some  next  research  questions  are:  to  what  extent  do  eye  tracking  observations  in  a \ndynamic 3D virtual environment correlate with memory recall, navigational decisions, and \npattern recognition about the overall design of the environment? More broadly, to what extent \ncan eye tracking help us understand how individuals form mental maps? In our experience, \nwe  noticed  several  situations  where  individuals  were  following  unique  building  features, \npeering through passageways, and scanning the topography of buildings. While these observations are anecdotal and with limited data, they do suggest these data could help validate \nthe importance of or focus on different architectural forms, textures, and aesthetics. \n\nImplementing eye tracking in 3D-controlled virtual environments shows promise for aiding \nthe examination of observational processes. This will have relevance in multiple fields. Certainly,  eye-tracking  has  been  used  in  3D  gaming,  but  studies  in  psychology,  architecture, \nurban design, interior design, and landscape architecture could benefit from having access to \nindividual patterns of observation data. Eye tracking is a well-established technique but employing it within 3D environments and determining how to associate these data with meaningful interpretations will provide new opportunities and insights for landscape studies.  \n

Parametric Planting Design: Algorithmic Methods \nfor Resilient Communities \n\nAbstract: Parametric applications in landscape architecture are gaining traction as designers realize the full potential of script-based analysis in various stages of design. Planting design is one realm of parametric landscape architecture that is traditionally done manually with books, websites, or other research on hand, thereby keeping its application within the grasp of landscape designers. This discussion proposes a method of using algorithmic design to analyze and specify plant species based on four different measures. Further, it is possible to expand this method in the form of a browser-based program for non-designers to take part in resilient landscape planting. \n\nIntroduction \n\nThe origin of computation-driven landscape analysis and design is often attributed to Carl \nSteinitz’s 1966 land evaluation and the SYMAP print of the Delmarva Peninsula (STEINITZ \n2014). Subsequent decades brought advancements in processing power and user interface, \nallowing a variety of software to gain traction in the design field including Photoshop, AutoCAD, and specifically for landscape architects, LANDCADD (ERVIN 2020, MACDOUGALL\n1984). ERVIN (2020) also describes the rise of optimization software and use of algorithmically-generated landscapes in response to the formation of the internet (ERVIN & HASBROUCK\n2001). As such, the success of spatial design computer applications has led to at least a partial \nreliance on digital workflows in the design process, if not a large portion of the work. \n\nIn an effort to explore emerging landscape architectural frontiers, designers echoed Steinitz’ \nexperimentation  and  began  programming  new  tools  for  greater  flexibility  in  their  work \n(CANTRELL &  MEKIES 2018). Development of programs continued with some tools being \ncodified  as  permanent  sub-tools,  such  as  Grasshopper  within  the  3D  modelling  software \nRhino. \n\nGeneral investigations of parametric landscape design problems can exist in the form of blog \nposts  (GENERATIVE LANDSCAPES),  online  videos,  and  academic  papers  (SERDAR & KAYA\n2019). Commercial tools have also been created and added to aid in the digital landscape \ndesign process (LANDKIT). Notable built projects include Eda U. Gerstacker Grove by Stoss \nLandscape Urbanism at University of Michigan, where student desire lines, drainage, and \nparametric bench profiles were incorporated into an algorithm to generate a site model that \nresponds to and supports the pedestrian experience (REED 2018). \n\nExisting Research \n\nWhile parametricism in landscape architecture is an ever-expanding area of study, limited \nresearch has been conducted on the use of algorithmic workflows regarding ecological factors including species selection and planting location. \n\nA thesis by Roasliina Luminiitty (2021) from Aalto University examines parametric planting \ndesign’s integration in the landscape design progress. LUMINIITTY (2021) analyzes prior software used for landscape design, as well as more modern investigations into algorithmic landscape architecture. The paper offers a detailed explanation into the components of parametric \nplanting design and offers a framework for digital planting design workflows, which can be \nexpanded further with the inclusion of measurements and real-world data to inform the final \nresult. Parametric patterns can be tailored for design continuity and be ecologically developed \nby an algorithmic plant selection process to create a holistic planting concept. \n\nOLIN Lab’s Tech- and Eco Labs have also developed research into digital workflows for \nplanting design. The process utilizes AutoCAD, Rhino, Grasshopper, Python, LandFX, and \nAdobe Suite products to derive functional planting plans for use in a landscape architecture \noffice setting (AREVALO 2020). The process is broken down into four parts: (1) Investigating \nplants as living material, (2) Speculating and experimenting with parametric components, (3) \n3D Spatial and aesthetic analysis, and (4) Seasons and time. AREVALO (2020) states that this \nworkflow was successful for the office-side design process, but documentation was still prepared manually, which requires work by a landscape architect. \n\nLandKit is a Grasshopper plug-in developed by LANDAU Design+Technology that creates \ncustom  components  in Grasshopper  for  users  to  more  effectively  design  fundamentals  including with specific components called TopoKit, PavingKit, and PlantKit (LANDKIT). What \nsets PlantKit apart from other parametric planting tools is that it does not automatically assign \nspecies to the plants it generates, but rather establishes certain plant typologies to be determined later by the user. The plant typologies refer to similar sizes, environmental considerations, and biodiversity, which gives the end user more agency when specifying species and \naccounts for regional climate variations. \n\nThe creation of these products is testament to years of dedicated research into accurate and \nefficient planting algorithms, for use in a landscape architecture office. The development of \ndifferent algorithms and tools has taken off regarding landscape design and will continue to \nbring new and improved iterations into the field. However, this computation-driven analysis \nis mostly locked behind software with intense learning curves and high price points. With the \ngoal of simplifying communication between designer and computer, parametric application \ndevelopers should also strive to lower the barrier of entry for the use of these tools. \n\nDigital Equivalents to Planting Design Concepts \n\nBefore attempting to develop any parametric tools, it is critical to understand the concepts \nbehind existing ecologically sound landscapes. Traditional landscape design practice includes \nresearching native or naturalized plants in a specific region, while placing them in a pattern \ncorresponding  to  a  design  intent.  This  varies  from  project  to  project,  but  is  generally  the \nformat of the planting design strategy. Many different categories of planting information exist \nfor each plant, and can be addressed through different ways to fit the project’s goals. \n\nQualitative information exists as a subjective rationale in landscape design. This may include \naesthetic considerations, natural plant communities (dependent on location and ecosystems), \nand features included on a site and how the site functions as a whole. In the digital realm \nthese  cannot  be  quantified,  though  studies  have  investigated  various  methods  to  evaluate \nlandscape perception (KARMANOV 2009). Use of qualitative data or input is still important, \nhowever, as it establishes “the meaning individuals or groups ascribe to a social or human \nproblem (CRESWELL 2014). Use of this type of information can be presented in the setup of \na project, or change with user preferences. \n\nQuantitative information is a numerical type of data representation where all possible results \nare accounted for, and often presented numerically. In landscape architecture, common uses \nof quantitative data are found in climate data, topography, and geotechnical properties. Data \ncan also be extracted from the site itself through analysis tools and simulations, unearthing \nunderlying layers of data otherwise hidden. \n\nPlanting for a Post-Wild World (RAINER AND WEST 2015) describes the structure of a designed landscape through a series of layers: a structural layer, groundcover layer, seasonal \nfiller layer, and dynamic filler. The clear distinction between individual plants work together \nto  form  the  identity  of  a  garden  which  can  establish  character  even before  more  complex \ndesign motifs take place. Additionally, thinking of plants in a binary manner lends itself to \ndigital applications where a machine must be programmed to receive input and output information in a highly controlled manner. \n\nRAINER AND WEST (2015)  also  signify  the  importance  of  employing  ecological  strategies \nwithin plantings to promote resilience in plant communities. “Resiliency” is a commonly used \nterm referring to the ecological health of a landscape and its ability to recover after periods \nof distress (RAINER & WEST 2015), but it also can be interpreted as a human community’s \nability to recover from a disturbance (FLINT 2010). Given the difficulty of evaluating a multifaceted  concept  such  as  resilience,  we  can  instead  look  at  establishing  environmentally \nsound  starting  points  (namely  regionally  accurate  plant  spreadsheets)  in  order  to  generate \nresilient planting communities. \n\nSoftware for this algorithm process utilizes the Grasshopper tool in Rhinoceros 3D. Because \nthis software allows for algorithmic design approaches, a single input can trigger a variety of \nsubsequent processes and analyses, culminating in an algorithm-derived final product. \n\nPreparation for this tool included development of a plant list extracted from eastern Nebraska \nnursery stock listings to ensure success in the Nebraska landscape. In total, 198 unique species \nand 80 cultivars or varieties were identified and constructed in a spreadsheet with important \ninformation such as mature height and width, shade tolerance, salt tolerance, drought tolerance, bloom timings and color, nativity to Nebraska, and hardiness zone range. All plants \nwere parsed based on their landscape function: overstory conifer, overstory deciduous, understory conifer, understory deciduous, perennial, tall grass, groundcover, and annual. \n\nAlgorithm \n\nGrid \n\nA  site’s  surface  can  be  divided  into  a  grid  of  any  size,  though  standard  1-,  2,  and  5-foot \nsquares work best. The grid allows for easy analysis of site features such as elevation, edge \nproximities, and sun exposure. The ideal scale for this depends on the overall scale of the \nsite, with smaller sites requiring a higher level of detail. The ideal resolution for a 7,000 sq. \nft.  (650  m2)  site  can  be  2  feet,  while  larger  sites  may  need  5-foot  resolution  to  minimize \ncomputing time. \n\nPlant and Environment Scoring \n\nA surface in Rhino is referenced in Grasshopper where four analyses take place: Elevation, \nShade, Salt intensity, and Structure proximity (Fig. 1). The interplay between envi-\nronmental factors plays a large role in identifying a “best fit” species for a particular location \non a site (CZAJA et al. 2020). Elevation analysis takes note of local high and low points on \nthe  site,  and  corresponds  with  low  points  requiring  less  drought  tolerant  plants  and  high \npoints requiring more drought tolerant plants. Shade analysis looks at sun exposure on the \nsite from existing buildings and trees to accurately place plants with regards to sunlight hours. \nSalt tolerance measures look at a point’s distance from paving surfaces or curves to account \nfor  road  salt  accumulation  in  winter  months  near  the  planting  surface  peripheries.  Lastly, \nstructure proximity  refers  to the distance between  a plant  and  a  structure, preferring  slow \ngrowth nearer to the structure to reduce risk of root damage to the foundation. \n\nThe  “environmental  scores”  (elevation,  sun  exposure,  and  salt  intensity) of each  potential \nplanting spot on the site are compared to each “plant score” of a particular plant typology \n(Fig. 2). The difference between the “environmental score” and “plant score” determines the \nresiliency of the proposed plant, with a lesser difference resulting in higher probability of \nresilience. It is possible for the algorithm to compare all 278 plants for every potential location, though it would leave too many options available for the user and result in an unclear \ndirection. Parsing plants into specific landscape structures provides opportunity for a better \nstructured landscape (RAINER & WEST 2015). \n\nFlexibility \n\nThe  uniform  analysis  and  subsequent  visualization  of  a  landscape  allows  for  a  variety  of \nplanting  regimes  to  occur.  Planting  locations  can  be  derived  from  the  algorithm  itself,  or \ndecided by a user making informed decisions from the data. \n\nAlgorithm-derived  planting  plans  can  be  applied  in  a  few  ways,  provided  a  distinction  is \nmade for the specific plant typology being used in each spot (overstory, understory, tall grass, \netc.).  Grids,  attractor  curves,  and  random  points  are  options  when  considering  automatic \nplanting proposals (Fig. 3). Further exploration in parametric design tools such as Grasshop-\nper may offer informed layouts with emphasis on user comfort and more complex designs. \n\nThe user  may  defer  to  stylistic  choices based on  a  desired  theme  (naturalistic  landscapes, \nEnglish gardens, etc.). For manual placement of plants, collections of points can be projected \nonto the site surface and analysis be drawn for those, where inputted plant patterns and sizes \nare matched to the “best fit” plant for that location. The variability of planting styles allows \nfor highly unique landscapes designed by the end user. To aid in this process, planting pattern \ndiagrams were developed to demonstrate the core principles of various landscape styles. A \nfew selected styles being presented to the user show successful patterns easily replicable from \nan amateur designer’s perspective and increase the appearance of legible design intent. \n\nAlgorithm to Browser-based Tool \n\nThis algorithm can be interpreted as a backend process for a planting design tool intended \nfor people inexperienced with landscape design. Given that the tool is able to suggest planting \nstrategies from topographic information, it is possible to derive this information from other \nsources using real-world data and leave the user with freedom to focus on designing their \nspace. Further, incorporating heterogeneous (containing structure and hierarchy, biodiverse) \nlandscape  design  in  homeowner-designed  landscapes  improves  landscape  perception \n(KHACHATRYAN et al. 2020). These positive attributes prompted the idea of a browser-based \ntool that can aid in the creation of a homeowner’s landscape design. \n\nTranslating a Grasshopper script to a programming language is a fairly straightforward task, \ngiven  that  Grasshopper  is  in  essence  a  visual  coding  language.  Python  and  Javascript  are \npopular programming languages capable of creating interactive maps, ones which users could \nuse to select site boundaries in their respective regions. An application programming interface (API) allows for communication between two or more computer applications, such as \nan individual computer requesting data from a large online database. Integration of Open-StreetMap (OSM) and United States Geological Survey (USGS) API allows for up-to-date \ninterpretation of landscapes with OSM providing location data, and USGS providing elevation data. \n\nOpenStreetMap is a free, open-source online mapping service that uses volunteer-provided \ninformation to gather location data, along with deriving maps from Bing aerial imagery and \nother mapping techniques (OPENSTREETMAP). Overpass API is a resource for an application \nto request read-only data in a variety of formats for any particular use due to the open-source \nnature of the service. To obtain specific site data, a bounding box is drawn over a map and \ncoordinate boundaries are established. The software requests any road and building geometry \nintersecting or within the boundaries from Overpass, which can be interpreted and shown in \nthe map view. \n\nThe National Map (TNM) is a project by the USGS’s National Geospatial Program to con-\nsolidate  downloadable  products  into  one  location  for  all  public  and  private  use  (UNITED \nSTATES GEOLOGICAL SURVEY). The TNMAccess API allows developers access to multiple \ndatasets, and will use the highest resolution dataset for the desired location request. To obtain \naccurate elevation data, the Elevation Point Query Service returns the elevation in requested \nunits at a specified latitude and longitude. Coordinates from the initial OSM bounding box \ncan be used to create a rectangular array of coordinate points and sent as a request to TNMAccess, where surface analysis can begin. Once a site is selected, the user can demarcate structures, roads, and potential barriers to plants that are present but not recorded to cull any areas \nincapable of supporting plant life. \n\nThe development of a user interface or user experience (UI\/UX) can lower the barrier of entry \nto individual landscape design. User interface is considered the format in which users see and \noperate the software, which should contain simple and concise language to explain the concepts at play in landscape design such as elements of analysis, plant environment descriptions, and list of results provided by the algorithm. This is considered front end development: \nthe side that the user is allowed to see. All the user’s inputs are relayed to the back end of a \nsoftware to be analyzed before a response is sent back to the front with a clear visual result \n(Fig.4). \n\nUser experience is the act of using the software and experiencing it through various steps to \nproduce a valid result. The flow of this process includes clear language to describe what each \ncomponent is and how it changes the result. This includes the language and response of any \nanalysis performed by the application. The goal of this experience is to provide the user with \na simplified approach to landscape design, performing site-specific landscape analysis in the \nback end to produce a tailored result for further consideration. \n\nDiscussion \n\nThe creation of a digital planting tool geared towards the general public provides an accessible platform that can elevate the ecological diversity and architectural quality of typically \nunderutilized landscapes. The use of this proposed process does not necessarily end with the \nindividual user in a single-family home setting, but can be applied in a broader application \nfor use in community organizations and people interested in improving the landscape of their \ncommunity spaces. \n\nWhile the project does accurately complete the task of planting design, it performs mainly as \nbackend development with complex inputs. Further exploration into user interface and user \nexperience front end could improve the clarity of language and process of the tool, and ultimately may be able to compile a custom document regarding maintenance and further resources for the end user for future planning. Further back-end analysis of landscape can increase the accuracy of the results regarding unique species preferences, or the variable shade from vertical layers of vegetation. \n\nCertain limitations apply to the accuracy and scope of an accessible planting tool, with datasets  needing  to  be  researched  and  formatted  to  a  uniform  spreadsheet.  One  approach  to \nexpanding this process into a United States-wide resource would be the use of the Federal \nHighway Administration’s Ecoregional Revegetation Application (ERA). This resource is a \ncompiled list of plant species and related information found in ecoregions across the entire \nUnited States, including Alaska and Hawaii (STEINFELD et al. 2007). \n\nThrough  this  framework,  it  is  possible  to  begin  the  development  of  a  tool  that  brings  informed, site-specific planting information to the general public. \n

Robots in the Garden: Artificial Intelligence and \nAdaptive Landscapes  \n\nAbstract: This paper introduces ELUA, the Ecological Laboratory for Urban Agriculture, a \ncollaboration among landscape architects, architects and computer scientists who specialize \nin  artificial  intelligence,  robotics  and  computer  vision.  ELUA  has  two  gantry  robots,  one \nindoors and the other outside on the rooftop of a 6-story campus building. Each robot can \nseed, water, weed, and prune in its garden. To support responsive landscape research, ELUA \nalso includes sensor arrays, an AI-powered camera, and an extensive network infrastructure. \nThis  project  demonstrates  a  way  to  integrate  artificial  intelligence  into  an  evolving  urban \necosystem, and encourages landscape architects to develop an adaptive design framework \nwhere design becomes a long-term engagement with the environment.  \n\nIntroduction \n\nIn the discipline of landscape architecture, a major epistemological framework has assumed \nthat the environment is a closed and static system that can be measured, predicted, and conceivably controlled by technology (LYSTRA 2014). The reality of severe climate change challenges this view. Although advancements in industrial technology have given humans some control over their environment, carbon continues to be released into the atmosphere at unprecedented rates. While science rigorously measures and predicts the increasingly grim im-\npact of human decisions, quantities of computer-processed data and complex control policies \nhave yet to provide straightforward solutions to climate change. This paper argues for a research paradigm where artificial intelligence (AI) helps adapt landscapes to a changing environment rather than control them. It considers the role AI can play within this new focus on adaptivity, and how they can contribute to an adaptive design framework that requires a long-term engagement with the environment. \n\nELUA, the Ecological Laboratory for Urban Agriculture, offers a case study for an evolving \necosystem, embedded with AI, that responds to the uncertainties of a changing climate. In a \ncollaborative endeavor among computer scientists, landscape architects and architects, two \ncommercial gantry robots and an extensive infrastructure support cultivation in two polyculture  gardens (Figure  1).  Our  work  includes  construction  and  customization  of  the  robots, \nincorporation of sensor arrays, an AI camera, and network infrastructure, as well as the design \nand construction of the garden beds.  \n\nIn the past two decades, many landscape programs have built laboratories with machinery \nand a “lab culture” as both research and education infrastructure. Examples include Alexander Robinson’s work at the Landscape Morphologies Lab of the University of Southern California(ROBINSON  &  DAVIS  2018);  Bradley Cantrell  and Xun  Liu’s work  with hydromorphology tables at the University of Virginia and Harvard University (LIU 2020); Matthew Seibert’s work at Milton Land Lab;1 and Ilmar Hurkxken and Christophe Girot’s Robotic \nLandscape work at ETH Zurich (HURKXKENS 2020). Each of these develops landscape laboratories that integrate physical spaces with customized tools and machinery. \n\nThis phenomenon mirrors the 21st-century development in landscape theory that prioritizes \ndynamic landscape processes and ecological evolution over static forms. Landscapes are imagined to evolve with recursive and process-based strategies over time, instead of as a one-time construction. Projects such as the Fresh Kills and Downsview Park competitions in the early 2000s weres examples of this design paradigm (CZERNIAK 2001, REED & LISTER 2014). Since then, many scholars have incorporated a broad range of ideas and concepts from both sciences and humanities to diversify and develop that paradigm. They include multispecies \nco-production,  novel  ecology,  feral  ecology,  and  cyborg  landscapes  (HOUSTON,  HILLIER, \nMACCALLUM, STEELE & BYRNE 2018, KLOSTERWILL 2019, PROMINSKI 2014). In addition, new tools have been imagined for integration into landscape systems that would execute process-based strategies and co-evolve with other landscape actors (CANTRELL & HOLTZMAN \n2015). \n\nOur research contributes to this body of work in theory and practice. We view an intelligent \nsystem, like its human counterparts, as an imperfect agent, rather than an omniscient, omnipotent black box. The perspective  of  collaborative  intelligence (EPSTEIN  2015) provides  an emergent, constructive view of artificially intelligent agents that participate in and support a collaborative  design  process.  We  envision  an  alternative  future  where  technology  plays  a more integral role in adaptation to rapidly changing environments. \n\nThis paper documents the design, construction, and preliminary testing of ELUA, and provides practical recommendations for such landscape laboratories. It also reflects on the ramifications of ELUA for landscape design and argues for a new research paradigm where AI \nis  an  integral  part  of  evolving  ecosystems.  From  our  perspective,  landscape  design  is  no \nlonger a finished product, but a long-term engagement and collaboration with an assemblage \nof actors, including AI systems, that co-creates an evolving ecosystem.  \n\nTechnologies and Design-build  \n\nELUA has two sites within the Spitzer School of Architecture at City College of New York, \nan outdoor garden on a rooftop and an indoor garden in a communal area near the landscape \nstudios. The outdoor garden (shown in Figures 1 and 2) focuses on growing food; the indoor \ngarden (shown in Figures 1 and 3) is used for education, prototype research, and experimental \ndevelopment.  \n\nInitial Hardware and Software \n\nBoth ELUA robots are from FarmBot,2 a California-based firm that designs and markets open-\nsource commercial gardening robots, and develops web applications for users to interface with \nthose robots. These are gantry robots that operate in three dimensions and employ interchange-\nable tool heads to rake soil, plant seeds, water plants, and weed. FarmBots are highly customi-\nzable; users can design and replace most parts to suit their individual needs. For ELUA, we have \ndesigned and 3D-printed our own watering nozzles, seeders, seed troughs, and camera mounts. \n\nFarmBot’s supporting code for farm design and robot control is also open source; users can \ncustomize  it  through  an  online  web  app.  This  allows  us  to  revise  or  replace  the  provided \nexecutable programs and to introduce new functionality into ELUA. The basic FarmBot code \nvisualizes garden designs before planting, photographs the garden, and provides primitive \nsensing and behaviors. \n\nCustomization for Robot-assisted Gardening  \n\nWe customized each of ELUA’s robots in several ways for our indoor and outdoor gardens, \nand both systems function as intended. ELUA’s rooftop robot has 2-meter tracks from a third-party vendor tailored to the spatial parameters of its site; they replace FarmBot’s original (x-axis) 1.5-meter robot tracks. To install the tracks on the I-beams on the rooftop, we designed \nand built our own joints. We developed planters made with standard milk crates lined with a \nlayer of geo-fabric. This modular approach provides flexibility to the entire rooftop design \nand  installation.  The  5'x5'  structural  frames  are  custom-built  with  10-foot,  16-gauge  steel \ndrywall studs and tracks, cut to size and assembled on-site with L-shaped corner clips. The \nstructural frames fit between two I-beams and support milk crate planters or wooden planting \nboxes (Figures 1, 2 and 4). As shown in Figures 1 and 3, the indoor system consists of a \nblack-pipe armature for the robot and mobile garden beds, instead of gantry tracks fixed directly onto the garden beds as suggested by FarmBot Inc.. This armature design allows us to \nremove and replace mobile garden beds if needed without deconstructing the entire gantry.  \n\nThe rooftop garden will eventually be used by a student group to produce food. In contrast, \nthe indoor garden is intended for more advanced experiments, where we will prototype and \ndevelop new algorithms, tool heads, and operations to be used for both robots.  \n\nEach FarmBot includes a camera that photographs the garden, with software to roughly stitch \nthe images together. We developed algorithms to improve image mosaicing (DICKSON et al. \n2002, MOLINA & ZHU 2014). Meanwhile, we installed a second, more powerful AI camera \nOAK-D camera3 to perform more advanced computer vision tasks, such as depth detection, \nweed detection and plant identification. We have used this AI camera and developed seamless image stitching for two-dimensional aerial views in ELUA that are more accurate and \nmore visually appealing.  \n\nInitially,  the  user  describes  the  garden’s  contents  to  a  FarmBot  as  a  simple  placement  of \nplants from the provided “plant dictionary” on a garden map, a two-dimensional grid visualized by the web app. FarmBot stores the location of each plant as a datapoint (x, y) on that \nmap. Other emerging plants, if detected by the camera, are treated uniformly as “weeds” that \nshould be managed by the robot. FarmBot’s software has no plant identification algorithm to \ndifferentiate between different weed species. Some “weeds,” however, such as dandelion and \npurslane, are edible, while others, such as red clover, can fix nitrogen and support soil health. \nWe expect that these species could play important roles in an urban polyculture garden and \nincrease urban biodiversity and resilience. Thus, we intend to process images from the AI \ncamera with deep learning models to detect such opportunistic species, record their locations \nin the garden map, and have the robot cultivate all welcome but unanticipated plants.  \n\nMultimodal Sensing and the New Database \n\nIn  addition  to  the  FarmBot  armature,  our  gardens  are  designed  to  benefit  from  additional \nsensors. We have incorporated an array of capacitive soil-moisture sensors connected to a \nmicrocontroller with a WiFi module. Our outdoor garden also includes a personal weather \nstation  connected  to Weather  Underground. With  their  application programming  interface \n(API) service, we can access real-time and historical weather data, as well as a seven-day \nweather forecast from the rooftop. Additional sensors could be similarly installed to measure \nother environmental factors, such as solar radiation, CO2, and air pollution.   \n\nTo incorporate this sensor data into ELUA, we have created a virtual server that hosts our \nown database as well as any API services. This greatly expands ELUA’s capability because \nit connects each FarmBot to other types of open data and services. For example, with weather \nforecast data, ELUA could modify scheduled watering regimens for precipitation and drought.  \n\nMachine Learning and AI \n\nAI is pervasive in this research. Non-experts. including many landscape researchers, often \nthink of an AI system as a general artificial intelligence that addresses multiple goals simultaneously. In ELUA, however, AI algorithms are individually built for specific tasks.  \n\nIn ELUA, the AI camera we added processes images with OpenCV, an open-source computer \nvision  and  machine  learning  software  library.  This  provides  machine  learning  algorithms, \nincluding pre-trained deep neural network modules that can be modified and used for specific \ntasks, such as measuring plant canopy coverage and plant height. Machine learning and AI \nplanning can also be used with the multimodal sensory data described in Section 2.4 to provide data-driven guidance to improve garden management. \n\nAn AI system that relies on reinforcement learning (RL) develops a policy for its behavior \nwhen it is rewarded or punished for the outcome of its actions. Such systems have devised \nunexpected behaviors in Go, chess, and some video game that expanded human players’ understanding of these games and provided new insights (SCHRITTWIESER et al. 2020). Landscape architects and ecologists now also imagine how RL systems might manage the environment and construct wild landscapes (CANTRELL et al. 2017, ZHANG & CANTRELL 2021). One research team conducted RL experiments to prune a polyculture garden with a FarmBot \nto increase biodiversity (PRESTEN et al. 2022). We will perform RL experiments in a simulated environment with a virtual robot and later test the learned policies with a physical robot. \nWe envision a version of ELUA that will evolve and propose novel methods, such as combinations of different watering nozzles and watering paths in different scenarios. We hope some \nunexpected  combinations  will  surprise,  intrigue,  and  inspire  us  with  their  successful  outcomes that help the garden adapt to a changing climate.  \n\nResults and Discussion \n\nIn the fall of 2022, we planted herbs on the rooftop and lettuce and herbs in the indoor garden \nto learn and test the basic functions of the robots. (The indoor ELUA robot in action appears \nin  a  brief  video  at  https:\/\/youtu.be\/VTec_SXO5Lk.)  Both  ELUA  robots  captured  garden \nmaps and carried out watering events as expected, although maintenance and troubleshooting \nare needed from time to time. This is a design-build project and every aspect of ELUA was \nconstructed  by  faculty  and  students.  Our  team  has  gained  hands-on  knowledge  as  it  constructed ELUA. Here, we offer three recommendations.  \n\nFirst, in cities like New York, public and free resources are available for academic research \nand well worth the time to track down the networks of organizations and groups. We received \n3 cubic meters of free, clean mineral soil from the NYC Clean Soil Bank hosted by the NYC \nMayor's Office of Environmental Remediation. The soil was delivered by the crews from the \nNew York Restoration Project. We received 40 bags (1 cubic meter) of compost from the \nNYC Composting Project hosted by Big Reuse. In return, we took the Big Reuse crews to \ntour ELUA, and hope to maintain this relationship. We learned about our current soil mix (⅓ \nmineral  soil,  ⅓  compost,  and  ⅓  perlite)  during  a  free  guided  tour  of  a  green-roof  facility \nhosted by the NYC Department of Parks & Recreation. ELUA would have been far more \ncostly and difficult to construct without these public resources. We encourage prospective \ndevelopers to seek out similar assistance. \n\nSecond, “open-source” offers adaptivity to its users but also forewarns the necessity of substantial troubleshooting. Although FarmBots appears to be user-friendly, some knowledge of \ncomputer science and electrical engineering is required to set up such a system. Researchers \nand research assistants from our Departments of Computer Science successfully overcame \nmany issues during our installation. Moreover, a licensed architect on the team successfully \ndesigned and constructed the garden beds and installed a FarmBot onto the existing load-bearing rooftop structure. A multidisciplinary team of computer scientists and designers is \nhighly recommended to replicate ELUA.  \n\nFinally, institutional knowledge is important in academic research. Institutional structures in \nuniversities, especially public schools, can support or hinder academic research visions. Researchers need to be nimble and adaptive in pursuing their goals. For example, thanks to the \nArchitecture School, the Colleges, and the University, we received multiple seed funds for \nELUA.  The  Architecture  School  also  provided  space  and  infrastructure  to  house  ELUA. \nSome rules, however, could not be bent. Because the University’s security regulations blocked \nnetwork ports used by a FarmBot, we have had to set up alternative 5G Wi-Fi hotspots untill \nthe University can provide a research-only network. We recommend an ample buffer in the \nresearch schedule to account for unexpected circumstances, as well as a good rapport with \nuniversity offices.  \n\nConclusion  \n\nDespite ELUA’s practical focus on urban food production, it is also a thought experiment \nthat challenges landscape architects’ conventional views on agency and intelligence. With \nELUA, we want to formulate a theory that questions how the environment is conceived and \nconstructed.  To  some  extent,  ELUA  offers  a  glimpse  into  an  ecosystem  of  computerized \necology where the relationship between humans and plants is deeply mediated and, at the \nsame time, enabled by sensors, controllers, computer hardware, layers of computer code, and \nonline servers.   \n\nA  problem  in  the  perspective  of  landscape  architecture  is  its  current  conception  of  AI  as \nintelligence embodied in a single agent (CANTRELL & ZHANG 2018). Ideas in posthumanism, \nhowever, such as assemblage and sympoiesis become new concepts to reframe agency and \nintelligence as distributive (BENNETT 2010, HARAWAY 2016, TSING 2015). From this posthu-\nmanist  perspective,  intelligence  should  be  viewed  as  an  emergent  epiphenomenon  arising \nfrom the interactions of an assemblage of actors – humans, AI agents, animals, and plants. \nThis framing sheds light on a new landscape design paradigm based on co-evolution among \nbiotic  and  abiotic  agents.  We  could  allow  AI  systems  and  plant  and  animal  agents  to  co-evolve to create novel ecosystems that inspire us with new methods to construct the land-\nscape.  \n\nIn  this  new  landscape  design  paradigm,  AI  agents  would  no  longer  simply  model  human \nbehavior under human control. Instead, they would become co-creators among human and \nnonhuman  actors.  They  could  offer  novel  approaches  and  long-term  cultivation  strategies \nthat humans can learn from and use to adapt to the changing climate. ELUA is a physical \ndemonstration of this new paradigm of landscape design. It provides empirical evidence that \ncollaboration among AI agents and other human and nonhuman actors is within reach.  \n

Toward Acoustic Landscapes: A Digital Design \nWorkflow for Embedding Noise Reduction in \nGround-forming  \n\nAbstract: Noise pollution is considered the number two environmental health risk in Europe, and there \nis increasing global awareness of the health risks associated with noise exposure. As urbanization expands, a growing number of people are exposed to urban noise, to which airports and large urban infrastructure are significant contributors. Unlike indoor noise, which is extensively addressed using digital \ntools in architecture, there are limited parallel efforts in landscape architecture. In this context, mitigating outdoor noise through ground forming can replace the standard use of sound barriers and offer noise \nreduction means together with recreational use. The paper presents and demonstrates a digital workflow \nfor designing acoustic grounds. The workflow links environmental noise data, parametric design, and \nacoustic simulation in a single design environment. A case study site adjacent to Munich Airport is \nused to demonstrate the workflow and comparatively examine the acoustic performance of different \ndesign patterns. The results indicate a possibility of reducing noise levels through ground forming.  \n\nIntroduction: In Search of a Vast, Horizontal Acoustic Tile \n\nIn  2011,  noise  pollution  was  named  the  number  two  environmental  health  risk  in  Europe \n(WORLD HEALTH ORGANIZATION 2011). Since then, the World Health Organization (WHO) \nhas  constantly  been  updating  the  health  risks  associated  with  noise  exposure  (WORLD \nHEALTH ORGANIZATION  2022).  As  urban  areas  expand,  the  number  of  people  exposed  to \nurban  noise  grows.  Airports  and  large  urban  infrastructure  significantly  contribute  to  this \nnoise (BOUCSEIN et al. 2017). In addition, the propagation of noise caused by transport infrastructure has been shown to increase through poor urban design (MORILLAS et al. 2018). Existing methods for mitigating outdoor noise typically consist of prefabricated, vertical indus-\ntrial acoustic walls. In contrast to acoustics in architecture, where digital tools are used to \ndesign and fabricate site-customized acoustic tiling, outdoor noise is not  met with similar \nmeans. However, preliminary examples indicate that formed grounds could mitigate sound. \nDespite this potential, there is a lack of dedicated methods for creating acoustic grounds in \nlandscape architecture. Mitigating noise through acoustic grounds could be beneficial for urban airports, which are typically bordered by buffering open spaces. In such areas, ground \nforming can provide noise mitigation as well as public recreational use. \n\nContext and Objective \n\nDigital tools enable the introduction of preciseness into landscape architecture design and \nmaking (CANTRELL & MEKIES 2018). Preciseness is defined here as the process of highly-articulate tailoring of a design for mitigating a natural phenomenon. In this context, the design of acoustic grounds requires linking environmental data relating to noise to a landscape \ndesign aiming to mitigate it. The research seeks to promote noise mitigation through what \ncan be viewed as the horizontal, ground-made, site-tailored version of an acoustic tile. To this \nend, the paper presents a digital design workflow for embedding noise reduction and simulating \nacoustic performance in landscape architecture. The workflow is based on a method for incorporating  acoustic  analysis  in  landscape  architecture  design  developed  by  the  authors  (BARSINAI et al. 2023). This is demonstrated through a case study site in Hallbergmoos, adjacent to \nthe  Munich  airport,  which  is  amongst  the  ten  busiest  airports  in  Europe  (BOUCSEIN et  al. \n2017). Currently, no physical noise reduction measures exist in the area (Figure 1). \n\nState-of-the-art: Design and Simulation of Acoustic Grounds \n\nThere is a growing awareness of the need to protect from noise in outdoor spaces (SORVIG & \nTHOMPSON 2018). In the context of airport noise, mitigation is addressed through three levels: a primary level, which targets the noise source and is applied during the production \nof aircraft; a secondary level, which adapts aircraft arrival and departure procedures; and \na tertiary level which includes measures by the local airport of aviation authority grounds \n(NETJASOV 2012).  \n\nUntil recently, tertiary-level noise mitigation measures around airports did not include the \nformation of acoustic grounds. This possibility is beginning to be explored in landscape architecture projects, demonstrating a capacity to mitigate noise and vibrations through targeted \nground forming. For example, Buitenschot Park demonstrates a reduction of the Schiphol airport noise through the construction of ground ridges and furrows. The park design distorts \nand disperses low-frequency noise waves, which have been reported to reduce the noise surrounding the airport by 10 dB (TASHAKKOR et al. 2020). A similar approach employed ground-\nforming for mitigating vibrations around a MAX Lab IV in Sweden  \n\n(WALLISS & RAHMANN 2016). These two examples challenge the standard practice of constructing absorbing sound barriers surrounding urban noise sources. However, there is still a \nlack of methods for performing noise mitigation through ground forming.  \n\nAcoustic simulation is often performed using stand-alone tools (SAKUMA et al. 2014), and as \nsuch, they do not readily support design iteration. While there are dedicated frameworks to \nintegrate acoustic simulation in architectural design (PETERS 2015), there is a lack of similar \nmethods for embedding noise reduction in landscape architecture design processes. Pachyderm, a recently developed open-source tool, performs ray tracing-based sound propagation \nsimulation and visualization embedded in 3D design environments (VAN DER HARTEN 2013). \nHowever, despite the availability of Pachyderm, there is still a need for methods for applying \nit toward noise reduction in the design of open spaces. The lack of such methods currently \nlimits the possibility of addressing noise in landscape architecture and urban design. \n\nAcoustic Landscape Design Workflow \n\nThe paper addresses these gaps by developing a digital workflow that links noise, design, and \nsimulation for embedding acoustic performance in landscapes. The workflow consolidates \nthe design and simulation in a single digital environment. It consists of noise data analysis, \nthe design of parametric ground formations, and acoustic simulation and evaluation. \n\nNoise Data Analysis: Combining Online and On-Site Measurements  \n\nAirport  noise  consists  of  ground-level  noise  as  well  as  noise  produced  by  aircraft  during \ntakeoff,  landing,  taxiing,  and  idling  stages.  The  Munich  Airport  tracks  noise  in  real-time \nthrough monitoring stations and provides publicly available data (MUNICH AIRPORT 2022). \nHowever, only one of the monitoring stations is positioned on the perimeter of the site. In \naddition, the monitoring stations are situated 4 m above ground. This height is determined by \nthe  Environmental  Noise  Directive  (END)  (EUROPEAN COMMISSION  2002),  as  it  is  where \nreflections from the ground stop playing a major role. Measurements performed above 4 m, \ntherefore, allow the official comparison of different contexts. This calculation method is also \nthe basis for all the noise abatement measures. However, measurements at the height of 4 m \nlimit the possibility of understanding the noise as it is perceived by a listener on-site.  \n\nTo  sense  the  noise  as  it  is  felt  on-site,  the  study  combined  online  data  with  on-site  noise sampling using mobile phones. The use of mobile devices has become an increasingly prevalent method for collecting environmental data (MURPHY & KING 2016). The noise sampling \nwas conducted using five different devices. The devices simultaneously recorded the noise \nlevels during takeoff and landing on five points on-site for 90 consecutive seconds (Fig. 2). \n\nDespite their limited accuracy, and while at this point of the research, no identifiable rela-\ntionship between the official and on-site sampling could be specified, the mobile phone meas-\nurements provided a picture of the felt noise levels on the ground. This noise, therefore, also \nincludes the ground effect which the official measurement stations exclude. While the official \naveraged noise contour (BAVARIAN MINISTRY OF ENVIRONMENT 2021) is limited to the runway areas (Figure 1), the on-site noise sampling recorded peaks of 75 dB and above beyond \nthe airport fence and within Hallbergmoos, underscoring the need for noise abatement in the \narea notwithstanding the averaged noise levels. \n\nDesign: Parametric Ground Formations \nThe design first proposed a basic layout for the park and coupled the desired noise mitigation \nstrategy with urban design considerations. The plan defined the movement paths, program, \nand areas dedicated to acoustic ground-forming (Figure 3). In these areas, the research tested \nformations consisting of mounds with public paths located between them. Four design patterns were tested at three heights: 2.5 m, 5 m, and 7.5 m. These included: high-to-low (HL), \nundulating mounds in gradually decreasing heights (7.5\/5\/2.5 m – 0.5 m); low-to-high (LH), \nundulating  mounds  in  gradually  increasing  heights  (0.5  m  -2.5\/5\/7.5  m);  constant  height \nmounds (CM), featuring uniformly sized undulating mounds (tested in 2.5\/5\/7.5 m); and constant heigh solid ground (CS), an elevated ground without any undulation mounds (tested in \n2.5\/5\/7.5 m). The width of the mounds was set to 21 m to provide an inclination and slope \nthat allow public accessibility even in the higher-mound instances (Figure 3).  \n\nAcoustic Simulation and Evaluation \n\nThe simulation process includes several aspects:  \n\n1)  A  base  model  –  the  base  model  aimed  to  reproduce  the on-site  conditions  and  noise \nlevels  as  measured  before  any  ground  modification.  In  construction,  base  models  are \noften referred to as 'digital twins,' a digital environment that simulates the existing condition in the physical environment (BOSCHERT & ROSEN 2016). The topography and the \nbuilt fabric were created using Blender with imported layers derived from Shuttle Radar \nTopography Mission (SRTM) data, a GIS data source with a 9-16 m accuracy. The model \nwas then placed in a bounding box to support the acoustic simulations.  \n\n2)  Noise source (emitter) – the noise source was introduced into the model and situated on \ntakeoff lane 08R\/26L at a level of 75 dB. The simulation works as a transfer path that \ndisperses the noise. The transfer path holds a noise spectrum with several frequencies. \nHowever, simulations, including the presented ones, rarely include all frequencies.  \n\n3)  Simulation tool – for simulating sound propagation, the research employed Pachyderm \nRC 26, an open-source tool that integrates acoustic simulation and visualization in a 3D \ndesign environment (VAN DER HARTEN 2013). Pachyderm provided a ray tracing-based \nmethod and was integrated into Grasshopper and then linked to the base model. This \nintegration allowed performing the design and the simulations in the same digital environment. The research employed an i7 processor, 32GB of RAM, and an Nvidia Quadro \n1000 graphics card which could not perform a full-site acoustic simulation due to insufficient processing power, memory, and graphic processing.  \n\n4)  Noise sampling grid and points (receivers) – to address the simulation challenge, the \nstudy  developed  a  method  for  sampling  the  acoustic  performance  using  a  grid  with \n20\/20m size cells. Within this grid, three listener points were positioned as noise receivers at a height of 1.65 m (Figure 4). Point 1 was closest to the airport runway, point 2 \nwas in the middle of the site, and point 3 was in the residential area of Hallbergmoos. \n\nEvaluation – while the simulation calculated noise levels, the evaluation focused on the relative reduction in the noise levels as measured in the model before and after the design intervention and reformed grounds. The focus on the reduction levels allowed us to analyze noise \nreduction as a trend and compare the acoustic performance of the different design scenarios. \n\nResults \n\nThe results of the simulations, summarized in the table (Table 1) and graph (Figure 5), can \nbe viewed through three aspects: listener points, design, and height. Each scenario is referred \nto according to the initials of the design pattern, followed by the height in m (i. e., CS2.5). \n\nPerformance by Listener Point \n\nPoint 1: The best noise mitigation was achieved here with HL5 (28.4 dB reduction) followed \nby CS2.5 (27.5 dB reduction), C7.5, and CS7.5 (27.4 dB reduction). Last ranked the LH 2.5 \n(1.3 dB reduction).  \n\nPoint 2: The best noise mitigation was achieved with C5 (26.4 dB), poorest performance \nwith LH2.5 (1.3 dB reduction).  \n\nPoint 3: The best mitigation is achieved with CS7.5 (21.9 dB reduction), followed by C5 \n(21.6 dB), then CS5 and HL7.5 with a similar noise reduction (20.5 dB reduction). The poorest performance was CS2.5 (16.2 dB).  \n\nPerformance by Design Pattern \n\nHigh-to-low (HL): This design showed the most effective noise reduction levels at point 1 \nfor all heights. The best performance was achieved with 5 m mounds which showed an average reduction of 23.9 dB across the three points.  \n\nLow-to-high (LH): In this design, there was a significant difference in effectivity between \nthe lower mound patterns (2.5 m) and the 5 and 7 m high ones. The best average reduction \nacross the three points was attained by the 5 m high mounds (23.1 dB).  \n\nConstant  mounds  (C):  This  design  demonstrated  highly  effective  noise  reduction  levels. \nThe highest effectivity was attained by the CM2.5 (24.7 dB), followed by CS7.5 (24.7 dB), \nand CS5 (24.1 dB).  \n\nConstant solid height (CS): This design demonstrated that CS5 performed better than CS2.5 \n(4.3 – 2.2 dB difference), CS 7.5 performed better than CS5 at point 1, but CS5 performed \nbetter than CS 7.5 m at points 2 and 3. The average reduction of CS5 and 7.5 m was equal.  \n\nPerformance by Height \n\n2.5  m:  Constant  mounds  perform  best  at  all  points,  followed  by  the  constant  solid.  LH \nshowed the poorest performance at points 1 and 2 and then a significant increase in performance once height was reached toward point 3. At point 3, HL and LH performed with a \nnegligible difference.  \n\n5 m: At this height, the HL pattern demonstrated the best performance at point 1. In average \nperformance, however, constant mounds outperformed HL with an average reduction of 24.1 \ndB compared to 23.8 dB by HL. CS followed with an average reduction of 20.67 dB.  \n\n7.5 m: At this height, HL, C, and CS all lost effectiveness with distance. The LH section \nperformed marginally better at point 2 than at point 1 (0.4 dB). All the designs at this height \nshowed the lowest noise reduction at point 3. \n\nDiscussion \n\nAcoustic Performance Trends \n\nIn line with existing works, the results indicate a possibility of reducing noise levels through \nground forming. A few noteworthy trends can be summarized based on the acoustic performance results:  \n\n1)  Higher does not equal better – while we would expect the highest mounds to perform \nbetter,  the  simulations  showed  that  performance  is  impacted  more  by  design  than  by \nheight. The  best-performed mitigation,  HL5,  was not  achieved  by  the  highest design. \nThe lowest height, C2.5, outperformed other designs and showed an equal performance \nto C7.5. In addition, in comparing the same design at different heights, such as in the HL \npattern, the best performance was shown at 5m rather than 7.5 m.  \n\n2)  The benefit of patterns – the results indicate the benefit of the constant mounds (CM) \npattern in relation to constant solid ground (CS) for mitigating noise. This trend can be \nclearly seen by comparing the CS2.5 m to the other patterns, as well as in the high average performance of CM in relation to the CS 5 pattern.  \n\n3)  High-to-low  outperforms  low-to-high  mound  patterns–  overall,  the  patterns  that \nranged from high to low performed better than the designs ranging from low to high. A \nsignificant  difference  was  seen  with  the  LH2.5  pattern,  which  had  the  lowest  performance (1.3 dB reduction) compared to the other results. An exception is LH 7.5, with \nslightly better performance than HL7.5 at point 2. This trend may indicate that patterns \nincreasing in height in the direction of the noise dispersion provide less effective mitigation unless they are high enough to form a barrier adjacent to the noise source, which \nexplains the improved performance of LH5 and LH7.5.  \n\n4)  Constant height outperforms inclined, high-to-low slopes – constant mounds (CM) \nshow great effectivity regardless of their height, and mounds as low as 2.5 m can yield \nsignificant noise reduction. This result aligns with the effectivity demonstrated around \nthe Schiphol airport, where constant-height ridges and furrows were used and reportedly \nreduced the noise levels by 10 dB (TASHAKKOR et al. 2020). However, in comparison to \nridges, mounds can offer more accessible and versatile public use throughout the park, \nnot limiting public activity to the furrows. \n\nContribution and Current Limitations  \n\nThe paper contributed a novel digital workflow for designing acoustic grounds. The work-flow provides and demonstrates a step-by-step guide for parametrically designing, prototyping, simulating, and evaluating noise mitigation through ground forming. The digital work-\nflow includes several steps – noise analysis, parametric design, and acoustic simulations, and \nallows for comparatively evaluating ground formations to understand their noise mitigation \neffectivity.  \n\nThe  workflow  was  demonstrated  in  a  site  adjacent  to  the  Munich  airport.  The  parametric \ndesigns featured three patterns of undulating mounds and one constant-height solid ground \nat  three different  heights.  The  acoustic  evaluation  was  performed  by  comparing  the  noise \nlevels  before  and  after  the  ground  modification  in  selected  points  within  the  simulation \nmodel. The results indicate the benefit of using constant height, undulating mounds rather \nthan inclined patterns of higher flat elevated grounds as a noise mitigation strategy.  \n\nDue to the complexity of the task at hand, the study faced several limitations, which will be \naddressed in future work. First, the complexity of acoustic simulations increases significantly \nwith the size of the tested area and, along with them, the computing time. This complexity is \na  current  obstacle  in  using  such  tools  in  landscape  architecture.  In  the  future,  this  can  be addressed with improved processing power, memory, and graphics processing capabilities. \nAlternatively, a different simulation method with a lower sampling rate could be used. The site  could  also  be  divided  into  smaller  parts,  simulated  separately  before  their  results  are \ncombined. Second, the research focused on noise propagation and did not look at: the frequency aspect; the effects of weather, a factor known to influence noise; and the effect of \nnoise absorption on the ground surface and the way different ground covers may contribute \nto reducing noise transmission. Finally, future work will also look at additional design patterns, varying heights, and regularity vs. irregularity and consider these factors in relation to \nmultiple noise emitters in various locations. \n\nConclusion and Outlook \n\nThe presented workflow allowed us to comparatively assess the acoustic performance of different landscape designs and discern trends associated with their design features. This contributes to the capacity to embed acoustic performance in landscape architecture and mitigate \nnoise pollution through ground forming. The digital workflow can be used for addressing \nnoise around other airports as well as around other sources of urban noise or large transportation infrastructure. As such, the workflow also promotes a broader endeavor – the development  of  dedicated  methods  which  link  environmental  data  and  parametric  design  toward \nforming performative grounds in response to environmental challenges. \n

Urban Agriculture:  \nClimate-Responsive Design Strategies for \nBlue Infrastructure in the Context of Singapore \n\nAbstract: Global climate change poses various threats to densely developed cities and their urban infrastructures. As an archipelago and product of many significant land reclamation efforts over the centuries, Singapore is most vulnerable to rising seawater levels and faces increasing climate change-related challenges, including intense rainstorms, water scarcity, food shortage, and extreme heat. This \npaper proposes a Synergy System, or climate-responsive computational design method, to create a new \ntypology of urban agriculture integrated with the existing infrastructure. This study employs performance-based generative design to generate typologies and optimize them in terms of driving factors \nsuch  as  food  production  and  microclimate  adjustment.  The  performance  evaluation  model  as  a  tool \nsupports the design of multifunctional and climate-responsive infrastructure typologies. \n\nIntroduction: Climate Challenges in Singapore \n\nAs a highly urbanized city, Singapore has suffered from complex challenges related to climate change, putting climate adaptation in the spotlight for future urban development. Singapore’s main climate crisis includes intense rainstorms, water scarcity, food shortage, and \nextreme heat (NATIONAL CLIMATE CHANGE SECRETARIAT 2022). For instance, frequent extreme rainstorms have put overwhelming stress on the stormwater infrastructure and have caused flash floods in urban areas (PUB 2022). Half of Singapore’s fresh water supply relies \non imports from Malaysia (ROMAN & CHEOK 2016), which will end in 2060. In addition, \n90% of Singapore's consumer food is imported, motivating local food production to achieve \n30% food self-sufficiency by 2030 (TENG & MONTESCLAROS 2019).  \n\nThe increasing challenges on urban infrastructure and residents’ daily lives have raised increased concerns about building a climate-resilient city for the future. The concept of urban climate  resilience  emphasizes  city  and  urban  infrastructure  as  a  complex  adaptive  system \nwith the ability to maintain basic functions in the face of climate disruptions (MEEROW et al. \n2016).  \n\nThe main climate challenges Singapore faces are closely related to three aspects: stormwater \ninfrastructure, urban agriculture, and microclimate. The synergies between them serve as the \nbasic  logic  for  building  climate-responsive  infrastructures.  Concrete  canals,  one  typical \nstormwater infrastructure, cannot accommodate peak flows during extreme rainstorms and \nare ineffectively utilized during the dry season. They are required to be transformed into a \nnew generation of infrastructure bolstering ecosystem services (LIQUETE et al. 2015). This \ncan be achieved by integrating a new typology of urban agriculture with the concrete canals, \nwhich promotes “the well-being of urban ecosystems and resilience in a circular economy.”(DEKSISSA et al. 2021). \nMoreover, decentralized urban farming integrated with underutilized \nspaces is a sustainable way to enhance local food production (SINGAPORE FOOD AGENCY \n2021). Utilizing the rainfall collected in canals for food production contributes to water conservation,  which  is  highly  imperative  to  sustainable  vertical  farming  (ASSOCIATION  FOR \nVERTICAL FARMING et al. 2015). Furthermore, the food-growing structures provide shading \nfor  microclimate  mitigation,  as  vegetation  shading  can  effectively  reduce  solar  heat  gain \n(OLGYAY 1963).  \n\nIn terms of the identified research gap, the synergies among the above three aspects have not \nbeen fully investigated and implemented into a computationally driven design strategy. A \nnew approach is required to explore the material flow of this synergy system and design a \nclimate-responsive typology based on its performance on food production and microclimate.  \n\nCore Synergies \n\nThe core synergies are summarized as follows (Figure 1): The rainfall collected in the stormwater infrastructure can be supplied for food production, while the growing plants provide \nvegetation  shading  for  cooling  the  microclimate.  Stormwater  infrastructure  in  connection \nwith urban agriculture can increase drainage capacity, offer accessible community spaces, \nimprove green features, and create comfortable microclimates.  \n\nThe synergies set the fundamental logic for a new computational design approach to transform existing monofunctional concrete canals into an adaptable and multifunctional infrastructure  typology  for the future. They  not only serve  as a  drainage  facility,  but  also  as  a \nresponsive landscape that interacts with the water. They not only sustain an industrial food \nproduction factory, but also provide diverse spaces for community interaction. Establishing \nthis infrastructure typology requires a systematic approach that explores and evaluates the \nenvironmental interactions related to rainfall and sunlight. This complexity is viewed as a \npositive challenge, to create a new design language connecting the green, the blue, and the \ngrey elements of a city to produce new and vibrant patches of local identity. \n\nClimate-responsive Design Methodology  \n\nComputational Design & Performance-based Design \n\nThe proposed design method aims to generate and optimize a climate-responsive typology \nbuilt on the domains of computational design and performance-based design (CHAOWEN & \nFRICKER 2021). Computational design thinking understands architecture as a dynamic system \ncomposed  of  interactions  between  elements,  rather  than  as  a  static  entity  (MENGES  & \nAHLQUIST 2011). Working with systems rather than forms is the connecting thread between \nspace-making  and  environmental  challenges  of  design.  Performance-based  design  underscores the value of environmental performance that reflects how architecture continually interacts with the surrounding environment. Performance-based design method combined with \ncomputational design has potentials in generating a climate-responsive typology. First, as a \nsystematic approach, computational design enables designers to focus on the connections and \nmaterial flows between the three identified elements and exploit their synergies; second, this \nmethod takes environmental performance as a design driver that facilitates the generation and \noptimization of design solutions. The infrastructure can no longer be seen as static built environments, but as “’dynamic’ self-constructing, living, breathing, and even artificially intelligent (thinking) environments” (CANTRELL & MEKIES 2018, 28).  \n\nPerformance Evaluation Model \n\nThis study develops a new performance evaluation model for the generation and optimization \nof climate-responsive infrastructure by establishing the relationship between the parameters \nand performance related to the synergy system. Within the synergy system, three subsystems \nare defined and connected with the material and energy flow of water and sunlight: the water \nsystem, the food production system, and the microclimate system.  \n\nBased on the connection, the performance criteria and parameters are defined that inform \nhow to build the three subsystems (Figure 2). Firstly, the focus is set on the catchment area \nof the canal and the calculation of runoff variability and water storage capacity, informing \nthe amount of food that can be grown. Secondly, the food production system establishes a \ncomputational model and defines a set of performance evaluation criteria for the food production condition and capacity, including direct sun hours, yield, food type, and water consumption. This paper takes a planting tower from a previous study as the prototype and extracts its relevant parameters (SONG et al. 2022). Lastly, the microclimate system measures \nthe thermal comfort of public spaces, which refers to the radiation performance of the space \nshaded by planting geometries. The Grasshopper plug-ins, Ladybug and Honeybee, are used \nas tools for the simulation and visualization of direct sun hours and radiation. Therefore, the \nkey performance of the model is derived from three subsystems: direct sun hours, yield, water \nconsumption, and radiation of the shaded area. \n\nForm Optimization Experiment \nThe  performance  simulation  starts  with  the  adjusted  prototype  planting  tower  introduced \nabove to explore the impact of different geometric parameters on the performance of food \nproduction and microclimate (Figure 3). Four metrics of performance need to be evaluated: \nyield, direct sun hours, water consumption, and solar radiation of the shaded area. The simulation experiments explore the impact of the variation of three parameters of the planting geometry on these performances: slope, orientation, and curvature. The footprint of the planting tower was kept constant (12.5m2) in all experiments. \n\nThe results can be summarized below: \n\n1. When the slope is in the domain ranges from 60 to 75 degrees, the performance of direct \nsun  hours  and  yield  is  balanced  well.  The  area  with  radiation  lower  than  200W\/m²  is  the \nlargest when the shape of the footprint is similar to a square. \n\n2. The performance of direct sun hours and radiation performs best when the planting surface \nis facing east and west. It can be taken as the main direction of the design. \n\n3. The variation of vertical curvature creates planting areas with different direct sun hours, \nwhich is ideal for producing various types of vegetables. The yield and shaded area are pos-\nitively correlated with horizontal curvature(D), and performance can reach the optimal point \nwhen it is above 0.4m.  \n\nThese main findings inform the designer how to adjust and optimize the form to balance the \noverall performance and achieve the desired state. It ignites the thought that environmental \nperformance should also be considered as an essential design trigger, especially in climate-responsive infrastructure design. In addition to the described performance of food production and thermal comfort, the design needs to consider other aspects, such as function, structure, \nand spatial aesthetics. Nevertheless, the introduced computational workflow offers designers \nan iterative tool to measure the environmental performance of different design solutions and \nunderstand the relationship between parameters and performance (FRICKER 2022). This tool \nfacilitates design trade-offs between environmental performance and other aspects of design \nto enhance the overall quality of infrastructures.  \n\nDesign Speculation for New Urban Typology  \n\nWater Volume Calculation \n\nThe prerequisite for designing a food production factory is establishing a dynamic water system with associated flow processes, specifying the amount of water flowing into the canal \nand used for food production. The first step is identifying the catchment area and the population it serves; the second is calculating the dynamic water supply and consumption for the specific site to deduce the maximum volume of water storage. \n\nFirstly, the upper stream catchment of the Jurong canal can be roughly depicted based on \nland contours and simulation of water distribution using Kangaroo plug-in in Grasshopper. \nThe area is around 9,560,000m² (Figure 4). Among the eight districts in this catchment area, \nHong Kah  and Yuhua West are  the  two  adjacent  to  the  Jurong  Canal with  76,560 people \nliving there. The goal for food production is to reach 30% of the annual food consumption of \nthese populations, which implies an ideal yield of about 370 tons of leafy vegetables. Secondly, this system pumps water during heavy rainstorms to reduce peak flow in the canal and store them to use during the dry season. Taking the rainfall data in 2021 as a reference, the \nmaximum water volume of storage is roughly 4,000 m3, informing the generation of water \nstorage space.  \n\nWater flow and plant growth give this system a dynamic character, where the water storage \nlevel and the amount of food production adjust according to the rainfall patterns. This feedback  mechanism  enables  the  system  to  cope  with  extreme  rainfall  weather  by  effectively \nrelieving the pressure of heavy rainfall on the urban drainage system. This performance echoes the original purpose of developing climate-responsive water infrastructure. \n\nDesign Generation \n\nThe design generation begins with the selection of the optimized type of planting geometry \nby comparing the performance of different forms (Figure 5). The generated geometry of two \nparallel curves has the best overall performance, in line with the results of the simulation \nexperiments. The yield is higher due to the large horizontal curvature (D), which maximizes \nplanting area and allows sufficient sunlight to reach both sides. Moreover, the twisted surfaces  provide  enough  shaded  spaces  with  good  thermal  comfort.  The  key  to  this  iterative \nprocess lies in balancing the performance of yield, water consumption, and solar radiation, \nthus producing sufficient food and creating comfortable public spaces. \n\nThe  design  generation  includes  the  generation  of  planting  geometry,  building  details,  and \nlandscape. First, planting geometry is generated based on the selected type and optimized \naccording to the performance on yield, direct sun hours, and radiation, which is evaluated by \nthe computational model built in Grasshopper. The finalized geometry produces the targeted \nyield and provides several public spaces with solar radiation below 200 w\/m². Second is the \ngeneration of the water distribution system and functional spaces. The water storage space is \nset with a certain amount of flexibility added to the maximum demand to cope with the seasonal variation of future rainfall. The final step is to create a responsive landscape by reshaping the terrain according to water and pedestrian flow.\nGround-level planting areas for flood retention reduce the drainage pressure for the canal during peak periods. Overall, the generated example, the food factory on the lively riverfront offers communal space for people to \nmeet and allow new activities to emerge in the neighbourhood. \n\nConclusion and Discussion \n\nThe  design  of  climate-responsive  infrastructure  typology  driven  by  environmental  performance  is  of  vital  concern  for  creating  resilient  urban  habitats  in  the  future,  especially  for \nintensely developed cities like Singapore. The study fills in the existing gap in the computational design method for generating and optimizing an urban agricultural typology integrated with stormwater infrastructure that responds to climate change. The study explores the impacts of various geometric parameters on the performance for food production and thermal comfort, which serve as the guidelines for finding optimal design solutions for the typology. This design workflow demonstrates the iterative processes of design optimization according \nto the balance of the performance on yield, water consumption and solar radiation, which can \nbe applied to the renewal of other concrete canals. \n\nThis study presents a speculative design proposal combining multiple parameters \ninto a single design model instead of considering the economics of implementation \nand feasibility of the design. The discussion focuses on two aspects: Firstly, how much \neach performance factor contributes to the overall change in the design was not explicitly \nstated in the study, which might depend on the objectives of the design. Future research can \nhence set several design scenarios and discuss the contribution of multiple factors respectively. Secondly, social aspects, such as how to involve people in the food production process, deserve more careful consideration in order to ensure the long-term success of the endeavor. \nIn addition, project planning requires the collaboration of multiple stakeholders, which has \nbeen a constraint to large-scale government implementation of food production in the past. \n

Visualizing and Clustering Eye Tracking within 3D \nVirtual Environments \n\nAbstract: Visual perception is one of the most important sensory processes for most of the population. \nThis process plays a key role in how we navigate and way find in urban environments. A wide range of \nliterature offers insight into the relationship between the structure of urban spaces and navigability, as \nwell as literature identifying how individual differences play a role in how well people can recall elements and navigate environments. Measurement techniques that reveal these differences are often captured as procedurally based evaluations after individuals have navigated through an environment. However, these valuations do not necessarily help us understand the process of how observations link to \nrecall and navigation. In this paper, we show a new technique for conducting eye tracking in 3D virtual \nenvironments to assess the process of observation in urban environments. Further, we demonstrate how \nclustering techniques can be used to improve eye tracking data generated in these 3D environments. \nThe techniques we provide can offer a new means to better understand how form, function, and design \nelements are observed. \n\nIntroduction \n\nIn navigation, the visual perception of an environment plays a significant role in decision-making, as well as informs knowledge about the properties of spaces and the relationships of \nobjects that form the collective environment. One of the ways researchers have attempted to \nunderstand the decisions we make during navigation is to create highly controlled experiments using virtual environments (BRUNS & CHAMBERLAIN 2019). However, many of the \nprevious experiments lack a comparable diversity of objects, routes, scales, and relationships \nbetween these elements in comparison to the real world. The benefit of virtual environments, \nparticularly with modern gaming engines, is that the designer can control all elements within \nthe environment. This offers a means to simplify a problem and employ a more deductive \nscientific process, but it may come at the cost of understanding how perception works holistically within complex spaces. Unfortunately, quantifying perception holistically would require new methods for analyzing the process of perception. Further, a method like this would need to be implemented in complex environments, which can be self-defeating if it requires an oversimplification of the environment itself to operate. Thus, finding a technique that enables both the employment of complex virtual environments and a seamless integration of \nanalyzing  perception  holistically  would  be  a  major  step  in  understanding  the  relationship \nbetween human and environmental interactions. \n\nDesigning spaces for intuitive navigation is an important process for urban designers, campus \n(e. g. business parks) planners, and outdoor recreation trail designers. There are many design \nproblems to undertake in these instances, with navigation and wayfinding within the set of \nissues. Both these processes require individuals to recognize spatial patterns, comprehend \nrelationships of elements, make determinations of how to focus their attention, and remember \nimportant objects or spaces. There have been many approaches to assessing these processes, \nsuch  as  measuring  response  times  and  accuracy  in  remembering  landmarks  or  locations \n(CHRASTIL & WARREN 2015, ERICSON & WARREN 2020, GAGNON et al. 2018, WEISBERG et \nal. 2014) and assessing map drawings of paths and spatial layout (BRUNS & CHAMBERLAIN \n2019, GARDONY et al. 2016, WANG & SCHWERING 2015). However, these measures are usually done post hoc rather than in real time. Further, the measures are usually procedurally \nbased (e. g., memory recall), rather than processes based (formation of memories). To improve our understanding of how process-based navigational activities unfold, we need to understand what drives perception and decision-making. A better understanding could help designers support meaningful relationships between objects to facilitate these perceptual processes. Fortunately, computational techniques can be created and then combined with cognitive science and urban and landscape design principles to better understand how individuals \nobserve and make inferences about those spaces. \n\nEye tracking is one technique that has been used by researchers to better understand the process of observation. It has been used for decades to understand how and why an individual \nfocuses on particular objects, areas, and elements of space. Implementations of eye tracking \nhave been primarily conducted in 2D environments (e. g. looking at a screen or flat image). \nThis includes architectural-related studies (XIANGMIN et al. 2021, ZHANG et al. 2019), with \nlandscape studies emphasizing 2D static images (DUPONT et al. 2016). In landscape and architectural studies, eye tracking is used to identify fixations within scenes, and in psychology \ncan help describe visual attention and arousal (KIM & LEE 2021). With many metrics that \ncan be analyzed from these data, broadly, one major advantage is to provide an objective \nmeasure of perception (DUPONT et al. 2014). Yet, relative to 2D eye tracking studies, there \nis little literature showcasing implementation in 3D dynamic environments. \n\nIn this study, we combine the generation of virtual environments and eye tracking to visualize \nindividual observational patterns in a virtual space. We extend previous work (FERNBERG et \nal. 2022) to showcase a new open-source software package we developed, as well as an analytical framework for representing these data. This paper deviates from the previous by showing specific visualizations and analyses of large datasets that have been analyzed, whereas the previous version was introducing the construct. The purpose of this study is to showcase how eye tracking data can be represented in a dynamic 3D environment and how those data can be analyzed using mathematical clustering mechanisms. We ask, to what extent can eye \ntracking be implemented in 3D gaming environments and analyzed post-hoc to determine \nfixations of objects within virtual urban spaces? \n\nData Collection from the Virtual Environment \n\nFor this work, we employ the Unity gaming engine and prefabricated 3D assets (from Kitbash) to create a procedurally generated urban environment which can be explored in VR. \nThe design of the environment was not intended to be complex or realistic world because this \nis not necessary for the primary purpose of implementing eye tracking and testing different \nclustering techniques. The environment consisted of 40 x 100-meter-long blocks, where each \nblock was one of five different architectural styles. The purpose of this setup was to observe \nif the pattern of clustering was different closer to transitions between different architectural \nstyles compared to areas where changes did not exist. However, in this study, we were merely \nattempting to identify how we might cluster data, the actual test of these transition zones is \nmeant for a later study. For now, all elements are static, the user can make observations freely, \nbut cannot change their location or speed of experience. Further, we have not included any \nother cues, such as sound, lights, or atmospheric changes. Each object was placed along a \ntwo-lane road in succession, with variable spacing between each building.  \n\n\nThe eye tracking software we developed, was created for implementation in the Unity gaming \nengine  only.  For  this  implementation,  the  Vive  Eye  Pro  virtual  reality  headset  was  used. \nWithin Unity, the headset was established as the user camera and the scripts were then associated with this camera. Movement through the environment was maintained at a consistent \nspeed, but the viewer can fully move their head around. The eye tracking software stores the \nlocation and rotation of the user’s eyes at every frame that gets rendered (about 60\/sec). The \ndata from each eye is averaged to create one point and one direction. This direction is, of \ncourse, where the user is looking. Using this information, we create an invisible virtual ray \nthat extends from the eye outwards (see Figure 2). Once this ray hits an object, the collision \npoint (location of the intersection of the eye tracking ray and the surface of the object) is \nrecorded along with the object’s name and position (recorded for accurate reproduction of \nthe data before participation). Other metrics are recorded and computed such as eye angle \n(looking left\/right\/etc.), distance from eye to collision, and whether they are blinking or not. \nThe figure below is a representation of rays produced along the route as a user looks at objects \non the buildings’ surfaces. \n\nThe data produced from a single experiment can result in a substantial number of data points \ncollected. With such a vast amount of data being produced, it is important to identify the most \nrelevant data that could provide researchers with meaningful interpretations of observational \npatterns. So, we needed to identify ways to reduce the amount of data by reducing noise and \njitter. Then, we needed to cluster the remaining data into meaningful groups, referred to as \nfixations. From this data, we can determine the total dwell time and total fixation count for \nspecific areas of interest. The areas of interest are regions in the environment that are important (HOLMQVIST et al. 2011) and could be identified as specific objects of general areas \nalong an object. \n\nTo  produce  clusters  from  denoised  data,  we  tested  a  clustering  method  called  DBSCAN. \nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. This technique uses an unsupervised machine-learning algorithm to identify clusters of observations. \nAs the name implies, it uses the spatial density of the data points (in any number of dimensions) to create clusters and eliminate noise. One important function of this algorithm is that \nyou can use more than the 3-dimensional distance to find spatial density. It can include factors \nsuch as eye rotation and time in its calculations. This can be useful because in a dynamic \nenvironment participants can first look at an object in the distance, then as they move forward \nthrough an environment, look back at the object again. This dynamic  facet of 3D gamine \nenvironments makes it critically important to ascertain what is a fixation across distances, \nversus random noise that could have been part of a rapid eye movement across an area and \nalong an object. \n\nImplementation and Outcomes \n\nIn this section we highlight the results from the clustering technique to show: 1) the volume \nof data produced by a single participant, 2) the observational patterns of the participant, and \n3) the effects of implementing the clustering algorithm on the previous two. In this section, \nwe provide context to the implementation (for each of these three), takeaways from our experience, and general statistics to highlight an overview of the outcomes. \n\nIn our experiment, a single individual produced 39404 observations over the entire 9 minutes \nand 35 seconds of the experience. This averaged about sixty-eight observations per second. \n\nThe rate of eye tracking data collection depends upon Unity’s internal update function, which \nis the same as the framerate. Framerate is affected by how many objects are in view and need \nto be processed by the GPU. Therefore, the framerate can vary throughout the experiment. \nWhile it can be helpful to maintain a high-frequency rate to minimize motion sickness and \nimprove realism, it is unknown the extent to which the rate of data collection would impact \nthe results, for whatever results are being sought. \n\nUsing these data, we developed a simple metric to highlight how often an individual may \nlook at objects versus other elements in the environment. In our implementation, the objects \nwere buildings, and the other elements included the ground (terrain), the street, and the sky. \nIn our implementation, the ground and street are objects because they have a surface with a \ncollider that enables the collection of eye tracking data points when the vector of the observation intersects with that surface. Unlike buildings, these two surfaces are continuous through-out  the  entire  environment,  whereas  buildings  are  separate  objects.  Our  eye  tracking  also indirectly collected observations of the sky (or distant void), in which there is a frame with no distinct object with a collision surface. Figure 3 shows global statistics of the proportion of observations made between the sky, terrain, road, and buildings. The figure also compares those data after removing saccades. Saccades and their removal are explained below. \n\nEye tracking data can be difficult to interpret. This is particularly true in 3D, where there are \nvery few studies that have attempted to validate how observational patterns in 3D are associated  with  meaningful outcomes  of  navigation (UGWITZ  et  al.  2022).  One  crucial  step  in \ngenerating interpretations of data is to remove irrelevant data (e. g., noise), such as saccades. \nA saccade is essentially a rapid view of an object, then a focus away from that object with \nanother quick return to the original object. In terms of perception, little to no information is \ngleaned during a saccade. Therefore, in addition to general noise (single random observation \nin space), eliminating saccades can also help streamline the data analysis. \n\nHowever, identifying these saccadic movements requires playing with the clustering parameters. This is because eye tracking data is generated based on a collision point for each frame, \nbut how the algorithm determines if a single data point belongs to a cluster or not is a little \ntricky. To reduce noise and eliminate saccades, we implemented DBSCAN. DBSCAN takes \nthe {X, Y, Z} vector position where it collided with the object, but also the time dimension \nof when it collided. Clusters are determined by locational and temporal similarities of vectors \nby turning two parameters. First, epsilon is the distance threshold from one observation to \nanother (in 4D). Second, minimum points is the number of minimum points that constitute a \ncluster. Our parameters were an epsilon of two meters and a minimum points of seven, which \nrepresents roughly a tenth of a second or the approximate minimum amount of time for a \nfixation. Again, Figure 3 highlights the global statistics before and after the removal of saccades, or points that did not belong to a cluster.  \n\nFigure 4 further depicts an example of different clusters using different colored dots. In this \nfigure, similar colored dots represent a single observational point. Find the set of green dots \nright below the purple dots. The large cluster of green dots shares a similar proximity with a \nsingle black dot right on the roof ridge. This black dot seems to be part of that green dot \ncluster. However, DBSCAN identified that the observation point represented as the black dot \nshould not belong to a cluster. This was because the observational point was created several \nseconds prior as part of an earlier saccade (singular rapid observation), whereas the other \nobservations were made in sequence (suggesting a focal point or area). \n\nDiscussion and Conclusion \n\n3D gaming platforms offer the ability to produce vast amounts of user-centric data. Having \ntools to analyze these data can help designers identify environmental cues or triggers that \ncould influence the perception of a design or plan. Eye-tracking offers a non-intrusive, process-based technique for collecting very precise observations within a space. However, finding a robust algorithm to cluster thousands of eye data points is essential to making meaningful interpretations of these perceptions. Using the software we produced, these patterns can be visualized and reduced for making assessments about areas or objects favored by users. DBSCAN is one of several techniques available but has been shown to produce good \nresults (ESTER et al. 1996). This paper was not intended to conduct a systematic comparison \nacross these techniques and variations, but instead to demonstrate the potential for eye tracking data in combination with a clustering technique to produce useful data. \n\nThe next major step in this research is to understand how these data can be related to meaningful observations to help form decision-making and recall. Understanding this link can help \ndesigners better associate the placement and patterns of objects, such as landmarks (BRUNS \n& CHAMBERLAIN 2019), within the environment to improve wayfinding and navigation. As \nThus,  some  next  research  questions  are:  to  what  extent  do  eye  tracking  observations  in  a \ndynamic 3D virtual environment correlate with memory recall, navigational decisions, and \npattern recognition about the overall design of the environment? More broadly, to what extent \ncan eye tracking help us understand how individuals form mental maps? In our experience, \nwe  noticed  several  situations  where  individuals  were  following  unique  building  features, \npeering through passageways, and scanning the topography of buildings. While these observations are anecdotal and with limited data, they do suggest these data could help validate \nthe importance of or focus on different architectural forms, textures, and aesthetics. \n\nImplementing eye tracking in 3D-controlled virtual environments shows promise for aiding \nthe examination of observational processes. This will have relevance in multiple fields. Certainly,  eye-tracking  has  been  used  in  3D  gaming,  but  studies  in  psychology,  architecture, \nurban design, interior design, and landscape architecture could benefit from having access to \nindividual patterns of observation data. Eye tracking is a well-established technique but employing it within 3D environments and determining how to associate these data with meaningful interpretations will provide new opportunities and insights for landscape studies.  \n

Concepts and Techniques for Large-Scale Mapping \nof Urban Vegetation Using Mobile Mapping Point \nClouds and Deep Learning \n\nAbstract: In urban environments, roadside vegetation provides important ecosystem services. Reliable \nand up-to-date  information  on  urban  vegetation  is therefore  needed  as  a  basis  for  sustainable urban design and regular tasks such as vegetation maintenance. Mobile laser scanning (MLS), i. e., the use of \nvehicle-mounted laser scanners, offers strong potential for capturing 3D point clouds of road environ-\nments on a large scale at a low cost. In this paper, the potential and challenges of using MLS for vegetation mapping are discussed. To lay a foundation for MLS-based inventories of roadside vegetation, a \nconcept for the automatic detection and analysis of vegetation in MLS point clouds using deep learning \nis presented. The proposed workflow covers vegetation detection and classification, delineation of individual trees, and estimation of tree attributes. In a case study, an initial implementation of the work-flow is tested using MLS datasets from two German cities and the results are evaluated through visual \ninspection. It is demonstrated that the proposed deep-learning approach is able to detect and classify \nvegetation in MLS point clouds of complex urban road scenes. When delineating individual trees, accurate results are obtained for solitary trees and trees with little canopy overlap, while the delineation \nof trees with strongly overlapping canopies needs further improvement in some cases. The results indicate that geometric tree attributes such as tree height and trunk diameter can be accurately estimated \nfrom MLS point clouds if the accuracy of the preceding processing steps is sufficiently high. \n\nIntroduction \n\nIn urban environments, roadside vegetation provides important ecosystem services, including \nsequestering carbon, mitigating air pollution, regulating microclimate, providing habitat, and \npromoting human well-being (SÄUMEL et al. 2016). Urban trees also help mitigate the effects \nof  climate  change  through  local  cooling  and  stormwater  absorption  (PATAKI  et  al.  2021). \nTherefore, maintaining and expanding roadside vegetation is essential for sustainable urban \ndevelopment. Reliable and up-to-date information on urban vegetation is needed to guide the \ndesign of urban green spaces toward the provision of ecosystem services (ELDERBROCK et al. \n2020)  and  to  develop  appropriate  green  space  management  programs  (SCHIPPERIJN  et  al. \n2005). Many municipalities, especially in North America and Europe, therefore, conduct inventories of public green spaces. These inventories often focus on surveying individual trees, \ni. e., mapping the location of individual trees, and collecting tree attributes such as tree species, height, and trunk diameter (MA et al. 2021). Since manual vegetation inventories are\ncostly and time-consuming, the use of LiDAR systems (Light Detection and Ranging) for\nautomated or semi-automated vegetation mapping has become an important research topic.\nThese systems capture the environment in the form of high-resolution 3D point clouds and\ncan be used with various acquisition platforms. The acquisition can be categorized as terrestrial laser scanning (TLS) with tripod-mounted laser scanners, personal laser scanning (PLS) \nwith handheld or backpack-mounted laser scanners, mobile laser scanning (MLS) with vehicle-mounted laser scanners, unmanned aerial vehicle-borne laser scanning (UAV-LS), and \nairborne laser scanning (ALS). Compared to TLS, PLS, and UAV-LS, the use of vehicle-mounted laser scanners significantly reduces the acquisition effort, while providing 3D point \nclouds with higher resolution and fewer occlusions than ALS. Because of these characteristics, MLS is a well-suited technology for the large-scale mapping of roadside vegetation and \nthus could become a complement or alternative to conventional vegetation surveys. Compared to conventional field surveys, MLS-based vegetation mapping would reduce labor and \ncost, while providing a richer, three-dimensional representation of vegetation. To realize the \npotential of MLS for vegetation mapping, an automated approach is needed to derive semantic information about vegetation from raw 3D point clouds. To lay a foundation for building \nsuch a system, this paper presents a general concept for the automatic detection and analysis \nof  vegetation  in  MLS  point  clouds.  In  contrast  to  previous  work,  the  proposed  workflow \nbuilds on a modern deep-learning approach for 3D point cloud segmentation. In a case study, \nan initial implementation of the proposed workflow is tested on MLS datasets from two cities \nand first results are provided. \n\nPotential of MLS-Based Vegetation Inventory \n\nCompared to conventional field surveys, MLS-based vegetation inventories would provide \nseveral advantages: First, the labor and cost of vegetation surveys would be reduced, allowing \nlarger areas to be mapped and vegetation inventories to be updated more frequently (e. g., \nquarterly to cover all seasons). Second, capturing vegetation in the form of 3D point clouds \nwould provide additional and more detailed data on urban vegetation. For example, shrubs \nand hedges could be mapped, which are not recorded in most conventional vegetation surveys \ndespite their ecological value and aesthetic impact. Furthermore, conventional tree invento-\nries usually only capture a limited number of tree attributes (ÖSTBERG et al. 2013). Using 3D \npoint clouds, additional geometric tree attributes such as trunk orientation or crown volume \ncould be captured (HERRERO-HUERTA et al. 2018). 3D point clouds could even be used to \nmodel the entire branching structure of trees (DU et al. 2019). Overall, the data collected in \nMLS-based  vegetation inventories could be used to build  comprehensive  tree information \nmodels, as proposed by SHU et al. (2022). Such information models would address the information needs of a wide range of stakeholders, including city officials, arborists, ecologists, \nand  landscape  architects.  For  landscape  architects,  information  derived  from  MLS-based \nvegetation inventories could be particularly useful for the following applications: (1) At the \nbeginning  of  the  design  process,  more  detailed  plant  models  could  be  created  to  enable  a \nmore accurate representation and analysis of existing vegetation. While generic plant models \nhave been commonly used to visualize vegetation (OEHLKE et al. 2015), plant models derived \nfrom  3D point  clouds  could  be  used  to  create  more  realistic  visualizations  of  vegetation. \nMoreover,  plant  models  derived  from  3D  point  clouds  could  also  be  used  to  estimate  the \necosystem services and disservices provided by existing vegetation. For example, the shading \nprovided by trees could be modeled, or the carbon storage and oxygen release potential of \nvegetation could be estimated (SCHOLZ et al. 2018). (2) Conducting MLS-based vegetation \nsurveys on a regular basis could also provide ground truth data for building and validating \nsimulation models of plant growth (WHITE et al. 2022). More accurate simulation of plant \ngrowth would support decisions between different planting regimes in the design of green \nspaces. (3) Once a certain planting regime has been established, MLS could be used to continuously monitor the green space (e. g., growth and condition). In this way, design decisions \ncould be evaluated, and vegetation maintenance measures could be planned and aligned with \nthe design goals. \n\nChallenges in MLS-Based Vegetation Inventory \n\nWhile MLS offers significant potential for the large-scale mapping of roadside vegetation, \nseveral challenging data characteristics must be considered when developing systems to automatically process MLS point clouds for vegetation mapping: \n\nLarge data volume: MLS produces large amounts of data with hundreds of points per square \nmeter. To be able to process MLS point clouds of large road segments or entire city districts, \nall processing steps must be automated and implemented efficiently. This requires algorithms \nthat allow parallel processing with modern multicore systems or graphics cards. \n\nVarying point density: The point density of MLS point clouds depends on the scanner type, \nthe acquisition speed, and the distance to the scanner trajectory. Systems for processing MLS \npoint clouds must therefore be robust to varying point densities. \n\nOcclusions: In MLS point clouds, vegetation  may be completely or partially occluded by \nother objects. The algorithms for detecting and analyzing vegetation in MLS point clouds \nmust therefore be able to cope with incomplete data, i. e., 3D point clouds that cover only a \npart of the surface.  \n\nLimited per-point attributes: Most LiDAR systems provide the 3D coordinates and reflection intensity for each scanned point. More advanced systems can capture additional attributes such as surface color (e. g., from panoramic images), temperature, or humidity. To support a wide range of scanner types, however, algorithms for vegetation detection and analysis should only rely on point coordinates and reflection intensity as input attributes. \n\nIntegration of  multi-temporal MLS data: Approaches for storing and processing  multi-temporal MLS data are needed to enable continuous vegetation monitoring. Since inaccuracies  can  occur  in  the  georeferencing  of  MLS  point  clouds,  techniques  for  co-registering \n3D point  clouds  from  different  acquisition  runs  are  needed.  In  addition,  approaches  are \nneeded to increase the coverage and merge redundant information from MLS point clouds \nacquired at different times and to identify areas that have changed between acquisition runs.  \n\nA Concept for the Detection and Analysis of Vegetation in \n\nMLS Point Clouds Using Deep Learning \n\nIn the following, a concept for the automatic detection and analysis of vegetation in MLS \npoint clouds is presented. The proposed workflow is divided into three steps: (1) In the first \nstep, deep-learning models are used to extract vegetation points from raw MLS point clouds \nand classify them into different vegetation types. (2) In the second step, the tree points detected  by  a  deep-learning  model  are  segmented  into  individual  trees.  (3)  Using  the  point \nclouds of individual trees, tree attributes such as tree height and trunk diameter are derived. \n\nDetection and Classification of Vegetation \n\nBesides vegetation, urban MLS point clouds contain a variety of other objects such as city \nfurniture, vehicles, and buildings. An automated approach is required to segment 3D point \nclouds of such complex scenes into vegetation points and non-vegetation points. In addition, \ndifferent vegetation types, such as low vegetation and trees, need to be distinguished. \n\nRelated Work \n\nWhile the present work aims to detect both low vegetation and trees, most previous work has \nfocused on tree detection in MLS point clouds. To segment urban MLS point clouds into tree \npoints and non-tree points, many works use either rule-based approaches (HAO et al. 2022, \nHUI et al. 2022) or statistical machine learning approaches (WEINMANN et al. 2017, CHEN et \nal. 2019). While rule-based approaches are usually not able to detect other vegetation types \nthan trees and are often very dataset-specific, statistical machine learning approaches often \nlack the capability to combine geometric features of different spatial scales. The recent work \nof CHEN et al. (2021) is among the first to use a deep-learning approach for vegetation detection in urban MLS point clouds. However, the PointNLM architecture proposed in their work \nis trained on hand-crafted geometric features of supervoxels and does not yet exploit the full \npotential of recent deep-learning architectures for 3D point cloud segmentation. \n\nMethodology \n\nIn general, deep-learning architectures for processing 3D point clouds can be divided into \narchitectures that operate on intermediate representations of point clouds such as 2D images \nor 3D voxel grids, and architectures that process point clouds directly (BELLO et al. 2020). \nOur workflow is built upon architectures that process 3D point clouds without intermediate \nrepresentations. These architectures usually are more efficient than architectures that use intermediate representations and are designed to self-learn geometric features at different spatial scales. Recent examples of such architectures include KP-FCNN (THOMAS et al. 2019), \na fully convolutional neural network (FCNN) based on a kernel-point (KP) convolution, and \nRandLA-Net (HU et al. 2020), “an efficient and lightweight neural architecture to directly \ninfer per-point semantics for large-scale point clouds” based on random sampling (Rand) and \nlocal feature aggregation (LA). In our implementation, the KP-FCNN Rigid architecture is \nused. Different variants of this architecture exist, targeting different processing tasks such as \nclassification  or  semantic  segmentation  of  3D  point  clouds.  In  this  work,  the  detection  of \nvegetation in MLS point clouds and its classification into different vegetation types are modeled as a single semantic segmentation task. To this end, models are trained to distinguish the \nfollowing classes: \n\nLow vegetation includes all types of low vegetation, e. g., shrubs, hedges, and potted plants. \n\nTree trunk includes tree trunks, defined as the segment ranging from the base of a trunk to \nthe first branching. \n\nTree branch includes the main branches of a tree that are not covered by foliage. \n\nTree crown includes tree foliage and the branches and twigs covered by it.  \n\nOther includes all non-vegetation points, e. g., ground, buildings, and city furniture. \n\nThis classification scheme is more fine-grained than the classification schemes used in previous studies. Segmenting trees into trunk, branch, and crown areas provides additional semantic information that can be used to delineate individual trees and estimate certain tree \nattributes. However, the proposed classification scheme also requires more detailed ground \ntruth annotations to train deep-learning models, which increases the annotation effort. \n\nSince large-scale MLS point clouds usually contain millions of points, they cannot be processed as a whole by deep-learning models. Therefore, our workflow includes two preprocessing steps to prepare raw MLS point clouds for processing by deep-learning models (Figure 1). First, the resolution of the large-scale point clouds is reduced by grid subsampling. \nSubsequently, small subsections of fixed size (4 m radius, 4096 points) are sampled from the \ndownsampled large-scale point clouds. These small-scale point clouds are processed by the \ndeep-learning model. The model predictions are mapped to the large-scale point clouds and \nare interpolated for points that were not covered by the small-scale point clouds. \n\nDelineation of Individual Trees \n\nThe deep-learning approach described in the previous section performs point-wise segmentation, where each point is assigned to a semantic class. However, for points that are classified \nas tree points, the deep-learning approach does not provide information about which individual tree a point belongs to. Therefore, an additional processing step is required to segment \nthe tree points identified by a deep-learning model into individual trees. Especially in areas \nwith high tree density and overlapping tree canopies, delineating individual trees can be a \nchallenging task. \n\nRelated Work \n\nIn  previous  work,  different  algorithms  have  been  proposed  to  delineate  neighboring  trees \nwith overlapping crowns, including graph-based approaches (ZHONG et al. 2017), clustering \napproaches (LI et al. 2021), and region growing approaches (LI et al. 2016). Some of these \napproaches rely on the correct detection of tree trunks and are therefore not robust to occlusions and misclassifications of tree trunks. Additionally, many algorithms for delineating individual trees are based on voxel representations of 3D point clouds, which limits their accuracy. \n\nMethodology \n\nTo improve robustness to incomplete data and achieve more accurate segmentation of overlapping tree canopies, we propose a multi-step approach to delineate individual trees. The \nproposed approach is inspired by several previous works (WU et al. 2013, ZHONG et al. 2017, \nXU et al. 2020, YANG et al. 2020) and consists of the following steps: \n\n1)  Identification of tree locations: In the first step, the locations of individual trees are \nidentified. To improve robustness against incomplete data, tree trunks, main branches, \nas well as treetops are considered for identifying tree locations. To identify tree locations \nbased on tree trunks and main branches, trunk and branch points identified by the deep-learning approach are clustered using the DBSCAN algorithm (ESTER et al. 1996). Adjacent clusters with similar growth direction are merged and the midpoints of the remaining clusters are used as approximate tree locations. To identify tree locations based on crown tops, a 2D canopy height model is constructed and searched for local maxima. \nThe positions of local maxima whose distance from the already found tree locations is \nabove a threshold are added to the set of tree locations. \n\n2)  Coarse delineation of individual trees: The tree locations obtained in the previous step \nare used to determine the coarse boundaries of the individual trees and to identify regions \nwith overlapping tree canopies. Different algorithms can be used for this purpose, e. g., \n2D Voronoi segmentation can be performed (ZHONG et al. 2017), or a canopy height \nmodel can be constructed and segmented using the 2D marker-controlled Watershed algorithm (KORNILOV & SAFONOV 2018). For the implementation in this work, a combination of both algorithms is used. \n\n3)  Refined delineation of overlapping tree canopies: If the coarse tree delineation indicates that two trees are close to each other, their canopies may overlap. In such cases, \nthe segmentation of the tree canopy is refined. Different approaches can be used to delineate  trees  with  overlapping  canopies,  including  graph-based  approaches,  clustering \napproaches,  or  region  growing  approaches.  Our  workflow  uses  a  region  growing  approach since it reflects the natural growth direction of trees and allows the delineation of \ntrees with different shapes. Specifically, we implement a custom, density-based region \ngrowing algorithm that is inspired by the DBSCAN algorithm (ESTER et al. 1996). In \nthis algorithm, for each tree to be processed, a set of seed points is selected (i. e., points \nthat belong to the tree with a high degree of certainty). In an iterative process, neighbor \npoints of the seed points are assigned to the respective tree, if they have not yet been \nassigned to another tree. Neighbor points that satisfy the core point criterion as defined \nin the DBSCAN algorithm (ESTER et al. 1996) become seed points themselves. To ensure  that  neighboring trees  grow  evenly,  points  are  sorted according  to  their  distance \nfrom the crown boundary determined during coarse tree delineation and processed in \nthat order. \n\n4)  Removal of implausible trees: In the final processing step, trees with implausible shape \nor size are discarded. Specifically, trees with few points and trees whose height is below \na threshold are filtered out. \n\nEstimation of Tree Attributes \n\nAfter obtaining point clouds representing individual trees, different tree attributes can be estimated. Most existing studies focus on the estimation of geometric tree attributes (HERRERO-HUERTA et al. 2018, WU et al. 2013, XU et al. 2020), while few authors also derive non-geometric attributes such as tree species or vitality (WU et al. 2018, CHEN et al. 2019). Since \n3D point clouds are particularly suitable for deriving geometric tree attributes, we also focus \non these attributes. However, algorithms for estimating non-geometric tree attributes such as \ntree species, carbon storage capacity, or oxygen release potential may be integrated into the \nworkflow in the future. For the estimation of the attributes tree location (WU et al. 2013), tree \nheight, trunk direction, crown width (HERRERO-HUERTA et al. 2018), trunk diameter at breast \nheight (CHEN et al. 2019), and crown volume (LI et al. 2020), we adopt the approaches used \nin previous work. Other tree attributes, namely under-branch height, and crown base height, \ncan  directly  be  derived  from  the  deep-learning-based  segmentation  of  trees  into  trunk, \nbranches, and crown. \n\nCase Study \n\nA test application was implemented to demonstrate the potential of the concept presented in \nthis paper. MLS point clouds collected in the cities of Essen, Germany, and Hamburg, Germany, were used to evaluate the test application. The point clouds were acquired using Trimble MX8 scanners in the leaf-on season. We manually annotated the point clouds and split \nthem into a training set, a validation set, and a test set. In the following, some preliminary \nresults for the test set are shown. \n\nFigure 2 shows exemplary results of the deep-learning-based vegetation detection and clas-\nsification.  As  can  be  seen  there,  large  portions  of  the  vegetation  are  segmented  correctly. \nHowever, several smaller segmentation errors can be identified: In multiple cases, low veg-\netation and tree crowns are confused. In addition, some pole-like objects are misclassified as \ntrees. There are also a few cases in which tree trunks are missed by the deep-learning model. \n\nResults of the delineation of individual trees are shown in Figure 3. While solitary trees and \nadjacent trees with little canopy overlap are correctly delineated in most cases, the results for \nthe delineation of trees with strongly overlapping canopies are mixed. Trees whose crown \nhas a large extent are over-segmented in several cases, and some parts of tree crowns with \nlow point density are missed by the region growing algorithm. \n\nFigure 4 shows some results of the estimation of tree attributes. As can be seen there, accurate \nestimates of geometric tree attributes are obtained if the accuracy of the preceding processing \nsteps is sufficiently high. When large parts of a tree are missed during tree segmentation, \nattributes such as tree height, crown width, and crown volume are often underestimated. In \ncases where multiple trees are recognized as a single tree, the crown width and crown volume \nare often overestimated. \n\nDiscussion and Conclusion  \n\nIn this work, a concept for the automatic detection and analysis of vegetation in MLS point \nclouds has been presented. The implementation of the concept produced promising results \nfor representative MLS point clouds from two cities. However, to approach the accuracy of \nmanual vegetation mapping, the method needs to be further improved. Since the accuracy of \nvegetation detection and classification affects the accuracy of the following processing steps, \nfurther improvement of the deep-learning approach for vegetation detection and classifica-\ntion would be of major benefit. Despite this need for improvement, the preliminary results of \nthis work suggest that the proposed deep-learning approach can detect and classify vegetation \nin complex street scenes that would be difficult to model using rule-based approaches. However, this comes at a price: The training of deep-learning models requires extensive annotated \ntraining data and high computing power of graphics hardware. To improve the practicality of \nthe approach, techniques to reduce the labeling effort (e. g., active learning or transfer learning) and improve model speed (e. g., model pruning) could be incorporated into the work-\nflow. \n\nWhile  the  test  application  presented  in  this  paper  was  limited  to  capturing  geometric  tree \nattributes,  it  would  be  desirable  to  derive  even  richer  tree  information  models  to  cover  a \nbroader range of use cases. For example, detailed models of tree branching structures could \nbe derived from 3D point clouds to enable estimation of aboveground biomass and carbon \nstorage capacity, as well as realistic vegetation visualization. Furthermore, it would be useful \nto integrate approaches for processing multi-temporal MLS data into the workflow. In this \nway, point clouds representing vegetation in leaf-on and leaf-off conditions could be combined. Capturing vegetation in leaf-off conditions would avoid the occlusion of woody components by foliage and thus facilitate the acquisition of woody biomass and the delineation \nof individual trees. In addition, approaches for predicting non-geometric tree attributes such \nas tree species and tree vitality could also be integrated into the workflow, e. g., by combining \n3D point clouds with spectral data from panoramic images or aerial photographs. \n\nSince MLS is a cost-effective method to survey very large areas, the present work focused \non vegetation mapping using MLS. However, MLS is only suitable for capturing vegetation \nin the proximity of drivable roads,  while PLS or UAV-LS  are  more suitable for  mapping \nvegetation in parks or private gardens. To provide a complete mapping of urban vegetation, \nthe concept presented in this work should be transferred to other LiDAR platforms. Since \nPLS point clouds have similar characteristics to MLS point clouds and a generic deep-learning approach is used in this work, transferring the approach to PLS point clouds should be \npossible with little effort. \n\nBuilding on the concept presented in this paper, a data processing tool could be developed \nfor large-scale and cost-effective urban vegetation mapping. Such a tool would enable continuous monitoring of urban vegetation and thus provide a basis for sustainable design and \nmaintenance of urban green spaces. In particular, it would enhance the study of ecosystem \nservices provided by urban vegetation and could thus help to guide the design of urban green \nspaces towards the provision of ecosystem services. \n

Ecological Robotics \n\nAbstract: This research presents a novel method for paste-based robotic planting. Our method for robotically extruding seeds in a paste of clay and planting media enables precise, algorithmic planting. \nThis additive manufacturing process builds microtopography, while planting seeds. With this 3D printing process designers can engage with the geomorphological and ecological processes that shape landscapes. Microtopography can be designed to direct flows of water, while planting can be designed to \nfoster biodiversity, form ecotones, and control erosion. As a proof of concept, we demonstrate how \nalgorithms can generate precise planting patterns  such as pseudorandom gradients. We envision  unmanned ground vehicles with seed printing systems planting entire landscapes with algorithmic designs. \n\nIntroduction \n\nResearch on autonomous construction in architecture has explored the novel creative, material, tectonic, performative, and aesthetic potential for the computational design of the built \nenvironment (GRAMAZIO & KOHLER 2014, MENGES 2015). Similarly, the autonomous construction and planting of landscape promises unique aesthetic opportunities and new ways of \nengaging with ecology and geomorphology. Designers have experimented with robotic processes for constructing landforms such as soil 3D printing (MITTERBERGER & DERME 2019, \n2020) and autonomous excavators (JUD ET AL. 2021, HURKXKENS ET AL. 2022). Robotic processes for planting have been developed such as vacuum seeding robots (GOLDBERG 1995, \nFARMBOT 2020, PRESTEN et al. 2021), autonomous seed drilling tractors (GROß 2013), and \nseed sowing with unmanned aerial systems (MOHAN et al. 2021). While sowing seeds is an \nimprecise process with low survival rates, seed drilling is more precise and has higher survival rates, but mechanically disturbs soil, increasing the risk of soil erosion. Our paste-based \nextrusion method for autonomous planting has millimeter precision, high survival rates, does \nnot disturb soil, and creates microtopography. Potential applications include landscape architecture, land art, ecological restoration, and precision agriculture. \n\nMethods \n\nIn our method for robotic planting, seeds are extruded in a paste of clay, planting media, and \nwater (Fig. 1). As an additive manufacturing process, paste-based extrusion of seeds builds \nlandforms layer by layer. The proportions of the paste are calibrated so that it can be extruded \nsmoothly, while supporting seed germination. Clay is used for plasticity for the sake of ex-\ntrusion. Planting media is used to provide nutrition for plants, retain water, and provide pore \nspace for root growth. Water is used to wet the clay and germinate the seeds. The paste is 3D \nprinted, i. e. extruded, directly onto soil. After printing, the seeds embedded in the paste have \nthe shelter, nutrients, and moisture they need to germinate. Once the seeds have germinated, \nthe roots of the seedlings grow into the soil below. \n\nAs a proof of concept, we developed a prototype with a linear actuator ram mounted on a 6-axis robotic arm. Industrial robots are reliable, versatile systems that can easily be adapted to \nnew tasks, making them well suited for creative experimentation with novel material processes  (GRAMAZIO  & KOHLER  2014,  16).  For  this  prototype,  we  used  a  UR10e  industrial robotic arm because its 12.5 kg payload was enough for a large extruder with 2000 ml of paste and its reach of 1300 mm allowed for a large build space. Grasshopper, a visual programming environment for computational design (MCNEEL 2021), was used to generate geometry for robot path planning. The Robots ex Machina framework (GARCÍA DEL CASTILLO Y LÓPEZ 2019) was used to control the robot and extruder.  \n\nTo test this method, a series of planting patterns were robotically printed in the lab. We tested \ndesigns such as space filling curves, landforms derived from trigonometric waves, landforms \nderived from cellular texturing, landforms derived from procedural noise (Fig. 2 & Fig. 3), \nand generative typography (Fig. 4). For ease of printing and cultivation in a laboratory setting, designs were printed in growing trays and stored on racks with grow lights. Each 250 \nby 250 mm tray was filled with planting media as a substrate for the print. We used plants \nsuch as perennial ryegrass (Lolium perenne), annual ryegrass (Lolium multiflorum), arugula \n(Eruca vesicaria), radish (Raphanus sativus), alfalfa (Medicago sativa), Siberian kale (Brassica napus) and broccoli (Brassica oleracea var. italica). For the landforms, the extruder was \nfilled with layers of different seeds to create an elevation gradient of species. Over the course \nof the study, the prints were photographed daily to record the growth of the plants. \n\nResults \n\nThe planting designs printed cleanly and precisely as the crisp letterforms in Figure 4 demonstrate. Plants grew most healthily and vigorously in prints that were 5 or 10 mm tall, composed of 1 or 2 layers, because the seedlings’ roots had easier access to the porous, nutrient rich substrate of planting  media below. Furthermore, plants grew  more vigorously in narrower forms with widths of 20-40 mm because this ensured that seedlings had less competition and more access to sunlight and substrate. While paste-based extrusion of seeds creates microtopography, the scale of appropriate landforms is highly constrained by growing conditions.  \n\nFuture Work \n\nTo further this research, we are investigating alternative media for the paste, testing different \ntypes of extruders, integrating sensors into the system, and integrating the system onto an \nunmanned ground vehicle for landscape-scale planting (Fig. 5). We are testing pastes composed of biochar that release nutrients slowly and biopolymers that biodegrade rapidly. We are integrating moisture sensors, depth sensing, lidar scanners, and multispectral imaging for adaptive planting and monitoring. Using these sensors, we plan to conduct experiments with controls, replicates, and quantitative measures to assess the efficacy of this method with different pastes. To plant at landscape-scale, we plan to integrate the robot arm, extruder, and sensors onto an unmanned ground vehicle with real-time kinematic positioning. We plan to \nuse lidar, positional data, and simultaneous localization and mapping algorithms for autonomous navigation and for positioning the extruder relative to the ground in real time. For field \ntrials, we will use lidar and multispectral imaging on unmanned ground and aerial vehicles \nto conduct repeated surveys at high spatial and temporal resolution to assess plant growth.  \n\nConclusion \n\nWith paste-based robotic planting, computational designs for planting patterns can be autonomously seeded with high precision. While this experiment was conducted in the lab, robotic \nplanting could be done at scale in the field with unmanned ground vehicles. With generative \ndesign enacted by field robots, landscape architects would be able to design dynamic landscapes as ongoing performances – as ecological processes guided by design interventions. \nUnmanned aerial systems could  collect imagery and elevation datasets at high spatial and \ntemporal resolution that could inform the ongoing design and management of landscapes. As \nunmanned aerial systems monitor how landscapes evolve, unmanned ground vehicles could \nadaptively plant and replant in response, catalyzing new assemblages of plants and wildlife. \nWe hypothesize that algorithmic planting based on ecological principles could foster spatial \nheterogeneity, complexity, and thus biodiversity. We envision field robotics giving rise to an \nalgorithmic aesthetic of ecology.  \n\nNew media scholar Laura Marks describes algorithmic aesthetics as a semiotic process of \nenfolding and unfolding, a process in which infinite possibility is transcribed into information \nand then image (MARKS 2010). This is a performative aesthetics of infinity and contingency \n– an aesthetics that explores what is revealed and what is hidden; an aesthetic in which creativity is a  means of discovering the infinite. Design projects like  GRAMAZIO KOHLER Research’s  Endless  Wall  (2011),  SNØHETTA’S  MAX  IV  Laboratory  Landscape  (2016),  and \nMAEID’S Magic Queen (2021) evoke such an algorithmic aesthetic. With systems for precise \nautonomous planting, landscape architects would have the means to express ecological complexity  and  contingency  with  a  visibly  algorithmic  logic.  Computational  design  processes \nsuch as procedural noise, cellular noise, and fractional Brownian motion (Figures 2, 3 & 6) \ncould be used to generate planting patterns  with high spatial heterogeneity and ecological \ngradients  between  drifts.  Such  computationally  designed  planting  would  simultaneously \nevoke the accidental and intentional, the actual and virtual. \n

Expanding Digital Design Workflows with Geospatial \nAnalytics: Linking Grasshopper3D with Google \nEarth Engine \n\nAbstract:  The  following  body  of  work  introduces  a  plugin  that  links  the  visual  scripting  language \nGrasshopper3D (GH) to the Google Earth Engine (GEE) in order to easily fetch geospatial information \nrelative to various societal issues and for any geographical area under study, inside the Rhino modelling \nsoftware.  \nAiming  at  expanding the  field  of  Digital  Landscape  Architecture with  novel  content to  analyse  and \ndesign, it provides designers with more than thirty years of historical imagery and scientific datasets, \ncollected in GEE on a daily basis by several institutions around the world. Leveraging the intuitiveness \nof the visual scripting language, it computationally empowers designers with geospatial insights with-\nout the need for any GIS skills and has been proved a successful platform for teaching purposes. Encouraging learning through application, the paper discusses three teaching experiences, which adopted \nthe proposed tool to visualise river dynamics in time, resource-specific maps of land consumption for \ncities and street-sensitive accessibility maps through the additional integration of OpenStreetMap data. \n\nIntroduction \n\nDuring the last three decades, a constellation of computational applications has emerged that \nempowers architects and designers to respond to the challenges of the  AEC and planning \nsectors through highly technological and innovative means. Active since late 2007, the Rhinoceros’s visual programming language: Grasshopper3D (GH), has been proved to be among \nthe most successful examples of this kind. It has offered a visually-intuitive medium to teach \nand compute advanced computational pipeline without writing one line of code, and has become an asset for both the academic and the industrial realms (CASTELO-BRANCO & LEITÃO \n202). Additionally, whether to simulate microclimatic conditions (MACKEY et al.. 2017), calculate structural performances (PREISINGER & MORITZ 2014), or work with georeferenced \ndata (DOGAN et al. 2018) to name a few, over the years GH has collected an extensive amount \nof plugins, developed by an active community of computational designers, to expand its influence in many aspects of the design process and in relationship to the many actors involved. \nFinally, by reshaping the traditional drawing tools through mathematics and functions, it has \nprovided designers with novel ideas to design while deeply influencing all scales of the project, from Digital Fabrication to Digital Landscape Architecture. Placed within this line of \ninvestigation,  the  following  body  of  work  introduces  a  plugin  that  links  Grasshopper  to \nGoogle Earth Engine to instantly fetch selected geospatial layers for any geographical area \nof interest. In this sense, it enables designers to access spatial insights related to more than \nthirty  years  of  remote  sensing  data,  collected  by  a  plethora  of  satellites  and  processed  by \nresearch institutions from all over the world. Answering to the call for data democratization \nwhile rendering large-scale computing accessible to non-experts, the Google Earth Engine \n(GEE)  is  a  “cloud-based  platform  for  planetary-scale  geospatial  analysis  [that  seamlessly \ngives to] not only traditional remote sensing scientists, but also a much wider audience, [access to many] societal issues including deforestation, drought, disaster, disease, food security, \nwater management, climate monitoring and environmental protection” (GORELIK et al. 2017). \nFor this reason, GEE is built around a petabytes-large catalogue of georeferenced data and \nan Application Programming Interface (API) to access and process server-side the same layers; achieving in this manner high speed performances and becoming suitable not only for \nglobal-scale  calculation  processes,  but  also  for  more  explorative  and  experimental  approaches, common in design processes.  \n\nThe Toolkit \n\nAs an entry point for designers to explore geospatial analytics through Google Earth Engine, \nthe toolkit proposed consists mainly of five GH components to import, spatialize, process \nand calculate geospatial raster layers through the Earth Engine API Python library1 and via \nHops. Being a recent development in the Grasshopper3D suite, Hops is the first package to \nefficiently link the visual scripting tool to the real potentialities of the Python programming \nlanguage.  By  externally  running  CPython  code  via  a  Flask  application,  it  allows  Python \nscripts to be implemented in GH unconstrained by the limitations of predefined libraries and \nopen to the vastness of community-driven packages available online.  \n\nBeing one of these contributions, the Earth Engine API is the official Python client library to \ndynamically access GEE. Used within the tailored Flask application, it runs requests from \nthe inputs in GH to the online GEE server and consequently fetches the required information. \nMore precisely, the discussed custom components to connect GH to GEE are: \n\n1)  ee_image: it permits the download of images from GEE for any geographical area of \n\ninterest and functions as the primary component to explore GEE. \n\n2)  ee_imageColl: it enables to work with the more advanced imageCollection typology and, \ncompared to the ee_image, requires extra information in respect to the date, or permitted \ncloud coverage to consequently extract images. \n\n3)  ee_ND: it engages with GEE to create normalised difference indicators on the server side \nby providing multiple bands to work with, and functions as an entry point to the world \nof remote sensing indicators \n\n4)  ee_cumCost: it calculates cumulative cost analysis, which are commonly used to spatialize accessibility, provided a cost to travel over a territory and an initial set of origins \n\n5)  reproject_UTM: an utility component to manage coordinate reference systems and align data fetched from GEE with the outputs of other plugins for GIS operations \n\nThe requirements to use the aforementioned components are kept very concise and focus on \nproviding the maximum flexibility with the minimum amount of inputs, and always comprises: an area of interest to download the image from, a resolution -in metres- to balance the amount of information to be downloaded, and the layer, with respective bands, that we are interested in accessing. Additionally, more inputs can be requested to calibrate the functions of the specific components, like in the case of the ee_ND that requires more than one band to reciprocally subtract, or the ee_cumCost which requires locations of origin for the accessibility analysis to be calculated from. This being said, the toolkit automatizes a series of spatial operations common in Geographical Information Systems (GIS) to deal with raster layers, \nlike is the case of resampling operations to obtain custom resolutions, or mathematical operations to calculate remote sensing indicators. It is important to notice that it does so in the \nbackground, opening up possibilities for the users to interact with the tool only through specifically opinionated inputs in order to facilitate its generic usability and versatility. In this \nsense, the tool aims at providing a simplified pipeline for computational designers to investigate  the  immensity  of  the  Google  Earth  Engine  database  for  design  purposes  through  a \nscarce set of inputs, allowing them to obtain material for further analysis and manipulations \nwith conventional processes – through standard components in Grasshopper3D – in a fast, \nand interactive fashion and without the need to be GIS experts.  \n\nFinally, the open source nature of the tool – a Python Flask application – permits an in-depth \ncustomization of each component – if equipped with enough knowledge of the Python programming language – and it has been proved to be a fruitful case to learn and apply computational logics in a pedagogical sense. It balances levels of complexity  when approaching GIS  processes  through  visual  programming  while  maintaining  the  possibility  to  read  and study the back-end codes when necessary. \n\nLearning Through Application \n\nThrough  one  year  of  teaching  experience,  the  proposed  plugin  has  been  tested  on  several \noccasions and has been proved to be versatile enough for different case studies, enhancing in \na broad sense the toolset that designers possess when approaching a territorial project – for \nvisualisation or analytical purposes – and not focusing on highly specific outputs. In the following section, the paper discusses three such occasions where the methodology has been \nshared with students to analyse river patterns in time, resource-specific maps of land consumption for cities, and street-sensitive accessibility maps through the integration of Open-StreetMap data relative to high resolution street networks.  \n\nMore precisely, the case studies hereby collected are the results of the Geomining lecture for \nthe Master in Landscape Urbanism at the Architectural Association School of Architecture \nin London, the Earthy Indexes workshop at the CAADRIA conference 2022\/23, and the one-week Urban Analytics workshop for the IAAC Global Summer School 2022. \n\nAnalysing River Patterns in Time \n\nThe analysis of river patterns in time is an important tool for understanding the dynamics of \nriver systems and the impacts of natural and human-induced changes on these systems. Used \nto inform a wide range of decisions related to the management and protection of river systems \nand the resources they provide, such as flood risk assessment, water resource management, \nenvironmental impact assessment, and land use planning, it is a fundamental asset for Landscape Architecture, which usually requires tedious data research and modelling.  \n\nUnequipped with a specific plugin, these studies are commonly carried out in GH through \nad-hoc scripting via iterative logics, such as for river meandering and oxbow lake simulation2 \nor water runoff studies which can be used as support material. Despite being excellent case \nstudies to teach iterative logics and loops (i. e. using the Anemone plugin3), they often fail to \nreach a high level of  specificity and end  up in the realm of design exercises compared to \nterritorial studies: fruitful to inspire design processes but insufficient for serious analytical \npurposes.  \n\nOn the other hand, there are several methodologies that can be used to map river dynamics \nin time using GIS, including time-enabled data, dynamic modelling and finally time series \nanalysis. Despite being able to provide highly specific and precise assessment of river dynamics, studies via time-enabled data or computational models are generally expensive or \ndemanding to implement due to the requirements of on-site equipment or highly trained professionals to set up and run hydrologic models. On the contrary, remote sensing has been \nwidely used in the analysis of river patterns in time over the past few decades as it allows for \nthe collection of large amounts of data over a broad area in a relatively short period of time; \npermitting a wide range of spatial and temporal scales to be included in a interoperable medium for the experts of the field to disseminate the results of their research. \n\nIn this sense, the JRC Global Surface Water mapping layer4 offers an unprecedented synthetic image of more than 4 million scans from Landsat 5, 7, and 8 to describe in high-resolution  the long-term changes  happening in river  systems  from early 1984 until the end of \n2021. Hosted in GEE as a multi-band 30m resolution image, it can be easily queried via the \nproposed ee_image component to visualise for any area of interest the patterns of extension, \nseasonality and recurrence of water to name a few. These layers precisely have been class \nmaterial during the Geomining lecture at the Architectural Association School of Architecture where participants drew a synthetic line-map of temporal river dynamics (Figure 1) almost-instantly and without geographical restraints. Taking advantage of the extensive repre-\nsentational possibilities of GH and adopting colour, angle and length as parameters, the map \nreported not only where it is possible to find water resources, but also their permanent loss \nand yearly frequencies, thus providing a wider understanding of the ephemerality of water \ncompared to a standard layer by layer visualisation. Additionally, and only thanks to the implemented pipeline, no particular download was required to compute the analysis. Avoiding \nto redundantly download entire databases by running area-specific queries in GEE is far more \nthan secondary as it prevents common issues concerning not only memory availability but \nalso computational power and computing times on the designer’s machine. \n\nMaps of City Consumption \n\nThe majority of people in the world live in cities, which currently only take up 3% of the \nEarth's surface but have transformed 70% of the planet through human activities (CIESIN \n2016). In this sense, cities around the world are interconnected and constantly exchanging \nresources, but the traditional link between places of consumption and places of extraction \nthat was once vital for a city's prosperity has been disrupted. As geographical proximity became less important for urban success, the environmental impact of this shift was overlooked, \ncontributing to the unsustainable nature of modern society, particularly due to the physical \nseparation of consumption and resource extraction. \n\nAiming to shorten the awareness gap that current planetary urbanisation has produced, the \nEarthy Indexes workshop held by the author and Erzë Dinarama at the CAADRIA conference \n2022\/23 presented a computational methodology – strongly supported by the discussed pipeline – to engage with the concept of ecological footprint at the city scale. More specifically, \nit challenged participants to crossread resource-specific demands of agricultural, pasture or \nforest land with context-specific land availability and land accessibility; finally envisioning \nup to which extension a city would consume if operating only by proximity logics (Figure \n2).  \n\nIn line with another study on Spatialized Metropolitan Ecological Footprints (Neri 2021), \nthe analysis computes pro-city land consumption values to fulfil the annual demand of a selected resource for its entire population and consequently queries exact amounts of land, filtered and ranked by its infrastructural accessibility. It exploits mainly two data layers: the \nGlobCover 20095 for a 300 m resolution global land cover map, and the Oxford’s Global \nFriction Surface 20196 layer to feed a territorial road-sensitive cumulative cost analysis via \nthe ee_cumCost component. More specifically, the latter offers a map where every pixel is \ngiven a speed to travel based on the local road infrastructure at approximately 900 m scale \nand based on a combination of national and global (OSM) data. \n\nFig. 2:  Ecological footprint studies for coffee, beef and wood, for the city of Barcelona as \npart of the Earthy indexes workshop led by Erzë Dinarama and Iacopo Neri at the \nCAADRIA 2022 – Post Carbon conference  \n\nBridging statistical data (e. g., demography and land consumption values) with geographical \ndata (e. g., land use and accessibility maps), this approach offers a fruitful pedagogical platform to engage with territorial indexes, while discussing the role of critical cartography in \nsupport of sustainability-related studies.  \n\nUrban Accessibility Maps \n\nFinally, the proposed pipeline has been adopted to study micro-scale mobility patterns. Reflecting on the aforementioned Oxford Global Friction Surface layer, the ee_cumCost component offers the possibility to alternatively use an ad-hoc friction layer to run cumulative \ncost analysis, therefore, exploiting GEE only for computing purposes and not for data collection. Again, OSM data provides a valuable medium to fulfil this goal, and can be easily accessed in GH via many workflows (i. e. Urbano7, Gismo 8) and geographically aligned with \nthe proposed pipeline via the utility reproject_UTM component. Technically, the cumulative \ncost component welcomes any sort of curve-based geometry to paint an image on the GEE \nserver with custom values and for any provided resolution, permitting the modelling of district-scale isochrone studies unlimited by the coarser standard friction layers of GEE (Figure 3).  \n\nThis was the subject of the one-week Urban Analytics remote workshop for the IAAC 2022 \nsummer school led by the author together with Eugenio Bettucchim, where the international \naudience of participants mapped for their home-towns a series of accessibility maps to various amenities and public services, collectively discussing by comparison the manifold forms \nof the x-minutes city. \n\nOther scholars have been using OSM data to create intuitive pipelines for accessibility studies \nvia network graph (Geoff 2020). Despite being widely used in mobility studies as an excellent \nmedium to represent complex relationships between the different elements of a street network \n(i. e. intersections, roads, and traffic flow), graph modelling requires extensive cleaning operations in its set up phase, which – for a crowd-sourced and in-development database such as OpenStreetMap – may disincentive non-expert users in comfortably interact with the algorithm. Trading specificity over usability, the proposed pipeline suggests a raster based approach  to  model  street-sensitive  accessibility  maps,  solving  the  incongruencies  within  the \nOSM network with a choice of pixel-resolution. \n\nConclusion and Outlook \n\nIn conclusion, the Grasshopper3D addon discussed in this text allows designers to access and \nutilise geospatial data from the Google Earth Engine platform in their design process. The \nplugin consists of five components that import and process geospatial data through the Earth \nEngine  API  Python  library  and  Hops.  The  Earth  Engine  API  is  the  official  Python  client \nlibrary for accessing GEE and is used within a tailored Flask application to fetch the required \ninformation in response to inputs from the GH components. These components allow designers to explore and use GEE data, specifically imagerial data, for various purposes and scales: \nfrom digital fabrication to digital Landscape Architecture, and integrating it with more traditional computational pipelines. \n\nAs proved through one year of teaching experience, this plugin represents a valuable tool for \ndesigners seeking to use geospatial data in their work and expands the capabilities of GH by \nlinking it to GEE's extensive data catalogue and powerful processing capabilities. \n\nFurther steps can be taken to extend the plugin with GEE’s Machine Learning algorithms \nlike the ones used for classification, clustering, regression or feature extraction, to name a \nfew. Related to a higher level of expertise, these algorithms allow to reproduce at will many of the pre-processed layers of GEE, which might be used to accomplish adhoc or higher resolution maps, similarly to the example of the district-scale isochrone studies via externally \nfetched OSM data, as well as to include design inputs in the forecast of their impacts. \n

As we move forward in the 21st century, the world is facing unprecedented global changes \nthat require a new approach to landscape architecture. Resilience has become a crucial factor \nin designing and man-aging our landscapes in response to natural and man-made hazards, \nsuch as climate change, urbanization, and environmental degradation. Therefore, the theme \nof this issue of the Journal of Digital Landscape Architecture, “Future Resilient Landscapes”, \nis timely and relevant. The term “Resilient Landscape” refers to a landscape that can withstand and recover from shocks and stresses caused by  various hazards, including extreme \nweather events, sea level rise, and biodiversity loss. Achieving resilience requires an integrated approach that considers ecological, social, and economic factors and leverages tech-\nnology and innovation. This issue of the journal features a range of articles that showcase \nhow digital tools and techniques can contribute to building resilient landscapes. The articles \ncover various topics related to resilience, including global change and hazard response, landscape and building information modeling, geodesign approaches, digital technologies, and \nrelated case studies. The use of UAV imagery and remote sensing in landscape architecture \nis explored in detail, along with the role of mobile devices, the internet-of-things, and ‘smart’ \nsystems in landscape architecture. Algorithmic design and analysis of landscapes, visualization, animation, and mixed reality landscapes are also dis-cussed. Finally, the issue examines \nthe role of digital fabrication in landscape architecture and how to teach digital landscape \narchitecture in academia and professional practice. In conclusion, this issue of the Journal of \nDigital Landscape Architecture provides a comprehensive overview of the current state of \nresearch and practice in the field of resilient landscapes. We hope that the articles will inspire \nand inform landscape architects, planners, and researchers to adopt a more integrated, col-\nlaborative, and innovative approach to building a sustainable and resilient future. We thank \nall the authors, reviewers, and editors who have contributed to this issue and made it possible. \n\nIf you’ve managed to read this far, then we’re lucky. Didn’t you get the feeling that the lyrics \nseemed somehow interchangeable, terribly generic and oddly impersonal? Did you perhaps \nthink something like “what in the world happened to this guy (the author), wasn’t he writing \nmore reflectively and engagedly before”? Well, then you are spot on. The above part of the \nforeword was written by a chatbot, built on top of large language models, fine-tuned using \nboth supervised and reinforcement learning techniques. Any simpleton can use this relatively \nnew breed of AI technology to generate endless texts while leaning back and staring at the \nscreen of their mobile phone themselves, following, for example, the latest news from the \nNorwegian Flat Earth Society. Or watching cat videos. Or consuming some other compara-\ntively important thing. Great, isn’t it? Now, right now, we have arrived in the digital age. Not \nquite as exciting as we might have thought. Recently, when an exhilarated and nervous young \ninterviewer  asked  the  famous,  now  95-year-old  Noam  Chomsky,  whether  software  like \nChatGPT would replace people’s language learning in the future, for example French, Chomsky twisted the corner of his mouth for a tenth of a second. Then he muttered something like, \n“Don’t worry about it too much”. Anything new is eyed anxiously until it goes out of fashion, \nbecomes commonplace, and then is forgotten. Yes, AI will be-come big, but no, DALL-E \nand the other applications will not put the landscape architect out of work. Let’s be happy \n\nand also a little proud that the DLA conference has been ploughing the field of digital landscape architecture  for  well over 20 years  without getting obsolete or out of fashion. Such \nstamina is the only way to prevent flat hype and create what is called substantial progress. \nProgress is happening slowly but steadily, and each year a new tenuous layer of research and \napplication is laid over the substance that has been worked out so far. Finally, back to the \nmain theme of the conference. Creating or preserving resilient or even sustainable landscapes \nis a noble goal and an important task. We don’t know anything with certainty, but we are \nrather sure that this task cannot be mastered without digital technology and methodology. \nThank you all for working diligently on this expedient challenge. \n\nJörg Rekittke, DLA Veteran \nNorwegian University of Life Sciences (NMBU) \n

Preface \n\nA network of engaged individuals teaching new information technologies at the International \nMaster of Landscape Architecture program MLA at Anhalt University established the annual \nConference on Digital Landscape Architecture DLA at our school in 1999. The DLA has to \ndate been held in Istanbul, Malta, Zurich, Munich, Aschersleben, twice in Boston, and frequently on our local campuses in Bernburg, Dessau and Köthen. In 2020 and 2022; the DLA \nwas hosted by Harvard University. Harvard  was able to organize the DLA 2022 as a full \nhybrid conference when the Pandemic still limited traveling. The Journal Digital Landscape \nArchitecture JoDLA which we have developed for the conference is listed in the international \ncitation database Scopus. This publication is supported academically by eighty reviewers and \nboard members. Here, we wish to thank them all for their committed long-term support.  \n\nHaving 64 papers (from more than 20 countries) which successfully meet the standards of \nthe review process coordinated by the founder of DLA, Prof. Erich Buhmann, and his editorial team once again guarantees a very substantial conference. \n\nThe 24th international conference on digital landscape architecture is now back at our internationally known campus in Dessau. Prof. Dr. Matthias Pietsch, this year's local host, is also \norganizing DLA 2023 as a hybrid conference in collaboration with Prof. Dr. Nicole Uhrig \nand Prof. Trevor Sears. Even now having more than two years of experience in organizing \nvirtual lectures and conferences, meeting all the additional needs for an international conference in a hybrid format is still a challenge and requires many university resources. We are \nvery thankful for the team spirit of so many colleagues at Anhalt University, and to the board \nmembers of the DLA for their support once again. \n\nThis  year’s  main  theme  “Future  Resilient  Landscapes”  is  a  core  issue  in  several  research \nefforts of Anhalt University. Our keynote speakers will widen our view on the challenges \nenvironmental design faces in coping with global change. \n\nAs we are able to work with a digital twin of our globe, we can focus on how to use the tools \nof digital landscape architecture in order to meet the challenges of global warming. \n\nAll positively reviewed papers are available as open access papers at Wichmann publisher \nand the outcome of the conference will be published DLA 2023 in Dessau at https:\/\/www.dla-\nconference.com\/ as in the past. \n\nWe are looking forward to welcoming many of you again in person in Dessau in 2023 and \nhopefully in the following years as well. At the same time, we looking forward to seeing the \nmany participants  who for a variety of reasons  will be virtually attending the 24th Digital \nLandscape Architecture Conference. \n\nKöthen, March 15, 2023 \n\nProf. Dr. Jörg Bagdahn, President Hochschule Anhalt \/ Anhalt University \n\nIntroduction \n\nThe cover of the eighth issue of the Journal of Digital Landscape Architecture JoDLA 8-2023 \nshows imagery, captured using a drone, of the city of Bad Neuenahr-Ahrweiler in Germany, \na region which was severely impacted by a flood disaster in 2021. Showing damage caused \nby severe floods or extreme fires is one of the many ways digital landscape architecture can \ncontribute to Hazard Management, one of the issues of resilient landscape architecture. However, our profession would much prefer to use landscape modelling to prevent some of these \ndisasters. This issue of JoDLA presents the current capacity our profession has for doing so. \n\nAfter being listed in Scopus, the journal is now also listed in DOAJ (Directory of Open Access Journals). Wichmann publisher has made accessible the JoDLA, and its forerunner publication Digital Landscape Architecture, as open access papers since 2013 and therefore provides ten years documentation of research in the area of Digital Landscape Architecture. \n\nThe  DLA  2023  is  organized  by  the  next  generation  of  professors  at  Anhalt  University  in \nDessau, Germany, where the Digital Landscape Architecture – Con was founded.  \n\nProf.  Dr.  Matthias  Pietsch,  the  local  chair  of  the  2023  DLA,  suggested,  after  the  COVID \npandemic, to focus on the issues of our endangered landscape by calling the main theme for \nthis year’s conference and of the JoDLA 2023 Future Resilient Landscapes. \n\nIn addition to the main theme, we have provided a number of other possible areas for submitting papers on current research or outstanding practice in digital landscape architecture. \nWe received 98 extended abstracts and can now present the result of a rigid two-phase double-blind review process. \n\nThe eighth issue of the Journal of Digital Landscape Architecture 8-2023 covers 66 contributions on the following seven current areas of research and prototype applications in digital \nlandscape architecture: \n•  Resilient Landscape, Global Change and Hazard Response  \n•  Landscape and Building Information Modeling (LIM + BIM) and other Standardizations in Digital Landscape Architecture  \n•  Algorithmic Design and Analysis Landscapes  \n•  Geodesign Approaches, Technologies, and Case Studies  \n•  UAV Imagery and Remote Sensing and Digital Fabrication in Landscape Architecture  \n•  Visualization, Animation and Mixed Reality Landscapes (VR, AR)  \n•  Teaching and Hybridization in Digital Landscape Architecture \n\nWe are very pleased that the worldwide academic community continues to grow in spite of \ncontinuing crises. The accepted abstracts come from seventeen countries:  \n\ntwenty-eight entries from the United States, fourteen from Germany, ten from China, five \nentries each from Turkey, four from Canada, three each from Hungary, Italy and Switzerland, \nand two each from Australia, Finland, Indonesia, Netherlands, Norway, and Spain. \n\nAnd from the following countries one entry each was accepted: Brazil, Georgia, and Singapore. We are very happy to have authors from countries that have contributed in the past, as \nwell as those who contributed for the first time. \n\n\nWe hope you will appreciate the eighth edition. The printed copies will be sent out on request \nto all participants before the conference at the end of May 2023. \nYou  will find all the contributions online as open access publications at the  gis.Point and \ngis.Open platforms of Wichmann http:\/\/gispoint.de\/jodla.html. \n\nWe would also like to invite you to the next DLA conference. The 25th international conference on information technology in landscape architecture, Digital  Landscape  Architecture \nDLA 2024 with the main theme “New Trajectories in Computational Urban Landscapes and \nEcology”, will be held from June 5 to 7, 2024 at the Vienna University of Technology, Austria. \n\nThe Journal of Digital Landscape Architecture invites you to submit ideas for special issues \nand topics. Please follow our continuously updated announcements and call for papers and \nposters at www.dla-conference.com. Here you will also find the complete online documentation of the DLA beginning from the year 2013. For earlier contributions of DLA publications, you may ask our JoDLA office. \n\nErich Buhmann, Stephen Ervin, Pia Fricker, Sigrid Hehl-Lange, James Palmer, \nand Matthias Pietsch \n\nAs we move forward in the 21st century, the world is facing unprecedented global changes \nthat require a new approach to landscape architecture. Resilience has become a crucial factor \nin designing and man-aging our landscapes in response to natural and man-made hazards, \nsuch as climate change, urbanization, and environmental degradation. Therefore, the theme \nof this issue of the Journal of Digital Landscape Architecture, “Future Resilient Landscapes”, \nis timely and relevant. The term “Resilient Landscape” refers to a landscape that can withstand and recover from shocks and stresses caused by  various hazards, including extreme \nweather events, sea level rise, and biodiversity loss. Achieving resilience requires an integrated approach that considers ecological, social, and economic factors and leverages tech-\nnology and innovation. This issue of the journal features a range of articles that showcase \nhow digital tools and techniques can contribute to building resilient landscapes. The articles \ncover various topics related to resilience, including global change and hazard response, landscape and building information modeling, geodesign approaches, digital technologies, and \nrelated case studies. The use of UAV imagery and remote sensing in landscape architecture \nis explored in detail, along with the role of mobile devices, the internet-of-things, and ‘smart’ \nsystems in landscape architecture. Algorithmic design and analysis of landscapes, visualization, animation, and mixed reality landscapes are also dis-cussed. Finally, the issue examines \nthe role of digital fabrication in landscape architecture and how to teach digital landscape \narchitecture in academia and professional practice. In conclusion, this issue of the Journal of \nDigital Landscape Architecture provides a comprehensive overview of the current state of \nresearch and practice in the field of resilient landscapes. We hope that the articles will inspire \nand inform landscape architects, planners, and researchers to adopt a more integrated, col-\nlaborative, and innovative approach to building a sustainable and resilient future. We thank \nall the authors, reviewers, and editors who have contributed to this issue and made it possible. \n\nIf you’ve managed to read this far, then we’re lucky. Didn’t you get the feeling that the lyrics \nseemed somehow interchangeable, terribly generic and oddly impersonal? Did you perhaps \nthink something like “what in the world happened to this guy (the author), wasn’t he writing \nmore reflectively and engagedly before”? Well, then you are spot on. The above part of the \nforeword was written by a chatbot, built on top of large language models, fine-tuned using \nboth supervised and reinforcement learning techniques. Any simpleton can use this relatively \nnew breed of AI technology to generate endless texts while leaning back and staring at the \nscreen of their mobile phone themselves, following, for example, the latest news from the \nNorwegian Flat Earth Society. Or watching cat videos. Or consuming some other compara-\ntively important thing. Great, isn’t it? Now, right now, we have arrived in the digital age. Not \nquite as exciting as we might have thought. Recently, when an exhilarated and nervous young \ninterviewer  asked  the  famous,  now  95-year-old  Noam  Chomsky,  whether  software  like \nChatGPT would replace people’s language learning in the future, for example French, Chomsky twisted the corner of his mouth for a tenth of a second. Then he muttered something like, \n“Don’t worry about it too much”. Anything new is eyed anxiously until it goes out of fashion, \nbecomes commonplace, and then is forgotten. Yes, AI will be-come big, but no, DALL-E \nand the other applications will not put the landscape architect out of work. Let’s be happy \n\nand also a little proud that the DLA conference has been ploughing the field of digital landscape architecture  for  well over 20 years  without getting obsolete or out of fashion. Such \nstamina is the only way to prevent flat hype and create what is called substantial progress. \nProgress is happening slowly but steadily, and each year a new tenuous layer of research and \napplication is laid over the substance that has been worked out so far. Finally, back to the \nmain theme of the conference. Creating or preserving resilient or even sustainable landscapes \nis a noble goal and an important task. We don’t know anything with certainty, but we are \nrather sure that this task cannot be mastered without digital technology and methodology. \nThank you all for working diligently on this expedient challenge. \n\nJörg Rekittke, DLA Veteran \nNorwegian University of Life Sciences (NMBU) \n\nConcepts and Techniques for Large-Scale Mapping \nof Urban Vegetation Using Mobile Mapping Point \nClouds and Deep Learning \n\nAbstract: In urban environments, roadside vegetation provides important ecosystem services. Reliable \nand up-to-date  information  on  urban  vegetation  is therefore  needed  as  a  basis  for  sustainable urban design and regular tasks such as vegetation maintenance. Mobile laser scanning (MLS), i. e., the use of \nvehicle-mounted laser scanners, offers strong potential for capturing 3D point clouds of road environ-\nments on a large scale at a low cost. In this paper, the potential and challenges of using MLS for vegetation mapping are discussed. To lay a foundation for MLS-based inventories of roadside vegetation, a \nconcept for the automatic detection and analysis of vegetation in MLS point clouds using deep learning \nis presented. The proposed workflow covers vegetation detection and classification, delineation of individual trees, and estimation of tree attributes. In a case study, an initial implementation of the work-flow is tested using MLS datasets from two German cities and the results are evaluated through visual \ninspection. It is demonstrated that the proposed deep-learning approach is able to detect and classify \nvegetation in MLS point clouds of complex urban road scenes. When delineating individual trees, accurate results are obtained for solitary trees and trees with little canopy overlap, while the delineation \nof trees with strongly overlapping canopies needs further improvement in some cases. The results indicate that geometric tree attributes such as tree height and trunk diameter can be accurately estimated \nfrom MLS point clouds if the accuracy of the preceding processing steps is sufficiently high. \n\nIntroduction \n\nIn urban environments, roadside vegetation provides important ecosystem services, including \nsequestering carbon, mitigating air pollution, regulating microclimate, providing habitat, and \npromoting human well-being (SÄUMEL et al. 2016). Urban trees also help mitigate the effects \nof  climate  change  through  local  cooling  and  stormwater  absorption  (PATAKI  et  al.  2021). \nTherefore, maintaining and expanding roadside vegetation is essential for sustainable urban \ndevelopment. Reliable and up-to-date information on urban vegetation is needed to guide the \ndesign of urban green spaces toward the provision of ecosystem services (ELDERBROCK et al. \n2020)  and  to  develop  appropriate  green  space  management  programs  (SCHIPPERIJN  et  al. \n2005). Many municipalities, especially in North America and Europe, therefore, conduct inventories of public green spaces. These inventories often focus on surveying individual trees, \ni. e., mapping the location of individual trees, and collecting tree attributes such as tree species, height, and trunk diameter (MA et al. 2021). Since manual vegetation inventories are\ncostly and time-consuming, the use of LiDAR systems (Light Detection and Ranging) for\nautomated or semi-automated vegetation mapping has become an important research topic.\nThese systems capture the environment in the form of high-resolution 3D point clouds and\ncan be used with various acquisition platforms. The acquisition can be categorized as terrestrial laser scanning (TLS) with tripod-mounted laser scanners, personal laser scanning (PLS) \nwith handheld or backpack-mounted laser scanners, mobile laser scanning (MLS) with vehicle-mounted laser scanners, unmanned aerial vehicle-borne laser scanning (UAV-LS), and \nairborne laser scanning (ALS). Compared to TLS, PLS, and UAV-LS, the use of vehicle-mounted laser scanners significantly reduces the acquisition effort, while providing 3D point \nclouds with higher resolution and fewer occlusions than ALS. Because of these characteristics, MLS is a well-suited technology for the large-scale mapping of roadside vegetation and \nthus could become a complement or alternative to conventional vegetation surveys. Compared to conventional field surveys, MLS-based vegetation mapping would reduce labor and \ncost, while providing a richer, three-dimensional representation of vegetation. To realize the \npotential of MLS for vegetation mapping, an automated approach is needed to derive semantic information about vegetation from raw 3D point clouds. To lay a foundation for building \nsuch a system, this paper presents a general concept for the automatic detection and analysis \nof  vegetation  in  MLS  point  clouds.  In  contrast  to  previous  work,  the  proposed  workflow \nbuilds on a modern deep-learning approach for 3D point cloud segmentation. In a case study, \nan initial implementation of the proposed workflow is tested on MLS datasets from two cities \nand first results are provided. \n\nPotential of MLS-Based Vegetation Inventory \n\nCompared to conventional field surveys, MLS-based vegetation inventories would provide \nseveral advantages: First, the labor and cost of vegetation surveys would be reduced, allowing \nlarger areas to be mapped and vegetation inventories to be updated more frequently (e. g., \nquarterly to cover all seasons). Second, capturing vegetation in the form of 3D point clouds \nwould provide additional and more detailed data on urban vegetation. For example, shrubs \nand hedges could be mapped, which are not recorded in most conventional vegetation surveys \ndespite their ecological value and aesthetic impact. Furthermore, conventional tree invento-\nries usually only capture a limited number of tree attributes (ÖSTBERG et al. 2013). Using 3D \npoint clouds, additional geometric tree attributes such as trunk orientation or crown volume \ncould be captured (HERRERO-HUERTA et al. 2018). 3D point clouds could even be used to \nmodel the entire branching structure of trees (DU et al. 2019). Overall, the data collected in \nMLS-based  vegetation inventories could be used to build  comprehensive  tree information \nmodels, as proposed by SHU et al. (2022). Such information models would address the information needs of a wide range of stakeholders, including city officials, arborists, ecologists, \nand  landscape  architects.  For  landscape  architects,  information  derived  from  MLS-based \nvegetation inventories could be particularly useful for the following applications: (1) At the \nbeginning  of  the  design  process,  more  detailed  plant  models  could  be  created  to  enable  a \nmore accurate representation and analysis of existing vegetation. While generic plant models \nhave been commonly used to visualize vegetation (OEHLKE et al. 2015), plant models derived \nfrom  3D point  clouds  could  be  used  to  create  more  realistic  visualizations  of  vegetation. \nMoreover,  plant  models  derived  from  3D  point  clouds  could  also  be  used  to  estimate  the \necosystem services and disservices provided by existing vegetation. For example, the shading \nprovided by trees could be modeled, or the carbon storage and oxygen release potential of \nvegetation could be estimated (SCHOLZ et al. 2018). (2) Conducting MLS-based vegetation \nsurveys on a regular basis could also provide ground truth data for building and validating \nsimulation models of plant growth (WHITE et al. 2022). More accurate simulation of plant \ngrowth would support decisions between different planting regimes in the design of green \nspaces. (3) Once a certain planting regime has been established, MLS could be used to continuously monitor the green space (e. g., growth and condition). In this way, design decisions \ncould be evaluated, and vegetation maintenance measures could be planned and aligned with \nthe design goals. \n\nChallenges in MLS-Based Vegetation Inventory \n\nWhile MLS offers significant potential for the large-scale mapping of roadside vegetation, \nseveral challenging data characteristics must be considered when developing systems to automatically process MLS point clouds for vegetation mapping: \n\nLarge data volume: MLS produces large amounts of data with hundreds of points per square \nmeter. To be able to process MLS point clouds of large road segments or entire city districts, \nall processing steps must be automated and implemented efficiently. This requires algorithms \nthat allow parallel processing with modern multicore systems or graphics cards. \n\nVarying point density: The point density of MLS point clouds depends on the scanner type, \nthe acquisition speed, and the distance to the scanner trajectory. Systems for processing MLS \npoint clouds must therefore be robust to varying point densities. \n\nOcclusions: In MLS point clouds, vegetation  may be completely or partially occluded by \nother objects. The algorithms for detecting and analyzing vegetation in MLS point clouds \nmust therefore be able to cope with incomplete data, i. e., 3D point clouds that cover only a \npart of the surface.  \n\nLimited per-point attributes: Most LiDAR systems provide the 3D coordinates and reflection intensity for each scanned point. More advanced systems can capture additional attributes such as surface color (e. g., from panoramic images), temperature, or humidity. To support a wide range of scanner types, however, algorithms for vegetation detection and analysis should only rely on point coordinates and reflection intensity as input attributes. \n\nIntegration of  multi-temporal MLS data: Approaches for storing and processing  multi-temporal MLS data are needed to enable continuous vegetation monitoring. Since inaccuracies  can  occur  in  the  georeferencing  of  MLS  point  clouds,  techniques  for  co-registering \n3D point  clouds  from  different  acquisition  runs  are  needed.  In  addition,  approaches  are \nneeded to increase the coverage and merge redundant information from MLS point clouds \nacquired at different times and to identify areas that have changed between acquisition runs.  \n\nA Concept for the Detection and Analysis of Vegetation in \n\nMLS Point Clouds Using Deep Learning \n\nIn the following, a concept for the automatic detection and analysis of vegetation in MLS \npoint clouds is presented. The proposed workflow is divided into three steps: (1) In the first \nstep, deep-learning models are used to extract vegetation points from raw MLS point clouds \nand classify them into different vegetation types. (2) In the second step, the tree points detected  by  a  deep-learning  model  are  segmented  into  individual  trees.  (3)  Using  the  point \nclouds of individual trees, tree attributes such as tree height and trunk diameter are derived. \n\nDetection and Classification of Vegetation \n\nBesides vegetation, urban MLS point clouds contain a variety of other objects such as city \nfurniture, vehicles, and buildings. An automated approach is required to segment 3D point \nclouds of such complex scenes into vegetation points and non-vegetation points. In addition, \ndifferent vegetation types, such as low vegetation and trees, need to be distinguished. \n\nRelated Work \n\nWhile the present work aims to detect both low vegetation and trees, most previous work has \nfocused on tree detection in MLS point clouds. To segment urban MLS point clouds into tree \npoints and non-tree points, many works use either rule-based approaches (HAO et al. 2022, \nHUI et al. 2022) or statistical machine learning approaches (WEINMANN et al. 2017, CHEN et \nal. 2019). While rule-based approaches are usually not able to detect other vegetation types \nthan trees and are often very dataset-specific, statistical machine learning approaches often \nlack the capability to combine geometric features of different spatial scales. The recent work \nof CHEN et al. (2021) is among the first to use a deep-learning approach for vegetation detection in urban MLS point clouds. However, the PointNLM architecture proposed in their work \nis trained on hand-crafted geometric features of supervoxels and does not yet exploit the full \npotential of recent deep-learning architectures for 3D point cloud segmentation. \n\nMethodology \n\nIn general, deep-learning architectures for processing 3D point clouds can be divided into \narchitectures that operate on intermediate representations of point clouds such as 2D images \nor 3D voxel grids, and architectures that process point clouds directly (BELLO et al. 2020). \nOur workflow is built upon architectures that process 3D point clouds without intermediate \nrepresentations. These architectures usually are more efficient than architectures that use intermediate representations and are designed to self-learn geometric features at different spatial scales. Recent examples of such architectures include KP-FCNN (THOMAS et al. 2019), \na fully convolutional neural network (FCNN) based on a kernel-point (KP) convolution, and \nRandLA-Net (HU et al. 2020), “an efficient and lightweight neural architecture to directly \ninfer per-point semantics for large-scale point clouds” based on random sampling (Rand) and \nlocal feature aggregation (LA). In our implementation, the KP-FCNN Rigid architecture is \nused. Different variants of this architecture exist, targeting different processing tasks such as \nclassification  or  semantic  segmentation  of  3D  point  clouds.  In  this  work,  the  detection  of \nvegetation in MLS point clouds and its classification into different vegetation types are modeled as a single semantic segmentation task. To this end, models are trained to distinguish the \nfollowing classes: \n\nLow vegetation includes all types of low vegetation, e. g., shrubs, hedges, and potted plants. \n\nTree trunk includes tree trunks, defined as the segment ranging from the base of a trunk to \nthe first branching. \n\nTree branch includes the main branches of a tree that are not covered by foliage. \n\nTree crown includes tree foliage and the branches and twigs covered by it.  \n\nOther includes all non-vegetation points, e. g., ground, buildings, and city furniture. \n\nThis classification scheme is more fine-grained than the classification schemes used in previous studies. Segmenting trees into trunk, branch, and crown areas provides additional semantic information that can be used to delineate individual trees and estimate certain tree \nattributes. However, the proposed classification scheme also requires more detailed ground \ntruth annotations to train deep-learning models, which increases the annotation effort. \n\nSince large-scale MLS point clouds usually contain millions of points, they cannot be processed as a whole by deep-learning models. Therefore, our workflow includes two preprocessing steps to prepare raw MLS point clouds for processing by deep-learning models (Figure 1). First, the resolution of the large-scale point clouds is reduced by grid subsampling. \nSubsequently, small subsections of fixed size (4 m radius, 4096 points) are sampled from the \ndownsampled large-scale point clouds. These small-scale point clouds are processed by the \ndeep-learning model. The model predictions are mapped to the large-scale point clouds and \nare interpolated for points that were not covered by the small-scale point clouds. \n\nDelineation of Individual Trees \n\nThe deep-learning approach described in the previous section performs point-wise segmentation, where each point is assigned to a semantic class. However, for points that are classified \nas tree points, the deep-learning approach does not provide information about which individual tree a point belongs to. Therefore, an additional processing step is required to segment \nthe tree points identified by a deep-learning model into individual trees. Especially in areas \nwith high tree density and overlapping tree canopies, delineating individual trees can be a \nchallenging task. \n\nRelated Work \n\nIn  previous  work,  different  algorithms  have  been  proposed  to  delineate  neighboring  trees \nwith overlapping crowns, including graph-based approaches (ZHONG et al. 2017), clustering \napproaches (LI et al. 2021), and region growing approaches (LI et al. 2016). Some of these \napproaches rely on the correct detection of tree trunks and are therefore not robust to occlusions and misclassifications of tree trunks. Additionally, many algorithms for delineating individual trees are based on voxel representations of 3D point clouds, which limits their accuracy. \n\nMethodology \n\nTo improve robustness to incomplete data and achieve more accurate segmentation of overlapping tree canopies, we propose a multi-step approach to delineate individual trees. The \nproposed approach is inspired by several previous works (WU et al. 2013, ZHONG et al. 2017, \nXU et al. 2020, YANG et al. 2020) and consists of the following steps: \n\n1)  Identification of tree locations: In the first step, the locations of individual trees are \nidentified. To improve robustness against incomplete data, tree trunks, main branches, \nas well as treetops are considered for identifying tree locations. To identify tree locations \nbased on tree trunks and main branches, trunk and branch points identified by the deep-learning approach are clustered using the DBSCAN algorithm (ESTER et al. 1996). Adjacent clusters with similar growth direction are merged and the midpoints of the remaining clusters are used as approximate tree locations. To identify tree locations based on crown tops, a 2D canopy height model is constructed and searched for local maxima. \nThe positions of local maxima whose distance from the already found tree locations is \nabove a threshold are added to the set of tree locations. \n\n2)  Coarse delineation of individual trees: The tree locations obtained in the previous step \nare used to determine the coarse boundaries of the individual trees and to identify regions \nwith overlapping tree canopies. Different algorithms can be used for this purpose, e. g., \n2D Voronoi segmentation can be performed (ZHONG et al. 2017), or a canopy height \nmodel can be constructed and segmented using the 2D marker-controlled Watershed algorithm (KORNILOV & SAFONOV 2018). For the implementation in this work, a combination of both algorithms is used. \n\n3)  Refined delineation of overlapping tree canopies: If the coarse tree delineation indicates that two trees are close to each other, their canopies may overlap. In such cases, \nthe segmentation of the tree canopy is refined. Different approaches can be used to delineate  trees  with  overlapping  canopies,  including  graph-based  approaches,  clustering \napproaches,  or  region  growing  approaches.  Our  workflow  uses  a  region  growing  approach since it reflects the natural growth direction of trees and allows the delineation of \ntrees with different shapes. Specifically, we implement a custom, density-based region \ngrowing algorithm that is inspired by the DBSCAN algorithm (ESTER et al. 1996). In \nthis algorithm, for each tree to be processed, a set of seed points is selected (i. e., points \nthat belong to the tree with a high degree of certainty). In an iterative process, neighbor \npoints of the seed points are assigned to the respective tree, if they have not yet been \nassigned to another tree. Neighbor points that satisfy the core point criterion as defined \nin the DBSCAN algorithm (ESTER et al. 1996) become seed points themselves. To ensure  that  neighboring trees  grow  evenly,  points  are  sorted according  to  their  distance \nfrom the crown boundary determined during coarse tree delineation and processed in \nthat order. \n\n4)  Removal of implausible trees: In the final processing step, trees with implausible shape \nor size are discarded. Specifically, trees with few points and trees whose height is below \na threshold are filtered out. \n\nEstimation of Tree Attributes \n\nAfter obtaining point clouds representing individual trees, different tree attributes can be estimated. Most existing studies focus on the estimation of geometric tree attributes (HERRERO-HUERTA et al. 2018, WU et al. 2013, XU et al. 2020), while few authors also derive non-geometric attributes such as tree species or vitality (WU et al. 2018, CHEN et al. 2019). Since \n3D point clouds are particularly suitable for deriving geometric tree attributes, we also focus \non these attributes. However, algorithms for estimating non-geometric tree attributes such as \ntree species, carbon storage capacity, or oxygen release potential may be integrated into the \nworkflow in the future. For the estimation of the attributes tree location (WU et al. 2013), tree \nheight, trunk direction, crown width (HERRERO-HUERTA et al. 2018), trunk diameter at breast \nheight (CHEN et al. 2019), and crown volume (LI et al. 2020), we adopt the approaches used \nin previous work. Other tree attributes, namely under-branch height, and crown base height, \ncan  directly  be  derived  from  the  deep-learning-based  segmentation  of  trees  into  trunk, \nbranches, and crown. \n\nCase Study \n\nA test application was implemented to demonstrate the potential of the concept presented in \nthis paper. MLS point clouds collected in the cities of Essen, Germany, and Hamburg, Germany, were used to evaluate the test application. The point clouds were acquired using Trimble MX8 scanners in the leaf-on season. We manually annotated the point clouds and split \nthem into a training set, a validation set, and a test set. In the following, some preliminary \nresults for the test set are shown. \n\nFigure 2 shows exemplary results of the deep-learning-based vegetation detection and clas-\nsification.  As  can  be  seen  there,  large  portions  of  the  vegetation  are  segmented  correctly. \nHowever, several smaller segmentation errors can be identified: In multiple cases, low veg-\netation and tree crowns are confused. In addition, some pole-like objects are misclassified as \ntrees. There are also a few cases in which tree trunks are missed by the deep-learning model. \n\nResults of the delineation of individual trees are shown in Figure 3. While solitary trees and \nadjacent trees with little canopy overlap are correctly delineated in most cases, the results for \nthe delineation of trees with strongly overlapping canopies are mixed. Trees whose crown \nhas a large extent are over-segmented in several cases, and some parts of tree crowns with \nlow point density are missed by the region growing algorithm. \n\nFigure 4 shows some results of the estimation of tree attributes. As can be seen there, accurate \nestimates of geometric tree attributes are obtained if the accuracy of the preceding processing \nsteps is sufficiently high. When large parts of a tree are missed during tree segmentation, \nattributes such as tree height, crown width, and crown volume are often underestimated. In \ncases where multiple trees are recognized as a single tree, the crown width and crown volume \nare often overestimated. \n\nDiscussion and Conclusion  \n\nIn this work, a concept for the automatic detection and analysis of vegetation in MLS point \nclouds has been presented. The implementation of the concept produced promising results \nfor representative MLS point clouds from two cities. However, to approach the accuracy of \nmanual vegetation mapping, the method needs to be further improved. Since the accuracy of \nvegetation detection and classification affects the accuracy of the following processing steps, \nfurther improvement of the deep-learning approach for vegetation detection and classifica-\ntion would be of major benefit. Despite this need for improvement, the preliminary results of \nthis work suggest that the proposed deep-learning approach can detect and classify vegetation \nin complex street scenes that would be difficult to model using rule-based approaches. However, this comes at a price: The training of deep-learning models requires extensive annotated \ntraining data and high computing power of graphics hardware. To improve the practicality of \nthe approach, techniques to reduce the labeling effort (e. g., active learning or transfer learning) and improve model speed (e. g., model pruning) could be incorporated into the work-\nflow. \n\nWhile  the  test  application  presented  in  this  paper  was  limited  to  capturing  geometric  tree \nattributes,  it  would  be  desirable  to  derive  even  richer  tree  information  models  to  cover  a \nbroader range of use cases. For example, detailed models of tree branching structures could \nbe derived from 3D point clouds to enable estimation of aboveground biomass and carbon \nstorage capacity, as well as realistic vegetation visualization. Furthermore, it would be useful \nto integrate approaches for processing multi-temporal MLS data into the workflow. In this \nway, point clouds representing vegetation in leaf-on and leaf-off conditions could be combined. Capturing vegetation in leaf-off conditions would avoid the occlusion of woody components by foliage and thus facilitate the acquisition of woody biomass and the delineation \nof individual trees. In addition, approaches for predicting non-geometric tree attributes such \nas tree species and tree vitality could also be integrated into the workflow, e. g., by combining \n3D point clouds with spectral data from panoramic images or aerial photographs. \n\nSince MLS is a cost-effective method to survey very large areas, the present work focused \non vegetation mapping using MLS. However, MLS is only suitable for capturing vegetation \nin the proximity of drivable roads,  while PLS or UAV-LS  are  more suitable for  mapping \nvegetation in parks or private gardens. To provide a complete mapping of urban vegetation, \nthe concept presented in this work should be transferred to other LiDAR platforms. Since \nPLS point clouds have similar characteristics to MLS point clouds and a generic deep-learning approach is used in this work, transferring the approach to PLS point clouds should be \npossible with little effort. \n\nBuilding on the concept presented in this paper, a data processing tool could be developed \nfor large-scale and cost-effective urban vegetation mapping. Such a tool would enable continuous monitoring of urban vegetation and thus provide a basis for sustainable design and \nmaintenance of urban green spaces. In particular, it would enhance the study of ecosystem \nservices provided by urban vegetation and could thus help to guide the design of urban green \nspaces towards the provision of ecosystem services. \n\nEcological Robotics \n\nAbstract: This research presents a novel method for paste-based robotic planting. Our method for robotically extruding seeds in a paste of clay and planting media enables precise, algorithmic planting. \nThis additive manufacturing process builds microtopography, while planting seeds. With this 3D printing process designers can engage with the geomorphological and ecological processes that shape landscapes. Microtopography can be designed to direct flows of water, while planting can be designed to \nfoster biodiversity, form ecotones, and control erosion. As a proof of concept, we demonstrate how \nalgorithms can generate precise planting patterns  such as pseudorandom gradients. We envision  unmanned ground vehicles with seed printing systems planting entire landscapes with algorithmic designs. \n\nIntroduction \n\nResearch on autonomous construction in architecture has explored the novel creative, material, tectonic, performative, and aesthetic potential for the computational design of the built \nenvironment (GRAMAZIO & KOHLER 2014, MENGES 2015). Similarly, the autonomous construction and planting of landscape promises unique aesthetic opportunities and new ways of \nengaging with ecology and geomorphology. Designers have experimented with robotic processes for constructing landforms such as soil 3D printing (MITTERBERGER & DERME 2019, \n2020) and autonomous excavators (JUD ET AL. 2021, HURKXKENS ET AL. 2022). Robotic processes for planting have been developed such as vacuum seeding robots (GOLDBERG 1995, \nFARMBOT 2020, PRESTEN et al. 2021), autonomous seed drilling tractors (GROß 2013), and \nseed sowing with unmanned aerial systems (MOHAN et al. 2021). While sowing seeds is an \nimprecise process with low survival rates, seed drilling is more precise and has higher survival rates, but mechanically disturbs soil, increasing the risk of soil erosion. Our paste-based \nextrusion method for autonomous planting has millimeter precision, high survival rates, does \nnot disturb soil, and creates microtopography. Potential applications include landscape architecture, land art, ecological restoration, and precision agriculture. \n\nMethods \n\nIn our method for robotic planting, seeds are extruded in a paste of clay, planting media, and \nwater (Fig. 1). As an additive manufacturing process, paste-based extrusion of seeds builds \nlandforms layer by layer. The proportions of the paste are calibrated so that it can be extruded \nsmoothly, while supporting seed germination. Clay is used for plasticity for the sake of ex-\ntrusion. Planting media is used to provide nutrition for plants, retain water, and provide pore \nspace for root growth. Water is used to wet the clay and germinate the seeds. The paste is 3D \nprinted, i. e. extruded, directly onto soil. After printing, the seeds embedded in the paste have \nthe shelter, nutrients, and moisture they need to germinate. Once the seeds have germinated, \nthe roots of the seedlings grow into the soil below. \n\nAs a proof of concept, we developed a prototype with a linear actuator ram mounted on a 6-axis robotic arm. Industrial robots are reliable, versatile systems that can easily be adapted to \nnew tasks, making them well suited for creative experimentation with novel material processes  (GRAMAZIO  & KOHLER  2014,  16).  For  this  prototype,  we  used  a  UR10e  industrial robotic arm because its 12.5 kg payload was enough for a large extruder with 2000 ml of paste and its reach of 1300 mm allowed for a large build space. Grasshopper, a visual programming environment for computational design (MCNEEL 2021), was used to generate geometry for robot path planning. The Robots ex Machina framework (GARCÍA DEL CASTILLO Y LÓPEZ 2019) was used to control the robot and extruder.  \n\nTo test this method, a series of planting patterns were robotically printed in the lab. We tested \ndesigns such as space filling curves, landforms derived from trigonometric waves, landforms \nderived from cellular texturing, landforms derived from procedural noise (Fig. 2 & Fig. 3), \nand generative typography (Fig. 4). For ease of printing and cultivation in a laboratory setting, designs were printed in growing trays and stored on racks with grow lights. Each 250 \nby 250 mm tray was filled with planting media as a substrate for the print. We used plants \nsuch as perennial ryegrass (Lolium perenne), annual ryegrass (Lolium multiflorum), arugula \n(Eruca vesicaria), radish (Raphanus sativus), alfalfa (Medicago sativa), Siberian kale (Brassica napus) and broccoli (Brassica oleracea var. italica). For the landforms, the extruder was \nfilled with layers of different seeds to create an elevation gradient of species. Over the course \nof the study, the prints were photographed daily to record the growth of the plants. \n\nResults \n\nThe planting designs printed cleanly and precisely as the crisp letterforms in Figure 4 demonstrate. Plants grew most healthily and vigorously in prints that were 5 or 10 mm tall, composed of 1 or 2 layers, because the seedlings’ roots had easier access to the porous, nutrient rich substrate of planting  media below. Furthermore, plants grew  more vigorously in narrower forms with widths of 20-40 mm because this ensured that seedlings had less competition and more access to sunlight and substrate. While paste-based extrusion of seeds creates microtopography, the scale of appropriate landforms is highly constrained by growing conditions.  \n\nFuture Work \n\nTo further this research, we are investigating alternative media for the paste, testing different \ntypes of extruders, integrating sensors into the system, and integrating the system onto an \nunmanned ground vehicle for landscape-scale planting (Fig. 5). We are testing pastes composed of biochar that release nutrients slowly and biopolymers that biodegrade rapidly. We are integrating moisture sensors, depth sensing, lidar scanners, and multispectral imaging for adaptive planting and monitoring. Using these sensors, we plan to conduct experiments with controls, replicates, and quantitative measures to assess the efficacy of this method with different pastes. To plant at landscape-scale, we plan to integrate the robot arm, extruder, and sensors onto an unmanned ground vehicle with real-time kinematic positioning. We plan to \nuse lidar, positional data, and simultaneous localization and mapping algorithms for autonomous navigation and for positioning the extruder relative to the ground in real time. For field \ntrials, we will use lidar and multispectral imaging on unmanned ground and aerial vehicles \nto conduct repeated surveys at high spatial and temporal resolution to assess plant growth.  \n\nConclusion \n\nWith paste-based robotic planting, computational designs for planting patterns can be autonomously seeded with high precision. While this experiment was conducted in the lab, robotic \nplanting could be done at scale in the field with unmanned ground vehicles. With generative \ndesign enacted by field robots, landscape architects would be able to design dynamic landscapes as ongoing performances – as ecological processes guided by design interventions. \nUnmanned aerial systems could  collect imagery and elevation datasets at high spatial and \ntemporal resolution that could inform the ongoing design and management of landscapes. As \nunmanned aerial systems monitor how landscapes evolve, unmanned ground vehicles could \nadaptively plant and replant in response, catalyzing new assemblages of plants and wildlife. \nWe hypothesize that algorithmic planting based on ecological principles could foster spatial \nheterogeneity, complexity, and thus biodiversity. We envision field robotics giving rise to an \nalgorithmic aesthetic of ecology.  \n\nNew media scholar Laura Marks describes algorithmic aesthetics as a semiotic process of \nenfolding and unfolding, a process in which infinite possibility is transcribed into information \nand then image (MARKS 2010). This is a performative aesthetics of infinity and contingency \n– an aesthetics that explores what is revealed and what is hidden; an aesthetic in which creativity is a  means of discovering the infinite. Design projects like  GRAMAZIO KOHLER Research’s  Endless  Wall  (2011),  SNØHETTA’S  MAX  IV  Laboratory  Landscape  (2016),  and \nMAEID’S Magic Queen (2021) evoke such an algorithmic aesthetic. With systems for precise \nautonomous planting, landscape architects would have the means to express ecological complexity  and  contingency  with  a  visibly  algorithmic  logic.  Computational  design  processes \nsuch as procedural noise, cellular noise, and fractional Brownian motion (Figures 2, 3 & 6) \ncould be used to generate planting patterns  with high spatial heterogeneity and ecological \ngradients  between  drifts.  Such  computationally  designed  planting  would  simultaneously \nevoke the accidental and intentional, the actual and virtual. \n\nExpanding Digital Design Workflows with Geospatial \nAnalytics: Linking Grasshopper3D with Google \nEarth Engine \n\nAbstract:  The  following  body  of  work  introduces  a  plugin  that  links  the  visual  scripting  language \nGrasshopper3D (GH) to the Google Earth Engine (GEE) in order to easily fetch geospatial information \nrelative to various societal issues and for any geographical area under study, inside the Rhino modelling \nsoftware.  \nAiming  at  expanding the  field  of  Digital  Landscape  Architecture with  novel  content to  analyse  and \ndesign, it provides designers with more than thirty years of historical imagery and scientific datasets, \ncollected in GEE on a daily basis by several institutions around the world. Leveraging the intuitiveness \nof the visual scripting language, it computationally empowers designers with geospatial insights with-\nout the need for any GIS skills and has been proved a successful platform for teaching purposes. Encouraging learning through application, the paper discusses three teaching experiences, which adopted \nthe proposed tool to visualise river dynamics in time, resource-specific maps of land consumption for \ncities and street-sensitive accessibility maps through the additional integration of OpenStreetMap data. \n\nIntroduction \n\nDuring the last three decades, a constellation of computational applications has emerged that \nempowers architects and designers to respond to the challenges of the  AEC and planning \nsectors through highly technological and innovative means. Active since late 2007, the Rhinoceros’s visual programming language: Grasshopper3D (GH), has been proved to be among \nthe most successful examples of this kind. It has offered a visually-intuitive medium to teach \nand compute advanced computational pipeline without writing one line of code, and has become an asset for both the academic and the industrial realms (CASTELO-BRANCO & LEITÃO \n202). Additionally, whether to simulate microclimatic conditions (MACKEY et al.. 2017), calculate structural performances (PREISINGER & MORITZ 2014), or work with georeferenced \ndata (DOGAN et al. 2018) to name a few, over the years GH has collected an extensive amount \nof plugins, developed by an active community of computational designers, to expand its influence in many aspects of the design process and in relationship to the many actors involved. \nFinally, by reshaping the traditional drawing tools through mathematics and functions, it has \nprovided designers with novel ideas to design while deeply influencing all scales of the project, from Digital Fabrication to Digital Landscape Architecture. Placed within this line of \ninvestigation,  the  following  body  of  work  introduces  a  plugin  that  links  Grasshopper  to \nGoogle Earth Engine to instantly fetch selected geospatial layers for any geographical area \nof interest. In this sense, it enables designers to access spatial insights related to more than \nthirty  years  of  remote  sensing  data,  collected  by  a  plethora  of  satellites  and  processed  by \nresearch institutions from all over the world. Answering to the call for data democratization \nwhile rendering large-scale computing accessible to non-experts, the Google Earth Engine \n(GEE)  is  a  “cloud-based  platform  for  planetary-scale  geospatial  analysis  [that  seamlessly \ngives to] not only traditional remote sensing scientists, but also a much wider audience, [access to many] societal issues including deforestation, drought, disaster, disease, food security, \nwater management, climate monitoring and environmental protection” (GORELIK et al. 2017). \nFor this reason, GEE is built around a petabytes-large catalogue of georeferenced data and \nan Application Programming Interface (API) to access and process server-side the same layers; achieving in this manner high speed performances and becoming suitable not only for \nglobal-scale  calculation  processes,  but  also  for  more  explorative  and  experimental  approaches, common in design processes.  \n\nThe Toolkit \n\nAs an entry point for designers to explore geospatial analytics through Google Earth Engine, \nthe toolkit proposed consists mainly of five GH components to import, spatialize, process \nand calculate geospatial raster layers through the Earth Engine API Python library1 and via \nHops. Being a recent development in the Grasshopper3D suite, Hops is the first package to \nefficiently link the visual scripting tool to the real potentialities of the Python programming \nlanguage.  By  externally  running  CPython  code  via  a  Flask  application,  it  allows  Python \nscripts to be implemented in GH unconstrained by the limitations of predefined libraries and \nopen to the vastness of community-driven packages available online.  \n\nBeing one of these contributions, the Earth Engine API is the official Python client library to \ndynamically access GEE. Used within the tailored Flask application, it runs requests from \nthe inputs in GH to the online GEE server and consequently fetches the required information. \nMore precisely, the discussed custom components to connect GH to GEE are: \n\n1)  ee_image: it permits the download of images from GEE for any geographical area of \n\ninterest and functions as the primary component to explore GEE. \n\n2)  ee_imageColl: it enables to work with the more advanced imageCollection typology and, \ncompared to the ee_image, requires extra information in respect to the date, or permitted \ncloud coverage to consequently extract images. \n\n3)  ee_ND: it engages with GEE to create normalised difference indicators on the server side \nby providing multiple bands to work with, and functions as an entry point to the world \nof remote sensing indicators \n\n4)  ee_cumCost: it calculates cumulative cost analysis, which are commonly used to spatialize accessibility, provided a cost to travel over a territory and an initial set of origins \n\n5)  reproject_UTM: an utility component to manage coordinate reference systems and align data fetched from GEE with the outputs of other plugins for GIS operations \n\nThe requirements to use the aforementioned components are kept very concise and focus on \nproviding the maximum flexibility with the minimum amount of inputs, and always comprises: an area of interest to download the image from, a resolution -in metres- to balance the amount of information to be downloaded, and the layer, with respective bands, that we are interested in accessing. Additionally, more inputs can be requested to calibrate the functions of the specific components, like in the case of the ee_ND that requires more than one band to reciprocally subtract, or the ee_cumCost which requires locations of origin for the accessibility analysis to be calculated from. This being said, the toolkit automatizes a series of spatial operations common in Geographical Information Systems (GIS) to deal with raster layers, \nlike is the case of resampling operations to obtain custom resolutions, or mathematical operations to calculate remote sensing indicators. It is important to notice that it does so in the \nbackground, opening up possibilities for the users to interact with the tool only through specifically opinionated inputs in order to facilitate its generic usability and versatility. In this \nsense, the tool aims at providing a simplified pipeline for computational designers to investigate  the  immensity  of  the  Google  Earth  Engine  database  for  design  purposes  through  a \nscarce set of inputs, allowing them to obtain material for further analysis and manipulations \nwith conventional processes – through standard components in Grasshopper3D – in a fast, \nand interactive fashion and without the need to be GIS experts.  \n\nFinally, the open source nature of the tool – a Python Flask application – permits an in-depth \ncustomization of each component – if equipped with enough knowledge of the Python programming language – and it has been proved to be a fruitful case to learn and apply computational logics in a pedagogical sense. It balances levels of complexity  when approaching GIS  processes  through  visual  programming  while  maintaining  the  possibility  to  read  and study the back-end codes when necessary. \n\nLearning Through Application \n\nThrough  one  year  of  teaching  experience,  the  proposed  plugin  has  been  tested  on  several \noccasions and has been proved to be versatile enough for different case studies, enhancing in \na broad sense the toolset that designers possess when approaching a territorial project – for \nvisualisation or analytical purposes – and not focusing on highly specific outputs. In the following section, the paper discusses three such occasions where the methodology has been \nshared with students to analyse river patterns in time, resource-specific maps of land consumption for cities, and street-sensitive accessibility maps through the integration of Open-StreetMap data relative to high resolution street networks.  \n\nMore precisely, the case studies hereby collected are the results of the Geomining lecture for \nthe Master in Landscape Urbanism at the Architectural Association School of Architecture \nin London, the Earthy Indexes workshop at the CAADRIA conference 2022\/23, and the one-week Urban Analytics workshop for the IAAC Global Summer School 2022. \n\nAnalysing River Patterns in Time \n\nThe analysis of river patterns in time is an important tool for understanding the dynamics of \nriver systems and the impacts of natural and human-induced changes on these systems. Used \nto inform a wide range of decisions related to the management and protection of river systems \nand the resources they provide, such as flood risk assessment, water resource management, \nenvironmental impact assessment, and land use planning, it is a fundamental asset for Landscape Architecture, which usually requires tedious data research and modelling.  \n\nUnequipped with a specific plugin, these studies are commonly carried out in GH through \nad-hoc scripting via iterative logics, such as for river meandering and oxbow lake simulation2 \nor water runoff studies which can be used as support material. Despite being excellent case \nstudies to teach iterative logics and loops (i. e. using the Anemone plugin3), they often fail to \nreach a high level of  specificity and end  up in the realm of design exercises compared to \nterritorial studies: fruitful to inspire design processes but insufficient for serious analytical \npurposes.  \n\nOn the other hand, there are several methodologies that can be used to map river dynamics \nin time using GIS, including time-enabled data, dynamic modelling and finally time series \nanalysis. Despite being able to provide highly specific and precise assessment of river dynamics, studies via time-enabled data or computational models are generally expensive or \ndemanding to implement due to the requirements of on-site equipment or highly trained professionals to set up and run hydrologic models. On the contrary, remote sensing has been \nwidely used in the analysis of river patterns in time over the past few decades as it allows for \nthe collection of large amounts of data over a broad area in a relatively short period of time; \npermitting a wide range of spatial and temporal scales to be included in a interoperable medium for the experts of the field to disseminate the results of their research. \n\nIn this sense, the JRC Global Surface Water mapping layer4 offers an unprecedented synthetic image of more than 4 million scans from Landsat 5, 7, and 8 to describe in high-resolution  the long-term changes  happening in river  systems  from early 1984 until the end of \n2021. Hosted in GEE as a multi-band 30m resolution image, it can be easily queried via the \nproposed ee_image component to visualise for any area of interest the patterns of extension, \nseasonality and recurrence of water to name a few. These layers precisely have been class \nmaterial during the Geomining lecture at the Architectural Association School of Architecture where participants drew a synthetic line-map of temporal river dynamics (Figure 1) almost-instantly and without geographical restraints. Taking advantage of the extensive repre-\nsentational possibilities of GH and adopting colour, angle and length as parameters, the map \nreported not only where it is possible to find water resources, but also their permanent loss \nand yearly frequencies, thus providing a wider understanding of the ephemerality of water \ncompared to a standard layer by layer visualisation. Additionally, and only thanks to the implemented pipeline, no particular download was required to compute the analysis. Avoiding \nto redundantly download entire databases by running area-specific queries in GEE is far more \nthan secondary as it prevents common issues concerning not only memory availability but \nalso computational power and computing times on the designer’s machine. \n\nMaps of City Consumption \n\nThe majority of people in the world live in cities, which currently only take up 3% of the \nEarth's surface but have transformed 70% of the planet through human activities (CIESIN \n2016). In this sense, cities around the world are interconnected and constantly exchanging \nresources, but the traditional link between places of consumption and places of extraction \nthat was once vital for a city's prosperity has been disrupted. As geographical proximity became less important for urban success, the environmental impact of this shift was overlooked, \ncontributing to the unsustainable nature of modern society, particularly due to the physical \nseparation of consumption and resource extraction. \n\nAiming to shorten the awareness gap that current planetary urbanisation has produced, the \nEarthy Indexes workshop held by the author and Erzë Dinarama at the CAADRIA conference \n2022\/23 presented a computational methodology – strongly supported by the discussed pipeline – to engage with the concept of ecological footprint at the city scale. More specifically, \nit challenged participants to crossread resource-specific demands of agricultural, pasture or \nforest land with context-specific land availability and land accessibility; finally envisioning \nup to which extension a city would consume if operating only by proximity logics (Figure \n2).  \n\nIn line with another study on Spatialized Metropolitan Ecological Footprints (Neri 2021), \nthe analysis computes pro-city land consumption values to fulfil the annual demand of a selected resource for its entire population and consequently queries exact amounts of land, filtered and ranked by its infrastructural accessibility. It exploits mainly two data layers: the \nGlobCover 20095 for a 300 m resolution global land cover map, and the Oxford’s Global \nFriction Surface 20196 layer to feed a territorial road-sensitive cumulative cost analysis via \nthe ee_cumCost component. More specifically, the latter offers a map where every pixel is \ngiven a speed to travel based on the local road infrastructure at approximately 900 m scale \nand based on a combination of national and global (OSM) data. \n\nFig. 2:  Ecological footprint studies for coffee, beef and wood, for the city of Barcelona as \npart of the Earthy indexes workshop led by Erzë Dinarama and Iacopo Neri at the \nCAADRIA 2022 – Post Carbon conference  \n\nBridging statistical data (e. g., demography and land consumption values) with geographical \ndata (e. g., land use and accessibility maps), this approach offers a fruitful pedagogical platform to engage with territorial indexes, while discussing the role of critical cartography in \nsupport of sustainability-related studies.  \n\nUrban Accessibility Maps \n\nFinally, the proposed pipeline has been adopted to study micro-scale mobility patterns. Reflecting on the aforementioned Oxford Global Friction Surface layer, the ee_cumCost component offers the possibility to alternatively use an ad-hoc friction layer to run cumulative \ncost analysis, therefore, exploiting GEE only for computing purposes and not for data collection. Again, OSM data provides a valuable medium to fulfil this goal, and can be easily accessed in GH via many workflows (i. e. Urbano7, Gismo 8) and geographically aligned with \nthe proposed pipeline via the utility reproject_UTM component. Technically, the cumulative \ncost component welcomes any sort of curve-based geometry to paint an image on the GEE \nserver with custom values and for any provided resolution, permitting the modelling of district-scale isochrone studies unlimited by the coarser standard friction layers of GEE (Figure 3).  \n\nThis was the subject of the one-week Urban Analytics remote workshop for the IAAC 2022 \nsummer school led by the author together with Eugenio Bettucchim, where the international \naudience of participants mapped for their home-towns a series of accessibility maps to various amenities and public services, collectively discussing by comparison the manifold forms \nof the x-minutes city. \n\nOther scholars have been using OSM data to create intuitive pipelines for accessibility studies \nvia network graph (Geoff 2020). Despite being widely used in mobility studies as an excellent \nmedium to represent complex relationships between the different elements of a street network \n(i. e. intersections, roads, and traffic flow), graph modelling requires extensive cleaning operations in its set up phase, which – for a crowd-sourced and in-development database such as OpenStreetMap – may disincentive non-expert users in comfortably interact with the algorithm. Trading specificity over usability, the proposed pipeline suggests a raster based approach  to  model  street-sensitive  accessibility  maps,  solving  the  incongruencies  within  the \nOSM network with a choice of pixel-resolution. \n\nConclusion and Outlook \n\nIn conclusion, the Grasshopper3D addon discussed in this text allows designers to access and \nutilise geospatial data from the Google Earth Engine platform in their design process. The \nplugin consists of five components that import and process geospatial data through the Earth \nEngine  API  Python  library  and  Hops.  The  Earth  Engine  API  is  the  official  Python  client \nlibrary for accessing GEE and is used within a tailored Flask application to fetch the required \ninformation in response to inputs from the GH components. These components allow designers to explore and use GEE data, specifically imagerial data, for various purposes and scales: \nfrom digital fabrication to digital Landscape Architecture, and integrating it with more traditional computational pipelines. \n\nAs proved through one year of teaching experience, this plugin represents a valuable tool for \ndesigners seeking to use geospatial data in their work and expands the capabilities of GH by \nlinking it to GEE's extensive data catalogue and powerful processing capabilities. \n\nFurther steps can be taken to extend the plugin with GEE’s Machine Learning algorithms \nlike the ones used for classification, clustering, regression or feature extraction, to name a \nfew. Related to a higher level of expertise, these algorithms allow to reproduce at will many of the pre-processed layers of GEE, which might be used to accomplish adhoc or higher resolution maps, similarly to the example of the district-scale isochrone studies via externally \nfetched OSM data, as well as to include design inputs in the forecast of their impacts. \n\nLiDAR-Based Digital Modeling and Comparative \nAnalysis of Urban Street Tree Form and Dimensions \n\nAbstract:  Numerous  workflows  exist  to  computationally  generate  three-dimensional  tree  models \nwhich effectively simulate their real-world counterparts. This research hybridizes several existing and \nnew approaches to tree modeling to improve growth models based on point cloud data. The digital tree \nmodels are created in Blender using an add-on called The Grove, enabling digital trees to grow according to light availability, with full control over their branching structure and form. These three-dimensional tree models are then measured and their dimensions compared to the point cloud counterparts, \nwith average values indicating that the digitally-grown trees are relatively similar to the point cloud \ntrees though smaller in most dimensions. The results demonstrate that point-cloud models of trees can \nbe used to improve computational tree growth models leading to improved tree visualization. \n\nIntroduction \n\nOur objective is to create a hybrid workflow to model context-specific digital tree models \nbased on LiDAR data of existing trees. We also aim to visualize the tree models by placing \nthem within a rendered virtual environment. Our hypothesis is that point clouds of street trees \ncan be used to improve computational tree growth models, leading to improved visualizations \nof trees. Visual realism is a critical part of this research, since capturing the materiality, light, \nand atmosphere of the landscape can resonate with audiences. Our motivation for this project \nis to improve upon our recent research in computational tree modeling which generated three-dimensional tree models based on scientific data without sacrificing visual realism, yet which lacked  any  methods  for  comparing  the  modeled  trees  with  real-world  tree  information (ACKERMAN et al. 2022).  \n\nLandscape architects model sites of significant physical size, places which are naturally dynamic and which can take decades to fully develop according to the designer’s vision. Thus, \nit is logical that most 3D modeling software is not currently suited to simulating landscape \ncharacteristics. This territory is often taken up through adapting software intended for modeling trees and terrain for games and film, tools such as SpeedTree, a standard industry tool \nfor creating realistic trees and plants, and Terragen, software used to create large-scale photorealistic natural environments. In recent years, many landscape architects have begun using \nLumion, a real-time renderer which integrates with most 3D modeling software and has a \nlarge and accessible library of trees and other vegetation. \n\nConstantly improving imaging technology has allowed us to digitally model landscapes for \nseveral decades (LEWIS 2012). Yet distinct challenges exist in modeling natural elements of \nthe landscape. In contrast to the well-defined geometry of architectural and industrial objects, \nthe organic variation of natural elements and the multi-part composition of vegetation presents unique difficulties. For example, trees are comprised of a trunk, branches, twigs, and \nleaves, each following a similar pattern but with unique formation. Where a single model of \nan architectural element such as a window can be used repeatedly in a scene without creating visual dissonance, a single model of a tree cannot be multiplied without looking artificial. In order to overcome this artificiality, digital tree simulations must integrate architectural and self-organizing  components  of  tree  growth  to  create  scientifically-informed  tree  models \n(PALUBICKI et al. 2009). \n\nSeveral models have been created to computationally generate unique three-dimensional tree \nmodels. The Algorithmic Beauty of Plants describes the application of L-systems to generate \na variety of tree branching structures which build upon earlier work of others’ to computationally simulate branching patterns (PRUSINKIEWICZ et al. 1996). Weber and Penn developed \ndigital tree models which focused more on visual realism than strict adherence to botanical \nprinciples. Their model integrates branching, pruning, leaf orientation, stem bending from \nwind, and vertical attraction, and realistic materials (WEBER & PENN 1995). De Reffye et al. \nused characteristics of tree mortality and regeneration to generate digital models of tree species growth over time, a data-informed method to create vegetation that strictly adheres to botanical  structure  and  development.  While  effective,  this  method  requires  knowledge  of botany as well as their specific computational methods (DE REFFYE et al. 1988).  \n\nMethods for creating scientifically accurate, realistically-rendered digital trees has improved \nsignificantly in recent years; such as digitally modeling an endangered eucalyptus woodland \n(CHANDLER et al. 2021), and digitally modeling and visualizing fifty years of growth in a \nWisconsin forest (HUANG et al. 2021). This use of field specimens for material texturing is \nan emerging tree visualization practice that help capture the realism of the landscape being \ndepicted (DEMETRESCUE et al. 2020). Yet many of these techniques have been limited by the \nhigh  level  of  programming  skill  required  to  apply  these  techniques  in  a  meaningful  way, \nrunning the risk of stalling the use of landscape visualization in scientific communication \n(BELL 2001). \n\nHigh levels of  graphic realism in landscape  visualization  are valuable in part due to  their \nability to reliably replicate similar levels of human cognition that occur when viewing a physical landscape site (SHI 2020). Yet realism in digital landscape visualization is often challenging given the number of trees and the amount of detail required in tree geometry (BAO et al. 2011, COLDITZ et al. 2005). These digital tree models are burdened with simulating \ninteraction and competition between trees, responding to sun and shade, compete for light \nwith their neighbors, lose branches over time, and simulate a range of other tree growth factors which have long been understood as critical components of the development of a tree’s \nphysical form (BELLA 1971). However, it stands to reason that with a greater number of interrelated  factors  being  used  to  model  a  tree,  the  more  closely  we  must  understand  the \ncause\/effect relationship of the parameters being used. This understanding is especially critical when tree growth models also include context such as nearby architecture and neighboring trees. \n\nMethods  \n\nPoint Cloud Capture and Conversion \n\nA comprehensive model of urban street trees was developed using vehicular ground-based \nLiDAR to capture several sets of urban street trees in Zhengzhou, China. The capture was \nperformed by driving down several urban street corridors with LiDAR equipment on the roof \nof the vehicle. The goal of this LiDAR capture effort was to create several point cloud models \nshowing variation in tree maturity. Three different sets of tree maturities were captured: 5-6 \nyears, 8-13 years, and 13-18 years. We were able to capture different maturity levels because \nmultiple  tree  planting  efforts  had  occurred  in  the  area  over  several  years,  producing \nheterogeneous groupings of trees spanning over a decade of growth. All trees were planted \nin identical linear plans along urban streets with high traffic volumes. All trees were of the \nsame species, a large deciduous tree with the botanical name Platanus x acerifolia, commonly \nknown as the London Plane Tree. This species was ideal for our research as it is one of the \nmost common street trees throughout China and North America, making the project broadly \napplicable to several urban contexts (LI et al 2011, LU et al 2010).  \n\nOnce captured, the point cloud model was processed in Lidar360 software in order to classify \nthe  point  cloud  into  tree  canopy  and  ground  elements.  The  canopy  point  cloud  was  then \nseparated into individual tree point clouds for analysis. This separation was done in Lidar360 \nusing an algorithm based on the shortest-path metabolic ecological theory, which uses the \nfact that vascular plants optimize the shortest possible distance from leaf to root to determine \nevery  point’s  belonging  to  a  particular  trunk  (TAO  et  al.  2015).  Each  point  cloud  was \nclassified and colorized before exporting to an LAS dataset (Fig. 1).  \n\nThe LAS dataset then needed to be converted to physical geometry for visualization. This \nwas  done  by  using  the  three-dimensional  modeling  software  Rhino  and  its  parametric \nmodeling tool, Grasshopper. In Grasshopper, we used the Volvox component to load the LAS \ndataset, create a voxel for every point, and export to a mesh. We then used the Weaverbird \ncomponent to subdivide and smooth the mesh. The resulting geometry provided a reasonably \nlegible approximation of a three-dimensional tree model which delineated each tree’s trunk, \nbranching structure, and canopy fullness (Fig. 2). \n\nTree Growth Modeling \n\nWe selected the intermediate-aged group of 8-13 years for use in tree growth modeling, as it \nincluded enough branching and canopy detail for comparison, while also being small enough \nto minimize the computational load in iterative development of a growth model. The 8-13-\nyear  model  was  exported  from  Rhino  as  an  FBX  file  and  imported  into  Blender,  another \nthree-dimensional modeling program. Blender allows intuitive yet sophisticated tree growth \nsimulation  through  an  add-on  called The  Grove,  which  provides  an  interface  for  growing \ngroups of trees with precise control of growth characteristics, species, and age. The Grove \nalso enables tree growth in response to light and shade, competing with neighboring trees for \nlight and responding to shade from nearby objects. \n\nTo begin the growing process, we selected the Platanus x acerifolia from the preset list of \navailable species in The Grove. These presets, which included many common trees, provided \na starting point with settings for the unique growth characteristics for the selected species. \nWe then placed a seedling tree at spacing intervals identical to the point cloud trees and grew \nthe seedling trees to 10 years of age. Through a trial-and error process of visual comparison \nbetween the 8-13-year point cloud mesh and the 10-year Grove model, we adjusted specific \ngrowth characteristics and re-grew the seedling trees again to 10 years of age. With each step \nwe noted the adjustment and captured an image of the tree group (Tab. 1). \n\nThrough iterative adjustment and re-growth, we gradually improved the growth model. After \nnine iterations, the tree growth model bore enough resemblance to the LiDAR model with \nregard to branching, structure, and canopy that we were satisfied with the settings. A visual \ncomparison of the final growth model and the LiDAR model demonstrates the results (Fig. 3). \n\nResults and Discussion \n\nWe hypothesized that point cloud models of trees could be used to improve computational \ntree growth models. Referring back to Table 1, the change in form and structure from the first \niteration to the ninth is striking. It is possible that this process could be seen as a form of \nmodel validation against physical counterparts, forming the basis for accepting the landscape \ncognition  that  occurs  when  viewing  images  that  include  these  digital  tree  models.  This \npresents implications for use of digital tree models in performance calculations. Lidar360 has \nthe ability to generate a report showing dimensions for each tree within the point cloud model. \nThese values can be used in climate-based calculations such as the U.S. Forest Service Tree \nCarbon Calculator (USDA 2022). This and other tools allow calculation of carbon dioxide \nsequestration  and  aboveground  biomass,  critical  factors  in  mitigating  climate  change.  We \nperformed a comparative study of the dimensions of trees from Lidar360 and dimensions of \ntrees generated in this iterative study using the Grove. Trunk diameter of trees from the Grove \nwere taken by importing the tree meshes into Rhino and creating a clipping plane at 1.37 \nmeters from the ground, the approximate level at which DBH (diameter at breast height) is \ntaken. The diameter of each trunk was measured (Fig. 4). \n\nTo calculate tree canopy volume, we developed a crown envelope which essentially wrapped \nthe tree canopy in an enclosed volume. This builds on recent work in transforming urban tree \npoint clouds into three-dimensional crown models (GUO et al. 2021, MÜNZINGER et al. 2022). \nBecause we imported leafless trees into Rhino to reduce computing load, we added a small \nbuffer at the ends of branches to account for the larger volume when leaves are present (Fig. \n5). \n\nOverall, the Grove tree models average height was slightly higher than Lidar360 – an average \nof 10.173 meters vs. 9.910 meters. However, in every other measurement the Grove trees \naverage  values  were  somewhat  lower  than  those  in  Lidar360.  The  differences  are  small \nenough that we feel that we are close to a digital growth model that can produce trees for use \nin analysis and simulation. Our aim is to be as close as possible to approximating real-world \nvalues, and we will continue to iteratively adjust the Grove trees and analyze their dimensions \nuntil they more closely matched Lidar360. Narrowing the gap between digitally-grown trees \nand real-world counterparts will be critical if we wish to use the dimensions of trees grown \nwith The Grove and Blender to perform similar carbon calculations. This would provide a \nvaluable tool for responsibly determining future performance of urban street trees. \n\nConclusion and Outlook \n\nFuture directions of this work will aim to expand the workflow to include additional common \ntree  species  using  a  similar  workflow  of  point  cloud-based  iterative  growth  modeling.  As \npreviously mentioned, we also aim to quantify the digital models created with The Grove and \nBlender in order to perform predictive calculations for carbon sequestration in urban settings. \nCombining these calculations with visualizations of mature urban street trees may well be a \npowerful  advocacy  tool  for  promoting  increased  tree  planting  in  urban  areas  while \nresponsibly depicting the likely visual character of such plantings.  \n\nDespite these successes, there are several components of this research that are problematic. \nA primary issue is the number of software programs and skillsets required to move through \nthe phases of these workflows. Unless we are able to streamline the workflow significantly, \nthis  level  of  landscape  modeling  and  visualization  will,  as  Bell  (2001)  stated,  remain  an \nunderutilized tool in scientific communication. \n\nAn additional problem is the significant computational power needed to run the tree growth \nsimulations and Unity visualizations. Even with the high-end computer used to generate the \nmodels and visualizations, 10-20 minutes was needed to generate a group of 20 trees grown \nto 35 years of maturity – approaching an apparent upper limit on the amount of trees that can \nbe  modeled.  These  trees  also  experienced  some  lag  time  in  Unity  that  resulted  in  a  less \nresponsive experience using the software. This is a long-standing issue with rendering 3D \nvegetation using current high-end computing power, mentioned by Weber & Penn (1995), \nColditz et al. (2005), Bao et al. (2011), and others. This suggests that issues with realism and \nlimitations of computing power will persist and should be anticipated as a regular part of this \nwork. \n\nThis  research  has  overcome  some  of  the  primary  challenges  that  have  long  existed  in \nmodeling the natural landscape, primarily the difficulty in capturing the organic variation of \nnatural  elements.  It  has  added  new  knowledge  of  modeling  a  tree’s  physical  form  and  its \nbehavior among other trees, building upon decades of work to devise L-systems, botanically-based models, visual models, and others. Ongoing development of this research can further improve  the  ways  that  we  model,  analyze,  and  apply  computationally-grown  trees  to contemporary challenges. \n\nNeural Radiance Fields for Landscape Architecture \n\nAbstract: In this paper, we examine potential applications of Neural Radiance Fields (NeRF) in the \nfield of landscape architecture. NeRF is a state-of-the-art method for novel view synthesis and volumetric scene reconstruction based on real-world training data. Our paper addresses NeRF and its derived models with a focus on the use and application of Instant-NGP, a method developed by researchers from the technology company NVIDIA. We discuss experimental applications of NeRF based on the case study of the post-disaster landscape of Ahr Valley, Germany, affected by a 100-year flood in \n2021. In particular, we are interested in the benefits of NeRF in comparison to other landscape modeling \nmethods, such as Structure-from-Motion (SfM) or Multi-View-Stereo (MVS), which use similar data \nas input.  \n\nThis study shows that the application of NeRF technology can be a promising alternative for capturing \nand visualizing landscape scenes. The study focuses especially on tasks and situations where the larger \nspatial context – the landscape – is of interest and importance. The technological aspects of how NeRF \nmodels work are relevant, but our main focus is on their potential implications for the field of landscape \narchitecture. Technical development and research in the scientific field of computer vision are accelerating rapidly. As users, rather than developers, of digital tools, we believe that NeRF technology re-\nquires professional validation through real-world landscape projects. \n\nIntroduction \n\nUpon reviewing a corresponding article on computer science, we developed a more comprehensive understanding of what can be generated with a Neural Radiance Field (NeRF) and, \nconsequently, the relevance and potentials of this technology and method in future developments of spatial design in general and more specifically, in landscape architecture. The NeRF \napproach comes from the computer science field of self-learning systems – as “neural” refers \nto “self-learning” – and we recognize the task of introducing this method to digital landscape \narchitecture as an urgency, which our contribution is centrally dedicated to. In this context, \nwe set out to generate on-site images – starting with the university campus and ending with \na significant flooded area after a disaster – that we could use for our related experiments with \nNeRF technology.  \n\nNeRF – Neural Radiance Fields \n\nNeural Radiance Fields (NeRF) is a method for novel view synthesis and volumetric scene \nreconstruction based on real-world training data. NeRF was introduced by (MILDENHALL et \nal. 2020) and, since then, has gained traction in computer vision and related fields (GAO et \nal. 2022, TEWARI et al. 2022). “In its basic form, a NeRF model represents three-dimensional \nscenes as a radiance field approximated by a neural network. The radiance field describes \ncolor and volume density for every point and for every viewing direction in the scene” (GAO \net al. 2022). The original approach by Mildenhall et al. “represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x; y; z) and viewing direction (\n)) and whose output is the volume density and view-dependent emitted radiance at that spatial location” (MILDENHALL et al. \n2020). The NeRF model uses a set of two-dimensional RGB images, and their camera poses \nto create synthetic three-dimensional scenes. These scenes can be rendered into new images \nor video animations of photo-realistic quality (GAO et al. 2022). NeRF models can also be \nexported as simple mesh models. Emerging from the field of computer vision, the primary \nfocus of the NeRF method is to produce visual representations of a scene, surface, or object. \nUnlike other methods and sensors in remote sensing and environmental modeling, NeRF does \nnot originate from a surveying or measurement context. NeRF uses internal coordinate sys-\ntems instead of geographic reference systems, which were not a priority in its development. \nIn the field of landscape architecture, the fact that the NeRF model is not connected to a real-world coordinate system that potentially links data to a ground-truth reference might be un-\nusual at first. The rooting in scanning and surveying that led to the rise of lidar point cloud \nmodels  in  the  field  is  being  replaced  with  a  kind  of  ground  truth  of  images.  The  article \n“Ground truth to fake geographies: machine vision and learning in visual practices” by GIL-FOURNIER & PARIKKA (2021) is of importance to this discussion and, particularly where the \nauthors argue that “ground truth has shifted from a reference to the physical, geographical \nground to the surface of the images.” \n\nA NeRF model is not limited to generating a radiation field but can also be used to generate \npoint-based radiation fields (XU et al. 2022) or voxel-based models (YU et al. 2021). Since \nits publication in 2020, the paper ‘NeRF: Representing Scenes as Neural Radiance Fields for \nView  Synthesis’  by  MILDENHALL et  al.  (2020)  has  inspired  many  researchers  to  advance, \nadjust, and refine their methods (GAO et al. 2022, TEWARI et al. 2022). Especially the processing speed metric has been a major threshold in making the method available to a wider \narray of users, as it is directly connected to the complexity of scenes and the hardware necessary for their generation. Comparing the element of speed between the newer and older \nmodels, we can observe that the former outperforms the latter by several orders of magnitude. \nWhile processing a specific scene takes twelve hours in 2020 (MILDENHALL et al. 2020), the \nsame scene takes only about five seconds in the middle of 2022 (MÜLLER et al. 2022).  \n\nGAO et al. (2022) provide an overview of existing literature grouped based on their focus on \napplications such as three-dimensional reconstruction, image processing, or urban applications. Especially interesting is a model for large-scale scene reconstruction (TANCIK et al. \n2022) that lets us envision potentially global scale models. Significant improvements have \nbeen made to NeRF models, creating a wide range of applications, including “urban mapping \n\/ modelling \/ photogrammetry, image editing \/ labelling, image processing, and 3D reconstruction and view synthesis of human avatars and urban environments” (GAO et al. 2022). \nRecent advances in NeRF model performance have also made this technology more accessible to professionals in related fields outside of computer vision. More specifically, professionals from fields involved in digital visualization and aesthetics, such as landscape architecture, will be encouraged to test and develop their own models in relation to their specific \ntasks and topics. \n\nNeRF Aesthetics \n\nCustomary techniques for reconstructing three-dimensional landscape scenes, such as point \nclouds or vectors derived from photogrammetry, result in models whose aesthetics are detached from their physical context – the surrounding landscape. Where the lidar scanner rays \nend for a point cloud model, a black hole opens as the model background. Such models represent their own digital aesthetics and stand in stark contrast to the realism of photography \nand film. From an aesthetic point of view, there was always a significant difference between \nthe navigable three-dimensional model and the modeled real scene. Lidar scans, or photo-\ngrammetric scene reconstructions, seem to be functionally limited by their rootedness in tech-\nnical correctness and dimensional accuracy. It is difficult to implement diverse, complex, and \nassociative topical links in these models. In contrast, NeRF technology translates the ability \nof photography or film to capture the full context of a scene, including the background, into \na complex three-dimensional model with an identifiable background (Fig. 1 and 2). A NeRF \nmodel generates a detailed reconstruction of variables, which are key in registering a scene. \nIn addition to position and color, other variables transferred to the model include light intensity, darkness, and transparency. The ability to incorporate these elements presents an unprecedented three-dimensional realism (Fig. 3). Point cloud models, with high point densities and an even distribution of points, appear sparse up close and denser from a distance, where \nthis  higher  density  does  not  correspond  to  increased  information.  Combining  point  cloud \nmodels with different resolutions or resampled data can thus be useful for creating large models. Christophe Girot describes such combined models through the term he coined as cloud-ism (GIROT 2020). As we have established the possibilities of the NeRF method, we propose to counteract the newfangled term cloudism, again with a classic term – realism. Landscape \nin the form of a model is still most accurately understood – by laypersons and experts alike \n– when it corresponds to the common appearance of the surrounding landscape. \n\nInstant-NGP \n\nFor the generation of our experimental NeRF outcomes, we used Instant-NGP (Instant Neural \nGraphics Primitives), an open-source software framework developed by NVIDIA. The software framework processes Neural Graphics Primitives that, in addition to NeRF, can also be used for Gigapixel images, neural Signed Distance Functions (SDF), and Neural Radiance Caching (NRC). Our paper is limited in scope to NeRF models. Instant-NGP is proving to be \none of the most popular and regularly updated NeRF generation solutions. It trains a NeRF \nin seconds using multi-resolution hash encoding. The coordinates are hashed and used as an \nindex into a stack of multi-resolution data arrays, drastically reducing the number of parameters per model. The NeRF model is constrained by a unit cube bounding box set at a coordinate space of [0,1]³. The model has the highest resolution around a central point positioned \nat the center of the unit cube, at [0.5, 0.5, 0.5]. \n\nCase Study Ahr River Valley \n\nFor our initial experimentation with the application of NeRF technology, we focused on the \ncase of the Ahr Valley in Germany in the aftermath of the 2021 flood disaster. We obtained \nthe related fieldwork data through camera tours and UAV flights on-site. In the summer of \n2021, between July 12th and July 15th, the Ahr River Valley experienced a 100-year flood as \na result of pronounced heavy regional rainfall events in connection with a low-pressure system. In addition, the soils in the affected regions of Rhineland-Palatinate and South West-phalia could hardly absorb any additional water (GERMAN WEATHER SERVICE 2021). After the flood,  which took the lives of  many people and caused extreme destruction, the  hasty \nreconstruction activity did not necessarily lead to sustainable design and building. \n\n“The moment after a natural disaster is a window of time that can be used to adapt-to-climate \n(change), but this opportunity is in many cases demonstrably wasted. […] After a disaster, \namnesia leads people to forget about what primarily should be designed and built” (REKITTKE \n& NINSALAM 2022). Without a thorough analysis of a disaster, economically and ecologically \nsensible decisions become unlikely. There is a danger in conducting post-disaster analysis \nand the subsequent planning and design relying solely on documents like maps or legal texts, \nwhich operate on a high level of abstraction. It is imperative to incorporate what the people \nthemselves have seen (REKITTKE & NINSALAM 2022). We are interested in NeRF technology \nfor this particular reason, as it opens up new possibilities for visual realism and the ability to \nintegrate different temporal layers into a single landscape model. Disasters reveal snapshots \nof many aspects that should have been taken into account during planning phases and are \novershadowed once the developments are carried through a short time later. In this pursuit, \nwe  aspire  to  preserve  the  memories  of  a  flood  disaster  by  creating  appropriate  landscape models. Like in an autopsy, the aim is to fill the common gap between reality, recollection, and  forward  planning  with  evidence  that  is  supposed  to  trigger  cogitation (REKITTKE  & NINSALAM 2022). \n\nData Collection and Processing \n\nFor testing NeRF models in the context of post-flood Ahr River landscapes, we created an \nextensive dataset consisting of 106 video and image samples using UAV-mounted cameras \n(Fig. 4) and ground-based handheld smartphone devices. Our data were collected in two separate sessions. The first was during the flood event in July 2021– sporadic and ad hoc. The \nsecond was in September 2022, in the course of systematic fieldwork. Our aim was to create \ncases using one of the most common NeRF methods available. All NeRF models were trained \nlocally using Instant-NGP. The GitHub repository (GITHUB \/ instant-ngp 2022) provides documentation  on  software  and  hardware  requirements,  installation,  pre-processing,  training, \nand rendering NeRF, as well as exporting. Another document on GitHub provides additional \nadvice on the process (GITHUB \/ nerf_dataset_tips 2022). We created a separate NeRF model \nfor each set of input data, following a list of six sequential processing steps: 1) data acquisition, 2) data pre-processing and frame extraction, 3) pose estimation, 4) NeRF model training, \n5) video export, and 6) post-processing. \n\n1) Data acquisition for each site was carried out using lightweight, field study-ready collection devices: a DJI Mini drone and an iPhone 11 Pro. For all data acquisition, we used the \nhighest  possible  resolution  of  the  devices.  The  drone  videos  were  shot  at  2.7K  resolution \n(2720x1530), 23,97 fps, in MPEG-4 format. With the smartphone camera, we shot videos at \nFull HD resolution (1920x1080), 59,94 fps, in MPEG-4 format, and pictures at 12MP resolution (4032x3024), in JPEG format. Shots in the  wide-angle camera  mode (0.5x, 13 mm \nequivalent focal length, 120° field of view) were particularly effective. In total, we collected \n96 videos and 10 image sequences, a raw data package of 35 Gigabytes. \n\n2) For the video shots from the field, we extracted a set of sequential frames ranging from 50 \nto over 400 images. The image sequences were filtered to the same amount. NeRF models \nbased on the method of Müller et al. (2022) do not infinitely increase resolution or quality \nwith a more extensive set of input images. Mildenhall et al. (2020) and Müller et al. (2022) \nuse tens to hundreds of images to train NeRF models. We followed the recommendations \npresented on the GitHub forum (GITHUB \/ nerf_dataset_tips 2022). Furthermore, we tested \nthe application of digital image enhancement techniques such as sharpening, noise reduction, \nand super-resolution to improve matching image detection. \n\n3) We used the COLMAP pipeline that was part of the Instant-NGP codebase to process an \nestimate of the camera poses for each image set. The resulting JSON file containing the camera parameters for each image was saved in a folder along with the original images in the \nformat TRANSFORMS.JSON. \n\n4) We trained our NeRF models using the interactive GUI (Graphical User Interface) that \nwas included in the codebase. The GUI offers a variety of different tools for training, visualization, and export, as well as allowing the user to interactively move through a scene while \nthe  model  is  being  rendered  in  real  time.  Training  begins  by  launching  the  GUI  from  an \nAnaconda prompt, and within seconds, the model evolves from blurry noise to a clear representation of a scene. Once the training reaches a satisfactory level, the training progress can \nbe saved as a JSON file – called snapshot – which can be used to reload the NeRF model or \nto create an animation. The GUI facilitates interactive creation and saves a camera path along \na set of key frames that can be used to export a video animation. \n\n5) The codebase allows exporting of flythrough video animations of the NeRF model using \nthe previously generated training progress and camera path. The export is handled outside \nthe GUI in an Anaconda code prompt using Python bindings. We exported a range of video \nanimations up to 4K resolution at 30 fps. \n\n6) The exported videos can be easily edited using common video and image editing tools. \nSince both the input and output of the NeRF model are image-based, digital editing and processing  pipelines  such  as  image  sharpening,  noise  removal,  or  frame  interpolation  can  be \napplied before and after NeRF modeling. The user has full control over both pre- and post-NeRF model media, as would be the case with photography, photogrammetry, or map-making. \n\nNeRF for Landscape Architecture \n\nAlthough we have been working with NeRF technology for a limited time and therefore did \nnot  yet utilize its full potential,  we can already identify and highlight some of its specific \nstrengths.  We  offer  a  selection  of  tested  applications  for  NeRF  technology,  generally  for landscape architecture and, more specifically, in the context of our case study. In addition to \nthe enormous technological advances that have determined the rise of NeRF technologies in \nrecent years, the method has to be tested in relation to issues concerning landscape architecture. \n\nMulti-Resolution Models \n\nFor NeRF models, it applies that their resolution is not developed in relation to the model but \nto the depth of information captured from the input images. “The multiresolution aspect of \nthe hash encoding covers the full range from a coarse resolution \nmin that is guaranteed to \nbe collision-free to the finest resolution \nmax that the task requires. Thereby, it guarantees \nthat all scales at which meaningful learning could take place are included, regardless of sparsity” (MÜLLER et al. 2022). In the context of the Ahr Valley after the flood, we are able to \ncreate a lightweight but complex three-dimensional model that enables capturing environ-\nments at multiple scales: from the rocks in the Ahr riverbed to the flowing water, from the \nriverbanks and adjacent vegetation to patterns of the urban fabric, and from the mountains in \nthe background to the clouds in the sky above the valley. The main benefit we see in our \ncase-based NeRF models is that they feature a high spatial depth, capturing the sky, clouds, \nand even distant landscape features such as mountains, valleys, and urban areas. In the case \nof the Ahr Valley, the NeRF model consolidates all flood-relevant factors to be discussed \nsimultaneously in one model: the change in sediment flows in the river, the altered course of \nthe river, the destruction of urban settlements and agriculture in the Ahr valley near the floodplain,  the  topography  of  the  valley  where  water  has  accumulated  downstream.  More  advanced NeRF models can present the rich contextual depth of the mapping in the form of a \nnavigable three-dimensional model. Xiangli et al. (2022) outline the nature of such prospective models by expanding the notion of rendering scenes at multiple resolutions by “modeling \ndifferent scenes at multiple scales with drastically varying views on multiple data sources” \n(XIANGLI et al. 2022). \n\nObject Focus versus Open World Scene? \n\nBased on our current research and studies, we observe a certain level of contradiction regarding the  great landscape potential of NeRF  models and their technical  nature. NeRF is designed to feature a central point in the model, from which the resolution gradually decreases towards the edges of the bounding cube. This raises the question of whether NeRF models \nare inherently object-oriented (single object) and how this might impact the modeling of non-point-centric open-world scenes, such as landscapes. Is our positive assessment of the NeRF \nlandscape model accurate, or will the triumph of the NeRF models primarily extend to object \nmodels, for example, in the context of architectural projects? Instant-NGP NeRF models are \nconstrained by a maximum resolution bounding box at its center. But in our case study, we \nfed this “machine” exclusively with data from landscape photography and landscape videos, \nthereby obtaining effective landscape models. We suggest that future explorations are “to be \ncontinued.” \n\nComparison to Photogrammetry and Point Cloud Models \n\nThere are partial overlaps in data collection and processing methods between NeRF models \nand photogrammetric processing. Both methods use two-dimensional raster images as input \ndata and share further similarities in the initial processing of this input data. A wide range of \nNeRF methods, including the one of Müller et al. (2022), use COLMAP, a package for SfM, \nto extract camera poses. Models derived from lidar scanning or photogrammetric modeling \nstill offer higher geometric accuracy than NeRF models (LEHTOLA et al. 2022). For our own \ncomparison of the different methods, we created a set of 406 frames from a selected drone \nflight, which we used as input for corresponding NeRF and photogrammetric models. The \nNeRF model was generated with Instant-NGP, and the photogrammetry model was processed \nas a dense point cloud in Agisoft Metashape. The point cloud was exported as a file in LAS \nformat with 12 million points and a file size of 326 MB. \n\nThe output quality of a photogrammetry process is assessed based on the accuracy of where \nthe resulting points are positioned with respect to a ground truth reality. Passive sensing data, \nsuch as intensity return data or true color imagery captured by other sensors, may only suport subsequent analysis or enhance visualization. In many ways, the NeRF model sits somewhere between the perception of space and the perception of textures, materials, light, and color. There are various metrics can be used to assess the quality of a NeRF model. Many \nfundamental advantages of photogrammetry, such as a relation to a “real world” Coordinate \nReference System (CRS) and transformations of the model with respect to this CRS, are not \nyet realized in the NeRF system we used. Nonetheless, this does not preclude the possibility \nof implementing them in future applications. \n\nA NeRF differs from all the traditional three-dimensional scene formats commonly used in \nthe field, some of which include vectors or meshes, grids, and point clouds. Each format can \nhave distinct ways of formulating a representation, which can yield a unique set of advantages \nand disadvantages depending on the format used. It can therefore be difficult to judge a NeRF \nin relation to the qualities of these formats. Some of these metrics may be embedded in the \nprocess or acquisition technology used for data generation to determine the potential for accurately representing a scene from the start. Different metrics apply to data obtained from \nphotogrammetry or lidar scanning. Such comparisons exist in various areas that are tangentially linked to landscape architecture, for example, in heritage preservation. Data obtained \nthrough lidar scanning is among the most accurate geometric data available in modern scanning techniques but lacks the ability to accurately capture the texture and diagnostic color \ninformation  (DOSTAL  &  YAMAFUNE  2018).  Perhaps  the  most  fascinating  outcome  of  the \nmethod is not the NeRF model itself but the images and videos that are generated from the \nmodel (Fig. 5 and 6). MILDENHALL et al. (2020) state that the results of view synthesis are \nbest viewed as videos. \n\nMaterial Noise \n\nMetrics such as reflection or change in transparency and color as a function of position are \nnot considered part of the physical enterprise of the object or scene in the current literature \non photogrammetrically derived models. Rather, they are viewed as  factors that distort or \ncorrupt the signal in ways that may need to be eliminated in order to derive a high-quality \nmodel.  In  scenes  produced  by  photogrammetry,  removing  water-surface  reflection  effects \npresents a challenge (PARTAMA et al. 2018). Materials featuring difficult optical properties –including but not limited to absorptivity, reflectivity, scattering, challenging texture and complex shape or geometry – still pose challenges in photogrammetry (NICOLAE et al. 2014). The \ndistinction between signal and noise is pronounced in the literature on photogrammetry and, \nmore generally, in remote sensing and earth observation. The NeRF model, on the other hand, \nallows data otherwise defined as noise to be used to visualize unstable materialities, surfaces, \nand objects. In our case study, for example, we had the means to illustrate the unstable nature \nof the Ahr River – with its changing water levels up to extreme flooding conditions. \n\nMulti-Source NeRF \n\nA multi-source model is based on input data generated through multiple acquisitions for the \nsame or a similar area. This method can be used in situations where only a sparse set of input \ndata is available to increase the resolution of the NeRF model by adding additional input data \nfor angles or features not previously captured. For our case study, we trained a NeRF model \nof the Kalvarienberg Monastery and the surrounding area in the town of Ahrweiler with sev-\neral of our drone acquisitions and eventually improved the model’s resolution. In addition, \nour model captures the changing light conditions between diffused and direct sunlight caused \nby different cloud conditions during the recording period. The various parameters – geometry, color, lighting, texture, and translucency – captured by a NeRF model may be acquired \nindependently. Each parameter can be derived from a separate set of input data. The final \nNeRF synthesis model allows navigating between these parameters in relation to the position \nand direction of a particular view. In working with a 3D landscape model, this synthesis is a \nnovelty that opens up considerable potential. \n\nMulti-Temporal NeRF \n\nIt sounds almost unattainable within the limitations and resources of our current time, but a \nsimultaneous coupling of movement through time, and movement through space, is fundamentally possible with NeRF technology. This option is yet to be defined and therefore, we \npropose to use “Multi-temporal NeRF models” when referring to the visualization of changing layers of time in the course of changing positions. Multi-temporal NeRF models use mul-\ntiple sets of images captured at different times and utilize them as the input to produce novel \nviews that interpolate between the images. The model synthesizes the input data and allows \nit to move through time while moving through space. A Multi-temporal NeRF model makes \nit possible to capture movements, visualize ongoing processes, and depict all kinds of patterns \nof change. For example, the growth or the changing state of the health of vegetation can be \ndocumented in this way. The intensity of the changes captured by the model can be related \nto the temporal extent of the capture and the intensity of the change in the underlying object \nor  study  surface.  Multi-temporality  is  a  common  concept  and  method  in  geosciences,  in \nwhich remote sensing  observations collected at different times are combined into a single \nmulti-temporal image or model. Multi-temporal analyses enable the detection and visualiza-\ntion of changes in  spatial patterns over  time. The concept  has  taken root in areas  such  as \narchitecture and landscape architecture to understand changes in the built environment. The \nlandscape architect, in particular, can think of numerous possible uses. The depiction of seasonal changes in the city, landscape, and vegetation are only a few of them. We find this \nmethod to be the most effective to this date for purposes of representing the “before” and \n“after” conditions of a site. \n\nIn 2021, it was already demonstrated that NeRF models could be trained with unstructured \ncollections of photographs taken at different times, from different angles, and under different \nlighting conditions. The model registers the static geometries of the scene but interpolates \nbetween color and illumination in dependence on the view position (MARTIN-BRUALLA et al. \n2021). It is possible for a NeRF to process a sequential set of images of the same scene at \ndifferent  times  of  the  day,  times  of  the  year,  and  so  on.  The  associated  different lighting \nconditions of these different images, which show a time difference, allow the generation of \nan outstanding level of multi-temporality in a single NeRF result. The resulting model allows \nthe user to literally move through time as they move through space – made possible by adjusting the different radiation fields between the time-shifted images. \n\nIn our case study, the Multi-temporal NeRF shifts the digital model from a state of representation to a state of simulation of the underlying flood event. Our NeRF model captures and \ninterpolates  situations  found  in  two  self-contained  trajectories.  The  model  combines  two \ndrone flights – one in 2019 and the other in 2022 – over the Ahr Valley municipality of Rech, \nGermany. The acquisition from 2019 shows a historic bridge over the Ahr River, connecting \nthe two halves of the village. The second acquisition captures the same location in the autumn \nof 2022, after the flood event destroyed parts of the bridge, swept away several buildings \nsouth of the bridge, and visibly changed the course of the river (Fig. 7). Both drone flights \nhave different trajectories and viewpoints, allowing us to create a model relating one set of \nviews to the 2019 acquisition and the other set of views to the 2022 acquisition. In the resulting NeRF model, we can navigate through the internal digital coordinate system and observe \nthe changing states in quasi-real-time. \n\nFlooding as a Multi-Temporal NeRF Application \n\nDue to the presence of temporal components – such as large amounts of water that flows in \nand out of a particular site – flood zones are generally considered to be suitable for the application of multi-temporal NeRF. Instant-NGP’s interactive GUI also provides a set of visual \ndebugging tools that can be used to uncover the internal structure of input and output neurons. \nThe parameters of these tools are recorded as part of a keyframe animation. For our case \nstudy dealing with speculative scenarios of a flood-ravaged valley, we used the partial acti-\nvation of neurons as a rising green light field to simulate the rising waters of the flood through \nvisuals (Fig. 8). Through this experimentation, we found a simple yet very powerful tool for \ndigital flood simulation with full visibility of all model elements. Supporting this hypothesis \n(LI et al. 2022) have shown that NeRF models can be used to simulate ultra-complex climate \nevents. \n\nConclusion \n\nThis paper presents a baseline study on  various approaches and  methods of  working  with \nNeRF models in landscape architecture. The aim is to inspire and encourage researchers in \nrelated fields to develop in-depth studies on the applications of NeRF. Our interest in NeRF \nis fundamentally rooted in the idea that landscape is anything but static, which is, at the same \ntime, where we find significant potential in this approach. NeRF models can be useful for \n“wandering” through changing light conditions or addressing moving objects, such as water, \nclouds,  birds,  cars,  trains,  and  others.  Different  materials  can  be  evaluated  through  direct \ncomparison.  Reflections  and  light,  as  well  as  structure,  can  be  included  in  their  changing \nappearance. Changed terrain, for example, differing terrain heights in the course of a construction project, can be evaluated. In addition, the same scenes and objects could be recorded \nunder different lighting conditions in order to enable a critical evaluation. For example, early \nin the morning, at noon, in the evening, or in cloudy weather. The NeRF interpolates between \nthe input images, allowing for seamless switching between different states within the result-\ning model. It is an important task to think of a landscape model not as a static set of coordi-\nnates, i. e. point clouds, raster, or vector data, but as a set of parameters that are constantly \nchanging. As in a landscape as such, the model changes depending on the viewer’s position \n– an unstable model. The fact that NeRF crosses the border between modeling and simulation \nsuits the instability and openness of the landscape subject. We are dealing with a technology \nthat is still very new and largely untested but whose potential seems enormous. Most com-\nputer graphics algorithms and techniques developed over more than half a century assume \nmeshes or point clouds as three-dimensional scene representations for rendering and editing. \nNeural rendering, on the other hand, is such a recent field that the term was first used in 2018. \nFor this reason, there is an inevitable gap between the available methods that can work with \nclassic three-dimensional representations and those that can be applied to neural representa-\ntions (TEWARI et al. 2022). This is definitely true for the field of landscape architecture, and \nwe look  forward to the developments and publications that  will qualify NeRF  models for \nlandscape architecture in the years to come. \n\nIn 2019, Christophe Girot described how the point cloud model overcomes the separation \nbetween model, architectural drawing, renderings, or other visualizations. “In the “cloudist” \napproach, there exists no separation between a model, a section and a plan: they all stem from \nthe same cloud of design information. Separate renderings or visualizations become quite \nunnecessary, since the views generated are directly derived from the model, with their own \nsingular  aesthetic”  (GIROT  2019).  NeRF  models  remove  the  threshold  between  different \nforms of representation (Fig. 9). The NeRF method offers qualities similar to point clouds \nbut significantly reduces the separation or the visual contrast between the model and reality.\n\nThe Influence of Perceived Landscape Qualities on \nEconomic Vitality: A Case Study of a Retail Coffee \nChain \n\nAbstract: As a crucial aspect of vitality, the economic facets of vitality at the store level have yet to be \ninvestigated in greater detail, and its relationship with micro-level perceived landscape qualities in the \npublic realm requires further examination. The recent advancements in big data and Machine Learning \n(ML) have presented an exceptional opportunity to empirically investigate vitality and its association\nwith the urban built environment. This research aims to comprehensively gather various dimensions of\neconomic vitality for retail coffee chain, using Starbucks stores in Hong Kong as a case study. The\nstudy incorporates the previously under-researched dimension of customer sentiment, which is interpreted through the Natural Language Processing (NLP) model. Additionally, the study collects both\nsubjectively measured landscape perceptions and objectively extracted visual features from street view\nimagery (SVI) using ML algorithms and crowdsourced surveys. Results indicate that micro-level perceived landscape qualities, such as scale and signage, have a greater impact on economic vitality than\nconventional macro-level planning characteristics. The findings of this research have the potential to\ninform and support a successful and economically dynamic retail model at the neighbourhood scale,\nfurther emphasizing the economic significance of human-scale landscape design in the public realm.\n\nIntroduction \n\nUrban vitality has long been considered a critical aspect of successful cities in place making, \ncontributing to resilience, creativity, and innovation for sustainable development (CHEN et \nal. 2022, MONTGOMERY 1998). First introduced in JACOBS (1961)’ seminal book, and initially defined as the presence of active street life, the concept of vitality has evolved into a \nmulti-faceted connotation that encompasses various dimensions, with economic vitality being regarded as a critical component (HUANG et al. 2020). \n\nQuantifying vitality remains a challenge due to its complex nature, encompassing both social \nand economic aspects. Previous studies have used macro-scale indicators, such as the number \nof entertainment facilities within a city, to measure vitality. However, with the rise of big \ndata,  new  opportunities  have  emerged  to  quantify  vitality.  Despite  this,  researchers  have \npointed out that some big data sources, such as cell phone records, have relatively low data \nquality. Therefore, researchers have shifted to using the intensity of geo-tagged catering businesses from POIs to measure economic aspects of vitality (XIA et al. 2020, YE et al. 2018). \nAlternatively, LONG & HUANG (2019) compared economic vitality across hundreds of cities \nin China using crawled numbers of reviews from popular social media websites that collects \nratings for restaurants. More recently, researchers have proposed more complex frameworks \nthat utilize information available from online service evaluation platforms, such as incorpo-\nrating service quality and scale in addition to popularity (LI et al. 2022). In conclusion, the \nspatial organization of small food establishments plays a significant role in reflecting human \nactivity patterns. Utilizing customer reviews to gather information about economic vitality \nhas also proven to be a valuable approach. However, these methods fail to consider the emotions  and  sentiments  of  user  groups,  which  play  a  crucial  role  in  the  human-environment \ninteraction and contribute to the economic and social aspects of individual businesses at a \nmicro-level (LIU et al. 2020). Additionally, calculating an overall vitality index using certain \nweighting methods for each dimension may be inexorably subject to bias, as different businesses may provide different types of services and target distinct demographic groups with \nvarying economic statuses. \n\nIn view of these factors, exploring the economic vitality of chained catering services, specifically  coffee  retail,  can  provide  an  innovative  approach  for  comparison  across  different \nstores. Coffee retail is often considered a crucial type of \"third place,\" where people gather \nfor socializing without an obligation to stay. It creates a sense of place, a key aspect of promoting  vitality  (OLDENBURG  1989).  The  success  of  coffee  retail  is  influenced  by  various \nfactors such as the environment, service quality, context, and food and beverage offerings. \nHowever, literature in the hospitality industry suggests that chained coffee shops often use \nstandardization and intra-regional diversification strategies based on ‘portfolio theory’ to reduce costs, providing a tactical advantage and greater survival rates over single-location franchises (PARK & JANG 2022). As a result, the differences in services and food offerings between stores within the same chained business can be largely ignored when comparing their \neconomic vitality, offering a solution to the limitations of previous methods. \n\nOn the other hand, previous research has shown that the design of the built environment can \naffect vitality. For instance, building morphology, density, typology, and land use mix have \nall been linked to vitality (HUANG et al. 2020, LONG & HUANG 2019, XIA et al. 2020, YE et \nal. 2018). However, these studies focused merely on objective environment factors at a macro \nand planning scale, but  neglected the nuances of daily life experience and the micro-level \nperceived landscape qualities which can be critical in promoting vitality. \n\nPerceived landscape qualities can be measured objectively, subjectively, or through a combination of the two measures. Advancements in Computer Vision (CV) technology have allowed for more efficient and high-throughput methods like using emerging urban data such \nas SVI to measure perceived qualities (DUBEY et al. 2016, ITO & BILJECKI 2021, ZHANG et \nal. 2018). In a nutshell, the perceived landscape qualities can be largely categorized into subjectively measured design perceptions and objectively measured visual elements (QIU et al. \n2022). These human-centric perceived qualities, which can proxy how people perceive the \nenvironment when walking down the street, have been used to examine the impact of microlevel  perceived  landscape  qualities  on  walking  behaviour  or  housing  prices  (BASU  & SEVTSUK 2022, SONG et al. 2023, SONG et al. 2022).  \n\nThough several recent studies have sought to reveal the correlation between perceived land-\nscape qualities and street vitality (CHEN et al. 2022, JIANG et al. 2022), they mainly focused \non pedestrian volume as a representation of vitality and only  studied a limited number  of \nperceived landscape qualities. Further research is needed to investigate how perceived environment qualities contribute to the economic vitality of coffee retail at a micro-scale. Additionally, due to differences in measurement methods, subjectively measured perceptions have \nbeen shown to exhibit different spatial heterogeneity patterns in comparison with their objective counterparts (SONG et al. 2022). Thus, their separate impacts on economic  vitality \nwarrant further understanding. \n\nIn conclusion, our research endeavours to address the existing knowledge gaps by integrating \nthe human-environment interaction into the measurement of economic vitality across chained \ncoffee stores, and examining how economic vitality is influenced by quantifiable micro-level \nperceived landscape qualities measured through both subjective and objective methods. Our \nresearch offers new insights in the following aspects: \n1) It sheds light on the multiple dimensions of economic vitality of chained coffee shops at \nthe store level and incorporates customer sentiment using advanced Natural Language \nProcessing (NLP) techniques. \n\n2)  The study measures both subjectively measured perceptions and objectively measured visual elements from SVI with ML tools.  \n\n3)  The relationship is disclosed between the perceived landscape qualities within the walking  radius  around  each  store  and  the  various  dimensions  of  economic  vitality,  by  the \ncomparison of the perceived landscape qualities with macro-level factors in relation to \neconomic vitality. \n\nData and Methods \n\nStudy Area and Data Source of Economic Vitality \n\nThe study area is Hong Kong, a high-density city that is one of the world's largest financial \ncentres with over 7 million residents. To control for factors that could impact the economic \nvitality of different services, chained coffee stores were selected for our investigation using \nbig data. By choosing stores from a single brand, the research can mitigate the influence of \nfood quality, service, and interior design and focus instead on other key factors such as the \nquality of the outdoor street environment and macro-level spatial qualities such as accessi-\nbility to transportation and points of interest (POI). This approach offers a straightforward \nresearch  design,  which  provides  a  clearer  understanding  of  the  economic  vitality  of  these \nstores compared to previous studies that have utilised a more broad-brush approach, observing citywide vitality in a coarser grid. Specifically, Starbucks coffee is chosen, a global market leader with over 150 stores in our study area. The analytical framework of this study can \nbe seen in Fig. 1. \n\nThe search results were finalised using the Google Map API, which returned information for \n158 Starbucks stores (Fig. 2) located throughout Hong Kong after initial data cleansing. It is \nworth mentioning although data was also obtained from the local restaurant evaluation platform 'Open Rice', the number of reviews for Starbucks Coffee on this platform was insufficient, so this data was not included in the study. Information gathered from the web crawl for each store comprised its geographic coordinates, address, and, most importantly, information on reviews, including the number of reviews, the number of review images, overall review score, review score distribution, and detailed review text (the 20 most recent reviews after January 2017).  \n\nNLP  is  a  field  of  Artificial  Intelligence  and  computational  linguistics  concerned  with  the \ninteractions between computers and human (natural) languages. It enables the interpretation \nof the human language in a meaningful way, for instance, to understand the emotions. Senti-\nment scores were calculated for each store based on its review texts using the state-of-the-art \nBidirectional Encoder Representations from Transformers (BERT) model, an NLP technique \nthat  uses  a  self-attention  mechanism  and  eliminates  biases  from  left-to-right  momentum \nwhich was used in previous models. The use of BERT has been increasing in recent studies \n(ALAPARTHI & MISHRA 2021). The  model  was pre-trained on a review dataset containing \n150k reviews and reported to achieve an exact prediction accuracy of 67% and an off-by-1 \nscore prediction accuracy of 95% for reviews in English. The sentiment score for each store \nwas calculated as the average of the sentiment scores interpreted from its crawled reviews, \nwith a score between 1 and 5, where 3 represents a neutral sentiment, 5 represents the most \npositive sentiment, and 1 represents the most negative sentiment. \n\nMeasuring Micro-level Perceived Landscape Qualities  \n\nThe street network for this  study  was obtained from  OpenStreetMap and points along the \nHong Kong road network were sampled every 50m using the QGIS platform (ZHANG et al. \n2018). To examine the correlation between perceived landscape qualities and economic vitality, points were selected within the 250m walking radius around each of the chained coffee \nstores located on the ground floor. Points located on highways were excluded as they do not \nreflect the pedestrian experience. The coordinates of points were fed into the Google Street \nView Static API and street view images (SVI) were obtained (heading = 0, field of view = \n90, image size = 800 x 400 pixels). After discarding grey or indoor images, 2,110 SVIs were \nleft and used for further analysis. \n\nTo extract objectively measured visual features from SVIs, the widely used ML algorithm \nPSPNet pre-trained on the ADE20K cityscape dataset was adopted. Around 20 streetscape \nelements, such as sky, sidewalk, trees, buildings, etc., were successfully extracted from each \nSVI. The view index of each visual feature (i. e., the percentage of an element  within the \nentire image) was calculated and the average value of each element within the walking radius \nwas used as the perceived objective feature quality for each store. A randomly sampled SVI \nand its semantic segmentation result using the PSPNet algorithm are shown in Fig. 3. \n\nMeanwhile,  in  accordance  with  previous  studies,  we  aimed  to  quantify  eight  subjectively \nmeasured design perceptions: typology, order, ecology, enclosure, aesthetics, accessibility, \nrichness and scale (EWING et al. 2006, SONG et al. 2022; TIAN et al. 2021). To achieve this, \nwe utilised perceived quality scores obtained from 300 SVIs, with 80% used for training and \n20% for testing, gathered from crowdsourcing surveys in a previous study (TIAN et al. 2021). \nThese scores served as the dependent variables, while the view indices of key street elements \nserved as the independent variables for the prediction task. \n\nEight ML algorithms were used and compared for prediction performance: K-Nearest Neighbours (KNN), Support Vector Machines (SVM), Random Forest (RF), Decision Tree (DT), \nGaussian Process (GP), Voting Selection (VS), ADA Boost (ADAB) and Bagging Regression  (BR).  The  ML  models  were  evaluated  by  using  R-squared  (R2)  and  Mean  Absolute \nError (MAE). The best-performing model for each subjectively measured landscape quality \nwas then selected to predict the scores for the entire SVI dataset in Hong Kong. \n\nMacro-level Conventional Planning Qualities and Correlation Analysis \n\nIn  addition  to  the  micro-level  perceived  landscape  qualities,  macro-level  planning  factors \nwere also computed to evaluate their impact on vitality. A 1.5 km buffer was created around \nthe centre of each store location, and the relevant  variables  were obtained from the Hong \nKong Geodata Store (https:\/\/geodata.gov.hk\/gs\/) and processed in QGIS. The variables included ‘number of POIs’, ‘number of hotels’, ‘number of Airbnb’, ‘number of elderly facilities’, ‘number of bus stops’, ‘distance to metro station’, and 'size of park area', which have \nbeen reported to contribute to street vitality. For example, accessibility to transportation facilities such as the distance to metro stations and the  number of bus stops can impact the \npotential crowds around the station. The accumulation of destinations (number of POIs) can \nattract people and promote vitality. Meanwhile, park size represents the neighbourhood-scale \nenvironmental quality, which contributes to subjective well-being and often attracts people. \nAdditionally, Airbnb can attract tourists and is essential to vitality. \n\nThe  study  conducted  further  statistical  analysis  to  determine  the  correlations  between  the \ndifferent dimensions of economic vitality of ground-floor stores and various groups of built \nenvironment qualities. Pearson correlation analysis was applied to provide a comprehensive \ncomparison between macro and micro-level factors and their relationship with economic vitality.  \n\nResults \n\nComparison of Economic Vitality \n\nThe 158 Starbucks Coffee outlets in Hong Kong are dispersed throughout various regions of \nthe city. Among these outlets, about 13 stores are located in the ‘Central’, which is its most \nconcentrated area, 91 stores of them on the ground floor and 67 stores on other floors ranging \nfrom -1st to 9th level. Because the intent of this research is to assess the economic vitality \nand its relationship with the surrounding context and integrates the Sentiment analysis based \non reviews, those review numbers were excluded if they were less than 30 times, and 127 \nstores were left. And because the stores located on the ground level have more direct interactions with the built environment, the stores on other levels were further removed, and 79 \nstores located on the ground level left. The detailed statistics are demonstrated in Table 1. \n\nAlthough only ground-floor stores were utilized for further analysis, a preliminary comparison was performed with stores located on other floors to identify potential biases. And the \nstatistics revealed similar results. In summary, ground-floor stores make up approximately \n60% of the total stores analysed. The average rating for these stores is slightly higher compared to those on other floors. The image count suggests that customers are more likely to \ntake and post photos in stores located on the ground floor, which may be due to the surrounding built environment. With regards to sentiment scores, it can be concluded that stores received overall positive sentiment scores, as a neutral emotion is rated as 3.0 on the scale used. \n\nML Model Performances \n\nMultiple  ML  algorithms  were  used  to  determine  the  most  effective  models  for  predicting \nsubjectively measured perceptions. As shown in Table 2, while four out of eight variables \n(Typology, Order, Aesthetics and Richness) had low  R-squared (R2) values and  were ex-\ncluded from further analysis, the qualities of Access, Ecology, Enclosure, and Scale achieved \nR2  values  ranging  from  0.40  to  0.53.  These  prediction  accuracies  are  deemed  acceptable \ngiven the size of the training sample, and they partially outperformed results from previous \nstudies  (DUBEY  et  al.  2016,  ITO & BILJECKI  2021,  SONG et  al.  2022).  Therefore,  the  four \nselected perceived landscape qualities were predicted for all SVIs using the best-performing \nmodels (i. e., Gaussian Process, Voting Selection, and Bagging Regression). After determining the qualities of each street view, we linked them to the corresponding Starbucks store \nlocations  and  obtained  the  mean  value  for  each  store  as  the  neighbourhood's  subjectively \nmeasured perceptions. \n\nCorrelation Analysis and Discussion \n\nThe Pearson Correlation analysis was conducted to investigate the correlation between the \nfour dimensions of economic vitality and selected macro-level spatial attributes (Fig. 4). The \nresults indicate that many macro-level factors had a moderate to weak positive correlation \nwith the review count. For instance, besides the most prominent impact of Airbnb (0.3), bus \nstops, POIs, and hotels also showed a similar positive association with the review count (0.26 \nto 0.27). The image count showed a weak positive relationship with bus stops (0.21). However, the correlations between the overall score and sentiment score and macro-level planning \nfactors were negligible. Despite this, POIs (-0.16) and hotels (-0.16) had the highest strengths \nin correlation coefficients with the sentiment score, suggesting that they may have a poten-\ntially negative impact on visitors' emotions, which in turn may negatively affect the economic \nvitality of the stores. \n\nWe conducted a separate analysis for the micro-level perceived landscape qualities (Fig. 5). \nOn the one hand, out of the four subjective perceptions, the quality of Scale, showed a mod-\nerate positive correlation with the review count (0.35), while its correlation with the image \ncount was weaker (0.23). Additionally, the review count demonstrated a weak positive correlation  with  Enclosure  (0.27)  and  a  weak  negative  correlation  with  Ecology  (-0.27). The \noverall review score showed a positive correlation with Ecology (0.2). Similar to the macrolevel spatial qualities, the correlations between sentiment and subjective landscape qualities \nwere statistically negligible. However, Ecology reported the highest positive impact (0.15) \non sentiment. \n\nOn  the  other  hand,  the  correlations  between  the  objectively  measured  visual  features  and \neconomic vitality dimensions were similar in strength to the subjectively measured percep-\ntions. The review count demonstrated a strong positive correlation with signboard (0.42). \n\nAdditionally, the review count showed comparable weak positive correlations with several \nother visual elements, including sky (-0.24), tree (-0.25), road (-0.26), streetlight (-0.26), and \nrailing (-0.26). It also reported a weak positive correlation with building (0.23). The image \ncount exhibited a moderate positive association with signboards (0.3) and weak negative re-\nlationships with the sky (-0.21) and road (-0.22). The sentiment demonstrated a positive cor-\nrelation with trees, showing its consistency of correlation with the Ecology perception (0.15), \nthough the correlation strength is statistically insignificant. \n\nOur study provides a brand-new insight into the hitherto poorly understood relationship between store-level economic vitality and built environment factors. Most importantly, it was \nobserved that compared to macro-level variables, micro-level perceived landscape qualities \nexhibit  stronger  correlations  with  economic  vitality  dimensions.  Additionally,  objectively \nperceived visual features were found to complement subjectively measured perceptions. For \nexample,  signboard  was  found  to  have  the  highest  impact  among  all  perceived  landscape \nqualities, offering meaningful design suggestions for enhancing future economic vitality. It \nproves that the wayfinding system is essential in the urban built environment. Conversely, \nsky, railing, and a few other visual elements had a negative impact, whereas building was \nfound to have a positive impact, potentially reflecting a different interpretation of enclosure \nquality, which supports walkability for pedestrians on the streets. This result is inconsistent \nwith the results of previous research that found sky and sidewalk to be positively related to \nvitality  using  SVI  (LI  et  al.  2022),  which  could  be  caused  by  differences  in  geographical \ncontext. It was also found that review count was negatively correlated with Ecology, which \nmight stem from a lack of available space in the public realm for street planting. This result \ncontradicts prior research, which suggests that street greenery has a positive impact on walking behaviours in Hong Kong (LU 2019) and that vegetation enhances street vitality (JIANG \net al. 2022). However, it did provide similar result which aligns with the findings of Y. LI et \nal. (2022), who found the greenery seem to show a degree of negative correlation when focusing  on  a  commercial  complex  site  in  Japan.  This  may  suggest  that  poorly  positioned \ngreenery may obstruct key commercial areas, potentially negatively affecting the economic \nvitality of stores. Future research should aim to conduct a more comprehensive analysis of \nthe relationship between street greenery and store-level economic vitality in Hong Kong. \n\nThe quality of Scale, Enclosure, and Access was all found to be positively related to review \nintensity, highlighting the importance of human-scale design quality in enhancing pedestrians’ experience and street vitality and supporting the benefits of appropriate street planning \nand design. Lastly, sentiment score was found to have a positive relationship with both greenery and ecology perception, albeit with relatively low strengths. This is in line with previous \nresearch that greenery can provide psychological benefits. Nevertheless, no statistically significant correlations  were established with other variables, suggesting the need for further \nresearch. The sentiments reported in reviews can be complex and might yield mixed results \ncompared to review intensity. \n\nRegarding macro-level factors, access to transportation was found to have a positive impact \non the dimensions of economic vibrancy, which is in line with previous research that suggests \npublic transportation facilities improve accessibility for non-local communities (HUANG et \nal. 2020). Additionally, the presence of visitor-oriented urban functions, such as hotels and \nAirbnb, were positively associated with economic vibrancy at the store level. The number of \nPoints of Interest (POIs) was found to have a relatively high impact among the macro-level \nvariables, which is consistent with earlier findings that land use density and functional mix \naffect economic vitality (LONG & HUANG 2019, XIA et al. 2020). However, our study sur-\nprisingly did not find significant correlation between park size and economic vitality, probably because park accessibility does not necessarily equate to increased economic activity in \nretail  or  commercial  stores.  Although  increasing  pedestrian  volume  can  increase  vitality, \nother driving forces, like perceived landscape qualities or urban micro-level amenities, are \nnecessary for attracting people to stay and linger so as to promote urban economic vitality. \n\nConclusion \n\nThis study offers several important contributions to the existing literature. Firstly, it provides \na unique perspective on quantifying  the dimensions of economic  vitality at the individual \nstore level, using a case study of chained coffee shops in Hong Kong. The study employs \ndata mining and NLP techniques to measure users’ sentiment scores from reviews, offering \na novel approach to this area of research. Secondly, the dimensions of economic vitality are \nstatistically evaluated in relation to both macro-level planning variables and micro-level perceived  landscape  qualities,  including  subjectively  measured  perceptions  and  objectively \nmeasured visual elements based on SVI dataset. This can help derivation of quantifiable design strategies and implementable guidelines to enhance the economic vitality of neighbour-\nhoods. Our preliminary data analysis suggests that compared to macro-level characteristics, \nsubjectively measured perceptions such as Scale, and objective visual elements, such as signboards, can have a significant impact on economic vitality. The objective visual elements in \nthe  streetscape  can  complement  the  subjectively  measured  perceptions  and  vice  versa. \nThirdly, although no statistically strong associations were found between sentiment scores \nand built environment factors, the study suggests that visual greenery and Ecology perception \ncould  play  a  positive  role  in  affecting  sentiment  scores,  which  is  beneficial  in  promoting \neconomic vitality. Lastly, this research adds to our knowledge of future recommendations for \nretail store location selection, and provides actionable insights for landscape architects on the \ndesign of streetscapes, with the goal of creating economically vibrant cities through meaningful placemaking. \n\nNevertheless, this research has several limitations. Firstly, the dataset could be enhanced by \nincluding comparison with other brands, such as Pacific Coffee, which holds a similar market \nshare  to  Starbucks  in  terms  of  coffee  outlets  in  Hong  Kong.  This  would  help  reduce  any \nbiases in the conclusion. Secondly, recent studies have shown that people's walking behaviour has a non-linear relationship with the built environment, and therefore, the vitality of an \narea may be influenced in a similar way. In future research, multiple walking radii could be \nemployed to gain deeper insights into this phenomenon. Thirdly, Thirdly, the accuracy  of \npredictions of subjectively measured landscape perceptions could be improved by either increasing the size of the training set or by employing more advanced machine learning algorithms such as Convolutional Neural Networks. Lastly, it would be beneficial to further explore other perceived landscape qualities or psychological perceptions, such as safety, in future studies. \n

Introduction \n\nThe cover of the eighth issue of the Journal of Digital Landscape Architecture JoDLA 8-2023 \nshows imagery, captured using a drone, of the city of Bad Neuenahr-Ahrweiler in Germany, \na region which was severely impacted by a flood disaster in 2021. Showing damage caused \nby severe floods or extreme fires is one of the many ways digital landscape architecture can \ncontribute to Hazard Management, one of the issues of resilient landscape architecture. However, our profession would much prefer to use landscape modelling to prevent some of these \ndisasters. This issue of JoDLA presents the current capacity our profession has for doing so. \n\nAfter being listed in Scopus, the journal is now also listed in DOAJ (Directory of Open Access Journals). Wichmann publisher has made accessible the JoDLA, and its forerunner publication Digital Landscape Architecture, as open access papers since 2013 and therefore provides ten years documentation of research in the area of Digital Landscape Architecture. \n\nThe  DLA  2023  is  organized  by  the  next  generation  of  professors  at  Anhalt  University  in \nDessau, Germany, where the Digital Landscape Architecture – Con was founded.  \n\nProf.  Dr.  Matthias  Pietsch,  the  local  chair  of  the  2023  DLA,  suggested,  after  the  COVID \npandemic, to focus on the issues of our endangered landscape by calling the main theme for \nthis year’s conference and of the JoDLA 2023 Future Resilient Landscapes. \n\nIn addition to the main theme, we have provided a number of other possible areas for submitting papers on current research or outstanding practice in digital landscape architecture. \nWe received 98 extended abstracts and can now present the result of a rigid two-phase double-blind review process. \n\nThe eighth issue of the Journal of Digital Landscape Architecture 8-2023 covers 66 contributions on the following seven current areas of research and prototype applications in digital \nlandscape architecture: \n•  Resilient Landscape, Global Change and Hazard Response  \n•  Landscape and Building Information Modeling (LIM + BIM) and other Standardizations in Digital Landscape Architecture  \n•  Algorithmic Design and Analysis Landscapes  \n•  Geodesign Approaches, Technologies, and Case Studies  \n•  UAV Imagery and Remote Sensing and Digital Fabrication in Landscape Architecture  \n•  Visualization, Animation and Mixed Reality Landscapes (VR, AR)  \n•  Teaching and Hybridization in Digital Landscape Architecture \n\nWe are very pleased that the worldwide academic community continues to grow in spite of \ncontinuing crises. The accepted abstracts come from seventeen countries:  \n\ntwenty-eight entries from the United States, fourteen from Germany, ten from China, five \nentries each from Turkey, four from Canada, three each from Hungary, Italy and Switzerland, \nand two each from Australia, Finland, Indonesia, Netherlands, Norway, and Spain. \n\nAnd from the following countries one entry each was accepted: Brazil, Georgia, and Singapore. We are very happy to have authors from countries that have contributed in the past, as \nwell as those who contributed for the first time. \n\n\nWe hope you will appreciate the eighth edition. The printed copies will be sent out on request \nto all participants before the conference at the end of May 2023. \nYou  will find all the contributions online as open access publications at the  gis.Point and \ngis.Open platforms of Wichmann http:\/\/gispoint.de\/jodla.html. \n\nWe would also like to invite you to the next DLA conference. The 25th international conference on information technology in landscape architecture, Digital  Landscape  Architecture \nDLA 2024 with the main theme “New Trajectories in Computational Urban Landscapes and \nEcology”, will be held from June 5 to 7, 2024 at the Vienna University of Technology, Austria. \n\nThe Journal of Digital Landscape Architecture invites you to submit ideas for special issues \nand topics. Please follow our continuously updated announcements and call for papers and \nposters at www.dla-conference.com. Here you will also find the complete online documentation of the DLA beginning from the year 2013. For earlier contributions of DLA publications, you may ask our JoDLA office. \n\nErich Buhmann, Stephen Ervin, Pia Fricker, Sigrid Hehl-Lange, James Palmer, \nand Matthias Pietsch \n

LiDAR-Based Digital Modeling and Comparative \nAnalysis of Urban Street Tree Form and Dimensions \n\nAbstract:  Numerous  workflows  exist  to  computationally  generate  three-dimensional  tree  models \nwhich effectively simulate their real-world counterparts. This research hybridizes several existing and \nnew approaches to tree modeling to improve growth models based on point cloud data. The digital tree \nmodels are created in Blender using an add-on called The Grove, enabling digital trees to grow according to light availability, with full control over their branching structure and form. These three-dimensional tree models are then measured and their dimensions compared to the point cloud counterparts, \nwith average values indicating that the digitally-grown trees are relatively similar to the point cloud \ntrees though smaller in most dimensions. The results demonstrate that point-cloud models of trees can \nbe used to improve computational tree growth models leading to improved tree visualization. \n\nIntroduction \n\nOur objective is to create a hybrid workflow to model context-specific digital tree models \nbased on LiDAR data of existing trees. We also aim to visualize the tree models by placing \nthem within a rendered virtual environment. Our hypothesis is that point clouds of street trees \ncan be used to improve computational tree growth models, leading to improved visualizations \nof trees. Visual realism is a critical part of this research, since capturing the materiality, light, \nand atmosphere of the landscape can resonate with audiences. Our motivation for this project \nis to improve upon our recent research in computational tree modeling which generated three-dimensional tree models based on scientific data without sacrificing visual realism, yet which lacked  any  methods  for  comparing  the  modeled  trees  with  real-world  tree  information (ACKERMAN et al. 2022).  \n\nLandscape architects model sites of significant physical size, places which are naturally dynamic and which can take decades to fully develop according to the designer’s vision. Thus, \nit is logical that most 3D modeling software is not currently suited to simulating landscape \ncharacteristics. This territory is often taken up through adapting software intended for modeling trees and terrain for games and film, tools such as SpeedTree, a standard industry tool \nfor creating realistic trees and plants, and Terragen, software used to create large-scale photorealistic natural environments. In recent years, many landscape architects have begun using \nLumion, a real-time renderer which integrates with most 3D modeling software and has a \nlarge and accessible library of trees and other vegetation. \n\nConstantly improving imaging technology has allowed us to digitally model landscapes for \nseveral decades (LEWIS 2012). Yet distinct challenges exist in modeling natural elements of \nthe landscape. In contrast to the well-defined geometry of architectural and industrial objects, \nthe organic variation of natural elements and the multi-part composition of vegetation presents unique difficulties. For example, trees are comprised of a trunk, branches, twigs, and \nleaves, each following a similar pattern but with unique formation. Where a single model of \nan architectural element such as a window can be used repeatedly in a scene without creating visual dissonance, a single model of a tree cannot be multiplied without looking artificial. In order to overcome this artificiality, digital tree simulations must integrate architectural and self-organizing  components  of  tree  growth  to  create  scientifically-informed  tree  models \n(PALUBICKI et al. 2009). \n\nSeveral models have been created to computationally generate unique three-dimensional tree \nmodels. The Algorithmic Beauty of Plants describes the application of L-systems to generate \na variety of tree branching structures which build upon earlier work of others’ to computationally simulate branching patterns (PRUSINKIEWICZ et al. 1996). Weber and Penn developed \ndigital tree models which focused more on visual realism than strict adherence to botanical \nprinciples. Their model integrates branching, pruning, leaf orientation, stem bending from \nwind, and vertical attraction, and realistic materials (WEBER & PENN 1995). De Reffye et al. \nused characteristics of tree mortality and regeneration to generate digital models of tree species growth over time, a data-informed method to create vegetation that strictly adheres to botanical  structure  and  development.  While  effective,  this  method  requires  knowledge  of botany as well as their specific computational methods (DE REFFYE et al. 1988).  \n\nMethods for creating scientifically accurate, realistically-rendered digital trees has improved \nsignificantly in recent years; such as digitally modeling an endangered eucalyptus woodland \n(CHANDLER et al. 2021), and digitally modeling and visualizing fifty years of growth in a \nWisconsin forest (HUANG et al. 2021). This use of field specimens for material texturing is \nan emerging tree visualization practice that help capture the realism of the landscape being \ndepicted (DEMETRESCUE et al. 2020). Yet many of these techniques have been limited by the \nhigh  level  of  programming  skill  required  to  apply  these  techniques  in  a  meaningful  way, \nrunning the risk of stalling the use of landscape visualization in scientific communication \n(BELL 2001). \n\nHigh levels of  graphic realism in landscape  visualization  are valuable in part due to  their \nability to reliably replicate similar levels of human cognition that occur when viewing a physical landscape site (SHI 2020). Yet realism in digital landscape visualization is often challenging given the number of trees and the amount of detail required in tree geometry (BAO et al. 2011, COLDITZ et al. 2005). These digital tree models are burdened with simulating \ninteraction and competition between trees, responding to sun and shade, compete for light \nwith their neighbors, lose branches over time, and simulate a range of other tree growth factors which have long been understood as critical components of the development of a tree’s \nphysical form (BELLA 1971). However, it stands to reason that with a greater number of interrelated  factors  being  used  to  model  a  tree,  the  more  closely  we  must  understand  the \ncause\/effect relationship of the parameters being used. This understanding is especially critical when tree growth models also include context such as nearby architecture and neighboring trees. \n\nMethods  \n\nPoint Cloud Capture and Conversion \n\nA comprehensive model of urban street trees was developed using vehicular ground-based \nLiDAR to capture several sets of urban street trees in Zhengzhou, China. The capture was \nperformed by driving down several urban street corridors with LiDAR equipment on the roof \nof the vehicle. The goal of this LiDAR capture effort was to create several point cloud models \nshowing variation in tree maturity. Three different sets of tree maturities were captured: 5-6 \nyears, 8-13 years, and 13-18 years. We were able to capture different maturity levels because \nmultiple  tree  planting  efforts  had  occurred  in  the  area  over  several  years,  producing \nheterogeneous groupings of trees spanning over a decade of growth. All trees were planted \nin identical linear plans along urban streets with high traffic volumes. All trees were of the \nsame species, a large deciduous tree with the botanical name Platanus x acerifolia, commonly \nknown as the London Plane Tree. This species was ideal for our research as it is one of the \nmost common street trees throughout China and North America, making the project broadly \napplicable to several urban contexts (LI et al 2011, LU et al 2010).  \n\nOnce captured, the point cloud model was processed in Lidar360 software in order to classify \nthe  point  cloud  into  tree  canopy  and  ground  elements.  The  canopy  point  cloud  was  then \nseparated into individual tree point clouds for analysis. This separation was done in Lidar360 \nusing an algorithm based on the shortest-path metabolic ecological theory, which uses the \nfact that vascular plants optimize the shortest possible distance from leaf to root to determine \nevery  point’s  belonging  to  a  particular  trunk  (TAO  et  al.  2015).  Each  point  cloud  was \nclassified and colorized before exporting to an LAS dataset (Fig. 1).  \n\nThe LAS dataset then needed to be converted to physical geometry for visualization. This \nwas  done  by  using  the  three-dimensional  modeling  software  Rhino  and  its  parametric \nmodeling tool, Grasshopper. In Grasshopper, we used the Volvox component to load the LAS \ndataset, create a voxel for every point, and export to a mesh. We then used the Weaverbird \ncomponent to subdivide and smooth the mesh. The resulting geometry provided a reasonably \nlegible approximation of a three-dimensional tree model which delineated each tree’s trunk, \nbranching structure, and canopy fullness (Fig. 2). \n\nTree Growth Modeling \n\nWe selected the intermediate-aged group of 8-13 years for use in tree growth modeling, as it \nincluded enough branching and canopy detail for comparison, while also being small enough \nto minimize the computational load in iterative development of a growth model. The 8-13-\nyear  model  was  exported  from  Rhino  as  an  FBX  file  and  imported  into  Blender,  another \nthree-dimensional modeling program. Blender allows intuitive yet sophisticated tree growth \nsimulation  through  an  add-on  called The  Grove,  which  provides  an  interface  for  growing \ngroups of trees with precise control of growth characteristics, species, and age. The Grove \nalso enables tree growth in response to light and shade, competing with neighboring trees for \nlight and responding to shade from nearby objects. \n\nTo begin the growing process, we selected the Platanus x acerifolia from the preset list of \navailable species in The Grove. These presets, which included many common trees, provided \na starting point with settings for the unique growth characteristics for the selected species. \nWe then placed a seedling tree at spacing intervals identical to the point cloud trees and grew \nthe seedling trees to 10 years of age. Through a trial-and error process of visual comparison \nbetween the 8-13-year point cloud mesh and the 10-year Grove model, we adjusted specific \ngrowth characteristics and re-grew the seedling trees again to 10 years of age. With each step \nwe noted the adjustment and captured an image of the tree group (Tab. 1). \n\nThrough iterative adjustment and re-growth, we gradually improved the growth model. After \nnine iterations, the tree growth model bore enough resemblance to the LiDAR model with \nregard to branching, structure, and canopy that we were satisfied with the settings. A visual \ncomparison of the final growth model and the LiDAR model demonstrates the results (Fig. 3). \n\nResults and Discussion \n\nWe hypothesized that point cloud models of trees could be used to improve computational \ntree growth models. Referring back to Table 1, the change in form and structure from the first \niteration to the ninth is striking. It is possible that this process could be seen as a form of \nmodel validation against physical counterparts, forming the basis for accepting the landscape \ncognition  that  occurs  when  viewing  images  that  include  these  digital  tree  models.  This \npresents implications for use of digital tree models in performance calculations. Lidar360 has \nthe ability to generate a report showing dimensions for each tree within the point cloud model. \nThese values can be used in climate-based calculations such as the U.S. Forest Service Tree \nCarbon Calculator (USDA 2022). This and other tools allow calculation of carbon dioxide \nsequestration  and  aboveground  biomass,  critical  factors  in  mitigating  climate  change.  We \nperformed a comparative study of the dimensions of trees from Lidar360 and dimensions of \ntrees generated in this iterative study using the Grove. Trunk diameter of trees from the Grove \nwere taken by importing the tree meshes into Rhino and creating a clipping plane at 1.37 \nmeters from the ground, the approximate level at which DBH (diameter at breast height) is \ntaken. The diameter of each trunk was measured (Fig. 4). \n\nTo calculate tree canopy volume, we developed a crown envelope which essentially wrapped \nthe tree canopy in an enclosed volume. This builds on recent work in transforming urban tree \npoint clouds into three-dimensional crown models (GUO et al. 2021, MÜNZINGER et al. 2022). \nBecause we imported leafless trees into Rhino to reduce computing load, we added a small \nbuffer at the ends of branches to account for the larger volume when leaves are present (Fig. \n5). \n\nOverall, the Grove tree models average height was slightly higher than Lidar360 – an average \nof 10.173 meters vs. 9.910 meters. However, in every other measurement the Grove trees \naverage  values  were  somewhat  lower  than  those  in  Lidar360.  The  differences  are  small \nenough that we feel that we are close to a digital growth model that can produce trees for use \nin analysis and simulation. Our aim is to be as close as possible to approximating real-world \nvalues, and we will continue to iteratively adjust the Grove trees and analyze their dimensions \nuntil they more closely matched Lidar360. Narrowing the gap between digitally-grown trees \nand real-world counterparts will be critical if we wish to use the dimensions of trees grown \nwith The Grove and Blender to perform similar carbon calculations. This would provide a \nvaluable tool for responsibly determining future performance of urban street trees. \n\nConclusion and Outlook \n\nFuture directions of this work will aim to expand the workflow to include additional common \ntree  species  using  a  similar  workflow  of  point  cloud-based  iterative  growth  modeling.  As \npreviously mentioned, we also aim to quantify the digital models created with The Grove and \nBlender in order to perform predictive calculations for carbon sequestration in urban settings. \nCombining these calculations with visualizations of mature urban street trees may well be a \npowerful  advocacy  tool  for  promoting  increased  tree  planting  in  urban  areas  while \nresponsibly depicting the likely visual character of such plantings.  \n\nDespite these successes, there are several components of this research that are problematic. \nA primary issue is the number of software programs and skillsets required to move through \nthe phases of these workflows. Unless we are able to streamline the workflow significantly, \nthis  level  of  landscape  modeling  and  visualization  will,  as  Bell  (2001)  stated,  remain  an \nunderutilized tool in scientific communication. \n\nAn additional problem is the significant computational power needed to run the tree growth \nsimulations and Unity visualizations. Even with the high-end computer used to generate the \nmodels and visualizations, 10-20 minutes was needed to generate a group of 20 trees grown \nto 35 years of maturity – approaching an apparent upper limit on the amount of trees that can \nbe  modeled.  These  trees  also  experienced  some  lag  time  in  Unity  that  resulted  in  a  less \nresponsive experience using the software. This is a long-standing issue with rendering 3D \nvegetation using current high-end computing power, mentioned by Weber & Penn (1995), \nColditz et al. (2005), Bao et al. (2011), and others. This suggests that issues with realism and \nlimitations of computing power will persist and should be anticipated as a regular part of this \nwork. \n\nThis  research  has  overcome  some  of  the  primary  challenges  that  have  long  existed  in \nmodeling the natural landscape, primarily the difficulty in capturing the organic variation of \nnatural  elements.  It  has  added  new  knowledge  of  modeling  a  tree’s  physical  form  and  its \nbehavior among other trees, building upon decades of work to devise L-systems, botanically-based models, visual models, and others. Ongoing development of this research can further improve  the  ways  that  we  model,  analyze,  and  apply  computationally-grown  trees  to contemporary challenges. \n

Neural Radiance Fields for Landscape Architecture \n\nAbstract: In this paper, we examine potential applications of Neural Radiance Fields (NeRF) in the \nfield of landscape architecture. NeRF is a state-of-the-art method for novel view synthesis and volumetric scene reconstruction based on real-world training data. Our paper addresses NeRF and its derived models with a focus on the use and application of Instant-NGP, a method developed by researchers from the technology company NVIDIA. We discuss experimental applications of NeRF based on the case study of the post-disaster landscape of Ahr Valley, Germany, affected by a 100-year flood in \n2021. In particular, we are interested in the benefits of NeRF in comparison to other landscape modeling \nmethods, such as Structure-from-Motion (SfM) or Multi-View-Stereo (MVS), which use similar data \nas input.  \n\nThis study shows that the application of NeRF technology can be a promising alternative for capturing \nand visualizing landscape scenes. The study focuses especially on tasks and situations where the larger \nspatial context – the landscape – is of interest and importance. The technological aspects of how NeRF \nmodels work are relevant, but our main focus is on their potential implications for the field of landscape \narchitecture. Technical development and research in the scientific field of computer vision are accelerating rapidly. As users, rather than developers, of digital tools, we believe that NeRF technology re-\nquires professional validation through real-world landscape projects. \n\nIntroduction \n\nUpon reviewing a corresponding article on computer science, we developed a more comprehensive understanding of what can be generated with a Neural Radiance Field (NeRF) and, \nconsequently, the relevance and potentials of this technology and method in future developments of spatial design in general and more specifically, in landscape architecture. The NeRF \napproach comes from the computer science field of self-learning systems – as “neural” refers \nto “self-learning” – and we recognize the task of introducing this method to digital landscape \narchitecture as an urgency, which our contribution is centrally dedicated to. In this context, \nwe set out to generate on-site images – starting with the university campus and ending with \na significant flooded area after a disaster – that we could use for our related experiments with \nNeRF technology.  \n\nNeRF – Neural Radiance Fields \n\nNeural Radiance Fields (NeRF) is a method for novel view synthesis and volumetric scene \nreconstruction based on real-world training data. NeRF was introduced by (MILDENHALL et \nal. 2020) and, since then, has gained traction in computer vision and related fields (GAO et \nal. 2022, TEWARI et al. 2022). “In its basic form, a NeRF model represents three-dimensional \nscenes as a radiance field approximated by a neural network. The radiance field describes \ncolor and volume density for every point and for every viewing direction in the scene” (GAO \net al. 2022). The original approach by Mildenhall et al. “represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x; y; z) and viewing direction (\n)) and whose output is the volume density and view-dependent emitted radiance at that spatial location” (MILDENHALL et al. \n2020). The NeRF model uses a set of two-dimensional RGB images, and their camera poses \nto create synthetic three-dimensional scenes. These scenes can be rendered into new images \nor video animations of photo-realistic quality (GAO et al. 2022). NeRF models can also be \nexported as simple mesh models. Emerging from the field of computer vision, the primary \nfocus of the NeRF method is to produce visual representations of a scene, surface, or object. \nUnlike other methods and sensors in remote sensing and environmental modeling, NeRF does \nnot originate from a surveying or measurement context. NeRF uses internal coordinate sys-\ntems instead of geographic reference systems, which were not a priority in its development. \nIn the field of landscape architecture, the fact that the NeRF model is not connected to a real-world coordinate system that potentially links data to a ground-truth reference might be un-\nusual at first. The rooting in scanning and surveying that led to the rise of lidar point cloud \nmodels  in  the  field  is  being  replaced  with  a  kind  of  ground  truth  of  images.  The  article \n“Ground truth to fake geographies: machine vision and learning in visual practices” by GIL-FOURNIER & PARIKKA (2021) is of importance to this discussion and, particularly where the \nauthors argue that “ground truth has shifted from a reference to the physical, geographical \nground to the surface of the images.” \n\nA NeRF model is not limited to generating a radiation field but can also be used to generate \npoint-based radiation fields (XU et al. 2022) or voxel-based models (YU et al. 2021). Since \nits publication in 2020, the paper ‘NeRF: Representing Scenes as Neural Radiance Fields for \nView  Synthesis’  by  MILDENHALL et  al.  (2020)  has  inspired  many  researchers  to  advance, \nadjust, and refine their methods (GAO et al. 2022, TEWARI et al. 2022). Especially the processing speed metric has been a major threshold in making the method available to a wider \narray of users, as it is directly connected to the complexity of scenes and the hardware necessary for their generation. Comparing the element of speed between the newer and older \nmodels, we can observe that the former outperforms the latter by several orders of magnitude. \nWhile processing a specific scene takes twelve hours in 2020 (MILDENHALL et al. 2020), the \nsame scene takes only about five seconds in the middle of 2022 (MÜLLER et al. 2022).  \n\nGAO et al. (2022) provide an overview of existing literature grouped based on their focus on \napplications such as three-dimensional reconstruction, image processing, or urban applications. Especially interesting is a model for large-scale scene reconstruction (TANCIK et al. \n2022) that lets us envision potentially global scale models. Significant improvements have \nbeen made to NeRF models, creating a wide range of applications, including “urban mapping \n\/ modelling \/ photogrammetry, image editing \/ labelling, image processing, and 3D reconstruction and view synthesis of human avatars and urban environments” (GAO et al. 2022). \nRecent advances in NeRF model performance have also made this technology more accessible to professionals in related fields outside of computer vision. More specifically, professionals from fields involved in digital visualization and aesthetics, such as landscape architecture, will be encouraged to test and develop their own models in relation to their specific \ntasks and topics. \n\nNeRF Aesthetics \n\nCustomary techniques for reconstructing three-dimensional landscape scenes, such as point \nclouds or vectors derived from photogrammetry, result in models whose aesthetics are detached from their physical context – the surrounding landscape. Where the lidar scanner rays \nend for a point cloud model, a black hole opens as the model background. Such models represent their own digital aesthetics and stand in stark contrast to the realism of photography \nand film. From an aesthetic point of view, there was always a significant difference between \nthe navigable three-dimensional model and the modeled real scene. Lidar scans, or photo-\ngrammetric scene reconstructions, seem to be functionally limited by their rootedness in tech-\nnical correctness and dimensional accuracy. It is difficult to implement diverse, complex, and \nassociative topical links in these models. In contrast, NeRF technology translates the ability \nof photography or film to capture the full context of a scene, including the background, into \na complex three-dimensional model with an identifiable background (Fig. 1 and 2). A NeRF \nmodel generates a detailed reconstruction of variables, which are key in registering a scene. \nIn addition to position and color, other variables transferred to the model include light intensity, darkness, and transparency. The ability to incorporate these elements presents an unprecedented three-dimensional realism (Fig. 3). Point cloud models, with high point densities and an even distribution of points, appear sparse up close and denser from a distance, where \nthis  higher  density  does  not  correspond  to  increased  information.  Combining  point  cloud \nmodels with different resolutions or resampled data can thus be useful for creating large models. Christophe Girot describes such combined models through the term he coined as cloud-ism (GIROT 2020). As we have established the possibilities of the NeRF method, we propose to counteract the newfangled term cloudism, again with a classic term – realism. Landscape \nin the form of a model is still most accurately understood – by laypersons and experts alike \n– when it corresponds to the common appearance of the surrounding landscape. \n\nInstant-NGP \n\nFor the generation of our experimental NeRF outcomes, we used Instant-NGP (Instant Neural \nGraphics Primitives), an open-source software framework developed by NVIDIA. The software framework processes Neural Graphics Primitives that, in addition to NeRF, can also be used for Gigapixel images, neural Signed Distance Functions (SDF), and Neural Radiance Caching (NRC). Our paper is limited in scope to NeRF models. Instant-NGP is proving to be \none of the most popular and regularly updated NeRF generation solutions. It trains a NeRF \nin seconds using multi-resolution hash encoding. The coordinates are hashed and used as an \nindex into a stack of multi-resolution data arrays, drastically reducing the number of parameters per model. The NeRF model is constrained by a unit cube bounding box set at a coordinate space of [0,1]³. The model has the highest resolution around a central point positioned \nat the center of the unit cube, at [0.5, 0.5, 0.5]. \n\nCase Study Ahr River Valley \n\nFor our initial experimentation with the application of NeRF technology, we focused on the \ncase of the Ahr Valley in Germany in the aftermath of the 2021 flood disaster. We obtained \nthe related fieldwork data through camera tours and UAV flights on-site. In the summer of \n2021, between July 12th and July 15th, the Ahr River Valley experienced a 100-year flood as \na result of pronounced heavy regional rainfall events in connection with a low-pressure system. In addition, the soils in the affected regions of Rhineland-Palatinate and South West-phalia could hardly absorb any additional water (GERMAN WEATHER SERVICE 2021). After the flood,  which took the lives of  many people and caused extreme destruction, the  hasty \nreconstruction activity did not necessarily lead to sustainable design and building. \n\n“The moment after a natural disaster is a window of time that can be used to adapt-to-climate \n(change), but this opportunity is in many cases demonstrably wasted. […] After a disaster, \namnesia leads people to forget about what primarily should be designed and built” (REKITTKE \n& NINSALAM 2022). Without a thorough analysis of a disaster, economically and ecologically \nsensible decisions become unlikely. There is a danger in conducting post-disaster analysis \nand the subsequent planning and design relying solely on documents like maps or legal texts, \nwhich operate on a high level of abstraction. It is imperative to incorporate what the people \nthemselves have seen (REKITTKE & NINSALAM 2022). We are interested in NeRF technology \nfor this particular reason, as it opens up new possibilities for visual realism and the ability to \nintegrate different temporal layers into a single landscape model. Disasters reveal snapshots \nof many aspects that should have been taken into account during planning phases and are \novershadowed once the developments are carried through a short time later. In this pursuit, \nwe  aspire  to  preserve  the  memories  of  a  flood  disaster  by  creating  appropriate  landscape models. Like in an autopsy, the aim is to fill the common gap between reality, recollection, and  forward  planning  with  evidence  that  is  supposed  to  trigger  cogitation (REKITTKE  & NINSALAM 2022). \n\nData Collection and Processing \n\nFor testing NeRF models in the context of post-flood Ahr River landscapes, we created an \nextensive dataset consisting of 106 video and image samples using UAV-mounted cameras \n(Fig. 4) and ground-based handheld smartphone devices. Our data were collected in two separate sessions. The first was during the flood event in July 2021– sporadic and ad hoc. The \nsecond was in September 2022, in the course of systematic fieldwork. Our aim was to create \ncases using one of the most common NeRF methods available. All NeRF models were trained \nlocally using Instant-NGP. The GitHub repository (GITHUB \/ instant-ngp 2022) provides documentation  on  software  and  hardware  requirements,  installation,  pre-processing,  training, \nand rendering NeRF, as well as exporting. Another document on GitHub provides additional \nadvice on the process (GITHUB \/ nerf_dataset_tips 2022). We created a separate NeRF model \nfor each set of input data, following a list of six sequential processing steps: 1) data acquisition, 2) data pre-processing and frame extraction, 3) pose estimation, 4) NeRF model training, \n5) video export, and 6) post-processing. \n\n1) Data acquisition for each site was carried out using lightweight, field study-ready collection devices: a DJI Mini drone and an iPhone 11 Pro. For all data acquisition, we used the \nhighest  possible  resolution  of  the  devices.  The  drone  videos  were  shot  at  2.7K  resolution \n(2720x1530), 23,97 fps, in MPEG-4 format. With the smartphone camera, we shot videos at \nFull HD resolution (1920x1080), 59,94 fps, in MPEG-4 format, and pictures at 12MP resolution (4032x3024), in JPEG format. Shots in the  wide-angle camera  mode (0.5x, 13 mm \nequivalent focal length, 120° field of view) were particularly effective. In total, we collected \n96 videos and 10 image sequences, a raw data package of 35 Gigabytes. \n\n2) For the video shots from the field, we extracted a set of sequential frames ranging from 50 \nto over 400 images. The image sequences were filtered to the same amount. NeRF models \nbased on the method of Müller et al. (2022) do not infinitely increase resolution or quality \nwith a more extensive set of input images. Mildenhall et al. (2020) and Müller et al. (2022) \nuse tens to hundreds of images to train NeRF models. We followed the recommendations \npresented on the GitHub forum (GITHUB \/ nerf_dataset_tips 2022). Furthermore, we tested \nthe application of digital image enhancement techniques such as sharpening, noise reduction, \nand super-resolution to improve matching image detection. \n\n3) We used the COLMAP pipeline that was part of the Instant-NGP codebase to process an \nestimate of the camera poses for each image set. The resulting JSON file containing the camera parameters for each image was saved in a folder along with the original images in the \nformat TRANSFORMS.JSON. \n\n4) We trained our NeRF models using the interactive GUI (Graphical User Interface) that \nwas included in the codebase. The GUI offers a variety of different tools for training, visualization, and export, as well as allowing the user to interactively move through a scene while \nthe  model  is  being  rendered  in  real  time.  Training  begins  by  launching  the  GUI  from  an \nAnaconda prompt, and within seconds, the model evolves from blurry noise to a clear representation of a scene. Once the training reaches a satisfactory level, the training progress can \nbe saved as a JSON file – called snapshot – which can be used to reload the NeRF model or \nto create an animation. The GUI facilitates interactive creation and saves a camera path along \na set of key frames that can be used to export a video animation. \n\n5) The codebase allows exporting of flythrough video animations of the NeRF model using \nthe previously generated training progress and camera path. The export is handled outside \nthe GUI in an Anaconda code prompt using Python bindings. We exported a range of video \nanimations up to 4K resolution at 30 fps. \n\n6) The exported videos can be easily edited using common video and image editing tools. \nSince both the input and output of the NeRF model are image-based, digital editing and processing  pipelines  such  as  image  sharpening,  noise  removal,  or  frame  interpolation  can  be \napplied before and after NeRF modeling. The user has full control over both pre- and post-NeRF model media, as would be the case with photography, photogrammetry, or map-making. \n\nNeRF for Landscape Architecture \n\nAlthough we have been working with NeRF technology for a limited time and therefore did \nnot  yet utilize its full potential,  we can already identify and highlight some of its specific \nstrengths.  We  offer  a  selection  of  tested  applications  for  NeRF  technology,  generally  for landscape architecture and, more specifically, in the context of our case study. In addition to \nthe enormous technological advances that have determined the rise of NeRF technologies in \nrecent years, the method has to be tested in relation to issues concerning landscape architecture. \n\nMulti-Resolution Models \n\nFor NeRF models, it applies that their resolution is not developed in relation to the model but \nto the depth of information captured from the input images. “The multiresolution aspect of \nthe hash encoding covers the full range from a coarse resolution \nmin that is guaranteed to \nbe collision-free to the finest resolution \nmax that the task requires. Thereby, it guarantees \nthat all scales at which meaningful learning could take place are included, regardless of sparsity” (MÜLLER et al. 2022). In the context of the Ahr Valley after the flood, we are able to \ncreate a lightweight but complex three-dimensional model that enables capturing environ-\nments at multiple scales: from the rocks in the Ahr riverbed to the flowing water, from the \nriverbanks and adjacent vegetation to patterns of the urban fabric, and from the mountains in \nthe background to the clouds in the sky above the valley. The main benefit we see in our \ncase-based NeRF models is that they feature a high spatial depth, capturing the sky, clouds, \nand even distant landscape features such as mountains, valleys, and urban areas. In the case \nof the Ahr Valley, the NeRF model consolidates all flood-relevant factors to be discussed \nsimultaneously in one model: the change in sediment flows in the river, the altered course of \nthe river, the destruction of urban settlements and agriculture in the Ahr valley near the floodplain,  the  topography  of  the  valley  where  water  has  accumulated  downstream.  More  advanced NeRF models can present the rich contextual depth of the mapping in the form of a \nnavigable three-dimensional model. Xiangli et al. (2022) outline the nature of such prospective models by expanding the notion of rendering scenes at multiple resolutions by “modeling \ndifferent scenes at multiple scales with drastically varying views on multiple data sources” \n(XIANGLI et al. 2022). \n\nObject Focus versus Open World Scene? \n\nBased on our current research and studies, we observe a certain level of contradiction regarding the  great landscape potential of NeRF  models and their technical  nature. NeRF is designed to feature a central point in the model, from which the resolution gradually decreases towards the edges of the bounding cube. This raises the question of whether NeRF models \nare inherently object-oriented (single object) and how this might impact the modeling of non-point-centric open-world scenes, such as landscapes. Is our positive assessment of the NeRF \nlandscape model accurate, or will the triumph of the NeRF models primarily extend to object \nmodels, for example, in the context of architectural projects? Instant-NGP NeRF models are \nconstrained by a maximum resolution bounding box at its center. But in our case study, we \nfed this “machine” exclusively with data from landscape photography and landscape videos, \nthereby obtaining effective landscape models. We suggest that future explorations are “to be \ncontinued.” \n\nComparison to Photogrammetry and Point Cloud Models \n\nThere are partial overlaps in data collection and processing methods between NeRF models \nand photogrammetric processing. Both methods use two-dimensional raster images as input \ndata and share further similarities in the initial processing of this input data. A wide range of \nNeRF methods, including the one of Müller et al. (2022), use COLMAP, a package for SfM, \nto extract camera poses. Models derived from lidar scanning or photogrammetric modeling \nstill offer higher geometric accuracy than NeRF models (LEHTOLA et al. 2022). For our own \ncomparison of the different methods, we created a set of 406 frames from a selected drone \nflight, which we used as input for corresponding NeRF and photogrammetric models. The \nNeRF model was generated with Instant-NGP, and the photogrammetry model was processed \nas a dense point cloud in Agisoft Metashape. The point cloud was exported as a file in LAS \nformat with 12 million points and a file size of 326 MB. \n\nThe output quality of a photogrammetry process is assessed based on the accuracy of where \nthe resulting points are positioned with respect to a ground truth reality. Passive sensing data, \nsuch as intensity return data or true color imagery captured by other sensors, may only suport subsequent analysis or enhance visualization. In many ways, the NeRF model sits somewhere between the perception of space and the perception of textures, materials, light, and color. There are various metrics can be used to assess the quality of a NeRF model. Many \nfundamental advantages of photogrammetry, such as a relation to a “real world” Coordinate \nReference System (CRS) and transformations of the model with respect to this CRS, are not \nyet realized in the NeRF system we used. Nonetheless, this does not preclude the possibility \nof implementing them in future applications. \n\nA NeRF differs from all the traditional three-dimensional scene formats commonly used in \nthe field, some of which include vectors or meshes, grids, and point clouds. Each format can \nhave distinct ways of formulating a representation, which can yield a unique set of advantages \nand disadvantages depending on the format used. It can therefore be difficult to judge a NeRF \nin relation to the qualities of these formats. Some of these metrics may be embedded in the \nprocess or acquisition technology used for data generation to determine the potential for accurately representing a scene from the start. Different metrics apply to data obtained from \nphotogrammetry or lidar scanning. Such comparisons exist in various areas that are tangentially linked to landscape architecture, for example, in heritage preservation. Data obtained \nthrough lidar scanning is among the most accurate geometric data available in modern scanning techniques but lacks the ability to accurately capture the texture and diagnostic color \ninformation  (DOSTAL  &  YAMAFUNE  2018).  Perhaps  the  most  fascinating  outcome  of  the \nmethod is not the NeRF model itself but the images and videos that are generated from the \nmodel (Fig. 5 and 6). MILDENHALL et al. (2020) state that the results of view synthesis are \nbest viewed as videos. \n\nMaterial Noise \n\nMetrics such as reflection or change in transparency and color as a function of position are \nnot considered part of the physical enterprise of the object or scene in the current literature \non photogrammetrically derived models. Rather, they are viewed as  factors that distort or \ncorrupt the signal in ways that may need to be eliminated in order to derive a high-quality \nmodel.  In  scenes  produced  by  photogrammetry,  removing  water-surface  reflection  effects \npresents a challenge (PARTAMA et al. 2018). Materials featuring difficult optical properties –including but not limited to absorptivity, reflectivity, scattering, challenging texture and complex shape or geometry – still pose challenges in photogrammetry (NICOLAE et al. 2014). The \ndistinction between signal and noise is pronounced in the literature on photogrammetry and, \nmore generally, in remote sensing and earth observation. The NeRF model, on the other hand, \nallows data otherwise defined as noise to be used to visualize unstable materialities, surfaces, \nand objects. In our case study, for example, we had the means to illustrate the unstable nature \nof the Ahr River – with its changing water levels up to extreme flooding conditions. \n\nMulti-Source NeRF \n\nA multi-source model is based on input data generated through multiple acquisitions for the \nsame or a similar area. This method can be used in situations where only a sparse set of input \ndata is available to increase the resolution of the NeRF model by adding additional input data \nfor angles or features not previously captured. For our case study, we trained a NeRF model \nof the Kalvarienberg Monastery and the surrounding area in the town of Ahrweiler with sev-\neral of our drone acquisitions and eventually improved the model’s resolution. In addition, \nour model captures the changing light conditions between diffused and direct sunlight caused \nby different cloud conditions during the recording period. The various parameters – geometry, color, lighting, texture, and translucency – captured by a NeRF model may be acquired \nindependently. Each parameter can be derived from a separate set of input data. The final \nNeRF synthesis model allows navigating between these parameters in relation to the position \nand direction of a particular view. In working with a 3D landscape model, this synthesis is a \nnovelty that opens up considerable potential. \n\nMulti-Temporal NeRF \n\nIt sounds almost unattainable within the limitations and resources of our current time, but a \nsimultaneous coupling of movement through time, and movement through space, is fundamentally possible with NeRF technology. This option is yet to be defined and therefore, we \npropose to use “Multi-temporal NeRF models” when referring to the visualization of changing layers of time in the course of changing positions. Multi-temporal NeRF models use mul-\ntiple sets of images captured at different times and utilize them as the input to produce novel \nviews that interpolate between the images. The model synthesizes the input data and allows \nit to move through time while moving through space. A Multi-temporal NeRF model makes \nit possible to capture movements, visualize ongoing processes, and depict all kinds of patterns \nof change. For example, the growth or the changing state of the health of vegetation can be \ndocumented in this way. The intensity of the changes captured by the model can be related \nto the temporal extent of the capture and the intensity of the change in the underlying object \nor  study  surface.  Multi-temporality  is  a  common  concept  and  method  in  geosciences,  in \nwhich remote sensing  observations collected at different times are combined into a single \nmulti-temporal image or model. Multi-temporal analyses enable the detection and visualiza-\ntion of changes in  spatial patterns over  time. The concept  has  taken root in areas  such  as \narchitecture and landscape architecture to understand changes in the built environment. The \nlandscape architect, in particular, can think of numerous possible uses. The depiction of seasonal changes in the city, landscape, and vegetation are only a few of them. We find this \nmethod to be the most effective to this date for purposes of representing the “before” and \n“after” conditions of a site. \n\nIn 2021, it was already demonstrated that NeRF models could be trained with unstructured \ncollections of photographs taken at different times, from different angles, and under different \nlighting conditions. The model registers the static geometries of the scene but interpolates \nbetween color and illumination in dependence on the view position (MARTIN-BRUALLA et al. \n2021). It is possible for a NeRF to process a sequential set of images of the same scene at \ndifferent  times  of  the  day,  times  of  the  year,  and  so  on.  The  associated  different lighting \nconditions of these different images, which show a time difference, allow the generation of \nan outstanding level of multi-temporality in a single NeRF result. The resulting model allows \nthe user to literally move through time as they move through space – made possible by adjusting the different radiation fields between the time-shifted images. \n\nIn our case study, the Multi-temporal NeRF shifts the digital model from a state of representation to a state of simulation of the underlying flood event. Our NeRF model captures and \ninterpolates  situations  found  in  two  self-contained  trajectories.  The  model  combines  two \ndrone flights – one in 2019 and the other in 2022 – over the Ahr Valley municipality of Rech, \nGermany. The acquisition from 2019 shows a historic bridge over the Ahr River, connecting \nthe two halves of the village. The second acquisition captures the same location in the autumn \nof 2022, after the flood event destroyed parts of the bridge, swept away several buildings \nsouth of the bridge, and visibly changed the course of the river (Fig. 7). Both drone flights \nhave different trajectories and viewpoints, allowing us to create a model relating one set of \nviews to the 2019 acquisition and the other set of views to the 2022 acquisition. In the resulting NeRF model, we can navigate through the internal digital coordinate system and observe \nthe changing states in quasi-real-time. \n\nFlooding as a Multi-Temporal NeRF Application \n\nDue to the presence of temporal components – such as large amounts of water that flows in \nand out of a particular site – flood zones are generally considered to be suitable for the application of multi-temporal NeRF. Instant-NGP’s interactive GUI also provides a set of visual \ndebugging tools that can be used to uncover the internal structure of input and output neurons. \nThe parameters of these tools are recorded as part of a keyframe animation. For our case \nstudy dealing with speculative scenarios of a flood-ravaged valley, we used the partial acti-\nvation of neurons as a rising green light field to simulate the rising waters of the flood through \nvisuals (Fig. 8). Through this experimentation, we found a simple yet very powerful tool for \ndigital flood simulation with full visibility of all model elements. Supporting this hypothesis \n(LI et al. 2022) have shown that NeRF models can be used to simulate ultra-complex climate \nevents. \n\nConclusion \n\nThis paper presents a baseline study on  various approaches and  methods of  working  with \nNeRF models in landscape architecture. The aim is to inspire and encourage researchers in \nrelated fields to develop in-depth studies on the applications of NeRF. Our interest in NeRF \nis fundamentally rooted in the idea that landscape is anything but static, which is, at the same \ntime, where we find significant potential in this approach. NeRF models can be useful for \n“wandering” through changing light conditions or addressing moving objects, such as water, \nclouds,  birds,  cars,  trains,  and  others.  Different  materials  can  be  evaluated  through  direct \ncomparison.  Reflections  and  light,  as  well  as  structure,  can  be  included  in  their  changing \nappearance. Changed terrain, for example, differing terrain heights in the course of a construction project, can be evaluated. In addition, the same scenes and objects could be recorded \nunder different lighting conditions in order to enable a critical evaluation. For example, early \nin the morning, at noon, in the evening, or in cloudy weather. The NeRF interpolates between \nthe input images, allowing for seamless switching between different states within the result-\ning model. It is an important task to think of a landscape model not as a static set of coordi-\nnates, i. e. point clouds, raster, or vector data, but as a set of parameters that are constantly \nchanging. As in a landscape as such, the model changes depending on the viewer’s position \n– an unstable model. The fact that NeRF crosses the border between modeling and simulation \nsuits the instability and openness of the landscape subject. We are dealing with a technology \nthat is still very new and largely untested but whose potential seems enormous. Most com-\nputer graphics algorithms and techniques developed over more than half a century assume \nmeshes or point clouds as three-dimensional scene representations for rendering and editing. \nNeural rendering, on the other hand, is such a recent field that the term was first used in 2018. \nFor this reason, there is an inevitable gap between the available methods that can work with \nclassic three-dimensional representations and those that can be applied to neural representa-\ntions (TEWARI et al. 2022). This is definitely true for the field of landscape architecture, and \nwe look  forward to the developments and publications that  will qualify NeRF  models for \nlandscape architecture in the years to come. \n\nIn 2019, Christophe Girot described how the point cloud model overcomes the separation \nbetween model, architectural drawing, renderings, or other visualizations. “In the “cloudist” \napproach, there exists no separation between a model, a section and a plan: they all stem from \nthe same cloud of design information. Separate renderings or visualizations become quite \nunnecessary, since the views generated are directly derived from the model, with their own \nsingular  aesthetic”  (GIROT  2019).  NeRF  models  remove  the  threshold  between  different \nforms of representation (Fig. 9). The NeRF method offers qualities similar to point clouds \nbut significantly reduces the separation or the visual contrast between the model and reality.\n

Preface \n\nA network of engaged individuals teaching new information technologies at the International \nMaster of Landscape Architecture program MLA at Anhalt University established the annual \nConference on Digital Landscape Architecture DLA at our school in 1999. The DLA has to \ndate been held in Istanbul, Malta, Zurich, Munich, Aschersleben, twice in Boston, and frequently on our local campuses in Bernburg, Dessau and Köthen. In 2020 and 2022; the DLA \nwas hosted by Harvard University. Harvard  was able to organize the DLA 2022 as a full \nhybrid conference when the Pandemic still limited traveling. The Journal Digital Landscape \nArchitecture JoDLA which we have developed for the conference is listed in the international \ncitation database Scopus. This publication is supported academically by eighty reviewers and \nboard members. Here, we wish to thank them all for their committed long-term support.  \n\nHaving 64 papers (from more than 20 countries) which successfully meet the standards of \nthe review process coordinated by the founder of DLA, Prof. Erich Buhmann, and his editorial team once again guarantees a very substantial conference. \n\nThe 24th international conference on digital landscape architecture is now back at our internationally known campus in Dessau. Prof. Dr. Matthias Pietsch, this year's local host, is also \norganizing DLA 2023 as a hybrid conference in collaboration with Prof. Dr. Nicole Uhrig \nand Prof. Trevor Sears. Even now having more than two years of experience in organizing \nvirtual lectures and conferences, meeting all the additional needs for an international conference in a hybrid format is still a challenge and requires many university resources. We are \nvery thankful for the team spirit of so many colleagues at Anhalt University, and to the board \nmembers of the DLA for their support once again. \n\nThis  year’s  main  theme  “Future  Resilient  Landscapes”  is  a  core  issue  in  several  research \nefforts of Anhalt University. Our keynote speakers will widen our view on the challenges \nenvironmental design faces in coping with global change. \n\nAs we are able to work with a digital twin of our globe, we can focus on how to use the tools \nof digital landscape architecture in order to meet the challenges of global warming. \n\nAll positively reviewed papers are available as open access papers at Wichmann publisher \nand the outcome of the conference will be published DLA 2023 in Dessau at https:\/\/www.dla-\nconference.com\/ as in the past. \n\nWe are looking forward to welcoming many of you again in person in Dessau in 2023 and \nhopefully in the following years as well. At the same time, we looking forward to seeing the \nmany participants  who for a variety of reasons  will be virtually attending the 24th Digital \nLandscape Architecture Conference. \n\nKöthen, March 15, 2023 \n\nProf. Dr. Jörg Bagdahn, President Hochschule Anhalt \/ Anhalt University \n

The Influence of Perceived Landscape Qualities on \nEconomic Vitality: A Case Study of a Retail Coffee \nChain \n\nAbstract: As a crucial aspect of vitality, the economic facets of vitality at the store level have yet to be \ninvestigated in greater detail, and its relationship with micro-level perceived landscape qualities in the \npublic realm requires further examination. The recent advancements in big data and Machine Learning \n(ML) have presented an exceptional opportunity to empirically investigate vitality and its association\nwith the urban built environment. This research aims to comprehensively gather various dimensions of\neconomic vitality for retail coffee chain, using Starbucks stores in Hong Kong as a case study. The\nstudy incorporates the previously under-researched dimension of customer sentiment, which is interpreted through the Natural Language Processing (NLP) model. Additionally, the study collects both\nsubjectively measured landscape perceptions and objectively extracted visual features from street view\nimagery (SVI) using ML algorithms and crowdsourced surveys. Results indicate that micro-level perceived landscape qualities, such as scale and signage, have a greater impact on economic vitality than\nconventional macro-level planning characteristics. The findings of this research have the potential to\ninform and support a successful and economically dynamic retail model at the neighbourhood scale,\nfurther emphasizing the economic significance of human-scale landscape design in the public realm.\n\nIntroduction \n\nUrban vitality has long been considered a critical aspect of successful cities in place making, \ncontributing to resilience, creativity, and innovation for sustainable development (CHEN et \nal. 2022, MONTGOMERY 1998). First introduced in JACOBS (1961)’ seminal book, and initially defined as the presence of active street life, the concept of vitality has evolved into a \nmulti-faceted connotation that encompasses various dimensions, with economic vitality being regarded as a critical component (HUANG et al. 2020). \n\nQuantifying vitality remains a challenge due to its complex nature, encompassing both social \nand economic aspects. Previous studies have used macro-scale indicators, such as the number \nof entertainment facilities within a city, to measure vitality. However, with the rise of big \ndata,  new  opportunities  have  emerged  to  quantify  vitality.  Despite  this,  researchers  have \npointed out that some big data sources, such as cell phone records, have relatively low data \nquality. Therefore, researchers have shifted to using the intensity of geo-tagged catering businesses from POIs to measure economic aspects of vitality (XIA et al. 2020, YE et al. 2018). \nAlternatively, LONG & HUANG (2019) compared economic vitality across hundreds of cities \nin China using crawled numbers of reviews from popular social media websites that collects \nratings for restaurants. More recently, researchers have proposed more complex frameworks \nthat utilize information available from online service evaluation platforms, such as incorpo-\nrating service quality and scale in addition to popularity (LI et al. 2022). In conclusion, the \nspatial organization of small food establishments plays a significant role in reflecting human \nactivity patterns. Utilizing customer reviews to gather information about economic vitality \nhas also proven to be a valuable approach. However, these methods fail to consider the emotions  and  sentiments  of  user  groups,  which  play  a  crucial  role  in  the  human-environment \ninteraction and contribute to the economic and social aspects of individual businesses at a \nmicro-level (LIU et al. 2020). Additionally, calculating an overall vitality index using certain \nweighting methods for each dimension may be inexorably subject to bias, as different businesses may provide different types of services and target distinct demographic groups with \nvarying economic statuses. \n\nIn view of these factors, exploring the economic vitality of chained catering services, specifically  coffee  retail,  can  provide  an  innovative  approach  for  comparison  across  different \nstores. Coffee retail is often considered a crucial type of \"third place,\" where people gather \nfor socializing without an obligation to stay. It creates a sense of place, a key aspect of promoting  vitality  (OLDENBURG  1989).  The  success  of  coffee  retail  is  influenced  by  various \nfactors such as the environment, service quality, context, and food and beverage offerings. \nHowever, literature in the hospitality industry suggests that chained coffee shops often use \nstandardization and intra-regional diversification strategies based on ‘portfolio theory’ to reduce costs, providing a tactical advantage and greater survival rates over single-location franchises (PARK & JANG 2022). As a result, the differences in services and food offerings between stores within the same chained business can be largely ignored when comparing their \neconomic vitality, offering a solution to the limitations of previous methods. \n\nOn the other hand, previous research has shown that the design of the built environment can \naffect vitality. For instance, building morphology, density, typology, and land use mix have \nall been linked to vitality (HUANG et al. 2020, LONG & HUANG 2019, XIA et al. 2020, YE et \nal. 2018). However, these studies focused merely on objective environment factors at a macro \nand planning scale, but  neglected the nuances of daily life experience and the micro-level \nperceived landscape qualities which can be critical in promoting vitality. \n\nPerceived landscape qualities can be measured objectively, subjectively, or through a combination of the two measures. Advancements in Computer Vision (CV) technology have allowed for more efficient and high-throughput methods like using emerging urban data such \nas SVI to measure perceived qualities (DUBEY et al. 2016, ITO & BILJECKI 2021, ZHANG et \nal. 2018). In a nutshell, the perceived landscape qualities can be largely categorized into subjectively measured design perceptions and objectively measured visual elements (QIU et al. \n2022). These human-centric perceived qualities, which can proxy how people perceive the \nenvironment when walking down the street, have been used to examine the impact of microlevel  perceived  landscape  qualities  on  walking  behaviour  or  housing  prices  (BASU  & SEVTSUK 2022, SONG et al. 2023, SONG et al. 2022).  \n\nThough several recent studies have sought to reveal the correlation between perceived land-\nscape qualities and street vitality (CHEN et al. 2022, JIANG et al. 2022), they mainly focused \non pedestrian volume as a representation of vitality and only  studied a limited number  of \nperceived landscape qualities. Further research is needed to investigate how perceived environment qualities contribute to the economic vitality of coffee retail at a micro-scale. Additionally, due to differences in measurement methods, subjectively measured perceptions have \nbeen shown to exhibit different spatial heterogeneity patterns in comparison with their objective counterparts (SONG et al. 2022). Thus, their separate impacts on economic  vitality \nwarrant further understanding. \n\nIn conclusion, our research endeavours to address the existing knowledge gaps by integrating \nthe human-environment interaction into the measurement of economic vitality across chained \ncoffee stores, and examining how economic vitality is influenced by quantifiable micro-level \nperceived landscape qualities measured through both subjective and objective methods. Our \nresearch offers new insights in the following aspects: \n1) It sheds light on the multiple dimensions of economic vitality of chained coffee shops at \nthe store level and incorporates customer sentiment using advanced Natural Language \nProcessing (NLP) techniques. \n\n2)  The study measures both subjectively measured perceptions and objectively measured visual elements from SVI with ML tools.  \n\n3)  The relationship is disclosed between the perceived landscape qualities within the walking  radius  around  each  store  and  the  various  dimensions  of  economic  vitality,  by  the \ncomparison of the perceived landscape qualities with macro-level factors in relation to \neconomic vitality. \n\nData and Methods \n\nStudy Area and Data Source of Economic Vitality \n\nThe study area is Hong Kong, a high-density city that is one of the world's largest financial \ncentres with over 7 million residents. To control for factors that could impact the economic \nvitality of different services, chained coffee stores were selected for our investigation using \nbig data. By choosing stores from a single brand, the research can mitigate the influence of \nfood quality, service, and interior design and focus instead on other key factors such as the \nquality of the outdoor street environment and macro-level spatial qualities such as accessi-\nbility to transportation and points of interest (POI). This approach offers a straightforward \nresearch  design,  which  provides  a  clearer  understanding  of  the  economic  vitality  of  these \nstores compared to previous studies that have utilised a more broad-brush approach, observing citywide vitality in a coarser grid. Specifically, Starbucks coffee is chosen, a global market leader with over 150 stores in our study area. The analytical framework of this study can \nbe seen in Fig. 1. \n\nThe search results were finalised using the Google Map API, which returned information for \n158 Starbucks stores (Fig. 2) located throughout Hong Kong after initial data cleansing. It is \nworth mentioning although data was also obtained from the local restaurant evaluation platform 'Open Rice', the number of reviews for Starbucks Coffee on this platform was insufficient, so this data was not included in the study. Information gathered from the web crawl for each store comprised its geographic coordinates, address, and, most importantly, information on reviews, including the number of reviews, the number of review images, overall review score, review score distribution, and detailed review text (the 20 most recent reviews after January 2017).  \n\nNLP  is  a  field  of  Artificial  Intelligence  and  computational  linguistics  concerned  with  the \ninteractions between computers and human (natural) languages. It enables the interpretation \nof the human language in a meaningful way, for instance, to understand the emotions. Senti-\nment scores were calculated for each store based on its review texts using the state-of-the-art \nBidirectional Encoder Representations from Transformers (BERT) model, an NLP technique \nthat  uses  a  self-attention  mechanism  and  eliminates  biases  from  left-to-right  momentum \nwhich was used in previous models. The use of BERT has been increasing in recent studies \n(ALAPARTHI & MISHRA 2021). The  model  was pre-trained on a review dataset containing \n150k reviews and reported to achieve an exact prediction accuracy of 67% and an off-by-1 \nscore prediction accuracy of 95% for reviews in English. The sentiment score for each store \nwas calculated as the average of the sentiment scores interpreted from its crawled reviews, \nwith a score between 1 and 5, where 3 represents a neutral sentiment, 5 represents the most \npositive sentiment, and 1 represents the most negative sentiment. \n\nMeasuring Micro-level Perceived Landscape Qualities  \n\nThe street network for this  study  was obtained from  OpenStreetMap and points along the \nHong Kong road network were sampled every 50m using the QGIS platform (ZHANG et al. \n2018). To examine the correlation between perceived landscape qualities and economic vitality, points were selected within the 250m walking radius around each of the chained coffee \nstores located on the ground floor. Points located on highways were excluded as they do not \nreflect the pedestrian experience. The coordinates of points were fed into the Google Street \nView Static API and street view images (SVI) were obtained (heading = 0, field of view = \n90, image size = 800 x 400 pixels). After discarding grey or indoor images, 2,110 SVIs were \nleft and used for further analysis. \n\nTo extract objectively measured visual features from SVIs, the widely used ML algorithm \nPSPNet pre-trained on the ADE20K cityscape dataset was adopted. Around 20 streetscape \nelements, such as sky, sidewalk, trees, buildings, etc., were successfully extracted from each \nSVI. The view index of each visual feature (i. e., the percentage of an element  within the \nentire image) was calculated and the average value of each element within the walking radius \nwas used as the perceived objective feature quality for each store. A randomly sampled SVI \nand its semantic segmentation result using the PSPNet algorithm are shown in Fig. 3. \n\nMeanwhile,  in  accordance  with  previous  studies,  we  aimed  to  quantify  eight  subjectively \nmeasured design perceptions: typology, order, ecology, enclosure, aesthetics, accessibility, \nrichness and scale (EWING et al. 2006, SONG et al. 2022; TIAN et al. 2021). To achieve this, \nwe utilised perceived quality scores obtained from 300 SVIs, with 80% used for training and \n20% for testing, gathered from crowdsourcing surveys in a previous study (TIAN et al. 2021). \nThese scores served as the dependent variables, while the view indices of key street elements \nserved as the independent variables for the prediction task. \n\nEight ML algorithms were used and compared for prediction performance: K-Nearest Neighbours (KNN), Support Vector Machines (SVM), Random Forest (RF), Decision Tree (DT), \nGaussian Process (GP), Voting Selection (VS), ADA Boost (ADAB) and Bagging Regression  (BR).  The  ML  models  were  evaluated  by  using  R-squared  (R2)  and  Mean  Absolute \nError (MAE). The best-performing model for each subjectively measured landscape quality \nwas then selected to predict the scores for the entire SVI dataset in Hong Kong. \n\nMacro-level Conventional Planning Qualities and Correlation Analysis \n\nIn  addition  to  the  micro-level  perceived  landscape  qualities,  macro-level  planning  factors \nwere also computed to evaluate their impact on vitality. A 1.5 km buffer was created around \nthe centre of each store location, and the relevant  variables  were obtained from the Hong \nKong Geodata Store (https:\/\/geodata.gov.hk\/gs\/) and processed in QGIS. The variables included ‘number of POIs’, ‘number of hotels’, ‘number of Airbnb’, ‘number of elderly facilities’, ‘number of bus stops’, ‘distance to metro station’, and 'size of park area', which have \nbeen reported to contribute to street vitality. For example, accessibility to transportation facilities such as the distance to metro stations and the  number of bus stops can impact the \npotential crowds around the station. The accumulation of destinations (number of POIs) can \nattract people and promote vitality. Meanwhile, park size represents the neighbourhood-scale \nenvironmental quality, which contributes to subjective well-being and often attracts people. \nAdditionally, Airbnb can attract tourists and is essential to vitality. \n\nThe  study  conducted  further  statistical  analysis  to  determine  the  correlations  between  the \ndifferent dimensions of economic vitality of ground-floor stores and various groups of built \nenvironment qualities. Pearson correlation analysis was applied to provide a comprehensive \ncomparison between macro and micro-level factors and their relationship with economic vitality.  \n\nResults \n\nComparison of Economic Vitality \n\nThe 158 Starbucks Coffee outlets in Hong Kong are dispersed throughout various regions of \nthe city. Among these outlets, about 13 stores are located in the ‘Central’, which is its most \nconcentrated area, 91 stores of them on the ground floor and 67 stores on other floors ranging \nfrom -1st to 9th level. Because the intent of this research is to assess the economic vitality \nand its relationship with the surrounding context and integrates the Sentiment analysis based \non reviews, those review numbers were excluded if they were less than 30 times, and 127 \nstores were left. And because the stores located on the ground level have more direct interactions with the built environment, the stores on other levels were further removed, and 79 \nstores located on the ground level left. The detailed statistics are demonstrated in Table 1. \n\nAlthough only ground-floor stores were utilized for further analysis, a preliminary comparison was performed with stores located on other floors to identify potential biases. And the \nstatistics revealed similar results. In summary, ground-floor stores make up approximately \n60% of the total stores analysed. The average rating for these stores is slightly higher compared to those on other floors. The image count suggests that customers are more likely to \ntake and post photos in stores located on the ground floor, which may be due to the surrounding built environment. With regards to sentiment scores, it can be concluded that stores received overall positive sentiment scores, as a neutral emotion is rated as 3.0 on the scale used. \n\nML Model Performances \n\nMultiple  ML  algorithms  were  used  to  determine  the  most  effective  models  for  predicting \nsubjectively measured perceptions. As shown in Table 2, while four out of eight variables \n(Typology, Order, Aesthetics and Richness) had low  R-squared (R2) values and  were ex-\ncluded from further analysis, the qualities of Access, Ecology, Enclosure, and Scale achieved \nR2  values  ranging  from  0.40  to  0.53.  These  prediction  accuracies  are  deemed  acceptable \ngiven the size of the training sample, and they partially outperformed results from previous \nstudies  (DUBEY  et  al.  2016,  ITO & BILJECKI  2021,  SONG et  al.  2022).  Therefore,  the  four \nselected perceived landscape qualities were predicted for all SVIs using the best-performing \nmodels (i. e., Gaussian Process, Voting Selection, and Bagging Regression). After determining the qualities of each street view, we linked them to the corresponding Starbucks store \nlocations  and  obtained  the  mean  value  for  each  store  as  the  neighbourhood's  subjectively \nmeasured perceptions. \n\nCorrelation Analysis and Discussion \n\nThe Pearson Correlation analysis was conducted to investigate the correlation between the \nfour dimensions of economic vitality and selected macro-level spatial attributes (Fig. 4). The \nresults indicate that many macro-level factors had a moderate to weak positive correlation \nwith the review count. For instance, besides the most prominent impact of Airbnb (0.3), bus \nstops, POIs, and hotels also showed a similar positive association with the review count (0.26 \nto 0.27). The image count showed a weak positive relationship with bus stops (0.21). However, the correlations between the overall score and sentiment score and macro-level planning \nfactors were negligible. Despite this, POIs (-0.16) and hotels (-0.16) had the highest strengths \nin correlation coefficients with the sentiment score, suggesting that they may have a poten-\ntially negative impact on visitors' emotions, which in turn may negatively affect the economic \nvitality of the stores. \n\nWe conducted a separate analysis for the micro-level perceived landscape qualities (Fig. 5). \nOn the one hand, out of the four subjective perceptions, the quality of Scale, showed a mod-\nerate positive correlation with the review count (0.35), while its correlation with the image \ncount was weaker (0.23). Additionally, the review count demonstrated a weak positive correlation  with  Enclosure  (0.27)  and  a  weak  negative  correlation  with  Ecology  (-0.27). The \noverall review score showed a positive correlation with Ecology (0.2). Similar to the macrolevel spatial qualities, the correlations between sentiment and subjective landscape qualities \nwere statistically negligible. However, Ecology reported the highest positive impact (0.15) \non sentiment. \n\nOn  the  other  hand,  the  correlations  between  the  objectively  measured  visual  features  and \neconomic vitality dimensions were similar in strength to the subjectively measured percep-\ntions. The review count demonstrated a strong positive correlation with signboard (0.42). \n\nAdditionally, the review count showed comparable weak positive correlations with several \nother visual elements, including sky (-0.24), tree (-0.25), road (-0.26), streetlight (-0.26), and \nrailing (-0.26). It also reported a weak positive correlation with building (0.23). The image \ncount exhibited a moderate positive association with signboards (0.3) and weak negative re-\nlationships with the sky (-0.21) and road (-0.22). The sentiment demonstrated a positive cor-\nrelation with trees, showing its consistency of correlation with the Ecology perception (0.15), \nthough the correlation strength is statistically insignificant. \n\nOur study provides a brand-new insight into the hitherto poorly understood relationship between store-level economic vitality and built environment factors. Most importantly, it was \nobserved that compared to macro-level variables, micro-level perceived landscape qualities \nexhibit  stronger  correlations  with  economic  vitality  dimensions.  Additionally,  objectively \nperceived visual features were found to complement subjectively measured perceptions. For \nexample,  signboard  was  found  to  have  the  highest  impact  among  all  perceived  landscape \nqualities, offering meaningful design suggestions for enhancing future economic vitality. It \nproves that the wayfinding system is essential in the urban built environment. Conversely, \nsky, railing, and a few other visual elements had a negative impact, whereas building was \nfound to have a positive impact, potentially reflecting a different interpretation of enclosure \nquality, which supports walkability for pedestrians on the streets. This result is inconsistent \nwith the results of previous research that found sky and sidewalk to be positively related to \nvitality  using  SVI  (LI  et  al.  2022),  which  could  be  caused  by  differences  in  geographical \ncontext. It was also found that review count was negatively correlated with Ecology, which \nmight stem from a lack of available space in the public realm for street planting. This result \ncontradicts prior research, which suggests that street greenery has a positive impact on walking behaviours in Hong Kong (LU 2019) and that vegetation enhances street vitality (JIANG \net al. 2022). However, it did provide similar result which aligns with the findings of Y. LI et \nal. (2022), who found the greenery seem to show a degree of negative correlation when focusing  on  a  commercial  complex  site  in  Japan.  This  may  suggest  that  poorly  positioned \ngreenery may obstruct key commercial areas, potentially negatively affecting the economic \nvitality of stores. Future research should aim to conduct a more comprehensive analysis of \nthe relationship between street greenery and store-level economic vitality in Hong Kong. \n\nThe quality of Scale, Enclosure, and Access was all found to be positively related to review \nintensity, highlighting the importance of human-scale design quality in enhancing pedestrians’ experience and street vitality and supporting the benefits of appropriate street planning \nand design. Lastly, sentiment score was found to have a positive relationship with both greenery and ecology perception, albeit with relatively low strengths. This is in line with previous \nresearch that greenery can provide psychological benefits. Nevertheless, no statistically significant correlations  were established with other variables, suggesting the need for further \nresearch. The sentiments reported in reviews can be complex and might yield mixed results \ncompared to review intensity. \n\nRegarding macro-level factors, access to transportation was found to have a positive impact \non the dimensions of economic vibrancy, which is in line with previous research that suggests \npublic transportation facilities improve accessibility for non-local communities (HUANG et \nal. 2020). Additionally, the presence of visitor-oriented urban functions, such as hotels and \nAirbnb, were positively associated with economic vibrancy at the store level. The number of \nPoints of Interest (POIs) was found to have a relatively high impact among the macro-level \nvariables, which is consistent with earlier findings that land use density and functional mix \naffect economic vitality (LONG & HUANG 2019, XIA et al. 2020). However, our study sur-\nprisingly did not find significant correlation between park size and economic vitality, probably because park accessibility does not necessarily equate to increased economic activity in \nretail  or  commercial  stores.  Although  increasing  pedestrian  volume  can  increase  vitality, \nother driving forces, like perceived landscape qualities or urban micro-level amenities, are \nnecessary for attracting people to stay and linger so as to promote urban economic vitality. \n\nConclusion \n\nThis study offers several important contributions to the existing literature. Firstly, it provides \na unique perspective on quantifying  the dimensions of economic  vitality at the individual \nstore level, using a case study of chained coffee shops in Hong Kong. The study employs \ndata mining and NLP techniques to measure users’ sentiment scores from reviews, offering \na novel approach to this area of research. Secondly, the dimensions of economic vitality are \nstatistically evaluated in relation to both macro-level planning variables and micro-level perceived  landscape  qualities,  including  subjectively  measured  perceptions  and  objectively \nmeasured visual elements based on SVI dataset. This can help derivation of quantifiable design strategies and implementable guidelines to enhance the economic vitality of neighbour-\nhoods. Our preliminary data analysis suggests that compared to macro-level characteristics, \nsubjectively measured perceptions such as Scale, and objective visual elements, such as signboards, can have a significant impact on economic vitality. The objective visual elements in \nthe  streetscape  can  complement  the  subjectively  measured  perceptions  and  vice  versa. \nThirdly, although no statistically strong associations were found between sentiment scores \nand built environment factors, the study suggests that visual greenery and Ecology perception \ncould  play  a  positive  role  in  affecting  sentiment  scores,  which  is  beneficial  in  promoting \neconomic vitality. Lastly, this research adds to our knowledge of future recommendations for \nretail store location selection, and provides actionable insights for landscape architects on the \ndesign of streetscapes, with the goal of creating economically vibrant cities through meaningful placemaking. \n\nNevertheless, this research has several limitations. Firstly, the dataset could be enhanced by \nincluding comparison with other brands, such as Pacific Coffee, which holds a similar market \nshare  to  Starbucks  in  terms  of  coffee  outlets  in  Hong  Kong.  This  would  help  reduce  any \nbiases in the conclusion. Secondly, recent studies have shown that people's walking behaviour has a non-linear relationship with the built environment, and therefore, the vitality of an \narea may be influenced in a similar way. In future research, multiple walking radii could be \nemployed to gain deeper insights into this phenomenon. Thirdly, Thirdly, the accuracy  of \npredictions of subjectively measured landscape perceptions could be improved by either increasing the size of the training set or by employing more advanced machine learning algorithms such as Convolutional Neural Networks. Lastly, it would be beneficial to further explore other perceived landscape qualities or psychological perceptions, such as safety, in fu-\nture studies. \n

A Comparative Overview of the Transversal Connec-\ntivity of Natural Landscape Mosaics to Freshwaters \nin the Metropolitan Areas of Berlin, London, & Paris \n\nAbstract: Connectivity among blue-green infrastructure (BGI) is a vital condition for enhanced ecosystem services provision in metropolitan regions where the social-ecological systems are interwoven. \nIn this study, we conduct a comparative spatial assessment of transversal connectivity of natural surfaces to freshwater sources for the metropolitan areas of Berlin, London, and Paris. We focus on the \ntransversal  connectivity,  which  describes  the  lateral  connection  between  natural  surfaces  and  water \nsurfaces, as opposed to connections along a water surface geometry. Initially, we compare the distribution ratios of water surfaces, natural lands, and transportation network at both the metropolitan and the \nurban level, relying on the Urban Atlas (UA) land-use and land-cover data. Our results show that at \nmetropolitan level, Berlin has higher shares of water and natural surfaces (2.6 % and 40.1%) compared \nto  London  (1.32  %  and  13.4  %)  and Paris  (0.9  %  and  23.1  %). On  the  contrary,  the  transportation \nnetwork covers less surfaces in Berlin (2.00 %) than in London (4.71 %) and Paris (3.62 %). Second, \nwe investigate the connectivity among BGI elements, and most importantly, to the water surfaces. We \nconsidered the water surfaces as pivot elements in the landscape, while investigating the connectivity \nof natural areas to them. As part of the methodology, the transportation network is introduced in the \nanalysis as the main fragmenting geometry. Our results show that in all three cases, more than 80 % of \nnatural surfaces patches had no connection to freshwaters. Among them, the natural surface patches of \nBerlin record the highest level of transversal connectivity to freshwaters (17 %) On the other hand, \nLondon leads the ranking on the water surfaces that are not connected to any natural surface (>60 %). \nTransversal connectivity can be greatly increased with minor adjustments in spatial planning. For example, in Berlin the percentage of natural surfaces connected to freshwater jumps from 18% to 53%, if \nwe assume that natural areas that are no more than 32 m apart can be connected in a network. This so-called potential transversal connectivity reveals areas where the connectivity among BGI is improvable \nand can enhance the capacities for nature-based solutions against emerging metropolitan challenges. \n\nIntroduction \n\nWater sources are among  the  most endangered components  within urbanized lands. More \nand  more,  the  urban  agglomerations  are  becoming  dependent  on  freshwater  sources \n(LUNDQVIST et al. 2005). However, the complexity of the metropolitan zone regarding the \nwater  management  has  not  been  considered  enough  by  the  existing  literature  (VAN  DEN \nBRANDELER et al. 2019). Blue-green infrastructure (BGI) is advocated to be an efficient conceptual  tool  for  sustainable  urban  water  management  (HAMEL  &  TAN  2022).  Apart  from \nstorm water management (sustainable urban drainage systems), it includes greenways and \necological networks  which are important for biodiversity.  For example,  small  green areas \nsuch as private gardens, have significant impact on the urban biodiversity especially for specific specialized species such as insects (e. g. butterflies) in Prague (KADLEC et al. 2008) and \nbats (pipistrelle) as reported in the case of Paris (MIMET et al. 2020). However, a well-connected BGI system is vital for a well-functioning urban ecosystem. Landscape Fragmentation \n(LF) is an adverse phenomenon happening in continental lands mostly due to land-use alteration. Settlement development, alteration of forested lands to agricultural use (FAHRIG 2001) \nand transportation infrastructure expansions (GENELETTI 2004) are the major human activities causing fragmented landscapes. Landscape connectivity is targeted as an important part \nof decision and policymaking practices such as transportation and regional planning in Europe (EEA 2011) or the City Biodiversity Index (CBI) in Asia (CHAN et al. 2014).  \n\nHerein, we provide a rapid and simple method to assess the existing transversally connected \nBGI using the three different European metropolitan areas of Berlin, London, and Paris as \nstudy cases. We utilized the Urban Atlas database enabled by the European Commission via \nthe Copernicus program. The workflow of the study was modelled in the open-source software QGIS- Model Designer. We intentionally relied on open-source data and software for \nboth promoting open science and making the reproducibility of the method easier. We expect \nthat the selected metropolitan areas contain relatively high amounts of green surface areas. \nHowever, we hypothesize that their inter-connectivity and specifically connectivity to freshwater surfaces is relatively low. Since the mere existence of BGI elements is not enough, \nunless  they  are  well  inter-connected,  we  also  push  forward  this  study  as  a  call  for  future \nstudies to search for ways of improving the interconnection of BGI. \n\nMaterials and Methods \n\nStudy Area  \n\nThis study is based on a comparison between the metropolitan areas of Berlin, London, and \nParis (see Fig. 1). All three cases are among the largest metropolitan areas in Europe. Furthermore, they are pioneers in developing management plans for improving the green and \nblue layer of their metropolitan zones. For example, Berlin is shortlisted among the five forerunner cities in implementing green infrastructure for sustainable urban water management \n(LIU & JENSEN 2018). On the other hand, teams of diverse professionals have collaborated to \nput forward general guidelines for Paris urban area’s evolution by 2030. The enhancement \nof the surface water systems is highlighted as a core objective (MASSON et al. 2013). Similarly, London makes home for a vast number of parks and gardens which are well known as \nthe ‘green lungs of London’ providing space for escape from high levels of pollution and \nrecreation. The phrase ‘Lungs of London’ was used for the first time by William Pitt, during \nparliamentary debate as early as the beginning of the 19th century while discussing about the \nurban encroachment on Hyde Park (JONES 2018). We intentionally did this selection to show \nthat high levels of development and abundance of green surfaces are not a guarantee of well-\nconnected and effective metropolitan BGI.  \n\nThe raw material of this study is the Urban Atlas (UA) land cover data as provided via European Environment Agency (EEA). UA data consist of high-resolution land use and land cover \n(LULC) data for 36 EEA member and cooperating countries. The LULC classification consists of 17 urban and 10 rural categories having a minimum mapping unit (MMU) of 0.25 ha \nand 1 ha, respectively. First, the water surfaces including the wetlands are highlighted as the \nfocal  surfaces.  A  second  set  consists  of  the  natural  surfaces,  including  green  urban  areas, \nforests, and herbaceous vegetation. The artificial surfaces are disregarded and have been filtered out during the processing workflow. Indirectly, they silently remain as landscape fragmentation geometries within the metropolitan urban fabric. While the transportation geometries are considered as core fragmentation agents. \n\nWater-oriented Transversally Connected Natural Landscape Mosaics  \nThis study follows the conceptual approach developed by HYSA & BAŞKAYA (2018a), which \ngenerates consecutive bands (patch order) of landscape patches within the coastal zone, based \non their spatial relationship with the coastline. Initially, the concept has been developed into \nan  automated  model  in  ModelBuilder  (ArcGIS)  for  the  coastal  zone  (HYSA  &  BAŞKAYA \n2018b). Further on, the model is adjusted for the metropolitan level connectivity analysis to \nidentify the transversally connected natural landscape mosaics (TCNLM) (HYSA 2021). This \nstudy considers the freshwater surfaces as focal elements, to which the transversal connectivity of natural surfaces is investigated. Here the workflow is modelled in the open-source \nsoftware QGIS, contributing to open science and to enable reproducibility for other metropolitan study areas where the land cover data are available. Another important parameter is \nthe patch order (PO). It refers to the connectivity order of a landscape patch to water geometry. Patches that are located at the waterfront are assigned a PO value of 1. Patches that are \nconnected to PO 1 patches are assigned a PO value of 2, and so on. PO values can continue \ninfinitely by indicating the connectivity order a patch has with the water surface (Fig. 2b). \n\nFigure 2 shows how the potential TCNLM are derived. Transportation network is accepted \nthe main fragmenting agent in metropolitan areas (Fig. 2a). First, if we assume that patches \nless  than  4m  (approximately  a  single  lane  road)  apart  are  connected,  connectivity  greatly \nincreases, and we obtain a network of natural landscape patches of different PO values (Fig. \n2b) and the potential TCNLM for this fragmentation distance (Fig. 2c). Technically, this process is implemented by buffering the original patches with half the fragmentation distance \nand identifying overlapping polygons. Further details about the procedure are provided in the \nstudy introducing the Transversal Connectivity Index (HYSA 2021). \n\nResults \n\nComparative Highlights about LULC Inventory Based on UA Data \n\nThe comparison of three cases based on UA data  (2018) shows that the  metropolitan and \nurban surface area of Berlin and Paris are similar. While the metropolitan area of London is \nabout half of them, its urban core is double the size of the two other cases (see Fig. 1). According to Figure 3, Berlin is leading the ranking of water surface areas both in metropolitan \nand urban level, being 3 to 5 times higher than in the other two cities. A similar trend appears \nwhen considering the natural surfaces at metropolitan level. Regarding the surface area of \nroads and associated lands, Paris is leading with a small difference. Furthermore, Figure 3c \nand 3d deliver interesting ratios of natural lands surfaces per inhabitant. Berlin scores almost \nten times higher than London and Paris at metropolitan level (see Fig. 3d). While at urban \nlevel, Berlin marks twice higher than London, and four times higher than Paris (see Fig. 3c). \n\nRevealing the Water-oriented TCNLM within the Metropolitan Regions \n\nAccording to the results of the TCNLM analysis at metropolitan level, only 37.9 % of water \nsurfaces in London are connected to natural lands (Fig. 4a). While the water surfaces within \nmetropolitan areas of Berlin and Paris are at least two times more connected then the case of \nLondon. Furthermore, only 38 % of natural surfaces of Berlin, 21 % of Paris, and 14 % of \nLondon, are connected to freshwater surfaces (Fig. 4b). On the other hand, when calculating \nby counts of natural surface patches, the connectivity percentages drop to as low as 15.5 %, \n7.9 % and 6.0 % for Berlin, Paris, and London respectively. Nevertheless, the potential improvement is recognizable after the applied buffers. Figure 4b shows that after applying the \nbuffer of 32m, the transversal connectivity of natural landscapes to freshwaters increase to \n88 % in Berlin, 63 % in Paris, and 35 % in London. This fact implies that the natural land \nsurfaces at metropolitan level have the capacity to be better connected to available freshwater \nsurfaces. \n\nFigure 5a presents the distribution of natural landscape patches within the identified TCNLM \nat five buffer levels (buffer of 0 m, 4 m, 8 m, 16 m, and 32 m). At the first level of no buffer, \nin Berlin there are 2551(12.7 %) natural patches that currently have direct connection with \nfreshwaters  (PO  1),  followed  by  Paris  with  1265  (7.5  %)  and  London  with  721  (5.7  %) \npatches. The higher connectivity of Berlin’s BGI compared to London and Paris is shown by \nspatial distribution patterns as well (Fig. 6). In all three cases, there is an abrupt drop in the \nnumber of connected patches of second band (PO 2).  \n\nNevertheless, after applying the buffering procedure of potential connectivity, there is a significant increase in the number of connected patches of second band (PO 2). Furthermore, in \nall three cases, the patches of second band (PO 2) show the highest increase after each buffer \ndistance application, compared to the other bands (Fig. 5b). Thus, we conclude that further \nefforts must be put on analysing further in detail the potential patches of second band (PO 2), \nand especially those that are fragmented by narrow roads of up to 8m width. \n\nDiscussion on Implications and Limitations \n\nAn  important  implication  of  the  method  presented  in  this  paper  relates  to  the  concept  of \n“sponge cities”,  which  is accepted as a revolutionary concept to simultaneously deal  with \ndiverse metropolitan challenges such as water cycle management, social-ecological benefits, \nand addressing climate change consequences. Recent literature proposes the need for further \nresearch  in  comprehensive  computer-based  models  to  support  the  implementation  of  the \nsponge city concept (NGUYEN et al. 2019). In particular, this relates to quantitative availability of measures supporting sponge cities, while the qualitative aspects are underestimated. \nThis research  work contributes to the discussion of  sponge cities by providing an explicit \nmethod for long-term monitoring of connectivity among BGI at the metropolitan scale.  \n\nOur method is not aiming a new landscape metric, but instead targets at providing a novel \napproach to BGI analysis by focusing on the water surfaces. While it results useful for identification  of  the  freshwater-oriented  TCNLM,  the  structural  and  functional  diversity  of \nTCNLM requires further analyses. Consequently, next studies should expand the integration \nof various landscape metrics that targets not only the structural but also the functional properties of diverse habitats. Also, future research can focus specifically on roads that significantly fragment water-oriented BGI, e. g. to inform feasibility studies about road alterations \nto  reduce  landscape  fragmentation  and  enhance  connectivity  among  metropolitan  BGI.  A \nchallenging discussion will be on how much of this connectivity is needed (ratio thresholds), \nto effectively orient the stakeholders in decision making processes. As we are not yet sure if \nall natural surfaces within metropolitan areas should necessarily be connected to freshwater \nsurfaces and vice versa.  \n\nConclusions \n\nThis study reveals pronounced differences in the TCNLM to freshwater surfaces among the \nthree metropolitan areas of Berlin, London, and Paris. Based on the UA data from 2018, it \nhighlights that only 15.5 % of natural landscape patches within the metropolitan area of Berlin are connected to freshwaters, with even lower levels of only 7.9 % and 6.0 % for Paris \nand London, respectively. Similarly, a significant portion of water surfaces remain unconnected to any metropolitan natural surfaces. Yet, our potential connectivity analysis shows \nthat if proper interventions are made, the transversal connectivity of natural landscapes to \nfreshwaters  can  be  improved.  For  example,  through  scenarios  of  altering  the  fragmenting \nsections of the road network, the percentages of TCNLM to freshwaters rise to 86.5 %, 64.1 \n%, and 35.0 % for Berlin, Paris, and London. The current transversal connectivity of natural \nlandscapes to freshwaters in the selected three metropolitan areas is not satisfactory, particularly in light of climate change and emerging challenges for large urban areas. Yet, we revealed that if proper spatial planning and management measures are taken by the responsive \nmetropolitan administrative units, the situation could be improved significantly.  \n\nCalculation Method for Carbon Sequestration of \nUrban Green Space Vegetation – Based on Point \nCloud Technology \n\nAbstract: In the context of China’s carbon peaking and neutrality goals, green space is important for \nsustaining urban ecosystems, which perform the functions of emission reduction and sequestration enhancement. As a result, the carbon  sequestration benefits of urban green space have become an important research topic. Furthermore, carbon sequestration is an important indicator for evaluating the \nbenefits of urban green areas. However, most existing calculation methods are based on manual measurements or satellite remote sensing data, which are inefficient and have low accuracy. Our research \ndevelops a method to calculate the carbon sequestration of urban green space vegetation based on point \ncloud technology and the integrated use of several digital software platforms. First, the point cloud data \nwere acquired by light detection and ranging, and preliminary processing was performed; second, based \non  the  characteristics  of  urban  green  space  vegetation  structure,  we  used  manual  segmentation  and \nindividual tree segmentation to classify tree, shrub, and ground cover data, and perform voxelization \nand surface mesh modeling to obtain tree and shrub volume and ground cover three-dimensional surface \narea; and third, we imported the relevant data using the carbon  sequestration calculation formula to \nobtain the carbon sequestration of each vegetation type and total carbon sequestration of green space \nvegetation. In the last step, we applied the method to a case study of Mei’an green space in a college \ncampus in Nanjing, China. The proposed method can model vegetation more accurately and efficiently \nthan previous methods and provides an effective way to calculate the carbon sequestration by vegetation \nin urban green space. \n\nIntroduction \n\nIndustrialization and the rapid expansion of cities have led to a rapid increase in the amount \nof carbon dioxide and other greenhouse gases in the atmosphere, leading to global warming, \nwhich has resulted in frequent extreme weather events, rising sea levels, and a sharp decline \nin biodiversity, all of which seriously threaten human survival (SOLECKI et al. 2013). The \nIntergovernmental Panel on Climate Change was established internationally as early as 1988 \nto address climate change and promote sustainable urban development. The Paris Agreement \nin 2015 put forward the requirements for countries to reduce carbon emissions, promote carbon sequestration benefits, and achieve carbon neutrality. The European Union and developed countries, such as the United Kingdom and France, have set timeframes for achieving \ncarbon neutrality after reaching peak carbon dioxide emissions (COMMISSION 2020, JUSTICE \n2019, KINGDOM 2022). China has also proposed “peak carbon dioxide emissions and carbon \nneutrality goals,” aiming to achieve “peak carbon dioxide emissions” by 2030 and “carbon \nneutrality” by 2060 (ZHANG et al. 2021). Vegetation and other green space components can \nabate, absorb, and store carbon dioxide (ZHANG et al. 2022). As an important part of the urban \necosystem, green space performs the vital function of carbon sequestration and is the primary \nmeans  of  neutralizing  urban  carbon  emissions.  Thus,  the  carbon  sequestration  benefits  of \nurban green space have become an important research topic. \n\nA point cloud is an extensive collection of points that contain attribute information, including \nthree-dimensional (3D) spatial coordinate details, colors, and materials (YANG & HAN 2018). \nIt is acquired using light detection and ranging (LiDAR), which includes an airborne laser \nscanner (ALS) and terrestrial laser scanner (TLS). ALS can acquire a large range of point \ncloud data but has relatively low accuracy. Moreover, the vegetation information in lower \nspace cannot be accurately obtained due to the occlusion of the vegetation canopy. Although \nthe scanning range of TLS is limited, it can accurately scan information about lower vegetation. The acquired point cloud data have the characteristics of high accuracy and high density \n(XU et al. 2021), which are more suitable for small- and medium-sized urban green spaces. \nCompared with traditional manual measurements for vegetation data, the acquisition of point \ncloud data by LiDAR has the advantages of high efficiency and accuracy, no physical contact \nwith and damage to the measured object, and accessible data viewing and processing (YANG \net al. 2018). Additionally, the high reproducibility of point cloud data in reflecting the 3D \nstructural information of plants (MALHI et al. 2018) makes it a promising technique for plant \nmodeling (LIVNY et al. 2010, MILENNOVIĆ et al. 2012) and capturing plant data (MEDEIROS \net al. 2017, Sun et al. 2018).  \n\nIn terms of plant modeling, research on the application of point clouds has primarily concerned algorithm-based simulations of realistic plant morphology. For example, interactive \nsegmentation and spatial colonization algorithms are used to segment and model tree point \nclouds (ZHANG et al. 2021). However, further quantitative analyses and applications of plant \nmodeling are still required. Point cloud can also be used for measuring living vegetation volume (LVV) based on the application of plant modeling. LVV refers to the volume of space \noccupied by the stems and leaves during plant growth and is an important indicator of the \necological benefits of urban vegetation (ZHOU & SUN 1995). LVV can be applied for calculating  the  vegetation  carbon  sequestration  benefits.  Several  studies  have  used  point  cloud \ntechnology for LVV measurement. For example, point cloud data can be used to obtain plant \ncrown diameter and height, and LVV can be calculated using the volume calculation formula \n(MARZULLI et al. 2020). Alternatively, the convex hull method can be used to approximately \nsimulate the plant shape and calculate the LVV (LIU et al. 2016, LU & GAO 2018). However, \nthe process is generally complex, and the accuracy of plant simulation requires further improvement.  \n\nCarbon sequestration refers to the process, activity, and mechanism of absorbing and storing \natmospheric carbon dioxide (HOUGHTON et al. 1995). Carbon sequestration of urban green \nspace vegetation refers to the process of absorbing and storing carbon elements through photosynthesis by urban  green  space vegetation. It  is an essential indicator for evaluating the \nbenefits of urban green spaces. Existing methods for calculating the carbon sequestration of \nurban green space vegetation mainly include sample inventory, photosynthetic rate integration, micrometeorology evaluation, and remote sensing estimation (FANG et al. 2007, LIU et \nal. 2021). Most of the primary data originate from manual measurement or satellite remote \nsensing, which results in a large and complicated calculation process, and results with low \nefficiency  and  less  accuracy (XIE  et  al.  2022).  This  is  because  manual  measurement  data \ncannot accurately describe complex lifeforms, such as plants, and remote sensing data cannot \nobtain information on the vertical structure of the forest understory, thus, making it difficult \nto accurately reflect the spatial characteristics of urban green space vegetation. In addition, \nremote  sensing  is  mainly  used  for  carbon  sequestration  calculations  of  large-scale  green \nspaces (ZHANG et al. 2022). In recent years, scholars in China and abroad have attempted to \nuse  LiDAR  to  obtain  3D  point  cloud  data  and  combine  the  data  with  digital  modeling  to \nreflect the spatial characteristics of  green areas accurately  (MÜNZINGER et al. 2022). This \ntechnology can accurately identify the morphological characteristics of vegetation, topography, and other elements, thereby overcoming the limitations of conventional manual measurement and conventional manual measurement methods (FERNÁNDEZ-SARRÍA et al. 2013). \nAdditionally, it can meet the needs of accurate quantitative analysis while providing a new \ntechnical means for calculating the amount of carbon sequestration in urban green spaces. \n\nIn this study, we explored a method for calculating the carbon sequestration of urban green \nspace  vegetation  based  on  point  cloud  technology.  We  selected  a  green  space  known  as \nMei’an on a college campus in Nanjing, China, as the research area and used LiDAR to obtain \n3D point cloud data. We used several software platforms, including Cloud Compare, Trimble \nRealWorks, Rhinoceros 3D, Grasshopper, and MATLAB, to classify and process the point \ncloud data, construct a 3D vegetation model, and measure the total carbon sequestration of \nvegetation in the Mei’an green space. This paper proposes an efficient and accurate method \nfor calculating the carbon sequestration of urban green space vegetation and discusses the \nadvantages of point cloud technology in improving  the accuracy and ease of operation of \nvegetation modeling and visualizing the carbon sequestration calculation results. \n\nMethods  \n\nIn this study, we developed a carbon sequestration calculation method for urban green space \nvegetation based on point cloud technology, which includes four main components (Figure \n1). First, we used LiDAR to obtain the point cloud data of the site and performed preliminary \nprocessing of the data. Second, we classified the point cloud data according to the requirements of urban green space vegetation composition and carbon sequestration classification \ncalculation and extracted the vegetation point cloud data. Third, we performed voxelization \nmodeling and surface mesh modeling for point cloud data of trees, shrubs, and ground cover. \nWe then obtained the volume of tree and shrub crown and the surface area covered by each \nvegetation type. Finally, the carbon sequestration of the different vegetation types was obtained using the corresponding carbon sequestration calculation formula and aggregated to \nobtain the total carbon sequestration of the green space. \n\nData Acquisition \n\nWe selected LiDAR to collect point cloud data within the study area for small- and medium-\nsized urban green spaces. After scanning the sample sites using the substation method, the \nobtained data were aligned and merged to obtain the original point cloud data. We obtained \naccurate point cloud data by data cleaning and accurate resampling of the original point cloud \ndata using professional software for point cloud processing. \n\nData Classification \n\nThe carbon sequestration benefits for different types of vegetation differ (YAO et al. 2017). \nTo improve the accuracy of calculating the carbon sequestration of green space vegetation, \nthis study divided the pre-processed point cloud data into five types according to the vegetation structure and attribute characteristics, namely evergreen trees, deciduous trees, evergreen \nshrubs, deciduous shrubs, and ground cover. The operation comprised two processes: point \ncloud database classification using Trimble RealWorks software and segmentation for individual trees using the watershed algorithm. \n\nIn  the  base  classification  stage,  the  data  were  classified  into  four  groups  of  point  clouds: \nground, high vegetation, building, and remaining, using the automatic classification function \nof the Trimble RealWorks software. High vegetation referred to vegetation with a specific \nheight,  including  trees  and  shrubs,  and  others  referred  to  point  cloud  data  other  than  the \nground surface, high vegetation, and buildings. The point cloud data of buildings and others \nwere not necessary for the calculation of carbon sequestration of green space vegetation and \nthus, were excluded. Because an inevitable error exists in the automatic software classification, it was necessary to manually confirm and filter the surface and high vegetation point \ncloud data and further separate the surface, tree, and shrub point cloud data. Subsequently, \nwe removed the hard ground  point cloud data from the surface point cloud data to obtain \nground cover point cloud data. The growth conditions of trees are complicated, as the tree \ncrowns are interspersed with each other; therefore, manual classification is difficult. To improve the segmentation efficiency, we used a segmentation method for individual trees before \ncategorization.  However,  the  shrubs  in  urban  green  space  are  were  small  in  size,  mostly \nclumped and planted in patches; therefore, classifying them manually into two categories, \nnamely, evergreen and deciduous shrubs, was suitable. \n\nIndividual tree segmentation was performed using the watershed algorithm. First, the previously classified surface and tree point cloud data were used to generate a digital elevation \nmodel and digital surface model using a triangulated irregular network and inverse distance \nweight. The canopy height model was obtained by subtracting the two values. Regarding this, \nwe used the watershed algorithm to obtain the individual tree segmentation results. Subsequently, the segmentation results were identified and categorized by referring to the current \nvegetation distribution map of the site to obtain the point cloud data of evergreen and deciduous trees at the site. While calculating LVV, we considered only the crown part. Therefore, \nthe point cloud data of the tree outside the crown were excluded by segmenting the point \ncloud data. \n\nVegetation Modeling \n\nThe carbon sequestration benefit of a single plant is proportional to its volume (DONG & WAN \n2019); therefore, the calculation accuracy of vegetation volume needs to be improved. Since \nmost trees and shrubs in urban green spaces have complex and irregular forms, and there are \nmany gaps inside the tree crowns, it is difficult to accurately model the plant form and calculate the volume. Therefore, we used the voxelization modeling method for modeling trees \nand shrubs and calculated the volume accordingly. A voxel is an extension of a two-dimensional (2D) pixel in a 3D space (KAUFMAN et al. 1993). Voxelization modeling refers to the \ngeneration of multiple voxels based on the spatial distribution of plant point clouds to simulate plant morphology. Plant volume can be obtained by accumulating the volumes of multiple voxels. This method is highly accurate, convenient, and efficient. For ground cover, which \nis  generally  low  in  height  and  tightly  integrated  with  the  terrain,  carbon  sequestration  is \nmostly calculated based on the surface area (YIN et al. 2020). Point cloud data have a high \ndegree of reproduction in terrain modeling, which can calculate a relatively accurate 3D surface area of the ground cover and reduce the errors caused by the use of the 2D surface area \nto calculate ground cover carbon sequestration. In this study, we used Rhinoceros 3D software to model the surface mesh of the surface point cloud data and then calculated the 3D \nsurface area of the ground cover. The software can avoid the errors caused by calculating the \nsurface area based on 2D projections. \n\nVoxelization Modeling of Trees and Shrubs \n\nVoxelization model generation and volume calculation of trees and shrubs were achieved by \nbuilding components using Rhinoceros 3D and Grasshopper software (Figure 2). The accuracy of the simulated plant volume using the voxelization model is closely related to the voxel \nedge length. Owing to the limitation of the density of the original point cloud and the uneven \ndistribution of the point cloud, a small voxel edge length leads to increased gaps between \nvoxels with subsequent volume loss; therefore, the actual plant morphology cannot be effectively reflected. Thus, the accuracy of the volume calculation is affected. A large voxel edge \nlength results in a coarse simulation of plant morphology, consequently, resulting in a large \nvolume. To ensure modeling accuracy, we constructed a power function relationship between \nthe ratio of plant crown diameter to voxel edge length and plant volume to assist in determining the length of the voxel edge (WEI et al. 2013).Considering that the plant crown diameter \nis essentially constant and has no effect on the power function curve trend, it can be directly \nset to a constant of 1. The equation is as follows: \n\nX=1\/𝑘\nY=Vk \n\nwhere k is the voxel edge length in m and Vk is the volume of the plant corresponding to the k voxel edge length setting in m³. \n\nUrban green space contains a wide range of plant species that vary in form and size. To ensure \nthat the k-value is applied to all trees and shrubs in the sample area, several trees of different \nspecies and several trees of the same species  were selected as samples for the voxel edge \nlength test. To improve the efficiency of the computation, Grasshopper software was used to \nbuild a circular test path (Fig. 3), which enabled automatic voxelization modeling of the plant \npoint cloud data at different k-values, data computation, and an output of a list of X- and Y-\nvalues. MATLAB software was used to visualize the data results and generate analysis curves \nto obtain suitable k-values by comparing the power function curves. The point cloud data of \nevergreen and deciduous trees and evergreen and deciduous shrubs were then used to generate separate voxelization  models using Grasshopper software. Finally, the total volume  of \neach plant type was obtained by multiplying the individual voxel volume by the number of \nvalid voxels in the corresponding voxelization model, which can be calculated directly by \nthe software. \n\nGround Cover Surface Meshing Modeling \nMost of the ground cover in urban green areas grows close to the ground surface, and the \nundulation of the ground surface has a considerable impact on the ground cover area statistics. Therefore, the 3D surface area should be calculated accurately. Triangular grid modeling \nis a commonly used method to model the surface of point cloud data; however, its drawback \nis  that  the  generated  surface  is  uneven,  which  leads  to  a  sizeable  calculated  surface  area. \nMoreover, the ground cover point cloud data will be affected by other plants or ground debris \nduring collection and classification, and the local point cloud has a certain degree of irregular \nundulation, which affects the accuracy of triangle grid modeling. The patch tool in Rhinoceros 3D generates smooth surfaces based on the point cloud distribution and obtains a more \naccurate surface area. Therefore, in this study, we used this tool to model the ground cover \non a grid and used the area statistics tool to obtain the 3D surface area. \n\nCarbon Sequestration Calculation  \nThe total annual average carbon sequestration of urban green space vegetation comprises the \nannual average carbon sequestration of trees and shrubs and the annual average carbon sequestration of ground cover. The carbon sequestration of trees and shrubs was calculated as \nfollows: \n\nCts = Sts × Vts\nwhere  Cts  is  the  average  annual  carbon  sequestration  of  trees  and  shrubs,  in  kg;  Sts  is  the \naverage  annual  carbon  dioxide  absorption  of  trees  and  shrubs  (ZHOU  &  SUN  1995)  in \nkg\/(m2·a); and Vts is the LVV of trees and shrubs, in m2. \n\nCg = Sg× Ag\n\nwhere Cg is the annual average carbon sequestration of ground cover in kg, Sg is the annual average carbon sequestration rate of ground cover (YIN et al. 2020) in kg\/(m2·a), and Ag is \nthe surface area of ground cover in m2. \n\nResults and Discussion \n\nCase Study Background  \n\nThe campus is located in Xuanwu District, Nanjing, China. The specific Mei’an green space \ncase study location is in the northwest corner of the campus (Figure 4), covering an area of \napproximately 4,697 m², of which 3,037 m² is green space. The vegetation structure in green \nspaces is complex, with a multi-level community of trees, shrubs, and grasses, including evergreen and deciduous types. The primary tree species included Platanus acerifolia, Metasequoia glyptostroboides,  Ligustrum lucidum,  Osmanthus fragrans,  Chimonanthus praecox, \nPrunus yedoensis, and Trachycarpus fortunei. In addition, the green space had been planted \nearlier, and after years of growth, the site area was highly dense in vegetation, and the plant \nbranches were intertwined; therefore, it was challenging to identify the point cloud data of \nsingle plants. \n\nData Processing \n\nWe used the Trimble TX8 laser scanner to obtain 3D point cloud data within the study area \nby scanning in a substation manner (Figure 5). The parameters of the laser scanner were set \nto a scanning distance of 340 m, scanning speed of 1 million points\/s, and scanning accuracy \nof ≤2 mm. In total, 79,455,618 points for the study area were obtained from the scanned data. \nAfter applying the pre-processing methods, such as registration and data cleaning, to the point \ncloud  data,  the  automatic  classification  function  of  the  Trimble  RealWorks  software  was \ncombined with manual recognition to separate the ground cover, and evergreen and deciduous shrub point cloud data. The point cloud data of trees were obtained by segmenting individual  trees  based  on  the  watershed  algorithm,  and  the  point  cloud  data  of  62  trees  were \nidentified.  Among  them,  30  evergreen  trees  and  32  deciduous  trees  were  categorized  and \nintegrated to obtain the evergreen and deciduous tree point cloud data for the site (Figure 6). \nTo calculate LVV, the point cloud data for the crown of evergreen and deciduous trees were \nobtained by manually segmenting the trunk point cloud data, excluding the crown. \n\nVoxelization Modeling and Surface Mesh Modeling \n\nThe vegetation modeling for the study consisted of two components: voxelization modeling \nof trees and shrubs and mesh modeling of ground cover. In the voxelization modeling stage, \nseveral samples were first selected to test the voxel edge length k-values and determine the \nspecific values. The diameter at breast height of trees in urban green spaces is mainly in the \n0.05 – 0.4 m range (SHI et al. 2016); thus, the k-value test was limited to 0.02 – 0.5 m. From \nthe point cloud data of a single tree obtained by single tree segmentation, four trees of different species and four of the same species were selected as test samples and imported into the \nRhinoceros 3D software. The k-value cyclic test components built into Grasshopper  were \nthen used to calculate the X- (1\/k) and Y-(Vk)-values at different k-values, and the output \ndata were used to generate the corresponding data curves using MATLAB software (Figure \n7). The results showed that X and Y for samples were all power functions, with the Y-value \ndecreasing as the X-value increased. The convergence point of the decreasing Y-value for \neach sample was located at k = 0.04 – 0.08 m interval. By comparing the voxelization model \nof k-values in this interval with the actual plant morphology, a k-value of 0.06 m was selected \nas the voxel edge length for plant modeling (Figure 8). The determined k-value and tree and \nshrub crown point cloud data were input into the voxelization modeling Grasshopper components to generate the evergreen tree, deciduous tree, evergreen shrub, and deciduous shrub \ncrown point cloud voxelization models. Mesh modeling of the ground cover was carried out \nusing the Rhinoceros 3D software. First, the ground cover point cloud data were imported, \nand the patch tool was used to generate a surface mesh for each ground cover. Second, the \nground cover boundaries were projected onto the surface, and the redundant mesh was cut \noff to obtain a mesh model of each ground cover (Figure 9). \n\nCarbon Sequestration Calculation and Analysis  \n\nWe measured the volume of trees and shrubs and the 3D surface area of ground cover separately, and input them into the formula to obtain the total vegetation carbon sequestration. \nThe corresponding results are shown in Table 1. The total annual average carbon sequestration of vegetation was 3342.86 kg, of which the carbon sequestration of deciduous trees was \n1045.30 kg, accounting for the highest proportion of the total carbon sequestration of green \nspace vegetation. The carbon sequestration of evergreen shrubs was 10.57 kg, accounting for \nthe lowest proportion (Figure 10). On this basis, the carbon sequestration calculation results \nwere further analyzed by visualization methods (Figure 11), which reflected the carbon sequestration benefits of each vegetation type more clearly and intuitively. we observed that \nthe carbon sequestration of vegetation showed uneven spatial distribution, and was more in \nareas with rich vegetation layers, undulating terrains, and large slopes. \n\nConclusion \n\nIn this study, we developed a point cloud-based method for calculating carbon sequestration \nin urban green spaces. Using several software platforms, including Trimble RealWorks, Rhinoceros  3D,  Grasshopper,  and  MATLAB,  as  along  with  voxelization  and  mesh  modeling \nmethods, the classification of green space vegetation point cloud data, plant modeling, carbon \nsequestration calculation, and visualization were achieved. The features and advantages of \nthis method are reflected in two aspects. \n\n1) Calculation accuracy \nIn the volume calculation of plants, the traditional geometric simulation method is limited by \nthe tree species. It considers the crown of a tree as a regular geometry with geometric formulas selected according to different crown shapes. The results of the calculations are inaccurate, and the operation is complex. The use of the convex hull method to calculate the LVV \ndoes not consider the gap within the crown of a tree, but only the outer surface of the tree \ncrown and the accuracy of the results is uncertain. This study calculates the volume of urban \ngreen space vegetation through voxelization modeling, which not only eliminates the limitation of tree crown shape in volume calculation but also can effectively  solve the problem \narising from the gap between tree leaves and branches, resulting in a more accurate calculation.  For  the  calculation  of  ground  cover  carbon  sequestration,  compared  to  the  previous \nmethod of using projected areas for calculation, the use of point cloud data for the construction of a 3D mesh model of the ground surface results in an accurate 3D surface area of the \nground cover. In addition, the method considers the influence of topographic factors.  \n\n2) Process efficiency  \nIn this study, we used Rhinoceros 3D and Grasshopper software to build components as computing modules for batch rapid voxelization modeling and calculation of volume of different \ntree and shrub species. The entire process of this method can be completed by a single researcher. The process is efficient and easy to use as it eliminates the need for time-consuming \nfield data collection and avoids tedious data processing and calculations. \n\nThere are limitations in the data acquisition and processing component of this study owing \nto the limitations of the equipment and technology involved. The density of the point cloud \ndata and voxel size can impact modeling accuracy. The acquisition and processing of ultra-high-precision point cloud data remains challenging owing to the need for scanning and computing equipment. However, contemporary point cloud data processing software lacks accuracy  in  classification.  It  fails  to  identify  plant  species,  requiring  a  combination  of  manual \nscreening for calibration, which reduces the efficiency of data processing to some extent. \n\nWith the advancement of laser scanning equipment and supporting software and hardware \ntechnology, the above problems can be gradually optimized and solved. In the  future, the \napplication of point cloud-based green space carbon sequestration calculation  methods on \nlarger scales can be further explored, for example, by combining unmanned aerial vehicle tilt \nphotography for large-scale green space point cloud data acquisition and carbon sequestration \ncalculation. Various methods for calculating carbon sequestration can also be explored, such \nas calculating leaf area index based on point cloud data or calculating biomass by combining \npoint cloud data with the wood volume method, thus, enabling the calculation of vegetation \ncarbon sequestration. In addition, by measuring and analyzing a large number of plant samples, the construction of a more comprehensive and accurate standard conversion amount of \nplant  carbon  sequestration  and  the  construction  of  a  database  on  the  carbon  sequestration \nbenefits of different tree species need to be studied urgently.  \n\nDigital Representation of Marginal Landscapes: \n‘Agile’ 3D Modeling Workflows for an Italian  \nInner Valley \n\nAbstract: Nowadays, the urban design, planning and environmental management of areas suffering \nterritorial imbalances require exploiting the ever-increasing availability of data about the natural and \nbuilt environments, while promoting stakeholder engagement and empowering local communities. In \nthe framework of an ongoing doctoral thesis, this paper contributes to the topic of digital representation \nof marginal landscapes, presenting three rapid and low-cost 3D modeling workflows relying on available data from different sources. The case study for this experimentation is the Val di Sole, an Italian \ninner mountain valley in the Trentino-Alto Adige region, whose multidimensional representation could \nsupport information management activities relevant to landscape planning and urban design towards \nthe enhancement of its social-spatial resilience. \n\nIntroduction \n\nAn overview of the landscape planning and design tools in Europe – comparing those countries that have adopted the principles of the European Landscape Convention (2000) for their \nterritorial policies – reveals that the prescriptions of the Landscape Plans are represented in \nstatic documents such as the Landscape Charters (SALA et al. 2014). Despite their name, the \ncurrent ‘charters’ are no longer drawn on paper, but, although digital, they still take the form \nof bidimensional maps usually implemented in a Web-Geographic Information System (GIS) \nenvironment, in compliance with cartographic symbolization and generalization conventions. \nCostly to update, the Landscape Charters crystallize the territory’s assets, mirroring the long \ngestation period of the Landscape Plans. The long-term ambition of this research is to promote a shift from the ‘traditional’ 2D Landscape Charter concept towards the use of dynamic \nand responsive 3D landscape representations to support truly informed and participating decision-making processes, envisioning future territorial design at different levels of engagement.  \n\nNevertheless, several researchers (ERVIN 2001, NESSEL 2013, ZHANG 2021) have identified \nan enduring gap between mainstream tools used for landscape design and industry-standard \nmapping and modeling technologies – such as Landscape Information Modeling (LIM) –, \ncapable of producing meaningful information about the natural and the built environments, \nat different scales and levels of detail. Indeed, the idea of a ‘smart’, ‘responsive’ or even ‘sensory’ landscape (ERVIN 2018) is still in its infancy, also because of two relevant challenges \n(MOSHREFZADEH et al. 2020): the networking of distributed information resources (i. e., Big \nData in ROYDS 2018; or even ‘Enormous’ Data in ERVIN 2020), and the integration of realtime information in virtual landscape models, even in raw environmental conditions (i. e., \npoor Internet connectivity, lack of sensors, digital divide, etc.). Indeed, the smart development of natural and built environments are divided into two sets of frameworks: the urban \nframework, already well established (i. e., Helsinki’s 3D city models, Rotterdam 3D, Virtual \nSingapore);  and  the  intermediate,  rural  and  mountain  framework  which  have  only  recently \ngained momentum, but mainly lack technical innovations (i. e., Smart Villages in ZAVRATNIK \net al. 2018). \n\nHowever, the latter is not residual (Figure 1), hosting 58% of the European population for \nliving and working issues (EUROPEAN UNION 2020) and being the target of the last (2014-\n2020) and current (2021-2027) European Cohesion Policies that call for its smart, sustainable, and inclusive  growth. Indeed, climate and  inherent social-spatial vulnerabilities bring \nuncertainty about the capacity of intermediate, rural and mountain areas to achieve sustainable development. The prevalence of exploratory studies related to territorially imbalanced \nareas, mainly concentrated in Asia and in Europe, is reflected in the use of a variety of adjectives – as ‘peripheral’, ‘marginal’, ‘inner’ and ‘inland’ – to refer to these (OPPIDO et al. 2020). \nSpecifically in Italy (the second most active country globally, and first in Europe, in researching practices), the National Strategy for Inner Areas (SNAI) – an innovative territorial cohesion policy which has recently benefited from new funds from the 2021 National Recovery \nand Resilience Plan (PNRR) and the current European Cohesion Policy – aims to counteract \nthe marginalization and demographic decline of these contexts, fostering a sensitive, respectful,  and  sustainable  design-driven  approach  not  to  compromise  their  territorial  resources, \nknowledge, artifacts, and potential uses. \n\nOn the one hand, these areas need the support of digitalization to overcome their territorial \nimbalances and go beyond their infrastructural marginalization. On the other hand, they lack \nhuman and service support to address the most technologically advanced tools. The methodological and operational proposal of this study moves in this critical gap by contributing to \nidentify and test rapid and low-cost workflows for 3D landscape reconstructions in marginal \nareas: the exploitation of available data from different sources can provide realistic visualizations of these territories to support information management activities and help democratize decision-making. \n\nMaterials and Methods for the Val di Sole Case Study \n\nWithin the framework of an ongoing doctoral thesis and the research project of national interest “B4R Branding4Resilience. Tourist infrastructure as a tool to enhance small villages \nby drawing resilient communities and new open habitats” (FERRETTI et al. 2022), the authors \nare investigating the digital multidimensional representation of marginal landscapes, relying \non available data from different sources. One of the four pilot areas of B4R, the Val di Sole \n– an Italian inner valley in the Alpine context of Trento Autonomous Province – is also the \ntesting ground for our investigation towards the development of a cross-sectoral approach for \na virtual-physical system supporting landscape planning and design decision-making, promoting stakeholder engagement and public empowerment. \n\nThe Quali-quantitative B4R Database \n\nSince the representation of landscapes, an interdisciplinary theme cutting across STEM and \nHumanities disciplines, requires the processing of geo-data as well as the depiction of sensory \ninformation  (SALERNO  2019),  a  preliminary  quali-quantitative  exploration  of  the  valley  – \ncoinciding with the first phase of the B4R project – was oriented to gather and reorganize the \nexisting distributed information resources on it, also aiming to solve interoperability gaps. \nThe process was conducted from November 2020 to June 2021 via the collection, categorization, and spatialization of open-source and collaborative data from different databases at \nvarious geographic scales and levels of detail (European, national, provincial, valley community, and municipal) within four main dimensions (infrastructure, landscape, and ecosystems; built and cultural heritage, and settlements dynamics; economies and values; networks \nand  services,  community  and  governance  models)  to  create  a  digital  multi-domain  information profile for the valley, developed in a GIS environment (FAVARGIOTTI et al. 2022). \nThe result is an under-construction atlas of thematic digital maps, diagrams, and cross-cutting \nindicators to comprehend, store, and transmit, among others, the environmental resources of \nsuch a marginal territory – one above all, the thermal water resource (PASQUALI et al. 2022) \n–, identifying landscape heritage values and assets. The outputs of the digital mapping supported the activities of the four-days co-design workshop in the Val di Sole held in February \n2022 – coinciding with the second phase of the B4R project – during which the participants \nproposed  real  answers  to  local  design  challenges  mainly  related  to  maintaining  a  balance \nbetween the protection of valuable natural resources and their fruition during mass seasonal \nflows of tourists (FAVARGIOTTI et al. 2022). \n\nRecalling the premises of this contribution, even if the GIS environment is nowadays the \npreferred way to manage information layers about landscape in the Italian practice, from the \noperative point of view of the landscape planning and design, other approaches must be addressed (i. e., the ‘cloudism’ in GIROT 2020), and different software packages and workflows \nare  required  for  iteratively  modelling,  rendering  and  testing  the  various  design  solutions \n(WISSEN HAYEK et al. 2011). To advance from the 2.5D visualizations of the valley’s Digital \nTerrain Model (DTM), with other data ‘projected’ on it, in the current GIS environment (obtained, for example, with the Qgis2threejs plugin for QGIS software) towards the quali-quantitative B4R database implemented in a proper 3D environment – and thus to support the last \nco-visioning phase of the B4R project –, a landscape truly ‘intelligent’ model of the Val di \nSole is necessary. Namely, it has to promote the use of technology to develop micro-actions \nin accordance with local conditions and citizens demands (CERRETA & FUSCO 2016). \n\nThree ‘Agile’ 3D Modeling Workflows \n\nPrimarily, the updating of the valley topography data is necessary as a base because the digital \nmodels (i. e., the Digital Terrain Model, the Digital Surface Model, and the Digital Building \nModel with a pitch of 1x1 m or 2x2 m, depending on the area) freely distributed by the Autonomous Province of Trento were acquired with airborne LiDAR technologies in 2014 and \nintegrated with other flights in 2018. Also, the digital orthophoto (a 4-band RGBI orthophotomosaic  with  a  ground  resolution  of  0.2  m)  made  available  by  the  province  was  acquired \nthrough an aero-photogrammetric survey in 2014-2015-2016. Since now extensive and resource-consuming survey campaigns to acquire current data are not an option for the valley \ncommunity  nor for the province, the solution is to identify  expeditious and inexpensive  – \nboth in terms of economic resources and computational power – workflows for updated 3D \nlandscape reconstructions, relying on already available data from different sources. \n\nThree relatively rapid and low-cost 3D geometric modeling workflows have been identified \nand tested for virtually reconstructing portions of Peio municipality, in the upper Val di Sole. \nThe first two procedures rely on globally available and periodically updated satellite data, \nwhile the third procedure uses data acquired on a one-off basis and for an originally different \npurpose. As a result of all three procedures, a textured 3D mesh surface is obtained. \n\n1) ‘Automatic’ geometric modeling from the cloud by importing elevation data (i. e., terrain, buildings) of the selected location through the Lands Design plugin for Rhinoceros \nsoftware  and  obtaining  a  textured  mesh  surface  with  3D  buildings  (Figure  2a).  This \nworkflow requires an internet connection and about ten minutes of time; the main limitation is the extension of the exported rectangle of data whose long side can not exceed \n10 km. The source and details of the data can be directly set from the Lands Design tab \n‘Import Earth's elevation data’ (in this case, a satellite image from Jaxa, with accuracy \n100 and distance 10 m\/sample – for both parameters, the highest possible values were \nselected); the date of satellite image acquisition is not clearly declared, we can assume \nthat it is updated to 05\/01\/2022 at the latest, when it was imported. \n\n2) Image-based geometric modeling from Google Earth data by acquiring a video tour of \nthe selected location through Google Earth Pro software, processing frames in cloud with \nAutodesk ReCap Photo software, and obtaining a textured mesh surface to be imported \nin a proper 3D environment (Figure 2b). This workflow requires an internet connection \nand about one hour of time, but this may vary depending on the number of frames to be \nprocessed and the desired output quality (in this case, 97 frames – extracted one frame \nper second from the video tour – were processed with default setting in Autodesk ReCap \nPhoto). The source and details of the data used is clear from Google Earth Pro (in this \ncase, a satellite image from Landsat\/Copernicus, acquired on 24\/09\/2021). Note that, to \ndate, Google Earth can be used for research purposes without needing permission but its \nTerms of Service generally forbid to use its output to reconstruct 3D models. \n\n3) Image-based geometric modeling from UAV video by processing frames with Metashape \nsoftware and obtaining a textured mesh surface to be imported in a proper 3D environment (Figure 3). This workflow required about one half an hour of time, but this may \nvary depending on the number of frames to be processed and the desired output quality \n(in this case, 30 frames – extracted one frame per second from the video – were processed \nwith default setting in Metashape). The data were acquired by the B4R project’s official \nphotographer, Mr. Nicola Cagol, on 05\/11\/2021 during his photographic campaign. \n\nDiscussion \n\nAlthough some of the presented techniques were originally developed for photogrammetric \napplications (i. e., the image-based geometric modeling), this contribution focuses on their \nability to support the updated 3D documentation of the Val di Sole and improve the visualization of information from existing databases and available sensors, rather than on the accuracy of the result. \n\nIndeed, the textured 3D mesh surfaces of valley’s portions are obtained without conducting \nprofessional survey campaigns and could now be managed in a proper 3D environment (as \nRhinoceros software, having many specific plugins for the purpose) for iteratively modelling, \nrendering, and testing various landscape design solutions. These digital models are relatively \neasy to obtain because, as illustrated, the procedures involve few steps and software commonly used in the field of digital landscape architecture (i. e., Lands Design, Rhinoceros); \nalso, regarding the use of specific photogrammetric reconstruction software that may be less \nfamiliar  (i. e.,  Autodesk  Recap  Photo  and  Metashape),  the  proposed  workflows  follow  a \nstandard procedure – loading photos, aligning photos, building dense point cloud, building \nmesh, building model texture – with default setting. Finally, in the case study of the Val di \nSole’s inner area, the resulting models benefit from more up-to-date data than those distributed by the province, although they do not reach that level of detail and control. Nevertheless, \nsince the requirements of quality and metric precision for the output can sometimes be more \nrelaxed in landscape planning and design applications, the actualization of information technologies’ affordances in intermediate, rural and mountain areas may rely also on citizen participation that would ensure extra data and information, as in the third presented workflow in \nwhich data acquired during a documentary photographic campaign were used for modelling \npurposes. \n\nSince this investigation about landscape ‘intelligent’ models is still ongoing as part of a doctoral thesis, the 3D mesh surfaces generated with the proposed workflows are not yet linked \nto the semantic information stored in the quali-quantitative B4R database, mainly because of \nthe open issues related to Landscape Information Modeling (LIM). However, the digital mapping, oriented at increasing awareness of individuals and organizations about the use of environmental resources in the Val di Sole, proved to be an effective information tool – both \nduring and after the B4R co-design phase – by prompting the inclusion of the (thermal) water \nelement in the communication about the valley by territorial promotion agencies. \n\nConclusion and Outlook \n\nNovel opportunities are emerging due to the ongoing digital transformation and the rise of \nsmart technologies, which promise the implementation of ‘intelligent’ solutions for landscape \nplanning and urban design, even in areas suffering from territorial imbalances. However, in \ncomplex and poorly infrastructured systems as inner areas a smart development has to follow \nlocal conditions, knowledge and needs because of the lack of adequate technological tools, \nnetworks, and structures. This contribution illustrated how the digital mapping and modelling \nof marginal landscapes can effectively rely on already available data from different sources, \nin addition to the official data provided by public administrations which here are often outdated: specifically, elevation data (i. e., terrain, buildings) from remotely sensed images can \nbe used in expeditious and inexpensive workflows for 3D landscape reconstructions. Moreover, the global availability of smartphones, cameras and drones has transformed even nonprofessionals citizens into ‘sensors’: images collected and shared by tourists could be used \nto improve results obtained using remotely sensed images by integrating different point-cloud \nmodels from collaborative photogrammetry, also enabling the monitoring of places, with particular regard to local resources exposed to hazards. \n\nThe long-term vision of this research goes towards the application of the Digital Twin (DT) \nparadigm (BATTY 2018) to marginal landscapes: beyond the buzzword, a loop in which data \ncollected in real-time from the real world are processed to inform a virtual counterpart and, \nat its highest level of ‘maturity’, allow autonomous operations. Originally born in the field \nof production engineering, DTs are now applied to towns, cities, and even nations. The idea \nof  a  Territorial  Digital  Twin  (TDT)  –  both  a  three-dimensional  queryable  repository  of \nknowledge (i. e., an information system), and a simulator (i. e., a model) of more resilient \nfutures for the Val di Sole – requires further development but would contribute to closing \ngaps in strategic planning and process management at the landscape scale, also overcoming \nissues connected to the digital divide. It could support operative actions in collaborative decision-making sessions towards the management and design of complex territorial transformations from a truly holistic and integrated perspective. Indeed, by improving accessibility \nof distributed data and making the fruition of information more intuitive and interactive, in \nfuture research TDT could assist understanding of risk-related components in the territory by \nlocal stakeholders, prompt concerted spatial planning actions, and help assess if investments \nare risk-informed. \n\nDo Green Infrastructure Types Represent Land \nSurface Temperature? A Case Study of Stuttgart \n\nAbstract: To combat urban heat islands, comprehensive data on how urban morphological configuration affects temperature is needed. Remote sensing offers diverse products to characterize land surface \nand  to  monitor  its  character, and  evaluate  changing  urban  structure  typologies.  Green  Infrastructure \nTypology (GIT) by Bartesaghi-Koc et al. (2019) is a relatively new typology that combines physical \nstructure (vegetation strata, built structures and water) and spatial descriptors of urban structures as a \nbasis to describe urban climate parameters. Using the city of Stuttgart, Germany as a case study, we \nassessed the land surface temperature (LST) of GIT classes. Our results indicate that GIT can be adapted \nfor Stuttgart and be used as an approximation to identify urban heat islands and cooler areas, making it \na valuable tool for planning resilient and sustainable cities.  \n\nIntroduction \n\nIncreasingly, people are moving from rural areas to urban centres leading to the densification \nof cities and competition for space. These processes modify the landscape structure as natural \ncover is replaced with artificial surfaces, thereby altering ventilation and modifying the heat \nbalance (SANTAMARIOUS 2015). Such changes, in combination with Global Climate Change \n(GCC), lead to warming cities and can dangerously affect human well-being (NGARAMBE et \nal. 2022). Urban heat mitigation is a key issue for governments, scientists and planners world-wide (MOHAMMED et al. 2022). An important strategy for urban heat mitigation is to foster \nurban  resilience  and  sustainable  development  with  Green  Infrastructure  elements  such  as \nparks, lawns, trees, green roofs, green facades, and their effective inclusion into the urban \nmatrix. \n\nVegetation reduces land surface temperature (LST) through shading, advection and evapotranspiration,  and  mitigates  the  surface  urban  heat  island  (SANTAMOURIS  2015).  Satellite-\nbased remote sensing LST products have helped monitor and investigate micro-urban heat \nislands and their relationship with vegetation cover for at least two decades (ANIELLO et al. \n1995, KAPLAN et al. 2018, RANAGALAGE et al. 2017). Today, such data is easily available \nand accessible with global coverage and high temporal resolution (e. g. 16 days for Landsat). \n\nAssessment of the thermal benefits of urban vegetation, which is embedded in a complex and \nheterogeneous environment, requires a holistic analysis that considers the different types of \nvegetation strata, land cover proportion and spatial arrangements, as well as human-meteorological effects. Structural typologies of urban land surfaces help to translate urban and human-meteorological complexity into planning suggestions. With this aim, many typologies \nand protocols have been proposed, such as the Local Climates Zones (LCZ) by STEWART & OKE (2012), the High Ecological Resolution Classification for Urban Landscapes and Environmental Systems (HERCULES) suggested by CADENASSO et al. (2007), the Urban Vegetation Structure Types (UVST) introduced by LEHMANN et al. (2014) and more recently the \n\"Green Infrastructure  Typology  (GIT)\"1  by BARTESAGHI-KOC et  al. (2019a).  GIT,  though \nconceptually  similar  to  the  other  approaches  mentioned,  is  constructed  with  simpler  landscape \nmetrics and is easier to apply across multiple spatial scales.  \n\nGIT was essentially formulated for climate aspects, but can also support other performance-\nbased  analyses  across  different  ecosystem  services. Various studies on Sydney, Australia, \nhave successfully evaluated and compared the intra- and inter-type variability of land surface \ntemperatures  (LST)  (BARTESAGHI-KOC et  al. 2019a).  These  studies  have  demonstrated  its \ncapacity  for  fast  identification  of  hotspots  to  prioritise  urban  areas  for  heat  mitigation \n(BARTESAGHI-KOC et al. 2019b ) and have assessed the thermal seasonal behaviour of urban \nlandscape configurations (BARTESAGHI-KOC et al. 2020). \n\nMotivated by its simplicity, integral vegetation-oriented approach and the positive results of \nprevious climate-oriented studies, we adapted and applied the GIT approach for Stuttgart and \nevaluated the thermal behaviour of its types. The current study aims to answer the following \nquestions: (a) Is GIT capable of catching the urban structural variability of Stuttgart? (b) Does \nGIT reflect land surface temperature (LST) differences as a basis for indicating heat stress \nand heat relaxation? and (c) Does the applied size of the spatial units for applying the typology matter?. We hypothesize that an analysis of GIT is a good predictor of LST. We test that \nby applying a simple statistical analysis of spatial coincidence. \n\nDetection of Vegetation Structures \n\nIt seems simple, but a comprehensive inventory of urban vegetation structures requires the \ncomplex merging of data sets that are built up and maintained with very different intentions. \nThree of the most fundamental datasets are: (1) green space, tree and forest cadastres, (2) \nbiotope mappings and (3) surveys of roof and facade greening.  \n\nIn the context of blue-green infrastructure, vegetation structures are considered in terms of \nthe preservation of the vitality of vegetation under climate change conditions. Here, details \nof vegetation structures and the requirement to have a citywide inventory play an important \nrole, and there is a need for information on vegetation composition and height. Green volume \nand leaf area index have become urban GCC adaption quality assessment indicators (VOLK \net  al.  2022).  Both  provide  a  basis  for  a  rough  assessment  of  the  temperature-regulating \nevapotranspiration capacity of the vegetation. As a pragmatic solution in a large-scale study, \nthe combination of height and growth form or morphology types (e. g., arboreal, shrub, perennial, perennial vegetation, turf) are considered sufficient indicators.. However, the more \nwe address the climate-vegetation relation on a local scale, the more detailed and species-\nspecific the characterisation of the plant communities should be. \n\nAs  a  part  of  the  project  “Integrated  strategies  to  strengthen  blue-green  infrastructures \n(INTERESS-I)”  (https:\/\/www.interess-i.net\/),  object-based  image  classification  methods \n(e. g.  PLATT & RAPOZA  2008)  were  used  to  detect  vegetation  objects  at  very  high  spatial \nresolution (0.5 m) using both spectral and structural data. The spectral information refers to \n0.5 m resolution Pleiades-I images from the10th and 16th of May 2017, acquired as an ESA's \nThird-Party  Mission  within  the  ESA  TPM  project  research  ID  49634.  The  structural  data \nwere extracted from Light Detection And Ranging (LIDAR) data captured in 2016 (average \nfirst return point density 16.6 pts\/m2) and provided by the Landesamt für Geoinformation \nund Landentwicklung, Baden-Württemberg (LGL). The classification and preprocessing of \nthe data was carried out using the software eCognition developer 10.0. Due to missing data, \nonly 89 % of the city area could be classified, covering 184 km2 (hereinafter referred to as \nstudy area, shown in Figure 1). \n\nSeven land cover types were separated at a high level of accuracy2: water, no vegetation (i. \ne. buildings, streets, bare soil) and five vegetation classes that include three ground vegetation \nclasses – grass or low (≤ 1 m), bushes or medium (1 m – 2.5 m) and trees or high (>2.5 m) – \nand two roof vegetation classes – grass or low (≤ 1 m) and bushes\/trees (>1 m ) (Figure 1). \n\nLand Surface Temperature: Identifying Heat Spots and Cooling Islands \n\nUrban heat island (UHI) is defined as an urban built-up area that is significantly warmer than \nits surrounding rural countryside due to anthropogenic activities. This can be studied by two \nclosely  coupled phenomena:  the surface urban  heat  island (SUHI)  and the  meteorological \n(atmospheric) urban heat island (PHELAN et al. 2015). In particular, SUHI is governed by \nlocal aspects that depend on the physical properties of natural and man-made materials constituting the urban morphology (BARTESAGHI-KOC et al. 2020).  \n\nLST as a spatially distributed measurement is used to assess SUHI. It captures the differential \nthermal behaviour of surface materials. Built-up structures, roads and other sealed surfaces \nstore heat and accentuate heat islands, while vegetation and water surfaces promote cooling \neffects through evapotranspiration, shading and modification of heat exchange through advection (SANTAMOURIS 2015). Therefore, a thematic map of LST indicates the location of \nwarmer or cooler areas and should be included in all urban meteorological planning records. \n\nThe LST data for our study area were obtained from validated products provided by Landsat \n8 (https:\/\/earthexplorer.usgs.gov\/ collection 2 level 2). The images were acquired on 18 of \nJuly 2017 around 10am and delivered at 30 m resolution. They were scaled to Kelvin using \nthe scaling factors of the product and then rescaled to degrees Celsius (USGS 2022). The \noriginal rasters were cropped to the study area and clouds were masked out (one small one). \nLST was then aggregated by the mean to raster layers at 60 m and 90 m resolution. \n\nArea Typing: Morphology Makes Climate \n\nGIT Basics in a Nutshell \n\nThe Green Infrastructure Typology (GIT) from BARTESAGHI-KOC et al. (2019a) is a holistic \napproach  overarching  principles  of  multi-  functionality,  connectivity  and  dynamic  spatio \ntemporal heterogeneity (BARTESAGHI-KOC et al. 2020). The scheme comprises 34 standard \n\nGreen Infrastructure Types (GIt, plural: GIts), each defined by a unique combination of physical characteristics of land surface (proportion and composition of surfaces: water, imperious, \ngrass, bush and trees) and spatial descriptors of tree patterns (patch elongation, CIRCLE_AM3; \nand patch clumpiness, nLSI4 as described in MCGARIGAL (2015)). Each of the GIts is assigned to one of the following broad groups: a) 'Impervious areas with a high proportion of \nsealed surfaces and\/or buildings', b) 'Mixed areas comprising a variety of biotic and abiotic \ninfrastructure',  c)  'Pervious,  natural  areas  with  minimal  anthropogenic  structures'  and  d) \n'Aquatic areas with a significant proportion of water bodies'.  \n\nIf the typing is available as a spatial assignment to units like cells in a regular grid, human-\nmeteorological parameters, such as the cooling effect, can be spatially assigned to the units \nat  a  rank  scale  (BARTESAGHI-KOC  et  al.  2020).  Therefore,  GIT  can  guide  GCC-adaption \nmeasures, the restructuring of urban morphology or climate-adapted urban development. \n\nImplementing GIT in Stuttgart City \n\nWe divided the study area (see Figure 1) into grid cells at 30 m (division is abbreviated as \n30G), 60 m (60G) and 90 m (90G) resolution to calculate the metrics and assign the GIts. For \nthe implementation of GIT in Stuttgart, three changes needed to be introduced.  \n\nFirst, the original GIT distinguishes between irrigated and non-irrigated grasses, while our \nclassification does not – there is only grass (details in Table 1). \n\nSecond, the GIT for the study area also includes roof greening, which like the ground vegetation has cooling effects (ASADI et al. 2020). Low roof vegetation was considered as grass \nand higher roof vegetation as trees.  \n\nThird, the original GIT, though effectively classifying around 98% of the study area, fails to \nclassify around 400 ha in the core urban area. After a careful inspection of the unclassified \nareas, two important adoptions were implemented: (1) The upper limit of the trees percentage \nof classes IM4c, IM5c, IM6c was increased by 10 %, MX2c, AQ4c by 15 % and MX5c by \n25 %, while ensuring the main characteristics of the GIts were maintained. (2) River Neckar \nport was introduced as a type called \"highly impervious with water\" (HA). The adoptions \nlead to a decline of unclassified land to less than 0.3 %.  \n\nFinally, we applied a GIT consisting of 30 GIts (Tab. 1). Figure 3 shows the GIts percentage \noccurring in the study area and the mean distribution of land covers per GIt at 60G. These \nresults are very similar for the 30G and 90G divisions. \n\nCharacterising Land Surface Temperatures of GIts \n\nThe characterisation of the GIts by LST focuses on cooling effects and on whether grid resolution plays a role. These questions were targeted specifically for the urban area, thus all \nforest and agricultural areas greater than 4 ha were excluded from the analysis (Fig. 1).  \n\nThe total assessed area was 95 km2, 88 km2 and 82 km2 for 30G, 60G and 90G respectively. \nOnly the GIts represented by at least five samples were taken into account for this analysis. \n\nThe cooling effect (Ceff) of each GIt was calculated as its median temperature (mGIt) minus \nthe median temperature of the highly impervious GIt (mIM1): \n\n(Eq. 1) \n\n𝐶eff = mGIt − mIM1\n\nBy comparing the LST of the different GIts, it becomes evident that temperature levels of the \nmain  groups vary  from  warmest  to  coldest  according  to the  sequence: impervious-mixed-\npervious-aquatic (Fig. 3).  \n\nInspired by the study conducted by ZHOU et al. (2011), we hypothesised that the resolution \nof the grid cell size chosen for analysis influences the result. Our results show that the temperature difference between some types becomes much more evident in the 60G and 90G \nthan in 30G division (for example MX10, PV2, PV3) (Fig. 4). Similarly, the results indicate \nthat though in 30G Ceffs of the different GIts can be detected, this is less pronounced than \non  larger grid sizes  (Fig. 5). Additionally,  the data  dispersion  and  the number of  extreme \nvalues reduce with increasing grid size (Fig. 5). All these findings are in line with ZHOU et \nal. (2011) who argue that much of LST variance is explained at coarse resolution by land \ncover composition, while more factors need to be considered to understand the behaviour of \nLST at finer scales. \n\nFigure 5 allows for the identification of differences among the cooling capacities of the GIts. \nWithin the impervious types, the GIts with the largest percentage of tree cover (IM4c, IM5c, \nIM6c) consistently reveal a cooling effect of approximately 2 °C. Similarly, IM3, with the \noverall highest percentage of vegetation in the group, reached 1.9 °C cooling at 30G.  \n\nIn general, the aquatic group presented the highest cooling capacity. The maximum cooling \ncapacity is located in AQ1, reaching Ceff = 9.6 ºC in 90G. Next, the tree-dominated GIts \nshow Ceff between 5.6 ºC and 8.4 ºC. The remaining GIts of the group have lower Ceffs in \naccordance with the increase in impervious cover and decrease in tree cover.  \n\nIn the mixed GIts, three groups can be identified: GIts with (a) minor cooling capacity (Ceff \nbetween 0 °C and 2.6 °C) having low tree cover (average below 3 %) comprising MX2c, \nMX3,  MX4,  Gits  with  (b)  significant  cooling  capacity  (Ceff  between  2,7  °C  and  4,1  ºC) \nhaving medium tree cover (average 20 % to 40 %) and a medium percentage of impervious \narea  (approx. 40 %)  consisting of  MX5c, MX7,  MX9  and  GIts with  an  (c)  extraordinary \ncooling capacity (Ceff between 3,9 °C and 6,5 ºC) having high tree cover (average above 50 \n%) and a low percentage of impervious surface (average below 26 %) including MX6, MX8, \nMX10.  \n\nIn the pervious group, PV11 shows the highest cooling capacity in all resolutions, reaching \nCeff = 8.2 °C at 90G. As in the other main groups, the Ceff of the GIts with more tree cover \n(PV6, PV8, PV10) are the highest, averaging a Ceff of 5.1°C. Nevertheless, PV2 (mostly \ngrasses) and PV3 (mostly grasses and bushes) also achieve Ceff up to 4.6 ºC. PV4, associated \nmostly with urban vineyards, showed an astonishingly low cooling effect. This affect is attributed  to  the  sun  exposition of  the  terrain. In Germany,  grapes  are  mainly  cultivated on \nsouth-facing slopes to obtain the most sunlight and heat. As a result, the topographic position \nof PV4 makes it warmer and masks the cooling effect of vegetation. \n\nThe results show clear differences in the cooling capacity of the GIts driven mostly by their \nland cover composition. In general, the high cooling capacity of areas with significant tree \ncover indicates that this vegetation type is the most important for urban cooling as indicated in other studies (ZAIN ET AL. 2015). Besides trees and water surfaces, considerable cooling \neffects was achieved by large extensions of other vegetation covers. However, such configurations are rare in urban development plans due to high competition for space. \n\nConclusions and Recommendations \n\nOur study shows that GIT is a good predictor of LST and can be used as a proxy to urban \nheat island detection. \n\nThe study shows that the Stuttgart consists of a big range of various GIts. The applied GIT \nsufficiently typecasted structural arrangements and morphologies that are common in European cities such as Stuttgart, Germany. It's simple approach allows for adaptations like the \nincorporation of previously unincluded classes (e. g. river port type) or the extention of class \nthresholds. \n\nRegarding GIT implementation, attention must be paid when selecting the resolution for analysis. In the case of LST analysis, the thermal effects are stressed at larger evaluation units, \nbut there is a higher risk that some GIts will not be represented.  \n\nUsing GIT in conjunction with free satellite products such as Landsat is an excellent option \nfor evaluating the thermal behaviour of urban arrangements. Moreover, these images' regular \navailability could be used to track GIts throughout the year and for several time intervals. \nThis  would  allow  for  the  assessment  of  changes  in  their  thermal  characteristics,  which  is \nimportant since vegetation is a living and dynamic component of urban structures. \n\nOur results confirm that trees and water surfaces have considerable cooling effects. This can \nalso be achieved with large areas of other vegetation cover, however, such configurations are \nnot the most desirable in city planning due to the high competition for urban space. \n\nAlthough topographic effects on the characterisation of GIts in terms of LST were not evaluated, the results suggest that sun exposition is a relevant factor. We therefore suggest that \nfuture studies carry out LST analyses of GIts using a stratification according to topographic \nvariables (aspect, slope, elevation). Such studies could not only indicate favorable configurations, but also which are more appropriate according to their location within the terrain. \n\nFocusing solely on the relationship between LST and GIT is not representative the complex \ninteraction between cold and heat islands. Ventilation per se and fresh air exchange through \ncirculation  are  important for heat  stress relaxation. Nevertheless,  the  use  of GIts  in urban \nplanning, as a quick screening method to roughly predict the thermal behaviour of existing \nand planned structures, can contribute to the design and simultaneous assessment of interventions. This is particularly relevant when competition for space leads to the necessity to \ncontrol the efficiency and sufficiency of GCC adaption plans. \n\nDoes the Past Influence the Present or the Future? \nDeep Time, Land Use, and Remote Sensing in  \nSouthern Mexico \n\nAbstract: This paper reports the initial results of a pilot study investigating the relationships among \nlong-term land use, settlements, historic population, and their potential influence for understanding and \nevaluating current and future land use. Most of our work to date has been focused on evaluating changing patterns of historic settlement and its relationship to what we know about the historic environment \nand landscape. Here, we instead rely on remotely-sensed big data as a first step to see how patterns of \npast land use are correlated with what we know about current land use and land cover. The pilot study \ninitiates a broader research agenda that better incorporates what we know about past landscapes into \ncontemporary land use decisions and to offer critical insights into how the future could be shaped by \nintegrating information about the past. As a first step, the analysis is intentionally broad so that our next \nsteps can provide the fidelity and resolution to offer place based information for design and planning. \nNevertheless, it offers a unique window of perception into current land use and a platform for operationalizing evolutionary uses of the past for better managing, designing, and planning complex land \nsystems and moving beyond analogic uses. \n\nIntroduction \n\nIn 1973, Billie Lee Turner carried out a transformative research program to document terracing, agricultural intensification, and a chronology of ancient land use in the Southern Maya \nlowlands in Mexico. Based on research in the Rio Bec region of the southeastern Yucatan \npeninsula, T URNER (1974)  concluded  that intensive  agriculture  in  the form  of  agricultural \nterraces and raised fields must have been widespread throughout the Maya lowlands, linking \nideas about agricultural intensification to BOSERUP’s (1965) model of agrarian change. Beyond challenging long-held slash-and-burn (swidden) based models for Ancient Maya agricultural  practices,  TURNER’s  (1974)  work  was  influential  in  two  key  ways:  (1)  he  tightly \nlinked concepts of intensification with population pressure and ideas about Maya settlement \ndensities, and (2) he chronologically connected these systems to the ‘peak’ of the Ancient \nMaya Civilization (250 to 850 CE). Subsequent studies of the Ancient Maya also used this \nwork to consider the collapse of Maya civilization as a consequence of population pressure \nand the over-exploitation of resources across the region (WEBSTER 2002). \n\nSeveral decades later, DEARING et al. (2010) asserted the critical value that deep-time perspectives offered for understanding complex land systems and assessing their future. Their \ntransformative work concluded that understanding the interactions between communities and \nenvironmental processes within a complexity framework is the critical foundation for anticipating the future of land systems and planning. Two uses of the past to inform land systems are described in their paper, analog and evolutionary.  \n\nAnalog perspectives are commonly promoted as narratives for learning about the past. They \noffer snapshots in time about the past. They do not directly link the past to the present and \ncan be limited in their direct application for assessing current conditions and planning future \ncomplex systems. For example, numerous studies of the past have been used to describe processes  of  collapse  due  to  land  stress  or  challenges  of  sustained  landscape  management \n(MURTHA 2009, TAINTER 2006, WEBSTER 2002). While much of our work has been focused \non developing those narratives, here we take a first step towards a different approach. \n\nIn contrast, DEARING et al. (2010) suggest that an evolutionary view of the past across long \ntimescales offers an essential perspective for assessing current and future land systems because the present is tightly coupled to the past. Theoretically, this connection allows scientists, designers, and planners to address long timescale processes, like challenges of sustained \nfertility or large-scale climate patterns (e. g., El Niño) that are repeated regularly. According \nto their paper, evolutionary views provide a better understanding of the relationships between \nfast and slow processes. Furthermore, studying past legacies may also provide a more complete  understanding  of  the  cultural  and  environmental  elements  influencing  contemporary \nland use and settlement system dynamics (DEARING et al. 2010). But there are no clear methods for operationalizing the past for design and planning in this way. \n\nOur paper reports the initial results of a study across southern Mexico as an attempt to investigate  a  connection or  a disconnect between  past  land  use  intensity  and  current  landscape \nchange. Relying on remote sensing and geospatial modeling, we first summarize the analysis \nof 458 samples of environmental LiDAR documenting details about deep past land use, primarily relying on evidence of settlement and agricultural intensification. Using these samples, we calculate a built environment index for small watersheds across our sample geography. We then investigate the three decades of land use and land cover between 1992 and 2012 \nto quantify whether past land use intensity correlates to recent landscape change and how \nthese changes compare spatially. We aim to use these data not only to compare our inventory \nof the past to the current complex land systems of southern Mexico, but also to speculate how \ndeep time perspectives can be better integrated into geodesign, studies of land systems architecture, and territorial scale design and planning (TURNER et al. 2013). \n\nData and Methods \n\nThe primary historic data used in this study were collected by the NASA Goddard Space \nFlight  Center,  led  by  Dr.  Bruce  Cook,  using  NASA  Goddard’s  LiDAR,  Hyperspectral  & \nThermal  Imager  (G-LiHT)  system  (COOK  et  al.  2013,  GOLDEN  et  al.  2016,  HERNÁNDEZ-\nSTEFANONI  et  al.  2015).  The  mission’s  primary  objective  was  to  refine  measurements  of \nabove-ground forest carbon stocks in Mexico (see GOLDEN et al. 2016). The data were collected in April 2013 as part of a multi-institutional, bi-national study of above-ground biomass  (AGB)  and  species-richness  that  covered  large  swaths  of  Mexico  (HERNÁNDEZ-\nSTEFANONI et al. 2015). The research was designed to inform deforestation programs, including the United Nations REDD+ (Reducing Emissions from Deforestation and forest Degradation, plus conservation, sustainable management of forests, and enhancement of forest carbon  stocks),  and  to  aid  in  the  design of  effective  strategies  for  selecting  natural  protected \nareas (see http:\/\/www.un-redd.org\/aboutredd). When we processed and analysed bare earth \nmodels of these data, we were taken aback not by lost ancient megacities but by the complex \ndistributions of ancient households and associated landscape features. \n\nNASA captured and processed 610 LiDAR samples over southern Mexico, ranging in size \nfrom 3 ha to 4100 ha (GOLDEN et al. 2016). Importantly, these flight tiles connect well-sampled areas, such as the Central Maya lowlands, with western regions like the Usumacinta \nbasin that have been more sparsely surveyed (see figure 1 in GOLDEN et al. 2016). They also \nconnect coastal sites to landlocked upland sites reliant on rainfed agriculture throughout history. In prior papers, we documented the important structure and variety of these samples and \nwhy  they  are  unique  and  important  to  studies  of  the  Ancient  Maya  (GOLDEN  et  al.  2016, \nSCHRODER et al. 2020).  \n\nWe processed 458 samples and documented five (5) distinct, durable feature types as polygons: 1) structures, 2) platforms, 3) plazas, 4) aguadas or borrow pits, and 5) causeways and \nfive distinct (5) feature types as polylines: 1) architectural terraces, 2) agricultural terraces, \n3) walls, 4) canals or ditches, and 5) paths (SCHRODER et al. 2020). For this paper, we included agricultural terraces, walls, platforms, plazas, and structures as a currency for our built \nenvironment index. We documented 8,763 terraces, 10,287 walls, 3,659 plazas, 3,244 platforms, and 54,488 structures across the geography of the G-LiHT samples (SCHRODER et al. \n2020). \n\nIn terms of ancient settlement and land use, we observed unexpected patterns. Fifty-nine percent of samples analysed and located within 10 km of a recognized ‘large’ Ancient Maya city \nexhibited very high or high densities (upper 40%) of ancient structures and platforms. This \nconfirmed much of TURNER’s (1974) early interpretations about the relationships between \npopulations and agrarian change. But we also found that 38% of samples more than 10 km \nfrom a ‘large’ Ancient Maya city also exhibited very high or high densities, which implies \nan inverse relationship between population density and ancient city centers, contrasting observations from most regional LIDAR studies focused on lost cities and monumental architecture (CANUTO et al. 2018). \n\nFurthermore, overall structure densities from our analysis, which are used by archaeologists \nto reconstruct past population histories are higher than recent LiDAR only surveys in Western \nBelize and in the Peten, Guatemala, traditionally considered to be the densest area of Maya \nsettlements (CANUTO et al. 2018). These results are especially significant due to the distinct \nsampling strategies used in each study. All prior studies of LiDAR focus on surveying large \npolitical centers, but the NASA G-LiHT samples we inventoried capture a broader regional \nand territorial settlement and landscape. \n\nFinally, when we compare the distribution of ancient residential architecture to the distribution of ancient agrarian features, like terraces and walls, we noticed important but unexpected \npatterns. Extending a Boserupian framework, we expected high densities of intensive agricultural features to correlate with high densities of ancient structures, plazas, and platforms. \nArtifact features and structures are currencies of past population densities, commonly used \nto reconstruct population histories. Yet, there is not a uniform correlation between past settlement density and agrarian features. For example, many areas with high densities of structures exhibit little or no evidence of agricultural intensification, while we also identified areas \nwith terraces and walls with low densities of residential architecture.  \n\nOur  work  to  date  has  been  almost  exclusively  focused  on  how  these  data  can  be  used  to \nunderstand past distribution of populations and their relationship to regional environmental \nsystems. But another key question has been at the forefront of our research, does the legacy \nof the past influence the present or the future? Or better, can the legacy of the past inform \nwhat we plan and design moving forward? \n\nTo start the process of this inquiry, we first summarized the intensity of past land use, focused \non the built areas of our samples. For every structure, wall, and terrace, we established continuous polygons representing areas of built features, similar to a layer of modern impervious \nsurface. We compared these areas to the overall sample areas of the LiDAR to calculate a \nsimple built environment index (or ratio of built to unbuilt). Simply, the index captures the \nimproved area compared to the unbuilt. We then summarized each sample by intersecting \nthese samples with global L12 small watersheds. We examined the spatial distribution of the \nbuilt environment across the lowland geography and finally compared it to what we know \nabout  modern  land  cover  changes  between  1992  and  2012,  relying  on  a  macro  dataset  of \nGlobal Land Cover, published by the European Space Agency (ESA 2017).  \n\nResults \n\nDistribution of Past Built Environment  \n\nThe 458 samples we analysed crosscut 385 small watersheds from Chiapas, Mexico north \nthrough Campeche, continuing to the coasts and the modern states of Yucatan and Quintana \nRoo (Fig. 2). Summarized by watershed, some clear patterns of built environment intensity \nemerge from our analysis. While there are pockets of high density across the samples, there \nare clear hot spots of ancient settlement and land use, largely focused in the central Yucatan \nand northwest into the modern state of Campeche.  \n\nThree hundred and twenty small watersheds were impacted by ancient settlement and land \nuse. The majority of the watersheds exhibit very little development and impact (< 5 %), but \nseventy-two watersheds exhibit more than 10% developed, while only 5 % of the watersheds \nare impacted with development covering greater than 25 % of the total sample for each watershed. A handful of watersheds are impacted at levels exceeding 50 %. When compared to \n\n\nwhat we know about modern watersheds in general, levels of impervious surface with as little \nas 5 to 10 % can degrade water quality, while greater impairments are expected with levels \nat 20 to 25% (NOAA 2023). Simply, while there are clustered patterns of expected impairment by ancient settlement and land use, the impacted watersheds are dispersed and almost \nexclusively located in the central and northwest Yucatan, with almost no coastal impact. \n\nComparing Spatial Patterns of the Past with the Present  \n\nWe then compared the sampled small watersheds to land cover data processed by the European Space Agency (ESA 2017). This global dataset is a critically useful resource to investigate large-scale land use and land cover changes with a standardized global resolution of 300 \nm per pixel. We used two available land cover datasets from 1992 and 2012. Information \nabout the processing and geospatial data for annual coverages are processed and refined continuously (ESA 2017). We chose the earliest and latest reliable land cover datasets and summarized key land cover categories that best correspond to what we documented as ancient \nsettlement and land use in the G-LiHT LiDAR data. We combined intensive urban land, intensive cropland, and mosaic cropland with less than 50 % tree cover to compare with our \nbuilt environment ratios calculated for the G-LiHT LiDAR inventories. This includes raster \nvalues 10, 11, 12, 20, 30, and 190 for both the 1992 and 2012 datasets. We tabulated the area \nfor each sample period, comparing these areas to the overall area of the watershed (Fig. 3). \n\nWhile not surprising, it is clear that the pace and impact of modern development is substantially more intense, but it is also more widespread across the geography of our study area. \nIntensive coastal population growth and development are clear along the northwest and northern coasts of the Yucatan in 1992 and 2012.  \n\nAdditionally, between 1992 and 2012 intensive forest clearings and intensified land use are \nalso evident in southern Chiapas. From long-term field work and research this significant \nchange is not associated with population growth, pressure, and urbanization, but the expansion of cattle production adjacent to the Usumacinta River. \n\nIn this paper, we cannot begin to investigate all of the critical shifts in land use and land cover \nduring the recent decades. There are, however, several key observations we can put forward \nbased on this initial and macro analysis. First, while the Ancient Maya utilized coastal resources and there is evidence of coastal settlements, the intensity and density of urban development across southern Mexico in recent decades has been focused almost exclusively on \nthe coastal watersheds adjacent to Campeche, Merida, and Cancun.  \n\nSecond, it is also clear that there has been increased deforestation and intensification of agricultural fields adjacent to these rapidly urbanizing regions. Therefore, there are direct and \nmacro-regional impacts of development that diverge from what we now know about at least \n2,000 years of settlement and land use across the southern lowland landscape. \n\nWhile the current population across our entire study area is likely larger and generally denser \ntoday than it was during the peak of Maya civilization between 250 CE and 750 CE, it is \nclear from these data that Maya settlements were designed, planned, and constructed in very \ndifferent regional and territorial spatial patterns than we are witnessing in the modern era. \nCoastal  development  is  dominating  these  regional  systems  and  the  introduction  of  cattle \nfarming in Chiapas has transformed what has been historically a stable and resilient socio-\necological system adjacent to the Usumacinta River (SCHRODER et al. 2021).  \n\nToday, not  only  are  there  more urbanized and  impacted  watersheds  than  there were  1500 \nyears ago, but there are also areas deep in southern Mexico where the intensification farming \nhas impacted watersheds disconnected from the population growth and pressure adjacent to \nthe coasts.  \n\nDiscussion and Conclusion  \n\nOur analysis of these data is simply a step to leverage information about the deep past to \nevaluate and assess current land use systems with the hope that these data can provide useful \ninformation for design, planning, and specifically geodesign. Uniquely, the data we used to \ndocument ancient settlements and land use were captured to measure above ground carbon \nstocks in order to measure and maintain conservation of the tropical forests in southern Mexico. There are, however, many more critical uses of these data and we took this first step to \nestablish some base line data for further investigation of each small watershed in more detail \nthrough a combination of remote sensing and fieldwork.  \n\nIn 1965, BOSERUP (1965) offered a transformative perspective on population growth and agricultural change. Less than a decade later, TURNER (1974) radically changed our perceptions \nof the ancient Maya by linking his field observations with Boserup’s ideas about intensification. Today, almost 50 years after TURNER’s (1974) field work in the Yucatan, we may now \nhave the opportunity to better integrate information about the deep past into assessments of \ncurrent land use and to inform future decisions about settlements and land use. \n\nWhile Boserup’s assessment and investigation of the relationships among population growth, \npressure, and agrarian change were transformative, they did not factor in the critical spatial \ndimensions of how these changes occur (STONE 1996). As more data products become available across the planet, we need to establish ways to quickly capture information about the \ndeep past and compare it to our modern settlements and land use. In the case of the southern \nlowlands, we know from  extensive  work,  that  Maya  settlements  and  communities  thrived \nacross this geography for centuries. These data offer clear regional information about where \nto expand settlements and intensify agriculture. Our future research aims will now investigate \nwhether there are crucial details within these watersheds to inform future land use planning.  \n\nBeginning in the 1970s, environmental scientists established the critical value of capturing \nand processing raw data about our changing environment from remote sensing. Year by year \nand decade by decade these data document the impact our choices have on the resilience and \nsustainability of communities. Today, there are new data sources that can provide information \nfrom the deep past allowing us to compare our settlement and land use decisions with the \nlegacy of land use written in the landscape (LEWIS 1979).  \n\nIn this paper, we take a first step, albeit a very speculative first step to investigate the past not \nsimply to reconstruct the socio-ecological dynamics and the cultural history of one of the \nworld’s most critical landscape narratives. That work is ongoing and provides an important \nanalog view of the past. But perhaps there is a way to leverage information about the past to \nevaluate the present and inform future decisions, offering pathways for scientifically informed \ngeodesign and landscape ecological planning. MCHARG (1969) recognized the coupled natural and human systems critical for sustainable and resilient design. For decades since, we \nhave relied on advancements in remote sensing of the environment and GIS to inform the \ndesign and planning of future settlements. New data sources, such as LiDAR now offer a \nmore complete picture about these coupled natural and cultural systems. In this paper, we \nintegrate information about how deep time and cultural systems compare to the decisions we \nmake today with the hope that we can integrate these data into regional and territorial scale \ndesign. \n\nDynamic Mapping Based on Multi-functional Land \nValues, Measuring Land Use Resilience for Climate \nAdaptive Spatial Planning \n\nAbstract:  Land is a complex, dynamic and adaptive system,  where its socio-ecological interactions \ncreate continuous land processes that provide multifunctional values to society. Overlooking this dynamic condition and these multiple values – and their trade-offs – within decision making processes, as is the case in territorial planning, compromises these values and they are irreversibly lost, decreasing the resilience of our territories. The following body of work discusses the potentials of current digital \ntools by proposing a framework capable of understanding these complex interactions in a dynamic way. \nA computational pipeline and feedback-loop methodology is proposed and tested in the case study of \nVitoria-Gasteiz municipality. The resulting Decision Support System enables a new spatial planning \nmethodology capable of measuring land resilience for climate adaptive planning. \n\nIntroduction \n\nWithin current territorial and urban planning processes, we can observe that decision-making \nis often constrained through land use categorization. Within this context, land use and land \nuse  change  only  allow  recognizing  the  socio-economic  functional  dimension  of  the  land, \nbounding land into spatial categories. This bounded representation of land use categorization \ndoes not allow the recognition or planning of land as a system of systems and, consequently \nfurther  fragments  the  territory,  meaning  the  breaking  up  and  consequent disintegration of \nlarger  areas  of  natural  land  cover  considered  as  continuous  ecosystems  (MITCHELL  et  al. \n2015). In addition to this, land use change and landcover change represent an abrupt disturbance, causing the loss of several ecosystem functions or services. This disruption in structural ecological connections presents serious threats to the environment’s capacity to provide vital ecosystem services, resulting in a decrease in habitat resilience in the general context of climate change. \n\nA consequence of the fact that current spatial planning tools are based on land use categorization, is that more often than not these lead to oversimplified decisions, overlooking policy coordination − especially among sectoral policies − that do not respond to the interlinkages \nthat characterise land as a complex, dynamic and adaptive system. These mechanisms enable \nsolutions that tend to focus only on one of land’s values, neglecting land's dynamic multi-functional condition and ignoring the multiple values that land provides, among which multiple Ecosystem Services (ES) and habitat resilience (DEFRIES & NAGENDRA 2017). Ignoring \nthe diverse synergies and trade-offs (for example, soil fertility and biodiversity loss), aside \nfrom the expected result of a specific solution (food production increase due to pesticide use), creates  the  ideal  scenario  for  the  continuous  loss  of  multiple  land  values,  and  consequent impacts on the greater ecosystems. In a future characterised by uncertainty, the territory needs to be enabled to respond to different needs simultaneously in order to guarantee the multiplicity of functions and ensure its resilience (AA.VV. 2022). At the same time, the tools that we develop for territorial planning need to tackle the dynamicity of the processes occurring in land systems in order to provide the dynamic strategies needed for adaptation (EUROPEAN ENVIRONMENTAL AGENCY 2018). \n\nA Dynamic Approach to Land Use Resilience \n\nSeveral  land  classification  systems  have  addressed  either  the  multifunctional  condition  of \nland by exploring different service dimensions (NEIKER 2018), multiple abrupt variations of \nlandcover change in time (WINKLER et al. 2021) or the pressure based-impacts induced by \nhuman activity (THEOBALD 2020). Nonetheless, these classifications fail to understand these \nvariations as transitions characterised by continuous land processes. Moreover, neglecting \nthe transitional dynamics of land becomes an obstacle for spatial development to adapt, necessary in order for it to be able to respond to uncertainty. With this in mind, and the potentials \nof digital tools to manage complex and dynamic territorial conditions, the body of work that \nfollows discusses a framework and computational pipeline to re-understand land as a system \nfrom a multifunctional perspective, where dynamics of functional interactions and the resulting transitions are measured through geospatial data, and algorithmic analysis becomes essential in order to tackle aspects of climate change adaptation. The dynamic understanding \nof land represents an opportunity to create a feedback-loop methodology capable of measuring land resilience, not only acknowledging land’s multiple values, but also by adding the trade-offs, in the form of an impact assessment, of spatial decisions, allowing to make territorial decisions responsive to adaptation. \n\nThe  pipeline  relies  on  the  development  of  a  new  assessment  framework  that  plugs  into  a \nterritorial Decision Support System (DSS) for spatial development based on land's multifunctional value. ES, in this matter, have been recognized as the missing link between social and environmental subsystems, consequently recognizes the multiple values that land provides in \nterms  of  Natural  Capital  through  its  multiplicity  of  socio-ecological  functions  (HAINES-\nYOUNG et al. 2018). Current tools such as the VivaGrass tool (AA.VV. 2020) have resulted \npromising in terms of applications of ES as a concept to improve land management through \nintegrated planning by acknowledging land’s multifunctionality. This tool also provides the \nbasis for clustering methods to group land based on ES and recognize ES associations. These \nassociations are a “set of associated ecosystem services that are linked to a given ecosystem \nand that usually appear together repeatedly in time and\/or space” (RAUDSEPP-HEARNE et al. \n2010). These associations become crucial in order to classify the land based on the values \nprovided. Yet, the tool fails in the constraints of land use, this being the primary parameter \nto determine the provided ES and their associations. Moreover, the dynamic aspect of land \nthrough the continuous measurement of decision-driven impacts is not embedded in the tool. The limitations related to this approach, therefore, disable the possibility of the aforementioned policy coordination, as well as continuous reassessment, needed for adaptive and resilient territorial and\/or urban strategies. \n\nThe proposed DSS allows us to simulate and evaluate land resilience through a data-driven \napproach by creating a cyclical methodology with which to understand land as a dynamic system. Within the practice of urban planning and design, the DSS pipeline is thought as a knowledge-based communication platform, ideally facilitating the connection and collaboration between researchers, policy-makers, and spatial planners. Once implemented, the platform allows primary users to evaluate new strategies based on socio-ecological indicators, and after their implementation, these interventions can be monitored continuously. By challenging  current  spatial  development  methodologies  based  on  land  use  categorization,  the DSS pipeline makes the hidden patterns that characterise land as a complex, dynamic and adaptive socio-ecological system accessible in order to propose a dynamic evidence-based \nspatial planning tool.  \n\nFor the implementation of the pipeline, a spatial analysis based on several ES must be performed  so  as  to  consider  the  socio-ecological  and  functional  dimensions.  This  analysis  is performed in a 1ha grid rather than basing it on land use boundaries, to understand land as a continuous system. In order to establish the connections between different ES, and identify \nthe  required  associations  between  them,  a  data-driven  taxonomy  is  developed  through  an \nalgorithmic grouping system (clustering). The result of this process enables the creation of \ntaxons, or groups of land that have similar ES associations, that do not only recognize the \ndiversity of functions in each land unit but also the diversity of interactions between these \nfunctions. The method for the proposed taxonomy considers the dynamics of land and function based on a dynamic data-input, allowing it to take into account the ever-changing data \nflow that feeds the taxonomy itself.  \n\nFinally, a multicriteria assessment based on the multiple land values is integrated into the \nprocess. Considering multiple values together can enable the proposal of hybrid solutions. \nHybrid strategies are necessary in order to measure the synergies and co-benefits land can \nprovide simultaneously, as well as the trade-offs of strategies. Thanks to the consideration of \nsynergies and trade-offs, a comparative analysis of the strategies can be developed through \nan impact assessment. The comparison allows for better-informed decisions ensuring the recovery of a diversity of land functions, and the land's option value can increase in each decision. As the option value of the land increases, the ability of the territory to respond to uncertainty also increases. By reunderstanding land from a socio-ecological perspective, land \nresilience can be measured, and recognize the trade-offs and synergies of our decisions to \nplan the urban processes that constantly adapt the land. \n\nFeedback-loop Methodology for Resilient Spatial Planning  \n\nThe  methodology  developed  comprises  three  strategic  stages  that  allow  the  translation  of \nland's multifunctional dynamic condition into coherent strategies for policy-makers and spatial planners with the input of research knowledge. The methodology of this framework is \ncircular and proposes a continuous feedback-loop in order to ensure the continuous adaptation of territorial strategies. The experimentation process was conducted in Vitoria-Gasteiz \nmunicipality, in the north of Spain. \n\nA Data-driven Spatial Analysis Based on Land Functions \n\nTo measure land's multifunctional value, four ES are selected with methods that allow their \nspatialization (Fig. 1): Soil Fertility (SF), Habitat Maintenance (HM), Carbon Sequestration \n(CS) and Food Production (FP). The primary purpose of this stage is to measure the territory's \nperformance for each spatial indicator through a grid, avoiding the constraints of land use \nboundaries. Global, European, regional and local geospatial and statistical data are combined \nfor the purpose through GIS tools. These datasets are characterized by being or performed \nyearly or are based in remote sensing methodologies, which enable the possibility of ensuring \nthe monitoring process. \n\n•  To analyse the capacity − and potential capacity − of the territory regarding SF, the Soil Fertility Index model (SFI) (SAGLAM & DENGIZ 2014) is calculated. SFI allows scoring soil fertility's spatial variability based on  macronutrient and micronutrient availability (BALLABIO et al. 2019), as well as soil properties (KLIMATEK 2019). \n\n•  Different ecosystems create the needed habitats characterised by an adequate environment for different kinds of autochthonous flora,fauna, or microorganisms to thrive, and it can be measured through the Habitat Maintenance Index (HMI) (NEIKER 2018). The HMI scores the land based on the protected areas, the richness of autochthonous vegetation species, and the successional state or ecosystem maturity (GOBIERNO VASCO 2019). \n•  To measure and spatialize the Food Production (FP), Plant-Based Food, and Meat Production are analysed through Neikers’ approach (2018). Both indicators are measured with statistical (INSTITUTO VASCO DE ESTADÍSTICA 2021) data and GIS tools, the Plant-Based Food is spatialized only in the arable land, and the meat is spatialized only in the pastures (GOBIERNO VASCO 2022).  \n\nIn order to measure the Carbon Sequestration (CS) yearly capacity, both CS in Trees and CS in Soils is measured and developed in the following section to provide one example regarding the spatialization of one of the indicators. \n\nCarbon Sequestration Method \n\nTrees sequester carbon by capturing carbon dioxide through photosynthesis to transform it \ninto biomass. (UNECE n. d.) For the research, regarding CS in trees, only carbon sequestered \nin the form of biomass is taken into account, and the carbon stock in deadwood and litter is \nnot measured since dead biomass does not actively absorb carbon. The method applied was \nextracted from the Spanish Government allowing to measure CS depending on tree species \n(Ministerio para la Transición Ecológica 2019), which is usually used for evaluating Land \nUse, Land Use Change & Forestry practices. One main factor that compromises the CS in \ntrees is that aged trees almost do not grow and do not increase their biomass volume. Hence, \nthe carbon sequestered in their mass is negligible. Four geospatial datasets are used to filter \nout the old trees from the calculation. The first two are the tree heights from 2008 and 2012 \n(DEPARTAMENTO DE DESARROLLO ECONÓMICO, SOSTENIBILIDAD Y MEDIOAMBIENTE 2017). \n\nTwo  parameters  are  used  to  select  the  growing  trees:  height  increase  and  height  decrease \nfrom 2008 to 2012 (assuming that would mean that the trees have been replanted). Another \ntwo data layers are used to select more accurately growing trees: the apparent age of trees \n(DEPARTAMENTO DE DESARROLLO ECONÓMICO, SOSTENIBILIDAD Y MEDIOAMBIENTE 2017). \nTrees over 80 years old are taken out of  the analysis. With the resulting  geospatial information, a fifth data layer is used to estimate the number of trees per hectare (HAZI 2020). \n\nThis procedure allows for establishing the number of trees per hectare sequestering carbon. \nSince not all tree species absorb carbon the same, once growing trees and their amount by \nhectare are filtered, the type of tree species is embedded in the analysis. The regional database \nestablishes the percentages of the three main species  for different forest areas (GOBIERNO \nVASCO  2021).  As  a  result,  the  table  from  the  method  (MINISTERIO  PARA  LA  TRANSICIÓN \nECOLÓGICA 2019) is used to multiply the number of trees of each species by the amount of \ncarbon sequestered by tree per year. The resulting information is geospatialized in tonnes\/ha\/ \nyear. \n\nCarbon Sequestration in Soils (CSS) results from different interactions (photosynthesis, respiration, and decomposition) of soil processes resulting in Soil Organic Carbon (SOC) content. CSS is usually estimated through soil sampling. Nonetheless, there is no such database \nfor Vitoria-Gasteiz and a different approach is used. The FAO has estimated the current SOC \nstock (FAO 2020) and a Business As Usual (BAU) estimation for SOC sequestration in 20 \nyears (FAO 2020). BAU scenarios are future estimations based on the absence of new action. \nThe CSS is estimated by subtracting the current SOC stock from the BAU SOC stock and \ndividing this by the number of  years. Since the FAO dataset is a global spatial index, the \nraster pixels' scale is 1 km. In order to increase the resolution of the data, since the grid cell \nsize used is smaller than the data gathered, Smart-Map experimental QGIS plugin is used to \nreduce the scale of the rasterized data. The Ordinary Kriging interpolation allowed by this \nplugin has been recognized as an accurate method to extrapolate from in-situ soil data samples to geospatialized information (PEREIRA et al. 2022). Extracting the centroids of the raster \nlayers for the different data allows them to be used as samples for the interpolation. As a \nresult, 100x100m pixel raster layers are created and the CSS is spatialized in tonnes per hectare per year.  \n\nData-driven Spatial Analysis Results \n\nThe results allow us to understand the diversity of performances of land depending on the \nmeasured function. The distribution of each land function varies in each pixel for each indicator.  Hence,  from  this  multicriteria  analysis,  it  is  possible  to  conclude  that  all  indicators \nshow different territorial readings and that land indeed provides a multiplicity of functions \nsimultaneously. Acknowledging the spatial distribution of the land functions' performances \nenables us to discuss the resilience capacity of the land. The limitations of the methodologies \napplied  for  the  indicators  are  related  to  available  data  regarding  natural  stock.  In  the  CS \nmethod, on one side, some of the datasets used are in a smaller resolution than the needed \none.  On  the  other  hand,  assumptions  related  to  the  filtering  of  non-growing  trees  and  the \nBAU scenario have been made. However, these are the most accurate procedures enabled by \nthe available data to understand the CS in Vitoria-Gasteiz. \n\nDynamic Taxonomy and Systemic Relationship Evaluation \n\nThe main point of this step is to group the land based on the systemic relationship, or interactions, between the previously analysed land functions. K-means clustering is selected due \nto the possibility of grouping the land into significantly associated ES, as well as minimising \nwithin-group variability (AA.VV. (2020). Currently the Grasshopper3D tool has facilitated \nthe possibility of implementing K-means in a parametric way, by enabling fast and easy clusterization through the Owl component. The use of Grasshopper also enables the parametrization of the data input, as well as the results of the K-means clustering algorithm. The implementation of this logic enables a dynamic taxonomy since the group classification is based \nstrictly on the land values. Each time this classification is done, through the cyclical methodology,  an  evaluation  of  the  cluster  results  is  needed  to  understand  which  hidden  patterns emerge from the application of the algorithm. This feedback-loop process enables the reinterpretation of the land groups (clusters), enabling a data-based taxonomy that changes depending on the new values provided by the monitoring process of the last step of the methodology. \n\nThe K-means clustering method is applied through the Owls plug-in, taking into consideration the previous spatial analysis, in order to group the land based on shared properties. The \nresults showed that the municipality is grouped into 9 different clusters that correspond to \nsimilar ES associations. The taxons are based on the correlation between the values, and most \nimportantly, they are not associated with land use. Figure 3 shows how the arable land of \nVitoria-Gasteiz is composed of the 9 clusters, meaning that certain land units of arable land \nhave correlations with forest land use, among others. These results enable the interpretation \nof different land groups based on different land value relations, enabling in the next step the \nproposal and evaluation of spatial planning strategies. \n\n\nStrategies Proposal & Evaluation \n\nOnce the analysis of the clusters is done, the strategies proposed should be based on the type \nof  land  values  that  want  to  be  enhanced  in  the  territory,  by  also  considering  the trade-off \nvalues of these strategies. Thus, how specific or multiple strategies affect each land function \nshould be considered. Research on how those strategies affect land should be analysed to be \nable to evaluate their impact correctly. First, 3 clusters were selected inside the 9 clusters, \ndue to their similar trends of functionality (specifically cluster 1, 3 and 4). These clusters \nshow how FP has a significant trade-off in HM and CS. Through literature review, certain \nstrategies have been analysed and correlated to the increase or decrease of certain values to \nestablish the values for the impact analysis based on synergies and trade-offs. The objective \nof the hybrid strategies should focus on rebalancing the territory by changing these relationships. While in cluster 1, FP could increase by intensifying agriculture while increasing SF, the other clusters (2 and 3) could reduce FP in exchange for increased CS and HM.  \n\nAfter the most suitable hybrid strategies are selected, the synergies and trade-offs of the strategies are measured through a parametric simulation through Grasshopper3D. This procedure enables a better understanding of the strategies' added value to natural capital. At the same time, the ES loss is simulated through the accountability of the trade-offs. With these results, \npolicy-makers  and  spatial  planners,  together  with  researchers,  can  make  data-driven informed decisions through evidence and implement the strategies. Finally, the new data values are used to recluster the territory and understand the responsiveness of the closed feedback-loop methodology.  \n\nThe feedback-loop  enables  the  monitoring  of  implemented  strategies  to  re-measure  land \nvalue (the first step of the methodology) and can quantify the strategy success rate. The strategies can continuously improve, shifting the actions taken in order to add optional values, and therefore resilience, to the land. At the same time, new data values and input mean a new \ntaxonomy, enabling the dynamic condition of the land to be considered through responsiveness. In the experimentation process, 10 clusters emerged from the application of the complete cycle, instead of the previous 9 clusters. This methodology enables the creation of new \nknowledge that can provide feedback in improving research about the land itself.  \n\nDiscussion and Conclusions \n\nA new way of visualising the values of a territory has already proven of interest to Vitoria-\nGasteiz municipality, which has already implemented the map of CS developed in this research as part of its territorial planning tools, hence these methodologies can also be of interest for different territorial planning agencies. On top of the already implemented visualization, the  methodological taxonomy derived  from the  framework discussed supplies a  new way of approaching space-based territorial transitions. Monitoring the strategies can create constant  new datasets based  on land  value evidence. This  means that  new data inputs are \nconstantly feeding the taxonomy. Once the monitoring data enters the workflow, the clustering process restarts, and new taxons emerge. The new taxonomy that reflects the new land value is accomplished, and the land evolution is reflected thanks to the territorial transition \nthat emerges from the strategies. The feedback-loop process, therefore, creates a responsive \ntaxonomy based on the dynamic condition of the land. With the new values, new interactions \nbetween the land functions emerge, and hybrid strategies can be redirected. The territory is \nunderstood as a constant cycle of changing in an ever-evolving ever-mutating state. The hinterland-city relations are not understood as static problem-solving scenarios but as a steady \ndecision process. \n\nThe value of dynamicity is not only related to recognizing land’s dynamic conditions regarding interactions and transition. It is also a valuable mechanism that enables decisions based on shifting baselines. Proposing a taxonomy based on constant new values when the reassessment is done stimulates the understanding of constantly changing scenarios where uncertainty can be embraced. The strategies implemented due to the feedback-loop methodology can  start  to  be  ever-changing  instead  of  finalised  solutions.  Moreover,  understanding  the trade-offs generated continuously can be understood not as forever losses but as a regular \nmanageable part of the strategies. The pipeline represents the opportunity to create a framework based on territorial dynamics, where the constant feedback-loop creates a continuous strategic process, no longer working with finished silver bullet solutions, but rather designing \nfor and within climate change adaptation. \n\nEnvironmental Impact Assessment Software for Constructed Wetland Parks’ Sustainability Performance\n\nAbstract: Landscape Architecture is not particularly a tangible field; where most factors incorporated in this area are primarily intangible factors. However, the illustrative aspect of this realm is where the digitization age can shine. Accordingly, the architecture realm could definitely capitalize on the unlimited opportunities the digital space has to offer. In fact, actively demonstrating sustainable development using digital software for landscape architecture is one key means of taking the framework to the next level. The goal of this study is to create a new assessment instrument that is more appropriate for constructed wetland parks (CWPs) impacts and activities, in terms of convenient environmental features. The study is based on a structured questionnaire that examines the accuracy and application of selected indicators, with the participation of professionals from various related areas from across the world. A suggested assessment tool to evaluate the sustainability performance of CWP is discussed in the study, and raises the question of whether the created tool would be appropriate or practical as software. It appears that the proposed approach can be more extensively employed in landscape architecture projects if it is published as a globe app to enable comparison of global park performance.\n\nIntroduction\nThere are several ongoing problems and catastrophes that the world should indeed manage effectively. Climate change and water scarcity are two major issues that many nations face across the world. Several approaches to overcoming these challenges have been widely researched. Reducing the detrimental effects of climate change and natural catastrophes on urban areas has been a priority for researchers. The key to this mitigation's success in achieving beneficial environmental effects is to embrace nature-based techniques. This might be accomplished by implementing a catalyst project that encourages positive changes and has a beneficial influence on the environment. One of the most well-known nature-based initiatives that helps cities deal with the consequences of the two primary crises is constructed wetland parks (CWPs), which views wastewater as a resource for reuse. It is challenging to assess the performance benefits and how CWP contributes to attaining sustainability since there are few instruments available for assessing the effectiveness of CWP projects and their multi-functionality. Consequently, it is proposed that CWPs be evaluated using a set of key influencing impacts. These impacts are employed to develop an assessment method for determining the sustainability of CWPs. The anticipated performance is assessed using the suggested tool to determine the favourable benefits of CWP in attaining municipal sustainability. In this paper, this research gap is being tackled through addressing intangible factors in a modern, digitized way. The paper discusses the proposed assessment tool that could gauge the sustainability performance of CWPs, raising the question of whether the developed tool would be suitable or feasible as software.\n\nMethods\nMany researchers throughout the years have established and improved evaluation methodologies to highlight the relevance of environmental change in a clear and consistent manner. (MARTIM 2013). The ad hoc technique, checklists, interaction networks, system diagrams, overlaying charts, and matrices are among the most important environmental impact assessment methods (MORAES 2013). The simple matrix primarily consists of a collection of environmental factors that are presented on the vertical axis and is used to determine if an action will have a negative, neutral, or positive influence on the environment with a \"check mark\" in the relevant column. Different matrix approaches have been created throughout the years for evaluating various types of project in order to find the best assessment method based on each project's requirements. One of the early techniques, the Leopold Matrix, was first proposed in 1971 (LOHANI et al. 1997). In 1974, Environment Canada introduced a different kind of matrix, the Component Interaction Matrix, to systematically determine the indirect effects. After gaining international recognition, EIAs gradually began including matrices in their impact analyses (BABU 2017). Further advances included the creation of Modified Graded matrix, Impact Summary matrix, and Loran matrix (LOHANI et al. 1997).\n\nLeopold Matrix\nLeopold Matrix offers a simple method for summarizing and categorizing environmental impacts and full review of the project's activities, consequences, and impacted environmental factors to determine the most significant actions and conditions (FIGUEIREDO 2020). It offers a framework for mathematically examining and weighing potential implications. The analysis doesn't offer a comprehensive quantitative evaluation; instead, it shows a variety of value assessments, ensuring that the effects of various actions are evaluated and taken into consideration while designing a project is the major goal. It offers a thorough study of the connections between planned human activities and environmental factors as a qualitative indicator of a project's environmental\/social effect. A list of 100 activities indicating environmental actions is depicted on the horizontal axis. On the vertical axis, about 88 environmental\/social factors are presented, representing the current environmental aspects and impacts that can be altered by each of the project activities on the horizontal axis. Few interactions are expected to have impact of the degree and relevance to require extensive treatment (PONCE 2009). A custom developed matrix should be tailored according to the distinct situations and features of each park for a precise and accurate assessment of park performance. Table (1) shows a sample of a Leopold matrix, for a model of 5 activities with effects on 2 environmental aspects, with blank cells indicating no influencial activity. The technique enables systematical comprehension of the assessor's reasoning, and spot matches and contradictions. As a result, the matrix is actually a synopsis of the EIA text.\n\nLeopold Matrix for Environmental Impact Assessment, EIA\nThe Leopold Matrix EIA is an examination of the cells with higher Magnitude and Significance values. Regardless of the allocated numbers, columns containing several components and Rows containing several actions are explored in depth (PONCE 2009). The justifications for assigning the score values of the impact's magnitude and relevance are discussed in the EIA text. A symposium on the essential characteristics of the proposed action, including the associated ecology (PONCE 2009, IISD 2021).\n\nURBIO Index\nURBIO Index is a method for evaluating the sustainable design of green spaces and is meant to help various specialists and designers in creating green, sustainable infrastructure. It assesses the project's six thematic indicator groups (Planning, Materials, Amenity\/Value, Biodiversity, Climate\/Water\/Soil, and Management), totalling 25 indicators. Each theme is presented on its own sheet, and the overall assessment is presented on a collective sheet facilitating park evaluations based on all areas of sustainability (MÜLLER 2016).\n\nProposed CWP Assessment Index\nUrban sustainability indicators are developed to investigate the interconnections between environmental, economic, and social factors and their mutual impacts to assess the CWPs performance as a multifunctional sustainable landscape project. The suggested metrics are assessed for this purpose in connection to the National SDGs, the UN SDGs, and performance-based evaluation techniques for CW projects for wastewater treatment (LEE 2020, ROBATI et al. 2021). The developed new assessment tool is more suited to CWPs operations and activities. Including a diverse proposed tools to aid in the assessment of each criterion for providing practitioners a cohesive set of information and interpretation of the collected data for assessent and decision making.\n\nProposed Framework\nExisting methods in landscape design do not represent all aspects of sustainability, and owing to their complexity and problems in implementation, the proposed tool aims to provide a digital evaluation software that incorporates all aspects of sustainability and is reasonable and unexpensive, to aid landscapers and small local projects in improving sustainability internationally. The chosen strategy focuses on categorizing indications into criteria and subcriteria. As a result, each relevant environmental component's effect factors would be assessed independently and given a score for Impact Magnitude and significance. In addition to the conventional Leopold matrix, new criteria of Probability and Duration were applied.\n\nThe proposed tool used the Leopold matrix technique, with the CW Parks' convenient activities added to the horizontal axis and the recommended environmental aspects added to the vertical axis. Each was then classified according to quantifiable parameters. To overcome the limits of the Leopold matrix, the Social Impact and Economical-Technical Factors were added, and each was further subcategorized with detailed factors. Based on relating suggested indicators to the two primary phases of the CW Parks' life cycle: construction and operation (LOHANI et al. 1997). Each was assigned a weight based on its lifespan. While the third phase, Demolition Phase, was omitted owing to its limited impact as it is thought to have no specific substantial actions other than water path backfilling (DAVIS 1995).\n\nValidation Methodology and Criteria Weighting\nSince CWP encompasses several distinct impacts and factors influencing sustainability, it necessitates the development of a distinctively designed CWP assessment tool capable of effectively targeting those various impacts while also accommodating the different projects in significance to their different concepts, types, conditions, and attributes. A quantitative approach is necessary to validate the outcomes of the suggested indicator categories. The study is based on a structured questionnaire, that assesses the applicability of the chosen indicators, with the involvement of specialists of related disciplines from all over the world. The suggested CWP Index is a simple and dedicated assessment tool that evaluates the major three areas of CWP sustainability performance, each according to its significance importance weight that define their respective relevance, based on the findings of the questionnaire.\n\nSuggested Main Phases\nEfficient EIA relies on detailed management of project concerns, their effects on key aspects, and a clear mitigation plan for impacts reduction. It correlates impacts with the project phases in which they occur; construction, and operation. This Tackling clearly reveals which project's components need mitigation measures through design adjustments and alignment of mitigation options with the project execution timeline (LOHANI et al. 1997).\n\nPhases’ weight Assessment to attain a rational indicative overall sustainability achievement of the CWP. Each of its two stages, construction and operation, was examined based on their effect weight in the CWP's life cycle. Lifespan of CWP is ruled by sewage pollution, capacity to filter and store pollutants, and waste accumulation. It has so far demonstrated a lifespan of more than 20 years with minimal efficiency loss. While the construction phase typically lasts 1 to 3 years on average, the operating phase might last 20-30 years (DAVIS 1995).\n\n\nSuggested Main Categories of Impacts, Calculations and Outcomes\nEnvironmental, social, and economic sustainability are the three primary pillars on which the suggested tool is based. The environmental factors are then divided into four major categories, which are the most important vital environmental aspects that can explain the impact of parks on the surrounding urban areas. Climatic, Sustainability, Biodiversity, and Water are the four factors that are evaluated quantitatively and descriptively. A 4-division cell represents a full assessment of each impact factor (IF). According to a scoring scheme, the assessor evaluates each impact's magnitude, significance, probability, and duration. The proposed Matrix will automatically calculate the Impact value relevance (IV), total Environmental Impact Value (EIV), Ratio of Impact Factor (R), and IV Weight Relevance Value (IVWR), as well as the percentage achieved for each factor. All are displayed in charts that compare the assessed CW Park's obtained score to the overall score that might be reached allowing for a better understanding of the CW Park's performance and thus assisting in decision making.\n\nProposed CWPs Assessment Tool Outcome encompasses a comprehensive matrix, sustainability performance summary charts for overall park performance and for both construction and operation phases, a summary chart for sustainability categories assessment analysis, together with three detailed charts for each of the sustainability pillars. In the Matrix sheet, the assessor must simply provide the project name, location, and his personal evaluation score for both the construction and operation stages. Suggestions for various measuring methods and instruments are provided to assist the assessor in evaluating and assessing each impact.\n\nApplication of the CWP Index on a Project\nAs an initial test of the developed tool, the expected performance of a case study of CWP in an arid climate in Egypt was assessed using the proposed CWP Index to confirm the tool's effectiveness in assessing the positive impacts of CWP in achieving city sustainability.\n\nConclusion, Challenges and Recommendations\nThe proposed tool allows rigorous analysis of CWP's entire sustainability performance as well as during construction and operating phases. The assessment is simplified over quantitative matrix and simple visual charts for a better review and assessment of opportunities for development, as well as identify environmental weaknesses and strengths. It simplifies the evaluation of the CWP performance to achieve an optimal and feasible project that improves social, economic, and environmental aspects of a sustainable city. It could be used to examine predicted sustainability performance aiding in the management of both existing CWPs and plan and design of future CWP projects. The suggested tool evaluates each effect based on its important weight while measuring the park's overall sustainability performance based on the relevant value of the project's phases' sustainability success. To avoid having an oversimplified and hypothetical attribute rather than a coherent picture, the tool should be improved to consider the cross-interaction of the sustainability factors across categories. It is recommended that the introduced matrix be tested in a variety of test cases for potential adjustments and improvements. The proposed assessment matrix and charts are alleged to be a strong assessment tool, making the proposed CWP Assessment Index user-friendly and easy to grasp for users of all levels, and serve as a summary of the project's impact assessment reports for early project appraisal and assessment of improvements prospects as well as identification of environmental impacts' strengths and weaknesses for later application of appropriate mitigation measures using a set of quantitative matrices and simple visual charts. To pave the path for a new, digitized sustainable landscape era, this paper recommends that the proposed tool be further developed as a digital software, automated and shared as a globe app to allow for comparison of global park performance.\n\nEstimating Solar Energy Potential of Hungary \nBased on Raster Maps  \n\nAbstract: The transition to renewable energy sources is critical for reducing greenhouse gas emissions. \nThis process changes the landscape structure. Through the example of Hungary, we presented the methods of estimating the annual electricity production based on solar energy at the national level using a \nGIS tool. In connection with the calculations, we summarized the landscape and environmental effects \nof different land uses, and we covered the physical characteristics of electric current which affect the \nspatial structure of production. In addition to determining the potential of solar energy, the environmental and physical characteristics determined the tasks, possibilities and limitations of landscape architecture in using solar energy. \n\nIntroduction \n\nIn  our  research,  we  estimated  the  potential  of  solar  energy  at  the  national  level  using  the \nCorine Land Use Cover (COPERNICUS 2018) and the Ecosystem Map of Hungary (AGRÁR-\nMINISZTÉRIUM 2019) raster maps. In the documents defining the energy policy of the coming \ndecades (Második Nemzeti Éghajlatváltozási Stratégia 2018) (Nemzeti Energia-és Klímaterv \n2020), Hungary has set the goal of reaching 6,000 MW of solar energy capacity, which can \nproduce  8700  GW  electricity  in  a  year.  Recently,  several  studies  have  examined  the  efficiency of different solar technologies (ATSU et al. 2021), the solar potential and trends of \nHungary and the surrounding region (KUMAR et al. 2021), and analyzed the potential of renewable energy sources at the regional level (HARTMANN et al. 2017), to name a few from \nthe research of recent years. What new aspects can we shed light on from the perspective of \nlandscape architecture? According to cultural history research, the transformation of the energy system is accompanied by a change in the landscape structure (MALM 2016), and the \nintensity of different renewable energy sources is different (FRITSCHE et  al. 2017). In our \nresearch, we connected the solar energy potential and the land use classes of the two raster-based maps on a national level and determined which landscape categories have more favor-able landscape and environmental effects. Furthermore, we examined whether, in addition to landscape use, it is worth further narrowing the areas at the level of location and potential at \nthe national level. We also examined the possibility of narrowing down the possible locations, taking into account the physical characteristics of the electrical system at the national \nlevel and maps at the settlement level, while considering the location of the areas and terrain \nconditions. \n\nMaterials and Methods\n\nWe estimated the potential of solar energy based on territorial data; for this, we used two \nraster-based maps: Corine Land Use Cover and the Ecosystem Map of Hungary. The two raster-based maps define different land use categories and are highly accurate. To determine the territorial data, we followed the following steps: \n•  We downloaded the raster data of the European land use map here: https:\/\/land.copernicus.eu\/pan-european\/corine-land-cover (COPERNICUS 2018). We downloaded the Ecosystem Map of Hungary here: http:\/\/alapterkep.termeszetem.hu\/ (AGRÁRMINISZTÉRIUM \n2019). \n\n•  We  applied  the  following  steps  to  get  the  raster  data  map  of  Hungary  in  ArcGIS: \nArcToolbox – Spatial Analyst Tools – Extraction – Extract by Mask, and then we ex-\nported data into TIFF format. \n\n•  We  finally  used  the  following  processes  to  calculate  the  area  of  land  use  types: \nArcToolbox  –  Conversion  Tools  –  From  Raster  –  Raster  to  Polygon.  This  procedure \nconverts the raster data to vector data to calculate the area of land use types in Attribute \nTable in ArcGIS. \n\nAfter, we calculated the solar energy potential for suitable land uses. Even in the country, the \nconditions are different (Fig. 1), so we can define an average solar potential to count with. In \nHungary,  an  average one kW  panel  can  produce  1450 kW  of  electricity  per  square meter \n(VARGA 2005), and the average efficiency of solar panels is approx. 18 % (‘Most Efficient \nSolar Panels’ 2022), Therefore, 261 kWh of electricity can be produced per square meter in \na  year.  Can  we  count  on  an  increase  in  the  efficiency  of  producing  electricity  from  solar \nenergy? On the one hand, there are physical limits to the increase in performance: according \nto the laws of thermodynamics, energy conversion always involves energy loss (KLEIDON et \nal. 2016), which is also true for environmental systems (ODUM 2007). Research is currently \nbeing  carried  out  in  two  directions  for  the  development:  on  the  one  hand,  to  increase  the \nlifetime of solar panel systems (EL-KHAWAD et al. 2022), or the efficiency can be increased \nby possible cooling of the systems (SIECKER et al. 2017) (PENG et al. 2017), but we cannot \naccurately count on these contingencies at the moment. In Hungary, solar panels are installed \nwith two land covers: grassland and built-up area (MUNKÁCSY 2021). We counted these two \ncategories and connected them with the environmental impacts of installation, use and abandonment of energy production. \n\nAnother critical question from the point of view of the calculations is whether it is worth \nnarrowing down the possible places. The national radiation map shows a difference between \nthe  northern  and  southern  areas  (Fig.  1),  but  the  difference  is  not  significant.  The  annual \nradiation per square meter of Szeged in the south is 1600 kWh\/m2, while Miskolc in the north \nis 1503 kWh\/m2 (CATTANEO 2018), The difference is not relevant from an energy point of \nview. By examining the annual distribution, there is a difference between the yearly radiation \ndistribution. In the south, there are more minor differences between the summer and winter \nmonths (Fig. 2) than in the north (Fig. 3). \n\nResults \n\nFirst, we introduced the results of the Corine Land Cover and the energy potential in Table \n1. Based on the categories defined by the map, we found four suitable land used types for \ninstalling solar panels: natural grasslands, discontinuous urban fabric, continuous urban fabric and industrial or commercial units. In Table 2, the results of the solar energy potential \nreference to the entire area. The potential of the areas far exceeds the set goals based on the \nresults. According to the national plan, the production should reach 8700 GW, possibly with \n0,06 % of the grasslands and built-up area. If only considering the built-up area, the 0,07 % \narea should be covered with solar panels. For example, in the case of 10 % solar panel coverage of all the examined areas, the electrical capacity is 1,253,973 GW\/year. This result is more than 144 times the set goal. \n\nAs it is clear that the solar energy potential is higher than the calculated plan, we can compare \nthe impacts of solar installations in different land use classes to exclude those more exposed \nto negative effects. We summarized the results in Table 3 as there are not many differences. \nHowever, the environmental impacts of grassland installations are more intense than former \nstudies have shown (FRITSCHE et al. 2017). The significant differences appear primarily during  construction:  large-capacity  solar  farms  involve  significant  earthworks,  and  large  surfaces can harm wildlife during operation (TURNEY & FTHENAKIS 2011). Furthermore, built-up areas have significant advantages, as they are connected to the grid, which minimizes the \nenvironmental impacts. In this case, we can estimate the solar energy potential of the built-up area. With 10 % solar panel density, discontinuous urban fabric has 1,175,147 GW\/year, \nindustrial and commercial units have 18,544 GW\/year, and the continuous urban fabric has \n54 GW\/year electricity potential. 10 % of built-up areas have solar energy potential of more \nthan 44 times the country's electricity production in 2020, which was 26,738 GW (INTERNATIONAL ENERGY AGENCY n. d.).  \n\nWe also examined the Ecosystem Map of Hungary with the same method which has a higher \nresolution (AGRÁRMINISZTÉRIUM 2019). Based on the results, if grasslands and other herbaceous vegetation and urban area are taken into account, 0.18 % of the area needs to be covered \nwith solar panels to reach the set target of 8,700 GW, which is a three-fold difference compared to the Corine Land Cover calculation. If only the built elements are considered, then \n0.35 % must be covered to reach the set electricity production, which means a five-fold difference. Based on the more accurate raster maps, a larger surface area is needed to achieve \nthe objectives related to solar power generation. In the case of both raster maps, built-up areas \ncan include roads, railways, other linear facilities, and green areas next to buildings, reducing \nthe possible solar panel installation area. However, it is also essential to consider that, in the \ncase of buildings, the examined maps did not consider the different building roofs, which can \nalso modify the results. For example, a detailed solar map for Budapest examines the potential of building roofs: based on this, the settlement's potential with an area of 525140000 m2 \nis 4872 MW (Magyar Napkollektor Szövetség 2022). The less accurate Corine map is closer \nto this result, where an area of 297473276 m2 approximately covers this capacity. In the case \nof the ecosystem map, it is 2669365884 m2. At the settlement level, the estimated values \nneed to be specified, but at the same time, the roof shapes presumably increase the potential \narea. \n\nThe  land  use  restriction  also  leaves  room  for  significant,  high-capacity  installations;  presented through a few concrete examples, the red line (Fig. 4, 5, 6) in the figures represents \nthe existing electrical network. In the case of housing estates in densely built-up urban fabric \n(Fig. 4), in the example, a solar panel is located on top of one of the most extended blocks of \nflats in Budapest with the capacity of 1128 MW (FÜLÖP 2012). Significant capacity can also \nbe built on top of industrial plants, logistics warehouses, and shopping centers, such as the \nAudi  Logistics  Center  in Győr (Fig.  5),  the  capacity  of  the  facility  is  12  MW,  which  can \nproduce 9.5 GWh of electricity annually (Európa még nem látott olyan naperőművet, mint \namilyet az  Audi  épít Győrben 2019).  Mine  tailings  are  also  suitable  for  the  installation of \nsolar panels, as was the case with the open pit coal mine in Visonta (Figure 6). The 16 MW \nsolar power plant was installed in the tailings area of the lignite mine, which is connected to \nthe nearby high-voltage line via an underground cable (Visontai Naperőmű – Műszaki Magazin, n. d.). In the case of all three examples, it is essential to have the option of connecting \nto the electricity network. \n\nThe question is whether it is necessary to narrow down the results on a national basis based \non the location of the areas, which affects the degree of radiation (BARTHOLY et al. 2013)? \nTo do this, on the one hand, we examined the characteristics of the electrical network system \nand the possibilities of inserting solar energy. Solar energy is an uncontrollable energy source \nand can be described as a stochastic system (TSEKOURAS & KOUTSOYIANNIS 2014) (SHARMA \net al. 2012). For the electricity system to be able to handle sudden production changes, it is \nworth geographically \"scattering\" the production, and the network itself can only store energy \nto a limited extent (YAZDANIE et al. 2016) (D’ANDRADE & SHAHSIAH 2017). For example, \nin the solar map of Budapest completed in 2022, where only buildings were taken into ac-\ncount, can be seen that there are buildings in poorly located areas where electricity can be \nproduced efficiently (Fig. 7), thereby making use of the network's storage capacities. The \nsouthern orientation is the best in terms of insolation. However, in that case, it does not mean \na significant drop in production: a study carried out in Újhartyán, Pest county, for a planned \nlawn area where there was a row of trees on the south side of the area, is why the production \nwas simulated in the case of a southern orientation by cutting it, or by leaving the tree line, \nthey concluded that no significant difference in production is expected if the orientation deviates from the southern direction by leaving the tree line (BARANYÁK & ZALAI 2016). \n\nOn the other hand, suppose the system characteristics are taken into account. In that case, if \nwe limit the production of solar energy in the built-up areas, even in that case, we will significantly reduce the landscape and environmental effects. At the same time, we will give energy planning the opportunity to take advantage of the opportunities provided by the net-\nwork. \n\nDiscussion and Conclusion \n\nThe results show that multiple times of solar energy potentials can theoretically provide not \nonly the set goals but even the entire electricity requirement, but this is not possible due to \nthe unique properties of electricity (VAJDA 2014). In order to reduce the environmental effects, it is worth considering the already built-up areas when planning the utilization of solar energy since the environmental effects, especially concerning installation and abandonment, \nare more favorable. It is even more important that the conditions are more favorable from the \npoint  of  view  of  network  development.  The  differences  between  the  different  scale  raster \nmaps in proportions are slight, but there are already significant differences between the area \nsizes. Taking into account the physical characteristics of electricity and the existing network \nconditions, the first thing to determine at the national level is the solar energy potential of the \nurban environment if these provide ample opportunities for energy planning, as can be seen \nin the Hungarian data, biologically active surfaces must be excluded, after that the system \ncan support energy planning at the national level. At the national level, it is worth excluding \nland use categories with GIS tools and the orientation of the areas. The data show that the \nlocation does not significantly affect solar energy production. In spatial planning, it is worth \nconsidering that areas are falling apart (ÜRGE-VORSATZ & HERRERO 2012), where residential \nenergy development is not significant. Therefore, these settlements must be supported in a \ntargeted manner. These results call into question the establishment of new long-distance electrical networks between, for example, Azerbaijan and the European Union since EU countries \nlocated south of Hungary may have even more significant potential (EU and Azerbaijan Enhance Bilateral Relations, n. d.). \n\nExploring Suitable Indicators for Residential  \nDevelopment and Resilient Landscape:  \nA Case Study in Orlando Metropolitan Region \n\nAbstract: This study examines the form and distribution of public parks in the Orlando Metropolitan \nRegion in Central Florida, USA, to understand the impact of residential land use on the local landscape. \nResearch examining the landscape impacts of rapid urbanization is critical for a holistic understanding \nof planning and design. However, we have not yet thoroughly examined the urban landscape to assess \nthe full impact of urbanization regionally. Here, a novel methodology is proposed to evaluate the relationship between residential development and public parks based on the cumulative metropolitan structures (e. g., urban sprawl) in the study region. Three metropolitan zones in Orlando Region, namely the \ncore, intermediate and periphery, are defined, examining the structure, form, and distribution of parks \nin those zones. Two key indicators: park service areas and their proximity to residential development \nare  explored  and  measured.  The  spatial  distribution  of  residential  development  in  each  of  the  three \nzones is also analyzed to examine the influence of metropolitan patterns on park accessibility and prox-\nimity. These emerging spatial measures offer a broader angle to address landscape inequality and resilience in future design and planning.  \n\nIntroduction \n\nThe concept of resilience has been studied in interdisciplinary science and has progressively \nshifted into coupled terms such as disaster resilience and socio-ecological resilience (HOLLING\n1973, WANG et al. 2020). Landscape resilience is a connected term that integrates landscape \necology, resilience, and sustainability to reflect the coupled human-nature relations (CUMMING \n2011, PLIENINGER & BIELING 2012). Applying the principles of landscape resilience to urban \ndynamics studies is crucial in mitigating the effects of environmental degradation, climate \nchange, and disaster impact (MOREITZ et al. 2011). Rapid urbanization puts natural landscape \ninto urban land use, countering landscape resilience. Addressing the impact of urban sprawl \nis particularly important in this regard. While many efforts have been made to enhance the \nlandscape and minimize environmental degradation, some are limited to on-site design and \nfacilities planning, not fully addressing the role of regional urban growth. A comprehensive \napproach is needed to promote a resilient landscape that reconciles the relationship between \nlandscape efficiency and regional sprawling patterns. This study aims to capture the intrinsic \nvalues of the local and residential landscape by examining the impact of regional urbanization \non public parks. Exploring spatial indicators, the study investigates the influence, structure, \nform, and distribution of public parks in the urban environment. \n\nSelecting suitable indicators for measuring public parks and residential development remains \na challenge for landscape resilience research and landscape design. Some existing park indexes (e. g., Park Score Index) have been put into multiple metrics (e. g., acreage, access, investment, amenities, equity) to assess the quality of the city park systems (PARK SCORE INDEX 2022). ParkServe, for example, uses ParkScore to compare demographic features in \nU.S. cities (PARK SERVE 2022). However, finding what parks or park systems need in an area \nby  applying  only  common  indicators  at  the  city  level  is  inappropriate.  The  regional  level \nextent lacks interpretation. How can suitable indicators that are better applied and reflect the \nurbanized environment in the region be used? Resilient landscape design and planning rely \non a science-based decision-making process (DE GROOT et al. 2010), making the develop-\nment of metrics that better capture park access and residential trends critical. This study addresses two key research questions: 1) how can complex models of residential development \nand park accessibility\/proximity be linked to reflect metropolitan patterns? 2) What broader \ngeographic understanding is needed prior to the application of specific landscape plans and \ndesign details? To address these gaps, the study proposes a regional park access metric that \nincludes indicators of park service area and their proximity to residential development.  \n\nThe objective of this study, which is conducted in the Orlando Metropolitan Region, Central \nFlorida, USA, is to: 1) analyze the process of urbanization and its impact on parks in metropolitan zones; 2) examine any existing patterns of park distribution in relation to residential typologies by utilizing two key indicators: park service area and proximity to residential areas; 3) compare the differentiated regional patterns with residential typologies.  \n\nOur study is based on the following hypothesis: Park indicators are influenced by metropolitan patterns. This hypothesis is being tested against the null hypothesis, which states that park indicators are not influenced by regional urbanization patterns. The goal is to determine if parks display variability in discrete metropolitan zones, spatially. In return, the captured park patterns can provide a deeper understanding to inform regional landscape design and \nplanning for a more resilient landscape.  \n\nMaterials and Method \n\nWe conduct our study within the boundaries of the Orlando Metropolitan Region, which encompasses the two counties of Orange and Seminole in Central Florida, USA. To carry out our  research,  we  utilize  two  primary  datasets.  The  first  dataset  is  the  2019  parcel  dataset obtained from Florida Parcel Data Statewide (fgdl.org), which we clean and reclassify into \nthree categories: multiple-family, single-family, and other residential. The second dataset is \nthe  park  and  the  recreational  dataset  obtained  from  the  Florida  geographic  data  library \n(FGDL), which provides information on public parks in 2019. Our study area encompasses \n631 parks, including various types such as city parks, county parks, facilities, sports, nature, \ntrial, camping, boating, beach, and local parks. \n\nThe metropolitan zones of Orlando have been extensively documented, featuring a sprawl \nstructure that is typical of the region (WANG & MURTHA 2019, WANG & MURTHA 2023). The \nthree zones are defined based on the distance from the city center: core (2-8 miles), intermediate (8-12 miles), and periphery (12-18 miles). To analyze the relationship between park \nindicators and metropolitan patterns, we calculate Euclidean distances using the parcel level \ndata for two typical indicators: park service area and their proximate residential. We apply \nnetwork analysis in ArcGIS Pro to measure service areas of parks with respect to different \nresidential typologies. Based on previous research (NICHOLLS 2001, WOLCH et al. 2005), we \nset walking distances of 1 km and 2 km for residents, as these are considered suitable thresholds. The proximity of residential lands to parks is measured using a program written in Python language that calculates the average distance of residential to parks (DIST) and the average weighted distance from residential to parks (WDIST). The latter is calculated by weighing the distance to parks based on the park area.  \n\nResults \n\nMetropolitan Zones and Parks  \n\nThe Orlando Metropolitan Region has undergone significant urban development, particularly \nin areas located between 8 and 14 miles from the city center (WANG & MURTHA 2023). This \ngrowth has resulted in a historical association between the number of parks and metropolitan \ninfluences  shaped  by  the  urban  context.  Examining  four  decades  of  change  from  1970  to \n2010 (Chart 1), both the urbanized parks' context and urban land development show increas-\ning trends within 2 to 8 miles, and declining trends at 12 to 18 miles. However, in the 8 to 12 \nmiles sprawling structure, the trend is reversed, with increased urban growth correlating with \na decline in the number of urbanized parks. In other words, despite high rates of urban expansion, fewer parks are distributed within the sprawling structure between 8 and 12 miles \nin the Orlando Region. To reflect this theme of sprawl-parks, three typical metropolitan zones \nhave been defined in the region: the urban core area (2 to 8 miles), intermediate (8 to 12 miles), and periphery (12 to 18 miles). \n\nIndicators  \n\nPark Service Area  \n\nOur results reveal the specific residential typologies that can be accessed by parks based on \nthe number of residential lots and area of residential served by parks. Figure 1 illustrates the \ntotal area of parks served (within a walking distance of 0-1km and 1-2km) in the Orlando \nMetropolitan Region across three metropolitan zones. The clustering patterns are diverse in \neach zone, with the most pronounced clusters located in the core zone, near the city center. \nAlthough some scattered clusters are found at specific locations (e. g., near transportation \nroutes), the intermediate and periphery zones exhibit uneven trends in their serviced patterns. \n\nNote:1km means 0-1 km walking distance; 2km means 1-2 km walking distance.  \n\nIn detail, charts from 2 to 7 illustrate the number of residential lots and areas served by parks \nwithin 1 km and 2 km walking distance in the three metropolitan zones. The results show \nthat the core zone of the Orlando Metropolitan Region serves a greater number of households \nand residential areas than the intermediate and periphery zones. The core zone's early investments in landscape infrastructure are reflected in the higher number of households and residential areas served by parks within a 1 km walk as compared to a 2 km walk. Conversely, in the intermediate and periphery zones, parks serve a greater number of residential lots and \nareas within a 2 km walk than a 1 km walk, highlighting the trend of sacrificing park spaces \nfor housing in these areas. This shift away from public open spaces like parks in recent developments,  especially  in  the  form  of  single-family  housing  with  backyards,  leads  to  decreased access to parks for health purposes and undermines park space's social and environmental benefits. \n\nThe trend of increasing housing construction in the Orlando Metropolitan Region has brought \nto light the park availability issue for different residential areas. As shown in Chart 4, parks \nin the intermediate zone serve the least number of single-family households. The imbalance \nof park provision to residential areas in the 8-12 mile range from the urban center suggests \ndifferent demographic communities may be experiencing less access to park infrastructure. \nIn the region, single-family housing continues to be the dominant trend in urban development, while multi-family housing and other residential types are less influenced by park service area measures. However, it is worth noting that parks in the intermediate zone serve more multi-family housing compared to other zones. \n\nAt the regional level, our results illustrate a spatial disconnect between typical residential \npatterns and the supply of parks. Simply, while Orlando's metropolitan landscape has many \nparks servicing communities, there is an uneven distribution. First, greater park accessibility \nhas benefited people near the downtown core, which can be attributed to early historic urban \ngrowth. On the other hand, there is a lack of park access in the intermediate zone. This has \nbeen addressed in the newer developments in the periphery far from the urban center, but \nremains  a  factor  in  the  intermediate  zone.  As  a  result,  Orlando’s  intermediate  zone  is  a \nuniquely sprawling area that needs further examination.  \n\nWhen interpreting the above results to a regional extent, our findings demonstrate an inequality of park service areas in Orlando's metropolitan area. People residing near the downtown core have benefited from better park accessibility and recent urban design and planning efforts. However, those who chose to live away from the urban  center, specifically in the \nintermediate zone, have limited park access but with single-family housing, including backyards. How are parks being valued by residents? For example, what unique combination of features in the intermediate zone has made parks in a sprawling area that deserves further study.  \n\nPark Proximity to Residential  \n\nWe are also looking at how the proximity of parks to residential areas varies by metropolitan \nzone and type of residential, specifically on average distance and average weighted distance. \nIn Charts 8 & 9, parks at the periphery have a greater distance from residential between three \nresidential types. In contrast, the core zone of the study region shows shorter average distances to parks. Those distinct trends highlight the impact of sprawl patterns over the proximity of park infrastructures. As we move from the city center to peripheral areas, both the average and average weighted distances between parks and single-family and multiple-family housing increase for all three metropolitan zones. For other residential lands, there is no \nsignificant difference in proximity to parks between core and intermediate zones. However, \ncompared to multi-family and single-family, other residential lands are further away from the \ncore, indicating a less favorable location for this type of development.  \n\nConclusion and Outlook  \n\nThis study adopts two important spatial indicators, park service area and park proximity to \nresidential, to reveal the urbanization influences in Orlando Metropolitan Region. The areas, \nnumbers, and distances of residential land use to parks are examined based on the spatial \nmeasurements within typical metropolitan structures in Orlando Region. The study reveals \nseveral  significant  findings:  1)  The  core  zone  of  the  study  area  has  the  closest  proximity \nbetween residential and parks, with a large area of residential land use. However, in the metropolitan area beyond the core zone, there are declining trends in both park service areas and \npark  proximity.  2) Early  planning  actions  have  significantly  contributed to  the  core  zone, \nwhere has the dominant access to park infrastructures, but these benefits decreased as one \nmoves away from the urban center. 3) The study highlights that single-family housing is the \ntypical form of urban development and has a major impact on residents’ access to a nearby \npark. In particular, the periphery has the least number of single-family and multi-family that \nare serviced by parks. 4) The intermediate zone has a lack of parks, which calls for further \nstudy to examine the multiple factors behind this phenomenon.  \n\nAs a pilot study, our study proposes that two spatial measures of a regional landscape examination are an integrated approach to enhancing resilient landscape design and planning. By \nexamining the regional spatial patterns of parks and their coupled human-nature relations, \nour study offers a new perspective on evaluating parks' master plan priorities. In the future, \nwe aim to integrate demographic factors into this research by exploring the correlations between race, age, income, and residents’ preferences. In this regard, we aim to address inequality further in a resilient landscape. \n\nIntegration of Urban Green and Blue Infrastructure by \nMeans of an Interactive and Geo-spatial Webmap-Tool \n\nAbstract: As a contribution to the nationally funded project INTERESS-I, we developed a web-based \ntool that balances rain and grey water drainage on the one hand, and vegetation water demand on the \nother. The tool combines GIS-balances of rain and grey water harvest in a catchment area and a day-by-day calculation of water demand from different vegetation structures respective of local weather \nhistory, shading situation and soil water conditions. The tool shows the drought period length a specific \nwater storage volume can bridge. \n\nIntroduction \n\nGlobal climate change accelerates conflicts in urban water use. Hot and dry summers increase \nwater demand of vegetation and the need for irrigation of parks and other green infrastruc-\ntures.  Using  drinking  water  for  this  purpose  is  unacceptable,  and  many  conflicts  between \nmunicipalities are now under consideration regarding the export of water from the periphery \nto  urban  centres  for  irrigation.  This  is  particularly  relevant  when  local  water  use  must  be \nrestricted due to water scarcity. Storing rainwater runoff and locally treated domestic grey \nwater for irrigation purposes can be an appropriate solution to this conflict. Here, the first \nchallenge is to balance rain- and grey water harvest and irrigation demand on a daily basis. \nIn addition, the second challenge is to determine the size of a storage facility, which efficiently – as a precautionary option – secures irrigation water during hot periods.  \n\nTo link urban water harvest and urban irrigation demand, three main questions arise: (1) What \nquantities of water can a given area provide? (2) How much water is needed to maintain the \nvitality of vegetation in a given area under given meteorological conditions? (3) How much \nstorage  capacity  should  be  installed  to  bridge  the  time  lag  between  rainfall  and  irrigation \nperiods?  \n\nMany studies have been conducted analysing irrigation demand and water harvesting potential as an inevitable adaptation to climate change. However, most studies focus on irrigation demand of agriculture on a large scale. Available studies regarding irrigation water requirements in urban areas do not adequately answer all three questions stated above. For example, LUPIA et al. (2017) and CHIU et al. (2020) only estimate irrigation water demand on a monthly basis, thereby ignoring short-term soil water conditions and soil characteristics in their analysis of the water saving potential of harvesting rainwater. CAUTERUCCIO & LANZA (2022) improve on this by recognizing the water storage capability of soil and performing their calculations on a daily basis. However, they only focus on a single (converted military) area. In contrast to this, we provide a comprehensive framework that includes all parts of the system – supply,  demand,  and  storage  –  on  a  daily  and  local  basis,  which  is  also  available  at  a\ncitywide extent.\n\nAs a contribution to the nationally funded project INTERESS-I (https:\/\/www.interess-i.net\/), \nwe  developed  a  web-based  tool  that  interactively  supports  urban  planners  and  other  local \nstakeholders in deriving strategies for the use of urban water resources for irrigation and its \noptimization e. g. via the sizing of storage facilities. The tool was developed and implemented \nfor Stuttgart, Germany. \n\nMaterial and Methods \n\nUrban Water Harvesting \n\nUrban water harvest for irrigation can be obtained from roofs and runoff-generating streets, \nas well as grey water from residential and other buildings. To get data for this we used data \nfrom  Cadastre  Information  System  ALKIS  to  identify  corresponding  urban  surfaces.  In \naddition, data from the German Meteorological Service for Stuttgart city center station were \nused to calculate the formula for an estimate of precipitation runoff water (PWR): \n\nPWR = hN × A × Cm [mm] \nwhere hN is precipitation height [mm], A is area under consideration and \nCm is average runoff coefficient accoding to Table 1. \n\n(Eq. 1) \n\nTo  estimate  private  domestic  grey  water  production,  we  used  GIS  readable  data  from \nCadastre Information System ALKIS and district-related population statistics for the districts \nof Stuttgart. From these we achieved a rough estimate for the number of inhabitants of each \nbuilding  by  disaggregating  district  data.  There  are  different  empirical  rates  of  grey  water \nproduction depending on building use. For residential buildings, a rough estimate is 45 liters \nof  slightly  polluted  greywater  per  inhabitant  per  day  (BDEW  2019).  Production  rates  for \noffice-, commercial-, and bank-buildings can be assumed as 9 liters per employee per day \n(DVGW 2008). According to DENA (2017) usable floor space per employee is 31 m² (office \nand banks) and 34.9 m² (commercial and mixed-use). Using these estimates,we were able to \napply the equation for the estimate of grey water production per day (GWP) for a specified \nurban area: \n\nGWP = TF \/ FPH × GWPH ×1000 [m³] \nwhere TF is total floor, FPH is floor per inhabitant, GWPH is grey water \nproduction per Inhabitant [l per day] \n\n(Eq. 2) \n\nVegetation Structures \n\nUsing  methods  of  object-based  satellite  image  classification,  vegetation  objects  were  detected from data of the Pléiades Earth Observation Mission (0.5 m resolution) for the urban \nareas of Stuttgart. In addition, highly accurate surface heights were detected using Lidar technology. Both information layers together result in a nearly citywide spatial distribution of the \nthree vegetation types “grass\/herbaceous”, “shrub” and “tree”. More details and corresponding  results  are  published  in  NARVAEZ-VALLEJO  & SCHWARZ-V.RAUMER  (2023).  Figure  2 shows a sample of these results. \n\nVegetation Water Supply  \n\nWhen to irrigate and how much? This question refers to the dynamic character of soil water \nstorage, evapotranspiration and the necessary amount of water for irrigation. Irrigation for \nurban  green  spaces  often  follows  experience-based  irrigation  rates  and  rules  or,  as  we  do \nhere, rules that are compiled on a more scientific basis and communicated in handouts like \nFLL (2015) and ALB (2020). Plant available field capacity (PFC) plays the decisive role in \nFLL (2015) and ALB (2020). They assume that the aim of irrigation is to prevent water saturation of PFC below 30 % or above 80 % (see Tab. 2).  \n\nTo map irrigation demand spatially and temporarily during a year, it is necessary to consider \nmeteorological patterns. The  annual  variation in precipitation and evapotranspiration on a \nday-by-day basis determines the availability of soil  water for plants and the necessity and \namount of irrigation. For tree irrigation, however, species-specific interaction with groundwater and special irrigation techniques should be considered. Similar to FLL (2015) and ALB \n(2020), our approach does not consider this for the purpose of simplification. \n\nWe have implemented the interaction of vegetation structure, soil properties and meteorological patterns using a dynamic model as suggested by SWIM (KRYSANOVA et al. 2000). As a \nfirst approximation, our model follows the balance equation, and  thus,  neglects  surface  runoff  and  subsurface  flow.  Again,  simplifying  soil  water  processes, we assume that SW is equal to the saturation of PFC and Percolation PERC is assumed to happen only if PFC is filled.\n\nSW(t+1) = SW (t) + PRECIPt – ETt – PERCt \nwhere  SW(t)  denotes  soil  water  content  start  of  day  t,  PRECIPt  denotes \nprecipitation [mm], ETt denotes evapotranspiration [mm], PERCt denotes \npercolation [mm] during day t \n\n(Eq. 3)  \n\nCalculation of ET follows FLL (2015) and ALB (2020) and multiplies potential evapotran-\nspiration ETpot with site related factors as described in Table 3.  \n\nET = ETpot × V × M × T × S  \n\n(Eq. 4) \n\nFrom  the  two  calculations  above,  it  is  possible  to  calculate  the  necessary  water  supply  to \nsatisfy the demand following the rule, “If PFK saturation is less than 30% irrigate as much \nas necessary to get PFC saturated by 80%” (FLL 2015, ALB 2020). The calculation is possible for every location in the study area that is covered by vegetation on a daily\/5m-rastercell \nbasis in the period 2001-2019.  \n\nThe Water Storage Model \n\nThe storage model for the filling quantity S on day t of a storage facility (tank, reservoir etc.) \nwith the maximum filling quantity Smax is based on the simple water balance equation \n\nS(t) = S(t-1) + WH(t) – D(t) \nwhere  t  =  1,...,365  ,  S(0)  =  Smax  ,  if  S(t)  >  Smax  then  S(t)  =  Smax  , \nWH(t) is water harvest, D is drain during day t. \n\n(Eq. 5)  \n\nWater  harvest  is  available  as  a  daily  value  composed  of  three  components:  (1)  greywater \n(constant), (2) rainfall dependent roof runoff and rainfall dependent street runoff. The reservoir  is  partially  drained  on  days  when  irrigation  is  required  (as  explained  in  2.3).  This  is \nespecially necessary when drought periods must be bridged. Therefore, the size of the reservoir Smax determines the length of a drought period, which can be bridged. The simulation \nof the reservoir filling keeps a record of that length. This enables the implementation of an \noptimization routine that adapts size of the reservoir Smax  according to a given acceptable \nlength of a drought period. \n\nWeb-mapping and User Interaction \n\nWe developed a web-based planning tool1 that allows the delineation of a catchment region \nfor rain- and greywater harvest and specifies the green infrastructure that is to be irrigated \n(Fig. 3). The system can be used to compare less hot years with very hot years and considers \nlocal site conditions (soil, meteorology and shading). It supports the evaluation of site and \nirrigation management strategies. \n\nThe tool offers two main functionalities for a given year in the period of 2001-2022:  \n(1)  Area  based  analysis.  Interactively  select  the  area  for  water  supply  and  the  area  to  be \nirrigated. This leads to a) the selection of buildings and streets, which are analysed concerning their contribution to irrigation water and b) the highlighting of selected vegetation structures (Trees, shrub, grass). It is possible to combine grey water, roof and\/or street run-off. \nThe system’s output describes how much irrigation water the selected “blue” area provides, \nirrigation water demand of the selected “green area” and the drought period length a specific \nstorage facility is able to bridge.  \n(2) Localized analysis. Interactively select a location (5m raster cell). This leads to a display \nof a day-by-day history of water harvest and irrigation demand, as well as meteorological \nand soil water parameters. \n\nFigure 4 shows an example of the tool usage. We used our University buildings, which are \nlocated around a small park, and the attached streets as an example harvest area to compare \nthe roof and street water harvest with the irrigation demand of the central part of the park. \nFor the year under consideration (year 2022), the selected harvest area generally provides \nmore water than the yearly water demand for vegetation irrigation. The synchronisation be-\ntween water availability and water demand is possible if a maximum drought period of 10 \ndays is acceptable. Otherwise, such a period can be bridged by storing 218 m³ of harvested \nwater. To bridge shorter drought periods, storage must be increased. \n\nDiscussion and Outlook \n\nThe identification of irrigation water demand and supply for a total municipal area is an innovative challenge, which we solved with the intensive use of remote sensing and GIS data \ncoupled  with  simplified,  pre-established  models.  The  implemented  webmap-tool  supports \nirrigation management and the calculation of necessary storage capacities for rain- and grey \nwater retention. The model that our tool implements accepts many simplifications: \n•  Water harvest is based on a rough estimate of inhabitants per building, and it doesn’t respect detailed surface runoff coefficients and rainwater sewage systems. \n\n•  Vegetation is classified in only three types (grass, bush, trees), and the types do not perfectly correspond to the FLL\/ALB approach. \n\n•  The soil water balance model is simplified by neglecting different layers of soil depth and root systems, as well as surface runoff and percolation. \n\n•  The urban soil-map we used just gives a very inaccurate estimate of plant available field capacity and does not record urban area anthropogenic soils that occur as heterogeneous mixtures of undefined material in erratic patterns.  \n\n•  We only use meteorological data from one logging station.  \n\nBecause of these simplifications and the lack of calibration and validation for the FLL\/ALB \napproach, the other sub-models, and the complete approach and implementation, the current \nexample presented here only satisfies the requirements of a demo version. First analyses of \nthe model results demonstrate a realistic outcome with a proper order of magnitude. In this \nway, our approach presents a prototype for the implementation of a useful and productive \ntool that helps manage and maintain the vitality of urban green infrastructure. The tool potentially  supports  scenario  calculations,  and  thus  has  the  potential  to  trigger  decisions  for \ninterventions related to the urban water cycle. To achieve this, we are planning an extensive \nvalidation project. The optimized and calibrated tool will then be available to the public administration but also to private property owners. \n\nLongitudinal Water Pollution Monitoring and Retention Pond Capacity Assessment Using Smart Devices \n\nAbstract:  This study experiment uses low-cost smart devices to longitudinally monitor the level of \ncommon water pollutants, such as electrical conductivity (EC) and total dissolved solids (TDS), in a \nretention pond, and assess and quantify a retention pond's capacity for pollution reduction. Landscape \nperformance (LAP) is an important and emerging topic that quantifies the impacts of design practices \nand helps to improve future designs. Although previous research has suggested that retention ponds can \naid in cleaning surface runoff before water is discharged into downstream systems, most of this research \nhas been theoretical, with few studies measuring the water cleaning capacity of retention ponds. In this \nstudy, the research team installs several smart devices with various sensors at each inlet and outlet of \nthe retention pond water system. Environmental data is collected continuously and can be accessed by \nresearchers at any time through an SD storage card. This research presents an alternative way for professionals to evaluate water quality and provides a method for quantifying a retention pond's pollution-reduction ability. The results of this study can potentially improve the existing environmental performance  monitoring system, provide evidence-based data to guide future retention pond projects, and serve as a reference for landscape teaching to enhance the competence of future environmental professionals. \n\nIntroduction \n\nWater Quality and Retention Ponds \n\nIn urban settings, water quality is influenced by various factors, including precipitation, climate, soil type, vegetation, geology, flow conditions, groundwater, and human activities. Activities such as mining, urban development, and agriculture can affect water quality. Non-point  source  pollution  includes  nutrients,  sediments,  and  toxic  pollutants  (CHAUDHRY  & MALIK 2017). Runoff is a major source of water pollution. As water moves along a surface, \nit absorbs trash, oil, chemicals, fertilizers, and other toxic substances. Sustainable stormwater \nmanagement, such as the retention pond, aims to preserve and restore functional hydrology \nby using methods modeled after natural processes (LAF 2022). Sustainable stormwater management techniques lessen thermal pollution, minimize erosion, reduce floods, and reduce \nstormwater runoff. They can also enhance ecological and aesthetic value and help recharge \nthe groundwater (LAF 2022).  \n\nResearch on retention ponds and water pollution has shown that retention ponds can assist in \ncleaning and pure surface runoff before discharging polluted water into the downstream water systems. Retention ponds capture some microplastics in surface runoff and prevent them \nfrom being transported downstream (LIU et al. 2019). Retention ponds can provide a way to \neffectively and economically manage the quantity and quality of urban stormwater (WANG \n& SAMPLE 2014). However, most relevant studies do not quantify the capacity of retention \nponds to treat water pollution. Some studies have involved quantitative studies, but their data \nare usually derived from one or several data collections. Water quality change is a dynamic \nprocess influenced by other environmental factors such as temperature, humidity, weather \nconditions, and air quality. The fact that practically all measures are one-time snapshots is a \nlimitation of the study methodologies (LUO et al. 2015).  \n\nLongitudinal Tracking \n\nIn a longitudinal study, researchers study the same subject repeatedly to detect any changes \nthat may occur over a period of time. This longitudinal study greatly extended the time frame \nfor data collection. Short-term studies do not reveal long-term ties, whereas long-term studies \ncan follow short-term relationships. (GAILLE 2017). Although it is most commonly used in \nmedicine, economics, and epidemiology, longitudinal studies can also be found and play an \nimportant role in studies related to design. Especially in the field of LAP, longitudinal studies \nare particularly indispensable. Research teams on LAP often work separately, and the quantitative methods used and documented landscape benefits differ from each other. All research \nmethods share a common disadvantage: they are one-time  observations (LUO & LI 2014). \nOne-time observations can convey a cross-section of the benefits created by sustainable landscapes, and they are also useful for comparative studies of sustainable and traditional development. But if the goal is to accurately quantify LAP, the reliability and validity of many of these methods are open to question (LUO & LI 2014). \n\nSmart Devices and Environmental Data Collection \n\nEmploying low-cost smart devices may be an alternative way to deal with this issue. With recent advances in technology, miniaturization of electronic equipment, and computing power, \nenvironmental sensors are becoming more innovative, reliable, compact, and cheap (GRIMMOND \n2006, RUNDEL et al. 2009). More cheaply-made sensors are now able to be more numerous \nand densely spaced, with vastly improved temporal collection and rapid data transmission \n(MULLER et al. 2013). \n\nIn some studies, smart devices have longitudinally collected environmental data in many design-related fields. In the field of smart buildings, studies focus on building occupants' thermal  comfort  and  air  quality,  using  feedback  obtained  from  occupants  through  sensor  networks (CORGNATI et al. 2008, CHOI & ZHU 2019). The optimum range is achieved by combining many sensors, including ambient temperature sensors, CO2 sensors, humidity sensors, volatile organic compounds, and particulate matter. The indoor air quality of buildings was examined (CHOI & MOON 2017, JIN et al. 2018). Smart sensors are also rooted in the urban study  field.  In 2013, Zeile, P., Resch, B., Loidl, M., Petutschnig,  A.,  & Dörrzapf, L used sensors to identify places in the urban environment that were considered unsafe by cyclists. \nSpecifically, physiological parameters such as ECG, skin conductivity, skin temperature, and \nheart rate variability are analyzed to determine the moment of stress (ZEILE et al. 2013). The \nresults of the study show that the sensors can identify where there are emotional peaks, particularly fear and anger. This study provides urban planners  with data to support, thereby \nreassessing and planning for those unsafe places in cities (ZEILE et al. 2013).  \n\nResearch Hypothesis \n\nThis paper presents two particular study hypotheses that correlate to the aims, drawing inspiration from prior empirical research and Arduino-based sensors' capabilities: \ni.  Compared to traditional snapshot data collection methods, smart devices provide a more \nefficient and reliable way to monitor and collect water quality data longitudinally. \nii.  Smart devices help professionals better analyze and evaluate retention ponds' capabilities by offering more precise, frequent, and high spatial resolution data. \n\nMethod \n\nOverview \n\nThis  study  developed  a  low-cost  water  quality  detection  sensor  node  for  collecting  water \nquality data (i. e., water temperature, EC, and TDS). Several sensors and other supporting \ncomponents constitute a low-cost smart  water quality detection device for less than $100. \nFigure 1. shows the development of the experimental water quality sensor nodes. By minimizing the programming, installation, and cost, this smart device has the potential for further \neducational outreach and data collection in  LAP and landscape architecture. The research \nprocess used in this study includes the following parts: \n1)  Selection of sensors according to the variables to be studied \n2)  Sensor nodes assembled to meet outdoor sensor requirements: achieving a certain degree of waterproofing, low power consumption and long battery life, effective data \ncollection, and no environmental impact \n3)  Site installation of sensors and data collection for one week \n4)  Data analysis, results, and discussion \n\nThe method uses a longitudinal approach to provide a high level of validity. Additionally, \nthe  use  of  sensor-assisted  longitudinal  tracking  provides  high-precision  data  for  research \nteams.  \n\nAll collected data is analyzed using descriptive statistics, F-tests, and P-tests. Statistic analyses allow the research team to determine and quantify the difference among data gathered \nfrom sensor nodes and understand its statistical meaning. \n\nExperimental Site \n\nThe research team chose the duck pond on Virginia Tech's campus, with two retention ponds \nas the experimental site to test their ability to reduce surface runoff pollutants. The research \nteam installed three sensor nodes at the retention pond's inlets and outlets. Figure 2 shows \nthe experimental site and the location of three sensor nodes. \n\nThe upper pond area is about 5,684.52 m² (61,187.72 ft²) with approximately 2.2 m (7.2') \naverage depth. The lower pond size is about 20,676.59 m² (222,561.01 ft²), and the average \ndepth is about 3.8m (12.5'). \n\nSite Installation and Data Collection \n\nAt the test site, sensors have been placed at three locations – the inlets and outlets of the \nretention ponds. To ensure accurate readings, the sensors are installed close to the shore of \nthe water body, while water quality probes are suspended in the water using wires. The installation ensures that there are no obstacles in the water that would affect the sensor readings, \nas well as no obstacles that would block the solar panels. The solar-powered sensor nodes \noperate continuously, 24\/7, and collect longitudinal data on pollutant concentrations throughout the test period. \n\nThe  sensors  automatically  collect  data  every  hour,  which  is  stored  in  TXT  format  on  an \nonboard  SD  card. The  data  is  later  converted  to  CSV  format  for  statistical  analysis  using \nSPSS. The researchers manually collect the data from the SD card every 48 hours to retrieve \nit. \n\nResult \n\nStatistical analysis of the collected data shows that there is a significant difference in the EC \nand TDS values among the three sensor  modules. Table 1 presents  the  water quality data \ncollected from the sensor modules between December 12 at 12:00 AM and December 18 at \n11:00 PM, with a one-hour data collection interval. The data indicates that sensor 2 recorded \nTDS readings that were approximately 20 ppm lower than sensor 1, and sensor 3 readings \nwere about 30 ppm lower than sensor 2. \n\nTable 2 presents the results of the ANOVA test, which show a statistically significant difference between the observed group data. The F value and sig. (P value) indicate that the null \nhypothesis (H0) should be rejected. \n\nFigures 3 and 4 depict the variations in EC and TDS concentrations at the test site over time \nduring the experimental period. The data collected by sensor 1 at the upper pond inlet indicates that the EC and TDS concentrations in the inflowing water begin to increase at 6:00 \na.m. each day, reach their peak around 4:00 p.m., and then decrease from 6:00 p.m. onwards. \n\nImages generated from data collected by sensor 2 and 3 show that the changes in EC and \nTDS concentrations at the upper and lower pond outlets are positively correlated with the \ndata collected at the upper pond inlet, but with a delay of 2.5 hours and 5.5 hours, respectively. \n\nDiscussion and Conclusion \n\nInitial findings suggest that a low-cost smart device is a feasible method for collecting water \nquality data and assessing the water purification capacity of the retention pond. The experimental results support both research hypotheses, showing that sensor modules can track the \nimpact of human behavior on water quality in real-time and provide high spatial resolution \nwater quality data that can be quantitatively analyzed. Compared to traditional hydrological \ndata, which are typically measured at the river or watershed scale, the experimental smart \ndevice can provide more accurate and high-resolution data for landscape architectural pro-\nfessionals. The data collection interval in this experiment was one hour, but it can be adjusted \nto suit specific sites or projects, with a minimum interval of 2 seconds. The sensor modules \ncan be powered by solar panels and batteries, allowing for longitudinal operation and remote \ndata collection with low maintenance requirements. While the current experiment uses manual  data  collection  every  48  hours,  future  research  can  be  improved  by  adding  a  wireless \nnetwork module to enable remote data collection via cellular networks. The prototype cost \nfor this smart device is less than $100, and with a professionally printed circuit board (PCB) \ndesign, the cost can be further reduced in future development. Future studies could examine \nadditional water quality parameters, but the cost of some sensors, such as nitrogen sensors, \nremains prohibitive. Balancing the cost of additional sensors against the necessary data  is \ncrucial. As the cost of sensor elements continues to decrease, more types of sensors may be \ndeployed for future studies. Additionally, more sensor nodes could be placed in a more spatially dense configuration within the research site. \n\nThe experimental results indicate that retention ponds can effectively purify water, reducing \nTDS concentration by approximately 5.5 ppm and EC concentration by about 0.08 mS\/cm in \nnearly 6 hours for every 10,000 m3 of retention water. However, other variables such as water \nflow, water channel depth, edge roughness, and aquatic life may influence the water purification capacity of retention ponds, and future studies should take these variables into account \nfor more accurate results. Even though some errors might exist, this research, as a pioneering \nstudy, aims to provide data to support some empirical theories in landscape architecture. For \nexample, in this study, the generally accepted theory that retention pond purify water is assisted by  scientific data. The scientific data can be  used to test, support, and complement \nsome of the sustainability theories in the landscape architecture discipline. Landscape architecture is an evidence-based discipline that requires a sufficient basis to guide future design. \nIn order to advance sustainable design practices, it is critical to gather scientific evidence to \nsupport the design and demonstrate performance. The results of this study can provide scientific data to support the development of the discipline. \n\nThis research has implications beyond the field of landscape architecture. With the increasing \ndevelopment of \"smart cities,\" incorporating environmental data can enhance the efficiency \nof resource use and improve the quality of life for citizens. In cities where water resources \nare critical, water quality sensors can track and manage water resources across the city. Real-time monitoring can quickly identify and mitigate pollution leaks and improve resource use \nefficiency. Environmental sensors are not limited to water quality testing; similar technologies can be applied to air quality, soil analysis, and other areas. By mastering these technologies, landscape architecture professionals can become more involved in managing the entire \ncity rather than being viewed as merely initiators and designers that some people think they \nare. \n\nMethodology for Selecting Sites and Creating Urban \nCool Islands (UCIs) in Cities \n\nAbstract: Extreme hot weather causes deaths that are largely preventable. Urban Cool Islands (UCIs) \noffer a landscape architecture solution that can help mitigate mortality brought on by heat waves. UCIs \nare designated city shelters that provide people, especially at-risk population groups, protection from \nextreme heat events. The paper illustrates a methodology to identify Urban Cool Island locations for \nfurther development. This design-oriented methodology defines key UCI elements of public accessibility, good air circulation, shade and water availability and treats them as flexible values, instead of \nfixed, pre-existing features in scientific methodologies. This methodology uses free and open-source \nsoftware and is easily adaptable to mobile phone alert applications. The German city of Leipzig was \nused to illustrate the application of this framework. \n\nIntroduction \n\nGlobal surface temperatures have been rising at an increasing rate and the trend continues \n(HRC  2019).  Hot  weather  can  be  lethal,  killing  more  people  every  year  than  any  other \nweather disaster (NWS 2022). In the summer of 2019, large parts of the European Union \nbroke heat records. Paris recorded its highest temperature at 42 °C with fifteen consecutive \ndays over 31 °C (ACCUWEATHER 2019). Across Paris and the rest of France, 1,500 deaths \nwere attributed to the heat wave that year (GUARDIAN 2019). However, this mortality rate \nwas much lower than Europe’s 2003 heat wave, which was not as hot and lasted for a shorter \nduration but left 14,000+ dead in France (WHO 2007). \n\nWith longer durations of heat and higher temperatures, why didn’t Paris see more fatalities \nin 2019? What reduced the mortality rate so drastically? The answer is that Paris followed \nthe recommended guidelines listed in the World Health Organization’s (WHO) EuroHEAT \nreport  (WHO  2007).  Paris  implemented  a  plan  involving  a  public  awareness  alert  system \n(KERAMITSOGLOUA 2017), creating community shelters (CITES 2022), opening parks, swimming areas, and greening the city with “islands of freshness” (O'SULLIVAN 2022). \n\nA key finding of the WHO report is that “the adverse health effects of heat waves are largely \npreventable” (HALLEGATTE 2016). Reduced mortality in Paris during the 2019 extreme heat \nevents is a testament to how  well Urban  Cool Islands (UCIs)  and advanced  warnings can \nimprove  heat  health  resilience.  Paris’s  islands  of  freshness  are,  at  their  essence,  carefully \ndesigned urban cool islands that prevent loss of life from extreme heat events. And it is this \ndefinition of UCIs that this paper refers to: UCIs are designated city shelters that offer protection from extreme heat events, especially for at-risk population groups. The authors’ aim \nhere was to create a framework for identifying sites to create UCIs and making this information available to everyone. The framework’s design-oriented landscape architecture methodology provides a simple, step-by-step process. Completing all the steps of the methodology acts as a process of elimination before beginning detailed analysis; it saves time and effort \nby creating a short-list of results to be used by city planners for more complex calculations. \nThe  paper  focuses  on  outdoor  spaces  because  they  have  a  secondary  effect  of  improving \nurban environments (KOWARIK 2016). And while rolling blackouts and power outages often \nhappen during extreme heat, this methodology can easily be expanded to include architectural \nand air-conditioned spaces as emergency shelters (KENWARD 2014). \n\nMethodology \n\nFor this study, free or open-source digital applications and tools such as QGIS, OpenStreet-Map, and Google Earth were used. The resulting  map highlights high-risk  mortality areas \nand their proximity to potential UCIs shelters. The combination of this methodology with a \npublic alert system provides a powerful tool for combating suffering and  mortality during \nextreme heat events. \n\nThe steps are simple: Analyse the city for accessible sites with good air flow, then intersect \nthe results for potential UCI locations. Determine at-risk groups’ proximity to the areas, calculate occupancy and select the  most valuable locations.  Adjust the selected locations  for \nwater and shade as needed. Make the findings available through a public emergency address \nsystem during extreme heat events. \n\nAnalyse Accessibility \nThe first step is to create a map analysing areas that are accessible to the public and emergency vehicles; both must be able to travel to UCIs easily (Figure 1). Hard and softscapes are \nacceptable, such as public parks and train stations. This step simply asks if all people; the \nhandicapped, disabled, very young and old, can easily gain access to the UCI. Access also \nneeds to include emergency vehicles, so while forests might be valuable shelters, their distances from emergency personnel can make it ineligible as a UCI.  \n\nThe value of 1 is assigned to areas that are not accessible. These locations may have characteristics of a UCI, but they are ineligible because they are private or pose obstacles that prevent  the average person or emergency vehicles from gaining entry.  Semi-public lands are \nassigned a value of 2 and include such spaces as apartment complexes or sports grounds. \nThese areas might be privately owned but open to the public. For semi-public lands to be a \nUCI location, permission from the landowners must be obtained prior to creating the UCI. \nValue 3 spaces are public spaces with no access restrictions areas, including public parks, \nmarketplaces, plazas, or local sports grounds. \n\nAnalyse Air Circulation \n\nOnce accessibility is determined, the next step is to locate potential areas with good air circulation.  Air  circulation  is  determined  by  how  confined  a  space  is.  Like  the  accessibility \nevaluation, a 3-point scale is used to rate air circulation of an area. An area that is surrounded \nby structures by more than 76% of its circumference has a value of 1. Spaces that are surrounded by structures between 26% to 75% have a value of 2. Spaces surrounded by less than \n25% or not at all are given a value of 3. There may be cases where vegetation and trees impact air flow. Clearing brush and thinning trees may be needed to allow for accessibility and air \ncirculation. A reference chart to determine valuable spaces by percentage is shown in Figure 2. \n\nTo complete the supply identification phase of the methodology, the values assigned to accessibility and air circulation are averaged, producing a single total value for each UCI area. \nThe result is a simple scale of 1 = no, 2 = maybe and 3 = yes, which is easy to understand \nwhen working across many different disciplines. This result is the UCI potential site list. \n\nDetermine At-Risk Populations’ Proximity to Potential UCI Sites \n\nIdentifying demand requires a comparison of who are at-risk and their proximity to potential \nUCI sites. When considering who are at-risk, studies find a clear correlation between extreme \nheat and mortality in the elderly, young children, the mentally ill and the sick. In addition, \nsocio-cultural factors also contribute to those who are at-risk. These include people who face \nsocial injustice: the poor, the uneducated, and foreigners (Kowarik 2016). For this case study, \nbuilding\/land use data sets were used to locate hospitals, schools, and other at-risk structures, \nwhile land value maps were used to determine household income. Germany protects personal \nprivacy so precise data could not be used in this case study, but the land value maps revealed \nboth adequate and lacking areas for UCI sites. \n\nWhile UCIs accommodate everyone in a community, they must cater to those who need them \nthe most. Two basic demographic population categories to be considered are age and income. \nIn  this  step  of  the  methodology,  no  values  are  assigned.  Instead,  different  demographic \ngroups are assessed based on the distance they can safely travel. A buffer of 100-meter increments is used to estimate these distances (Figure 3). For instance, the very young and the \nvery old may have great difficulty walking a kilometer, but other groups will have no problem. More precise distance measurements can be calculated in various ways, but for the sake \nof  easily  identifying  risk  and  proximity,  a  buffer  is  sufficient  to  inform  city  planners  and \nhealthcare professionals where additional focus and attention should be applied. \n\nDetermine Capacity \n\nIt is also important to calculate UCI occupant capacity. The calculation answers the question: \nis the UCI large enough to serve its community? This capacity is an estimate of the number \nof people that can be helped using a standard of one person per 4 square meters. This number \nis based upon emergency Temporary Hospitals (THs) assessment, giving each person room \nto  lay  or  sit  comfortably  while  allowing  for  stretcher  clearance  and  emergency egress \n(CAMPOS 2022). \n\nTo calculate occupancy, a three-meter buffer is applied to all potential UCI sites. This allows \na tolerance for hedges, bushes, gates, and comfortable distances from roadsides and any other \nobstruction that would prevent a person from sitting and resting. The 3-meter buffer is not a \nfixed distance and can be based on cultural preference. \n\nNext, all spaces less than 10 meters square are eliminated. This 10-meter distance is Leipzig's \nobserved standard area needed to plant a shade tree (Tilia cordata). In most cases, a space \nthat cannot accommodate a full-grown tree is not suitable for a UCI. Small spaces that might \nqualify would include hardscape areas such as public markets and train stations. \n\nFinally, sealed areas are calculated. Sealed land is determined by buffering the appropriate \nwidth of all paths, sidewalks, and roads in the area. Sealed land is also subtracted from the \nusable space in UCI analysis. Zentrum, a sample area for this study, is almost entirely sealed. \nThat does not disqualify it as a UCI, but it does mean the temperature reduction provided by \nshade will never match that of a green space. The final formula is for calculating UCI occupancy is:  \n\nOccupancy = (Area of potential site) − (3-meter Comfort Buffer) − (all areas ≤10 meters squared) − (% of Sealed area) divided by 4 (people) \n\nThe UCI occupancy can now be compared with a city’s population and at-risk needs, creating \na list of valuable UCI locations. \n\nAdjust for Shade \n\nShade is a requirement for a UCI. However, good shade coverage at midday is not always an \nexisting feature. Therefore, shade is not factored into the UCI value at first. Unlike accessibility, air circulation or proximity, shade can be added or removed later. Trees can be planted \nin a green park with little shade. Marketplaces with 100% sealed grey space are unacceptable \nfor a permanent UCI, but temporary tents or awnings can transform them into valuable emergency shelters. Therefore, once areas of risk and their proximity to preferred UCIs are identified, shade can be provided to create a safe area. \n\nThere are two basic approaches to shade. Green Solutions include planting trees for shade, \nincreasing permeable materials for pathways and hardscapes, and maintaining wooded areas \nto keep ground cover from overtaking suitable areas. Clearing overgrown areas around natural water features such as riverbanks, lakes and canals allows people to keep cool by bathing \nand swimming. Grey Solutions include erecting temporary shade such as tents and awnings. \nPermanent and temporary water features such as fountains and pool, water misters, and evaporative cooling fans reduce the ambient temperatures at grey landscapes. \n\nAdjust for Water \n\nWater is essential to a UCI. Water is nearly always available in Leipzig. But if there is no \nexisting water, other sources (e. g., fire hydrants, hoses, misters, etc.) must be made available \nfor potable water and cooling  water alike. A supply of potable  water is easily accessed if \nUCIs are in public buildings such as schools or libraries. For parks or marketplaces, emergency services can provide water from fresh-water fire hydrants and water mains. In all cases, \nwater supply needs must be solved on a city-by-city basis. \n\nWater is also used for cooling the ambient temperatures of a space. Water evaporation is a \nnatural  form  of  cooling  that  works  well  in  environments  with  moderate  to  low  humidity. \nThere are many products on the market that cool the ambient temperatures of outdoor spaces. \n\nCreate Public Address System \n\nThe lack of an alert system was identified as the main preventable cause of mortality in Paris \nthe summer of 2003 (WHO 2007). That is why it is critical to alert the population of pending \nextreme heat events and guide them to shelter. Many cities already have public address systems in place, or volunteer emergency brigades that can help ensure public awareness and \nsafety. There are also existing free phone apps for extreme heat alerts (EXTREMA 2022). \n\nCase Study \n\nLeipzig (Germany) was chosen as an example to apply this methodology. Leipzig is home to \nnearly  600,000  people.  It  has  the  median  yearly  income  of  around  €40,000.  At-risk  age \ngroups  make  up  26.6%  of  the  residents  (STADT  LEIPZIG  2020).  For  this  case  study,  three \ndistricts were chosen for comparison: Gohlis, Grunau and Zentrum. Each district is distinct \nin its different income and housing styles. Gohlis tends to be upper middle class, inner city, \nand  a  traditional  residential  style.  Grunau  is  predominantly  lower  income,  locating  at  the \nperiphery of Leipzig’s boundary. It has a combination of single home dwellings and apartment complexes. Zentrum is the city centre with high-value properties and businesses but \nfew residents, and a large, fluctuating, daytime pedestrian population (Figure 4). \n\nIn all instances, the at-risk population of each district in Leipzig is below the UCI capacity. \nGermany restricts personal data access, so property value (instead of income) was used to \ndetermine the districts’ at-risk population sizes (Figure 5). \n\nGrunau district stands out in terms of need. Approximately 5% of the population lives more \nthan half a kilometer away from a potential site, and most of the population is poor. Using \nthis data, city planners can begin adding shelter for at-risk populations, knowing where to \ncreate UCIs, how big they should be, and how much shade and water will be needed to ensure \nadequate safety (Figure 6). \n\nThis analysis can be used for more than short term emergency planning. Resulting data can \nbe used to guide changes to zoning plans, determine where best to allocate resources for new \npublic  spaces,  influence  property  values,  initiate  community  support  measures  that  guide \npublic investment strategies, and mitigate climate displaced refugees (IOM 2008). Further \nanalyses on  wind direction and flow, public transportation  proximities, or specific private \ninformation for at-risk groups would further enhance the results. \n\nConclusion \n\nDangerous  heat  waves  are  increasing  in  temperature,  frequency,  and  duration.  As  power \nblackouts happen, electrical plants shut down and transportation comes to a halt (KENWARD 2014, WEHRMANN 2019, MUDGE 2019), there is an increasing need for landscape architecture  solutions  to  shelter  people  from  extreme  heat  events.  This  methodology  provides  a framework for putting UCI creation into practice, and it is expected to evolve and adapt to cities’ specific needs. It offers a solution to an imminent threat and encourages action, which will provide protection and save lives. \n\nRevealing the Power of Landscape in Mitigating the \nClimate Crisis  \n\nAbstract: Climate crisis demands decarbonization of our energy supplies. The upscaling of renewable \nenergy is accelerating around the world. Most renewable energy  projects fail to realize values other \nthan reducing greenhouse gas emissions and societal support is eroding. Many critics appropriate ‘landscape’ to oppose, postpone or reallocate climate action. The objective of this paper is to highlight another, often overlooked power of landscape: The power of landscape to enable learning, reflecting, and \nresolving critical questions regarding climate action. Landscapes, we sustain, deserve ever more attention in the pursuit of timely climate action. We illustrate that the transition to renewables provides the \nvery foundations for new cultural landscapes. This paper draws from a recently published book – The \nPower of Landscape – which presents our research and design at the Amsterdam Academy of Architecture, complemented by essays from renown architects, geographers, sociologists, and historians.  \n\nIntroduction \n\nThe objective of this paper is to introduce the reader to the power of landscape – three different kinds of power that we encountered while working as landscape architects on energy \ntransition. While most of that knowledge has been acquired through in our research on the \ntransition from fossil fuels to renewable energy, the findings are also relevant for other transformative landscape challenges of the 21st century. The first power of landscape refers to the \nfact that landscapes are affecting the quality of life of most if not all humans on Earth. Secondly,  landscapes  are  powerful  in  mirroring  human  successes  and  failures.  Thirdly,  landscapes have the power to resolve urgent questions many of us face. Have you, for instance, \nbeen wondering whether energy transition should be pursued in a centralized manner or more \ndecentralized way, bottom-up? In the book that provides the foundations for this paper we \nshow that landscape can provide answers to this and many other critical questions. Beyond \nthe agency of landscape, it has been a shared wish to collect and cultivate novel narratives \nthat can help to engage with the energy transition (STREMKE et al. 2022). \n\nHumans draw energy from landscapes around the world such as the open-cast coal mines in \nColumbia. A large share of coal from Columbia is exported to Europe and to the Netherlands, \nour country of residence, until the closure of the Amsterdam RWE power plant in 2019. The \nlandscapes  we create are not innocent. Quite the opposite is true: They affect the lives of \nothers – to paraphrase Andrea Cardoso from Columbia who contributed to the research presented in our book. Our consumption patterns, put simply, affect those living in or near energy landscapes. And yet we continue to alter the surface of the Earth. Unprecedented and \nunreversible transformation of landscapes like in the Appalachian Mountains in West Virginia in the United States. The life of residents in that energy landscape is challenging to say \nthe least. This specific kind of coal mining – mountain top removal – comes with frequent \nexplosions and heavy traffic. This share of the Appalachian Mountains may be a thing of the \npast, anytime soon. But also much closer, not far from Dessau\/Germany – the location of the \n2023 DLA conference − villages are erased from the surface of the Earth for coal mining. \nThe church of Heuersdorf, for example, is the only relict of an entire village. Gone are the \ndwellings, gardens, and the entire surrounding landscape of Heuersdorf, hundreds of years \nold. \n\nIn different ways but similar magnitude, renewable energy landscapes like the Vajont dam in \nNorthern Italy affect human lives. On the 9th of October 1963, a landslide caused a massive \ntsunami resulting in the death of more than 2.000 inhabitants. Not all energy landscapes are \nas violent as the coal mines in Columbia, the US and Germany, or the Vajont dam in Italy, \nbut our energy demand certainly entails costs that are scarcely mentioned in the newspapers. \nWhat we do read is that we are currently importing more oil and gas from Northern America. \nBut where exactly does that energy come from and what kind of energy landscapes are materializing? Much of the energy originates from the oil sands in Northern Alberta\/Canada. \nThis fossil energy landscape is larger than all of England. Three times larger than The Netherlands; the only energy landscape that can be seen from space. And those energy landscapes \nare powerful indeed, affecting those living inside or nearby. In the summer 2019, the entire \nlandscape was on fire causing thousands of people to evacuate the region. Despite the recent, \ngeopolitically motivated resurrection of fossil fuels they will be outperformed by renewable \nenergy sources in terms of greenhousegas emissions and because many are less expensive. \n\nFossil Thinking \n\nSadly enough, some of the renewable energy landscape dress their like fossil predecessors. \nWho could easily tell that Figure 1 features a geothermal power plant, that the resulting energy landscape is renewable in nature? Of course, they are effective in mitigating the climate \ncrisis, but renewable energy landscapes too can be monofunctional, blunt and reckless. Another example is the wind energy landscape near Palm Springs, the biggest one in the USA, \nan agglomeration of thousands of wind turbines. Every type of wind turbine that was ever \nsold in the US can be found in that energy landscape. The mountain range to the South of \nPalm Springs serves as visual pain killer but the daily reality of the local landscape users is \nharsh. \n\nMore traces of what we call‚ fossil thinking‘ can be found in renewable energy landscapes. \nIn the Sierra Nevada\/Spain, for example, the sublime mountain panorama helps to mitigate \nthe appearance of solar power plants. But their high fences, the mono-functionality, and the \nabsence of vegetation underneath the installations are a straight-forward expression of ignorance  for  our  planet.  Less  violent  than  other  energy  landscapes,  perhaps,  but  certainly  no \nexpression of stewardship for ‘Gaia’ – to quote BRUNO LATOUR (2017) who reminded us of \nour new responsibilities now that we have entered the Anthropocene. At some point, during \nour field work in Andalucía\/Spain, we encountered a sign outside a solar power plant stating: \nProhibited to take photographs: To us, this symbolizes a call for help. Have we created yet \nanother  ‚monster‘  to  paraphrase  Dutch  philosopher  MARTIJNTJE SMITS (2006).  We  know, \nsomething is wrong, and some may argue it cannot be dealt with. But is that really so? \n\nTo put things very clear: Nothing is wrong with renewable energy technologies per se, they \nare efficient and affordable, and help mitigating climate change. But once they arrive in our \nliving environment something is happening. Let’s take a closer look at the concerns people \nhave  about  energy  landscapes,  with  the  help  of  a  specific  example:  The  Cleve  Hill  Solar \nPower  Plant  −  one  of  the  largest  renewable  energy  landscapes  in  the  United  Kingdom. \nPHILIPPA RODDIS and her team conducted an online questionnaire to find indicators for community  acceptance  (2020).  More  than  800  people  participated.  Their  concerns  have  been \ncoded and ranked as following: 18% of all concerns relate to ‘wildlife and habitats’, 10% to \nthe ‘project scale’, and 8% to ‘landscape character’. Nine of the top 10 determinants of community acceptance are linked with landscape. Together they amount to circa 70% of all determinants. Put simply: More than two-thirds of all concerns that people have are related to landscape. This certainly does not align  with the  way applications for building permits or subsidies for solar power plants are evaluated in the Netherlands and many other countries.  \n\nAlternative Narratives \n\nIn our book, we question the strong focus on energy technologies such as power plants, photovoltaic panels, and wind turbines. We illustrate that energy transition is not about technology, but about a changing environment: the landscapes we live, work and recreate. And there \nis growing evidence of possible synergies between renewable energy and other challenges in \nour landscapes. Renewable energy, to begin with, can help recovering soil quality and biodiversity, especially in arid conditions such as in Southern Spain. Underneath the solar trackers \nin Las Gabias, for example, we found a closed vegetation cover with high biodiversity. This \nenergy landscape is called ‘solar orchard’ by the locals.  \n\nIn other places, the accessibility to the landscape has increased, for example through maintenance roads that came with the construction of wind turbines near Viticuso\/Italy and Tarifa\/ \nSpain. In fact, in Tarifa wind energy is celebrated. The terrace of the local Cafe Con Horizonte faces the wind turbines and many photographs in the restaurant feature the local wind \npark. In other places it is the very energy infrastructure that creates access and hosts new \nfunctions such as the public park on the crest of the hydropower plant in Canales\/Spain. \n\nSolar energy landscapes too, can increase accessibility to our living environment. In Bronck-horst\/The Netherlands, a piece of landscape that was largely inaccessible until the arrival of \nthe solar power plant is now open to the public during daylight hours. Some of those energy \nlandscapes host community gathering spaces. More importantly, we were told that the co-creation  of  renewable  energy  landscapes  such  as  in  Bronckhorst  or  in  the  Bioenergiedorf \nJühnde\/Germany were important cornerstones of active community building.  \n\nRenewables can provide the economic means to maintain cultural heritage such as Torrigiani \ngardens in Florence – an UNESCO world heritage site. In 2021, the owner told us that the \nPV panels allow for the greenhouse and the adjacent villa to be used for venues throughout \nthe year. Elsewhere, energy infrastructure like the Art Deco penstock towers at the Hoover \ndam in the United States have contributed to our architectural and thus cultural heritage. Even \nif the US no longer needed the electricity from this hydropower plant, the Hoover dam would \nnot be dismantled as it is listed as national heritage site. \n\nOther renewable energy landscapes have become landmarks, such as the Concentrated Solar \nPower Plant next to the Interstate 15 between Los Angeles to Las Vegas. This type of energy \nlandscape is frequently associated with post-carbon futures and featured in the blockbuster \nmovie Blade Runner. The movie caption says ‘California 2049’ which is somewhat puzzling \nbecause the Ivanpah power plant is already in operation for many years.  \n\nOther renewable energy landscapes have been featured in commercials for electric cars and \nare associated with contemporary fashion design, such as the Gemasolar Solar Power Plant \nin Andalucía (Fig. 2). Gemasolar even has a Google Map review. Are these first signs of our \nsociety to become more acquainted with those kinds of landscapes? Are we starting to associate them positively with our new responsibilities in the Anthropocene and the response to \nclimate change? \n\nEither way, energy landscapes can become part of very personal experiences as evidenced \nby the large number of wedding photographs that are taken in wind parks. The Irene Vorrink \nWind park near Lelystad\/The Netherlands is a particularly interesting energy landscape. For \nthe very first time in the Netherlands, people opposed the removal of the wind park. Should \nthe Cultural Heritage Agency of the Netherlands be called upon?  \n\nConcluding this section of the paper, we like to remind the reader that we found evidence of \nfossil thinking as well as promising synergies between energy development and other landscape challenges, across the portfolio of renewable energy landscapes. We also learned that \nthe very same energy technology can generate very different responses, depending on a multitude of factors (ENSERINK et al. 2022). What if  we as society have been focusing on the \nwrong thing – energy technology – during the initial phase of the transition, and what should \nwe be focusing on in the future?  \n\nThe Power of Landscape \n\nEnergy transition is not about technologies exclusively but about the landscapes that change \nwith the introduction of technologies. Predominant questions in the current discourse revolve \naround the what (choice of technology), the who (involved actors) and the when (urgency of \na timely transition). The discourse yet needs to adopt where and how questions. ‘Landscape’ \ncan interconnect these questions and in that sense provides a solid starting point for a more \ninclusive debate on energy transition (OUDES 2022, PICCHI et al. 2023). To exemplify the \npower of landscape for energy transition and beyond, we will now focus on one of the frequently voiced dilemmas: central versus decentral decisions making and implementation.  \n\nIn centralized energy projects such as large hydropower plants or concentrated solar power \nplants, the initiative is typically taken by stakeholders from outside the local landscape. In \nthe attempt to rapidly increase the installed capacity of renewable energy, national policy, \nlaws and subsidy schemes affect both project developers and energy infrastructure. The resulting intervention, however, affects local landscapes. Those in favor of centralized energy \ntransition stress the urgency of climate mitigation, advocate swift action and top-down development. Many decentralized projects, often entailing wind and solar energy, are initiated \nand (partly) owned by local communities. In the case of Southill Solar in the United Kingdom, the local community owns and decided about the design of the solar landscape (OUDES \net al. 2022). Advocates of a decentralized energy transition point to improved energy democracy and self-sufficiency. In the current energy transition discourse, the central and the de-central perspectives are often presented as if it is either one or the other. \n\nFor this dilemma, we argue that the power of landscape lies in the fact that both the central \nand decentral perspective  meet in the  physical environment experienced by landscape  users. Landscape thus provides a mirror of our success or failures. On the local level it is important to realize that any project is part of the challenge to achieve regional and (inter)national energy targets. In turn, national or regional policy − intended or unintended − affects \nthe choice of location, type of technology, spatial configuration as well as scale and number \nof  additional  functions.  Both  perspectives  are  therefore  needed,  and  their  hybridization  is \nbelieved to be beneficial in the long term. We will, in the following, illustrate the power of \nlandscape by drawing evidence from historical map analysis, spatial analysis and fieldwork \nin contemporary energy landscapes, and design research on future energy landscapes.  \n\nLearning from the Past \n\nThroughout history, both central and decentral perspectives resulted in tangible energy landscapes. Among the first large-scale energy landscapes developed in a centralized and topdown manner were the forest removals during the antique and medieval times, for example \nin  the  Northern  Italian  plain.  At  that  time,  wood  was  used  as  main  source  of  energy  and \nconstruction material in many regions around the world, before it was replaced by other energy sources such as coal. Almost simultaneously, small-scale, and locally developed vernacular energy landscapes such as in Mazara\/Italy appeared around the world (Fig. 3).  \n\nMoving ahead in time, a new form of energy landscape started taking shape once the first oil \nwell was drilled in Titusville, Pennsylvania in 1859. The resulting landscapes took on a much \ndifferent appearance than any of its predecessors. Exoskeletal drilling rigs starting marching \nacross the land like an invading army as frenzied speculators scrambled to extract the black \nliquid  from  the  land.  As  enthusiasm  spread,  those  energy  landscapes  started  appearing  in \nmany countries (PASQUALETTI 2022, 17). \n\nCentralized development of energy landscapes is still a default reality in the Global South. \nMultinational energy companies operate far away from most of their clients − us − in the \nlandscape of others, such as in the case of the Cerrejon coal mine in Colombia. Coal mining \nhas dried up drinking water and afflicts agriculture, air quality and causes diseases. The life \nof the local inhabitants is dominated by the presence of the mine. In 2015, the Cerrejón company began diverting 3.6 kilometres of the Arroyo Bruno for a mine expansion. The stream \nis one of the main tributaries to the Río Ranchería and a source of water for local Wayuu \ncommunities as well as for the town of Albania. The river diversion caused more drought and \nforces the members of the Wayuu community to collect water far away from their dwellings \n(CARDOSO & BANKS 2022). What we learn from these cases is that landscape indeed possesses the power to mirror our success and failures. Existing landscapes illustrate benefits \nand drawbacks of centralized as well as decentral approaches to energy development. They \npresent an almost indefinite source of knowledge for learning, reflecting, and improving.  \n\nQuestioning the Present \n\nSolar energy landscapes − a type of energy landscape that has become rather popular over \nthe past decade or two − originate both from central and decentral approaches to energy development.  In  the  early  stages  of  the  current  energy  transition,  the  Solarfeld  Gänsdorf \nemerged in the highly productive agricultural landscape of southern Germany (Fig. 4). This \nproject came into being when national subsidies were generous and local planning regulations scarce. A local entrepreneur seized the opportunity to develop this large solar power \nplant, making sure taxes remained locally and thus doubling the tax-income of the local municipality.  \n\nOther solar landscapes such as the earlier mentioned Kwekerij in Bronckhorst\/The Netherlands have been co-designed with and are responsive to the wishes of landscape users. This \nparticular solar park − unlike many others power plants that carry that name – is actually a \npark. Located at the edge of a small village, local inhabitants use the solar park for community gatherings, children as playground and others for lunchtime strolls. The Kwekerij has \nbecome a best-practice and draws visitors from all over the country and abroad (OUDES & \nSTREMKE 2021).  Please  note that  the  Power  of  Landscape book  presents  many  additional \ncase studies that help learning how the global energy transition can be localized. \n\nImagining Alternative Futures \n\nBuilding upon a critical reflection of the present, we can start imagining alternative futures. \nFor the next generation of landscape architects, the need for energy transition is no longer a \nquestion but a fact. While teaching at two Universities, we realized that for young landscape \narchitect, the goal is no less than designing the cultural landscapes of the 21st century.  \n\nThere are sufficient valid reasons and pressing issues that support their ambition to design \nnew cultural landscapes. However, centralized targets with regard to energy transition, climate adaptation, biodiversity crisis, food security and other 21st century grand challenges are \noften accompanied with quantitative models and criteria aiming to ‘mitigate impacts’. The \nnext generation of landscape architects, through their design research, emphasises the need \nto move beyond impact mitigation and, instead, explore local value creation.  \n\nThe Blue Heart project by Changsoon Choi can serve as an illustrative example (STREMKE \net al. 2022). It explores the multifunctional values of a new island near the coast of North \nHolland. The island improves coastal protection, enables the generation of renewable energy \nfor the city of Amsterdam, and creates a new habitat for seals. The location of the island is \ndetermined so it can function as an ecological steppingstone between the west-coast of the \nNetherlands and the Wadden Sea. In doing so, the project reveals the power of landscape \n(architecture) in the exploration of value creation rather than merely mitigating the negative \nimpacts of energy technology. \n\nIn the Land of Succession project, Hester Koelman takes a more decentralized approach employing a farmers-citizens cooperative and addressing local challenges such as nitrification \nand peat oxidation in the Lopikerwaard landscape near Utrecht\/The Netherlands (STREMKE \net  al.  2022).  She  proposes  for  this  landscape  to  host  30.000  oscillating  vertical  axis  wind \nstacks − a captivating new structure and experience for landscape users. The stacks form a \nnew grid, contrasting with but respecting the historical polder structure. Citizens can adopt \ntheir own wind stack − capable of generating power for ten households − and use the polder \nas a recreational natural landscape (Fig. 6). Centralized targets can help to push the imaginaries of local landscapes, yet the development of these designs is not top-down, but requires \na locally supported, coherent vision of landscape quality and other parameters relevant for \nthe acceptance of landscape transformation by the landscape users. \n\nPublic art, alike landscape design, can help to foster the societal dialogue on energy transition \nand the quality of our living environment at large. Since the second half of the 20th century, \nart advocated for social justice and pinpointed towards key environmental challenges. Artists \nand artwork can support local communities to imagine alternative futures. Art can, for example, interpret local identity and values such  as in  the Energy Duck project in Copenhagen \n(MONOIAN & FERRY 2022). It can provide opportunities for people to relate to their landscape, engage with other people, and strengthen their connection to nature as in the case of \nthe Solar Mountain project in Northern Nevada (MONOIAN & FERRY 2022). Public art can \nsupport  both  central  and  decentral  approaches  by  illustrating  the  beauty  and  promise  of  a \nworld that has moved beyond carbon and thus becomes an important constituent of the cultural landscape of the twenty-first century. \n\n\nEpilogue \n\nBy complementing the what, who and when with the where and how questions, a more encompassing energy transition narrative emerges. The answers to these questions, together, \nprovide the building blocks for more meaningful decisions and define how the energy transition takes place in different places, in different landscapes. Our preference for renewable \nenergy sources − effective to mitigate the global climate crisis − leaves us with many choices \nto be made during local implementation. Choices perhaps less related to quantity but rather \nto quality. Quality of implementation processes as well as the outcome of those processes: \nrenewable energy landscapes. Decisions taken during the local implementation will have implications for social and spatial justice, energy costs, energy security and, ultimately, the pace \nof energy transition. The resulting energy landscapes and their experience by landscape users \ndoes and will continue to mirror these decisions in most parts of the world. \n\nThe introduction to this paper argued that the energy transition is not so much about technologies but about changing landscapes. The landscapes where we live, work and recreate. Our \nselection of projects from the Power of Landscape book presented in this paper, in a complementary manner, provides evidence to sustain this proposition. We are confident that a more \nencompassing approach to energy transition involving landscape and landscape  users  will \nbenefit both the quality of the transformation and its timely realization.  \n\nDespite the ground-breaking work of fellow landscape architects such as Sylvia Crowe and \nGeoffrey Jellicoe in the 1950s, we are just starting to realize the true power of landscape. \nLandscape, of course, is not a standalone superpower, it needs to inform policy instruments \nsuch as renewable energy subsidies and planning regulations. Luckily, these instruments are \ndeveloped by people. People that we recommend to visit and explore energy landscapes, to \nconverse with locals and experts, to keep innovating and reflecting. \n\nDeparting from the landscapes that people use and cherish, instead of technological performance and economic growth that has dominated the post-modern era and thus much of the \ninitial phase of today’s energy transition, certainly leads to different conversations. Conversations in which memories of the past and images about the future are starting to coalescence \ninto the cultural landscape of the 21st century. \n\nThe Driverless City: The Urban Possibilities of \nAutonomous Vehicles and Navigation Safety \n\nAbstract: The advent of autonomous and ubiquitous co-robot technologies presents an opportunity to \nrecalibrate current automobile transportation infrastructure in dense urban cores. This research investigates the trade-offs between navigation safety, functionality, and experiential conditions to understand \nthe impact of driverless vehicles on urban design. The result will be a framework of forecast scenarios \nto advise urban designers, policymakers, stakeholders, and the autonomous vehicle industry on critical \nfactors to consider when deploying these technologies. The research examines how cities can leverage \nupcoming  mobility  technologies  to  retrofit  late-nineteenth-century  automobile  transportation  infrastructure into human infrastructure for the twenty-first century. This excerpt establishes a system for \nusing passive landscape objects as reference points to improve the navigation ability of driverless cars \nwhile addressing environmental issues like Urban Heat Island. \n\nThe research builds a technical framework based on current driverless technologies and IIT's interdisciplinary Driverless City project methodology to understand the limitations of GPS availability and landmark-based  navigation.  The  final  component  incorporates  speculative  scenarios  to  increase  the  localization \nsafety of autonomous cars by using landscape objects in a binary error-correcting code in State Street(Chicago). The results of these scenarios suggest an increase in landscape elements as landmarks that can be \ntranslated into an increase in canopy coverage, maximizing the benefits of the urban tree canopy. By employing our findings in a 3D simulation of State Street, we determined that our system improves not only \nnavigation accuracy but also environmental benefits like outdoor thermal comfort. \n\nIntroduction \n\nThe advent of autonomous and ubiquitous co-robot technologies offers society a unique opportunity to reshape our transportation infrastructure. To properly deploy Autonomous Technologies, we must balance the requirements for navigation safety, functionality, and experiential conditions. This interdisciplinary research investigates these trade-offs to understand \nthe impact of driverless vehicles on urban design and public policy. To do so, this research \ninvestigates the challenges of urban navigation for autonomous vehicles on State Street (Chicago).  Furthermore,  it  introduces  a  method  to  improve  autonomous  vehicle  localization \nsafety through an  urban design approach. This approach improves current urban practices \nwhile ensuring AV’s proper function.  \n\nDriverless  vehicles  will  need  to  operate  with  safety  levels  on  local  and  residential  streets \nsubject to corresponding accuracies at the centimetre level (REID et al. 2019), but urban environments can degrade navigation sensors’ accuracy and, therefore, fault-free integrity. Tall \nbuildings  can  severely  degrade  Global  Navigation  Satellite  System  Signals  (NAGAI  et  al. \n2020). Landmarks spaced too far apart for landmark-based navigation decrease location accuracy. At the same time, close landmarks can introduce a high probability of faulty measurement mis-association (HAFEZ et al. 2020). One could shape the environment to maximize a robot’s localization  safety to  mitigate safety risks. This could be done by creating ordinances that dictate the appearance of the  streetscape so that self-driving cars, drones, and other mobile co-robots can guarantee their trajectory. However, modifying the environment \nto maximize co-robot safety could have negative and  wide-ranging societal impacts if the \nprocess does not consider the needs of all the involved stakeholders. This highly interdisciplinary research project studies the relationship between landscape architecture, city planning, and mobile co-robot navigation safety. As a result, it develops a method that transforms passive landscape objects such as trees and light poles into binary error-correcting codes that enhance autonomous vehicle localization safety. \n\nMethodology \n\nGlobal Navigation Satellite System (GNSS) Evaluation \n\nThe research begins by defining safety requirements for driverless vehicles under fault-free \nconditions and developing measurement models for multi-sensor integrated navigation systems. The study evaluates satellite availability in 3-D Mapped Urban Environments to understand the limitations of Global Navigation Satellite System Signals Navigation. An array \nof sensors is utilized to collect data along the 8.9-kilometer State Street transect. This array \nconsists of Lidar sensors, GPS antennas and receivers, IMUs, and other GPS units. The collection of data generated a Point-Cloud dataset of State Street. The sensor array is positioned \non top of a test vehicle that ran through the transect multiple times in different weather and \ntime conditions. The team replicated an accurate 3-D Environment with the collected data to \nevaluate and assess the developed methods.  \n\nWe started by evaluating GNSS availability. The urban canyon is assessed through shadow \nmatching (GROVES 2011) to identify signal blockages. The availability of GPS-only positioning is determined to be less than 10 percent at most locations in State Street (Fig.2). Using \nfour entire GNSS constellations, availability improves significantly but is still lower than 80 \npercent at specific points rendering Satellite Navigation Systems unreliable for Autonomous \nVehicles in the selected transect (NAGAI et al. 2020).  \n\nPseudo-Random Landscape Strategy \n\nOne approach to solving the deficiency of Satellite Navigation availability is directly extracting navigation information from the local environment, using ranging sensors like LiDAR \nand RADAR. Unfortunately, the random arrangement of local landmarks makes it challenging to quantify  navigational safety. It  may be difficult, if possible, to calculate the risk of \ninaccurate  landmark  extraction  and  association  in  a  dense  setting  (DUENAS  ARANA  et  al. \n2019). In response, the research team combines multiple Ranging and Inertial Sensors with \na pseudo-random landscape strategy to provide quantifiable navigation integrity. All objects \nin urban environments are candidates for external ranging sources for LiDAR. However, we \nspecifically focus on extracting pole-like landmarks (e. g., trees and streetlamps) because of \ntheir location flexibility, relative ubiquity, and defined shapes (NAGAI et al. 2021).  \n\nThe  landscape  objects  are  used  in  a  binary  error-correcting  code  to  improve  localization \nsafety  for  autonomous  vehicles.  Prior  research  showed  that  LiDAR  measurements  using \npole-like landmarks improved vehicle localization in urban environments but that” the accuracy of the localization is highly dependent on the number and density of available landmarks \nat each scene” (BRENNER 2009). In this work, each landmark mapped in the selected transect \nis assigned a unique identification (ID) with a given geographical area (Fig.3). This results \nin bidirectionally decodable landmarks so that robots can read the code while traveling in \neither direction along a given path. To provide a realistic scenario, we included urbanistic \nconstraints on where these landmarks can be set up. The spacing between new landmarks \nrespects the landscape ordinance of Chicago, spacing trees between 6 and 7.5 meters.  \n\nRelated  work  includes  the  use  of  QR  code-based  localization  methods.  These  approaches \nmainly focused on optical-based codes, but the techniques were limited to small-scale indoor \napplications  (ZHANG  et  al.  2015,  LEE  et  al. 2015,  KOBAYASHI  2012).  In  contrast,  our  approach introduces a novel and reliable localization technique that applies to large outdoor \nenvironments, such as urban streets. The key benefit to this approach is that, without introducing new instruments to the current environment, through minimal modifications to the \nurban landscape, the technique can nearly eliminate measurement faults in range measurement-based localization using landmark maps and thus guarantee safe localization.  \n\nNavigation Safety Assessment \n\nTo test this method, we use the current conditions of State Street obtained through the site \nsurvey performed with Navigations Sensors and Ranging Sensors. We evaluate them with \nNavigation Integrity parameters and define an Alert Limit or maximum allowable error in \nthe measured position. Once the zones with low Navigation Integrity have been identified, \nwe modify the transect in a 3D environment. For instance, this method proposes the addition \nof 630 landmarks to the 2558 existent landmarks in the selected nine-kilometer transect of \nState Street. Then, the 3D environment is modified, and its integrity is evaluated under the \nexact parameters of the beginning of the process. \n\nIn this step, Integrity risk is measured, which is established as a function of the specific sensors  the  robot  is  carrying,  their  availability,  noise  characteristics,  feature  extraction  algorithms,  data  association  algorithms,  monitor  detection  thresholds,  and  the  density  of  local \nlandmarks. A monitor developed by IIT’s driverless city team is used to evaluate the data \nintegrity of the selected transects. Our monitor considers GPS availability, INS metrics, and \nextracted data from passive landmarks on the streetscape. We assume as an integrity requirement that the probability of exceeding a 1- meter position estimate error (Fig. 4 (a)) must be \nlower than 10−7 (Fig. 4 (b)). Given a position error standard deviation of σpos, the 1-meter \nintegrity alert limit corresponds to approximately 5σpos. \n\nEnvironmental and Urban Impact Assessment \n\nOnce autonomous vehicle navigation is guaranteed, the research evaluates the impact of the \nadded landmarks requested by the binary code approach in the selected transect. From 31st \nStreet to North Avenue, a transect of 8.9 kilometers, there are 2558 existent landmarks in \nState Street. The conditions vary along the transect, where there are areas with a denser number  of  landmarks.  The  simulation  of  the  geocode  suggested  the  position  of  630  geocoded \n\nlandmarks located in areas where GNSS is unreliable or unavailable. The Geocoded Landmarks are prioritized over the existing landmarks to be consistent with the system. The landmarks in the neighboring area must be removed in a radius of 0.40 meters on the X-axis and 5.60 meters on the Y-axis. There a 170 existent landmarks that had to be removed in the total transect. In total, the number of landmarks accounting for the existent and the geocoded landmarks, and subtracting the removed, is a total of 3018 landmarks, representing an increase of 17% landmarks along the transect. \n\nAdding more Urban Forestry to cities has a positive impact on navigation safety. Chicago’s \ncurrent road infrastructure occupies thirty-nine percent of surface coverage, while Parks and \npublic spaces account for only six percent (IBRAHIM et al. 2018); therefore, there is plenty of \nroom to improve the urban forestry on public roads, exponentially increasing their benefits. \nAn increase from 8\\% to 22\\% in canopy coverage can help mitigate the urban heat island \neffect. To assess this, we used 3D simulated environments of the selected transects. A 10,000 \nm2 area has been established in a section of state Street, measuring 125 meters north to south \nand 80 meters east to west.  \n\nThese models are subject to thermophysiological evaluations through open-source software \ncalled ladybug (PAK et al. 2013). The determined area is divided into small regions of around \n2 square meters. Each subdivision is evaluated considering the weather variables obtained \nfrom the EPW weather data from O’Hare airport. These variables are air temperature, solar \nradiation, relative humidity, and wind speed, and they have a combined effect on thermal perception. Buildings’ material, shape, morphology, vegetation presence, global radiation, evaporative cooling, and these parameters are determined. Increasing canopy coverage in State Street \ncan lower temperatures from 6 to 9 degrees Celsius on average across the transect (Fig. 5). \n\nWe can see from the computed data that the areas with high temperatures are shrinking. The \narea exposed to 45°C or more decreased by 455.38 m2. The area was exposed to 44 to 45°C \nby 607.45 m2. The area exposed to 43 to 44°C remained constant with a decrease of 40.15 \nm2, and the area exposed to 42 to 43°C decreased by 318.322 m2. There has been an increase \nin area in the lower ranges. The area exposed to 41 to 42°C expanded by 204.78 m2, the area \nexposed to 40 to 41°C climbed by 130.16 m2, and the area exposed to 39 to 40 °C increased \nsubstantially 1086.35 m2.  \n\nDiscussion \n\nStreetscape policies rarely considered functional values, let alone using streetscapes to improve navigation safety for Ubiquitous Robots. Cities like Chicago manifest in their landscape ordinances document the  following: The objective of the landscape ordinance  is  an \nattractive  city  of  tree-lined  streets  and  boulevards,  greener  neighborhoods,  and  enhanced \nproperty values. The people of Chicago benefit from a more beautiful city filled with trees, \nshrubs, and flowers. We all benefit when the high temperatures of the urban heat island are \nlowered by spreading canopy trees over hot asphalt paved streets and parking lots. Birds and \nother wildlife benefits from nesting and resting habitats, refuge, and food sources provided \nby the landscape in what could otherwise be a sterile urban environment. \n\nAlthough  this  operation  has  positive  consequences  for  the  integrity  of  navigation  and  the \nenvironment and society, it would mean a great effort to apply this system on a large scale. \nThe operational and environmental costs of removing mature street trees and planting young \ntrees are high for municipalities like Chicago. New Landmarks would take decades to reach \nmaturity  sacrificing  their  urban  benefits  in  the  short  term.  Likewise,  this  would  require  a \nthorough survey of the road system of already consolidated cities, which could take extensive \noperating times and costs.  \n\nHowever, systems like the geocoded streetscape can and should be considered in new developments in established cities and emerging cities, where there is greater flexibility to apply \nthese systems and base streetscape ordinances on navigation systems for navigation ubiquitous robots. \n\nConclusion and Outlook \n\nThe research proposes a solution to the current challenges for Autonomous Vehicles in urban \nenvironments. The importance of this method is the urbanistic design approach as a tool to \nimprove navigation safety. This allows us to propose different scenarios like the increase of \nurban  forestry  and  the  retrofit  of  street  space.  Although  these  technologies  are  still  in  the \nresearch and development stage, it is essential to discuss the cities' near and long-term future \nand how we can leverage their needs and capabilities to benefit the stakeholders. This method \nshows an approach in which the intersection of multi-disciplinary fields can achieve the required conditions for the technology to function and provide positive urbanistic values. If we \nfail to understand the requirements for technology to coexist with us, we can exacerbate the \ncurrent urban issues by segregating our infrastructure even more. However, suppose we understood the cooperative essence of technology and provided the meanings for it to function \nwhile considering the valuable opportunities it offers appropriately. We could be on the eve \nof creating sustainable, regenerative, inclusive, and equitable cities. \n\nUsing Spatial Technology to Support Community \nResilience for Landscape and Food Systems in the \nU.S. Virgin Islands \n\nAbstract: This case study will showcase the use of spatial data and technology to promote community, \nlandscape, and food systems resilience in the U.S. Virgin Islands. We explore the inventory and analysis \nstage of landscape architecture using place-based and public-interest design processes and then investigate the strengths and challenges of primary data collection for landscape design and food systems \ndevelopment in a community with gaps in internet connectivity and limited existing digital data. We \ndemonstrate the usefulness and intent for integrating primary data collection, mapping and community \ndecision making for place-based planning. This process, and use of a mobile application tool (Fulcrum), \nis believed to be a transferable process for similar communities with minimal data resources and internet \nconnectivity. \n\nIntroduction \n\nIn  early  2017,  our  cross-disciplinary  team  at  Iowa  State  University  (ISU)  was  teaching  a \nnational certification program for community food systems when we met with an organization from the U.S. Virgin Islands, which needed support to develop local food systems in the \nTerritory. Later in the year, as our teams started to work together, two Category 5 hurricanes \nhit the Territory. Farmers, food businesses and community members took action to help understand and alleviate some of the burden the natural disaster put on the local food system. \nThe impact of the hurricanes accelerated our partnership, spurring our team to provide assistance with hurricane response and development of the local food system.  \n\nOur team first responded by conducting on-site interviews and listening sessions designed to \nrecord the impacts of the natural disaster on businesses, organizations, farmers and commu-\nnity members. This initial response led to many discussions about creating a more resilient \nfood system in the future, including the ability to prepare, respond, recover and rebuild. Our \ndefinition of a resilient food systems is the capacity for place and values-based food systems, \nand the actors within, to be able to withstand shocks and disruptive pressures while maintaining basic structures, processes and functions of the community food system and supply chain; \nensure the ability to produce and access nutritious and culturally acceptable food over time \nand space; and create a responsive and functional paradigm (FAINSTEIN 2014, CAMPANELLA \n2006, SCHIPANSKI et al. 2020). Shocks can range widely from economic crises, natural disasters, environmental impacts of climate change and social and political forces, all of which \ncan evolve over time. These shocks impact the liveability of a community or landscape region.  \n\nTo prepare, plan and respond, it is important to have a baseline understanding of existing \nlocal conditions and resources. Based on the response protocols identified,  we recognized \nmajor gaps both in data availability and in organizational capacity to respond effectively to \nshocks. Digital landscape and place-focused tools are a necessary component to collect in-\nformation when extant data is not readily available. New data metrics are needed for future \nuse and development of a resilient landscape and local food system for the Territory. Robust \nspatial  data  would  allow  for  further  understanding  of  existing  conditions  and  locations  of \nfarms and food businesses. Therefore, the team worked to map out conditions of each island \nand determine design strategies for response. We prioritized digital solutions and mapping \ntechnology as an initial method to fill the gap that was identified. Over the following years, \nwe formed strong relationships with many organizations across the Territory and supported \nour partners through research, facilitation and conceptual design tactics related to priorities \ndetermined  from  spatial  data  analysis.  This  included  work  with  FEMA  on  a  food  system \nassessment, which was the first opportunity to highlight mapping and landscape design strategies (LONG et al. 2019). Following the assessment in 2019, we prioritized projects for farmers  and  food businesses,  including  development  of  virtual  farmers  market  framework  and \nresilient food system action plan.  \n\nIn this paper we detail a process for working with community members to identify and create \nnew data through digital geospatial technologies and utilize findings to support data driven \ndecision making. We demonstrate how technology innovation, within a community that lacks \ndata resources and has limited internet connectivity, can be developed though a mobile data \ncollection tool. We show how digital skills, tools and processes within Landscape Architecture, such as GIS analysis, site field research, occupancy identification and conceptual design \nallowed for identification and understanding of existing landscape and environmental conditions to inform decisions for resilient food system opportunities within the community. By \ninventorying and visualizing existing open-source spatial data, performing analysis of data \nto identify gaps, using open-source GIS software (QGIS) and creating a customized spatial \ndata collection app, community partners  will be able to support the development,  maintenance and administration of a virtual farmers market in the future. \n\nBackground  \n\nStudy Area Overview  \n\nThe U.S. Virgin Islands is a territory of the United States and consists of a group of four \nCaribbean islands: St. Thomas, Water Island, St. John and St. Croix. The islands to the north, \nSt. Thomas, Water Island and St. John, consist of hilly and at times very steep terrain due to \ntheir volcanic origins. St. Croix, the largest island in land area, lies to the south and has much \nflatter terrain. Because the islands have significant differences in physical geography, they \ndiffer in the type and availability of farmland and farming opportunities available to residents \nof each island. St. Croix is larger by size, least densely populated and economically industrial. \nSt. Thomas is smaller in size but has a denser population driven by a strong tourism industry. \nVirgin Islands National Park takes up 60 % of St. John’s land area, making the island a tourist \nattraction and limiting population capacity and farming. Tab. 1 displays population changes \nthat likely occurred due to natural disasters and economic factors. \n\nAround 12 % of the Virgin Islands territory is comprised of agricultural land (THE WORLD \nBANK 2018). As of 2017, the U.S. Virgin Islands had 565 farms, increasing from 219 in 2007 \n(USDA NASS  2018).  The  USDA  defines  a  farm  as  an  operation  where  $500  or  more  of \nagricultural products are sold within a year. The 565 farms account for 9,324 acres of land, \nwith 2,620 acres of cropland and 5,538 acres of pasture or grazing land. The average farm \nsize in the Territory is 16.5 acres. Of the islands, St. Croix has the largest average farm size \nof 17.9 acres, while St. Thomas and St. John has an average farm size of 10.2 acres. A majority  of  farms  in  the  Territory  (59  %)  operate  their  farm  business  from  their  residence \n(USDA NASS 2018). Farms not operating from private residences are typically on government-owned land. The government has defined policies and regulations related to land use \nand build infrastructure (see Fig. 1 for two examples of farms in the Territory). One of the \nmost significant policies that impacts renters is that only temporary structures are allowed. \nTenant farmers commonly use freight or shipping containers as temporary structures for their \nequipment storage needs. However, unless they are heavily weighted or anchored down, they \nare unable to withstand high winds and can flip over and cause additional damage to equipment and land.  \n\nProject Background \n\nWe  first  sought  to  understand  impacts  of  the  hurricanes  through  qualitative  analysis  with \ninterviews and focus groups. The ability to understand, identify and assess current conditions \nwas quickly seen as a constraint within our work. The Territory lacked accurate record keeping and updated census information. Additionally, when interviewing farmers about the impact from the 2017 hurricanes, many spoke about gaps in record keeping as well as losing paper copy receipts and other records during the storms. Because of this, many farmers and businesses  did  not  receive  adequate  compensation  for  rebuilding  their  farm,  and,  without updated Census information it was difficult to track the movement of people post-storm.  \n\nFollowing this work, we broadened our team to include data expertise, geospatial design and \nlandscape  architecture  to  enhance  the  efforts  around  inventory  and  analysis.  As  our  team \nworked to collect data, we identified significant gaps in the availability of geospatial data. \nWe also recognized differences in the addressing system that led to difficulties with general \nnavigation, and place-finding and geocoding. The addressing system originally designed in \nthe  Territory  used  estate  names  and  plot  numbers  rather  than  street  names  and  sequential \nnumbering. Plot numbers were not assigned in an orderly manner, as street numbers are in \nU.S.  states.  This  resulted  in  a  confusing  pattern  of  numbering,  making  finding  individual \nhouses or business locations difficult. Because of these complicating factors, it is common to \nhave multiple parcels with the same address in the same Estate. For example, the address 2 \nDA Nazareth and 2 DA Secret Harbor\/Nazareth are both located in Estate Nazareth but are \ndifferent addresses (Fig 5). The development and transition to a modern addressing system \nhas been underway for about a decade, but a staggered rollout has led to confusion and inconsistencies in address reporting.  \n\nThese addressing challenges made it difficult to understand where farm and food businesses \nwere located pre- and post-storm. Therefore, we needed to collect location data using coor-\ndinates  rather  than  relying  on  addresses.  To  achieve  this,  we  needed  to  identify  a  mobile \napplication that could collect  GPS coordinates site amenities, product inventory and sales \ninformation. A USDA Farmers Market Promotion grant funded this search. In the following \nsections we detail the process of utilizing digital landscape tools to digitize data, create an \naccurate directory of businesses, and identify a partner organization to support the farmers \nmarket services. Figure 2 lists the activities completed to date in partnership with U.S. Virgin \nIslands.  \n\n\n\n•2018: Hurricane response through interviews and focus groups \n•70 interviews and 18 listening sessions in collaboration with FEMA during \nthe first 6-12 months after impact \n•Stories shared identified needs for better response and action plans for \nfuture storms\n\n2019: Community food systems assessment\n•Hosted trainings\n•Developed vision, mission, and goal statements\n•Visualized data from publicly available sources \n•Identified priorities for each sector of the food system\n\n•2020-2022: Risk management needs for farmers related to production, \nfinance, and marketing\n•Finance trainings and lending programs\nMarketing support and campaigns for local foods \nProduction best practices across crops, livestock, and poultry\n\n•2020-2023: Development of farmers market and online directory\n•Fulcrum training to identify existing farm and food business locations\n•Validation of farm and food business locations\n•Data transfer from Fulcrum to Market Maker\n\n•2020-2023: Resilience study for response to COVID and natural disaster\nIdentified priorities for creating a resilient food system\nFacilitating sessions on a cooperative business model for farmers\n\nImplementation of Digital Tools and Methods \n\nBecause  the  U.S.  Virgin  Islands  has  limited  availability  of  up-to-date,  detailed  geospatial \ndata, our initial priority was to create accurate maps of the islands following the 2017 hurricanes. This included identifying data sources to map features of the land and physical geography as well as infrastructure and the built environment. This built a foundation for understanding the spatial relationship between the landscape, land use and potential impacts to the \nfood system from past and future hurricanes. To supplement the existing limited data, we \nutilized a combination of federal open data sources and OpenStreetMap (OSM). The data \nthat  was  available  through  OSM  was  downloaded,  analysed  and  visualized  in  QGIS.  The \ninitial  maps  produced  were  included  in  the  2019  community  food  systems  assessment  to \nshowcase changes in the landscape, and to depict existing  conditions after the  hurricanes. \nWhile this may be a rudimentary procedure in some communities, it was a necessary step to \nestablish a baseline inventory of existing farms within the built environment in order to move \nforward. \n\nIn  2020,  we  began  work  on  the  first  component  of  creating  an  online  farmers  market  by \nidentifying  interest,  location  and  capacity  of  farms  and  food  businesses  in  the  Territory. \nWhile some data on the locations of food businesses can be found in OSM, there  was no \npublicly available source that showed us the location of farms throughout the Territory. We \nneeded a tool to aid in the creation and collection of this spatial data. The tool needed to be \nable to create point locations, attribute information and photos; and at the same time, handle \nthe data collection challenges of unreliable internet and cell phone service, limited available \nfunds and nontechnical users and data collectors. After a review of technology options, the \nmobile application Fulcrum was selected and utilized for this project.  \n\nFulcrum is a mobile application that is compatible with iOS and Android mobile devices and \ncollects data using smart forms and GPS location. The Fulcrum platform is a simple option \nfor nontechnical users who need to create geospatial data (points) with detailed attribute information. The forms are easy to create with a drag and drop functionality. The application \ncan be deployed to multiple mobile device users and in field conditions where internet access \nis not available.  \n\nWith  a  tool  selected,  a  standardized  data  collection  form  was  created  with  23  conditional \nquestions in addition to the option to attach multiple site photos to each record. We worked \nwith a U.S. food business directory platform, Market Maker, to create specific and detailed \nquestions about the farm and food businesses related to products and business type. Questions \nranged from standard contact information (name, email, phone number) to business or organization type, and specifics related to each business,  such as type of product offered, sales \noptions and affiliations. Our team designed the Fulcrum survey with conditional logic and \nvisibility rules, along with single and multiple-choice form fields when possible (Fig. 3). For \nexample,  when  asked,  “What  is  your  business  or  organization  profile  type?”  individuals \nchose from ten different options. Based on their response(s), the next set of form questions \npopulated and displayed. If a participant identified their business as a farm, the next question \nwas about the type of farm. This logic continued throughout the form to keep the application \nstreamlined and easy to follow. Another important feature of Fulcrum was that the default \nsetting of using the location of the mobile device to collect the GPS location for the point \nrecord could be overwritten with a manual location selected from the displayed map. This \nfeature was crucial for this project because data collectors could adjust the point marker position if they were not standing at the business store front during the verification and collection process.  \n\nAs a starting point for populating data, the Virgin Islands Department of Licensing and Con-\nsumer Affairs provided a list of licensed farm businesses. This list gave us the business name, \nbusiness owner and the business address for 294 licensed farm businesses in the Territory \nfrom 2020. From there, the business addresses were geocoded using Esri’s ArcGIS World \nGeocoding Service and then imported into the Fulcrum platform (Fig. 5). This was another \nbenefit of selecting Fulcrum: existing data could be imported into the form and utilize the \nsubsequent data structure produced by the form. Utilizing existing farm business profiles, we \nhoped to be able to understand or visualize initial estimates of farmers across the Territory. \nHowever,  we quickly learned that  we did not have the ability to accurately  locate the addresses because of the addressing system used within the Territory.  \n\nDue to address and geocoding constraints, we identified the need to connect with community \nleaders to help ground truth the information provided by the Department of Licensing and \nConsumer Affairs to accurately depict where farms and food businesses exist. This required \ncommunity leaders to be trained to use the Fulcrum platform. Having previously used Fulcrum to support participatory mapping of community infrastructure for walkability, we knew \nthe technology would work well for this situation (SEEGER 2015). We held three different \ntraining sessions and followed up with virtual technical assistance to teach best practices and \nto  walk through the questionnaire flow. Participants  were granted access to Fulcrum only \nafter completion of the training sessions. To ensure data integrity, we utilized a validation \nmethod in Fulcrum so team members could track when profiles were completed. \n\nOf the 294 farm business addresses provided, geocoding resulted in 40 % matched addresses, \n50 % unmatched and 10 % tied match results. Unmatched and tied addresses were matched \nto the estate name when possible and the city name when not possible. This resulted in many \naddresses being located to the same position on the map, the centre of a town or estate. To \ndeal with this, we utilized a data verification method in Fulcrum to tag points based on the \nlocation or attribute accuracy or validation level. Points with correct and verified attribute \ninformation but with inaccurate or unverified location information could be tagged as such. \nThis method was used to help streamline the verification process.  \n\nResults and Conclusions \n\nThere are currently 364 farms and food business records in the Fulcrum application. This \nincludes the 294 provided, plus 70 additional farm and food businesses that have been added \nby trained data collectors. However, at this time, only 78 records have been fully validated \nthrough business information and data validation. During the Virgin Islands Agriculture Fair \nin February 2023, we will continue this process of validation. Once validated, business data \nrecords will be imported into the Market Maker system. This will increase opportunities for \nboth consumers to find farm locations and products and for food businesses to find markets \nas well as maintain their business profiles. While this project and case study was not landscape  site-based,  it  incorporates  digital  data  collection  techniques  for  meeting  the  unique \nchallenges of this community. \n\nHaving a reliable platform for identification, sales options and connectivity creates a unique \ndesign application for investigating strategies in the broader food system. The process also \nallows for collaborators to identify next steps that will increase the resilience of the Territory’s food system. Through data visualization, community dialogue and decision making, \nadditional landscape design strategies have been identified: \n•  Support site-specific designs for on-farm solutions using best practices for resource management such as water access, conservation and production.  \n\n•  Analyse Fulcrum data to understand key locations for aggregation and distribution of \nlocal foods. Site inventory and conceptual designs may be able to support the development of new food systems practices in the future.  \n\n•  Discuss a regional food system design approach with key decision and policy makers. \nThis may include aspects related to complex systems of food aggregation and distribution; best locations for farmers markets for consumer and farmer access; and increase in wholesale distribution through food cooperatives. \n\nThe development of a virtual farmers market platform will not only serve as a mapping resource of existing farms and businesses but will also provide an opportunity for increased \nsales and new market options across the Territory. Individual farmers will have the ability to \nsell their products through the platform and work with consumers to negotiate a preferred \npick-up location, whether at an existing farm store or another drop off site. Additionally, this \napplication will include a customer interface that allows for individuals to both identify and \nlocate farms selling local products, and purchase from them. It is believed that this new market will provide an added value and opportunity for farmers, and increase interest in producing local food, leading to a more resilient and diversified environment. The team anticipates \nthat  consumers  in  the  Territory,  as  well  as  tourists,  will  better  be  able  to  access,  gain \nknowledge about local products and increase direct to consumer sales. However, the community will need to continue to foster awareness, marketing and additional supports for their \nlocal and regional food businesses. \n\nBy understanding existing conditions, we can better prepare and determine appropriate resilient strategies for rebuilding (LONG et al. 2019). This case study highlights the transferability \nand connection between mapping, inventory analysis, and use of Fulcrum to collect primary \ndata in a community with limited internet and geospatial technology capacity. Opportunities \nto further connect  with the  government to streamline and  digitize procedures, such as the \nfarmer licensing form,  may be an opportunity to improve data collection. There is further \nwork  to  be  done  to  foster  local  food  production,  business  and  finance  best  practices,  and \nconsumer awareness about local foods. These initial steps towards creating a virtual farmers \nmarket framework and leading to a digital platform establish a baseline critical to move forward in the process. We suggest future explorations use the improved geospatial data created \nin this project to develop site-, community-, and regional scale solutions related to the Territory’s food system.  \n

3D Morphodynamic Visualizations of Storm Impacts \nfor Decision Support \n\nAbstract: 3D flood visualizations are commonly used by coastal managers and other experts to engage \nthe public regarding storm impacts, and to support management decisions. 3D flood visualizations do \nnot, however, capture physical changes to the landscape, such as erosion, that result from storms and \ndo significant damage to human habitations and change ecological systems. We address this gap by \npresenting novel 3D morphodynamic visualizations that depict physical changes to the coastal morphology  wrought  by  modelled  storms.  We  propose  these  visualizations  may  be  more  effective  than \nflood visualizations as decision support tools in situations where shoreline change is a factor. We describe the process of creating the visualizations for storm, sea level, and mitigation scenarios and make \nobservations of their possibilities and limitations. The visualizations plainly show profoundly different \noutcomes than flood visualizations for the same storm. These visualizations may be extremely useful \nin the sedimentary contexts considered. However, the lack of clear conventions and complexity of creating these visualizations means that more experimentation is required before such visualizations can \nbe considered for wide application. \n\nIntroduction \n\nHurricanes  (tropical  cyclones)  and  nor’easters  (extra-tropical  cyclones)  present  increasing \nuncertain  risks  to  ecosystems  and  coastal  communities  in  the  Northeast  United  States. \nNor’easters often occur in the fall, winter, and early spring, when shoreline sediments have \nbeen moved offshore by winter wave action (BOOTH et al. 2015, R ANGEL-BUITRAGO AND \nANFUSO 2 011).  These storms  are  thus  powerful  drivers  of  shoreline  change  because  they \nerode and over-wash dunes, redistributing sediments. This process gradually modifies coastal \nbarrier islands as the shoreline retreats (CONERY et al. 2018). Impacts to ecological systems \nvary. Storms may do damage to some ecological resources. Storms may also elevate marsh \nplatforms through sediment deposition and refresh coastal lagoons by cutting new inlets allowing  for  water  exchange,  causing  net  positive  effects  (GOBLER et  al.  2019,  OLIN et  al. \n2020). Impacts to infrastructure, and private property occur through a range of mechanisms, \nincluding storm surge, waves, and erosion of land, in addition to flooding, wind, and secondary hazards such as power outage and obstructed emergency access among others (STEMPEL \net al. 2018). This diversity of constructive and destructive impacts is not directly shown in \nconventional flood visualizations and can only be inferred.  \n\nWe hypothesize that visualizing flooding alone likely results in an understatement of impacts, \nespecially in sandy coastal barriers, because flood visualizations do not faithfully capture the \nextent of landscape change wrought by storms. Sedimentary coastal barrier systems such as \nthis comprise about 10 % of the world’s coastlines (STUTZ AND PILKEY 2011). Flood visualizations of the likeliest scenarios, such as nor’easters and storms that have generally greater \nthan 1 % chance of annual exceedance, may not show dramatic flooding at all, but nonetheless could result in significant impacts through shoreline change. The 1 % chance of exceedance storm is significant because it is the standard that many flood maps and assessments \nuse, such as  the  US Federal  Emergency Management  Agency  (HORN  AND BROWN 2017). \nFlood visualizations also make it difficult to assess the efficacy of mitigation measures such \nas vegetation restoration or implementation of offshore reefs that reduce the effect of wave \nenergy but have less visible effect on flooding.  \n\nTesting  this  hypothesis  requires  the  development  of  a  meaningful  alternative  to  3D  flood \nvisualization  that  can  sincerely  represent  impacts  and  depict  the  effects  of  mitigation \nmeasures. To that end, this paper presents a novel workflow for a set of model-driven 3D \nmorphodynamic visualizations that depict changes to the landscape wrought by storms and \ntest the effects of several mitigation scenarios. These visualizations are being developed and \nused as decision support tools for communities, the US Fish and Wildlife Service, and other \nnon-profit partners. We summarize the methods, present the visualizations, and observations \nand next steps based on the first application. \n\nProject Site \n\nThis research is being conducted in South Kingstown and Charlestown Rhode Island, USA, \ntwo communities on a south facing coastline open to the Block Island Sound and the Atlantic \nOcean. The geography consists of sandy coastal barriers and lagoons that support a rich ecology and vibrant coastal communities. The Ninigret Trustom National Wildlife refuge managed  by  the  US  Fish  and  Wildlife  Service  forms  a  significant  part  of  the  study  area,  and \nhabitat conservation, especially for shore birds such as plover is a significant concern as is \nthe diversity of the coastal ecosystem spanning a gradient of habitats from the intertidal zone, \ndunes, salt marshes, lagoons, and coastal shrub and woodlands. Initial management concerns \nincluded decisions regarding managing vegetation such as invasive species, and performance \nof a permanent breachway (constructed in 1958) with coastal structures damaged by Superstorm Sandy (2012). Other issues included prevalence of future breaching of the barrier system more broadly, the potential damage caused by successive storms, and extent of habitat \nzones for shorebirds.  \n\nMethods \n\nThe process for developing the 3D morphodynamic visualizations is following an approach \nthat allows interested parties shape both visualization outputs and modeling decisions in a \ncoherent, iterative process (STEMPEL AND BECKER 2019). This approach to developing hazard visualizations allows for significant exchange and calibration of information regarding \nrisk and uncertainty between persons with differing levels and types of expertise and varied \nbackgrounds through constant feedback (STEMPEL AND BECKER 2019, SALTER et al. 2010). \nIn practical terms, this involved several steps: elicitation of management concerns and desired  storm  scenarios,  measurement  of  existing  conditions,  modeling  of  water  levels  and \nwave heights, implementation of the  morphodynamic  models, and visualization. This  was \nundertaken with frequent contact between interested parties and team members. Each of these \nsteps is summarized in turn.  \n\nManagement Concerns and Scenarios \n\nSuperstorm Sandy was selected as the basis for the initial storm scenarios. The team refers \nto the modelled storm as a “Sandy-like” scenario because current morphological conditions \nand application of sea level scenarios necessarily change the dynamic conditions of the models as compared to conditions present in 2012. An important note regarding the magnitude of \nSuperstorm Sandy is that although Superstorm Sandy significantly affected coastal Rhode \nIsland, USA (sustained windspeed of 111 kmh, 69 mph), impacts were less severe than those \nto New York and New Jersey, USA (sustained windspeed of 177 kmh, 110 mph), further to \nthe south where the center of the storm made landfall. Although Superstorm Sandy began as \na hurricane (tropical cyclone), it made landfall as an extremely wide post-tropical cyclone \nwith many of the characteristics of large nor’easters (extra-tropical cyclones) while retaining \nits tropical cyclone core (HALVERSON AND RABENHORST 2013). The majority of shoreline \nchange in the Northeast USA is driven by nor’easters (extra-tropical cyclones) by virtue of \ntheir duration, size, and varied patterns of wave energy (HARLEY et al. 2017). \n\nThree variations were tested with a Sandy-like storm under current sea levels and with an \nadditional .33m of sea level incorporated into the models: current conditions, optimal vegetation cover, implementation of a segmented offshore coastal barrier. The coastal barrier is a \ncomplex topic not discussed here for reasons of brevity.  \n\nMeasurement of Existing Conditions \n\nThe recency of geographic information becomes a significant factor in assessing sedimentary \ncoastlines like the southern coast of Rhode Island, USA, because the starting condition is \nchanging seasonally and annually, and is not in equilibrium (OAKLEY et al. 2019, HOLLIS et \nal. 2016). A 1-meter topobathy DEM was created from a combination of recent LiDAR and \nSoNAR data sources using older data to fill gaps between more recent scans. NOAA and \nUSGS LiDAR data from 2018, 2014, 2012, 2011, and 2010 and aerial imagery from 1958 to \nthe present was analyzed in combination with sediment sampling to understand both shoreline  and  vegetation  changes  and  physical  modifications  to  the  system.  This  was  complimented by terrestrial LiDAR gathered with a Trimble X7 terrestrial scanner. \n\nWater Levels and Wave Heights \n\nWater levels and  wave heights  were assessed for the selected scenarios using the coupled \nADvanced CIRCulation (ADCIRC) and Simulating Waves and Nearshore (SWaN) (ATKINSON \net  al.  2004,  LUETTICH JR  et  al.  1992).  ADCIRC  uses  an  unstructured  grid  that  has  higher \nresolution in the nearshore area and wider node spacing in open ocean (Figure 1). ADCIRC \nSWAN outputs were used as a boundary condition for subsequent modelling. \n\nMorphodynamic Modeling \n\nMorphodynamic modelling simulates the interaction between sediments and hydrodynamic \nconditions, updating them continuously with the storm propagation. This was accomplished \nusing the Xbeach model (ROELVINK et al. 2009) in a high-resolution coastal grid forced in \nboundary conditions by the results of the larger scale simulations with ADCIRC-SwaN (Fig-\nure 2). Xbeach was used in the “Surfbeat” mode to predict the morphodynamic changes occurring (throughout storm events (GRILLI et al. 2020).  \n\nVisualization \n\nSpace delimited tables were produced for model intervals representing the xy coordinates for \neach node, terrain, base flood elevation and water velocity. These were first explored in ESRI \nArcGIS Pro so that the team could better understand the outputs. Tables used for 3D visualization  were  post-processed  using  python  scripts.  Data  was  organized  to  create  xyz  point \nclouds that could be read directly by McNeal Rhino. Colour ramps were applied to the data \nsuch that finished jpeg images could be generated directly from python, avoiding time consuming manual steps in GIS or other raster processing software.  \n\nElevation data was coloured using a colour ramp reminiscent of world maps to allow for easy \ncomparison of before and after conditions. Several unique graphic choices were made. These \nincluded treating base flood elevation as a 3D surface and mapping water velocity onto that \nsurface and using a highly distinctive red-light yellow colour ramp to distinguish velocity \nfrom more conventional flood depth maps.  \n\nThese outputs were combined with other geographic information in Rhino, and renders were \ncreated using conventional rendering processes in that platform. All steps were designed with \nfuture automation both inside and outside the rendering platform in mind to allow for production of animated sequences. An initial presentation was made  using still visualizations \npresented in sequence (Initial visualization was completed quickly, with days between model \noutput and presentation).  \n\nResults \n\nThe modelling and visualizations plainly demonstrated the significant impact of a Sandy-like \nstorm should it occur under today’s morphological conditions that have already been altered \nby Superstorm Sandy and subsequent smaller storms. Impacts included significant breaching \nof the barrier system, erosion behind fixed structures protecting structured breachways (openings between the lagoon and the open ocean) (Figure 3).  \n\nModelling and visualizations demonstrated that dune vegetation did make a significant difference  in  overtopping  of  the  dune  during  storm  events,  potentially  reducing  damage  and \nimpacts, and supporting the further study and implementation of improvements to these nature-based systems (Figure 4). Implementation of the shore parallel reef did reduce storm \nimpacts but did not prevent breaching. As seen in figure 2, the segmented nature of the barrier \nalso introduced the potential of rip currents between segments, a potential hazard for recreational uses.  \n\nObservations and Next Steps \n\nVisualizations  were  presented  to  local  interested  parties  on  October  27,  2022. This  group \nincluded local council and committee members, representatives of US Fish and Wildlife Service, National Park Service, and non-profit organizations involved in coastal management. \nSome audience members were noticeably stunned by seeing the transformation of the landform. Interested parties were highly engaged, scrutinizing details, for instance, rapidly pointing out the mislabelling of a coastal pond. Persons who managed the breachway remarked \nthat the effects on the storm were as expected but felt that the visualized outcomes clarified \ntheir understanding of the extent of the change. Visualizations of vegetation helped to express \na relative magnitude of potential effect of vegetation without promising protection. Unpacking the meaning of model choices such as “optimized vegetation” helped managers understand both limitations and possibilities for how modelling can inform decision making. The \nliterature  suggests  that  this  kind  of  interaction  bolsters  both  the  perceived  legitimacy  and \nacceptance of modelled outcomes (e. g. WHITE et al. 2010).  \n\nThe complexity of the modelling and effective characterization of outcomes would not have \nbeen  possible  without  the  implementation  of  a  highly  engaged  and  collaborative  process. \nWeekly meetings of the project team and continual engagement with interested parties not \nonly ensured that the team was coordinated with and understood each other and interested \nparties but aided with the communication of epistemic uncertainty related to model choice in \naddition to the aleatory uncertainty associated with the physical processes (MERZ & THIEKEN \n2009). Repeated, open discussions of modelling choices also helped to reduce the “crystalizing” effects of visualizations that can make an outcome appear fixed or more certain than it \nis (DEITRICK & EDSALL 2009). For instance, we elected not to show structures such as houses \nin these  first iterations of the  morphodynamic  visualizations simply because  more experimentation is needed to determine how to effectively represent them without implying out-\ncomes. Instead, this information was included in separate visualizations using identical viewpoints. \n\nThe crystalizing effects of visualizations seem particularly relevant in the case of morphodynamic visualizations because physical conditions continue to change and evolve throughout \nthe model timespan and beyond. The still visualizations presented here can show a maximum \nchange or a snapshot in time but fail to capture the continued evolution of the barrier in the \naftermath of a storm that continually reshape a barrier breach (something that will be resolved \nby animation). Thus, effectively characterizing the exact nature of  what  was being shown \nrequired careful attention of modelers and visualizers working in tandem with interested parties. For instance, “velocity” being used as a proxy for the intensity of the storm impact because it could be visualized (Figure 5). Waves, however, are the predominant damaging hazard, and it would be ideal to represent them directly. These issues will persist until subsequent \nmodelling is performed to better describe the phenomena. (GRILLI et al. 2020). \n\nThe representational uniqueness of the still visualizations also necessitated extreme care in \npresentation. The project team used visualizations in sequences that oriented interested parties to the physical geography first, before proceeding through sequences of visualizations, \nkeeping each set of visualizations in the same order. Further attention to issues such as the \nuse  of  terrain  exaggeration,  inclusion  of  structures  and  orienting  features,  and  developing \nconsistent  standards  for  colour  ramps  is  essential.  The  absence  of  conventions  and  rapid \ntimeframe of visualization development highlighted the value of familiar conventions in orienting audiences to what they were seeing.  \n\nWe thus conclude that morphodynamic visualizations offer both possibilities and challenges. \nAnecdotal evidence suggests that they can be extremely effective in engaging interested parties in management decisions by depicting tangible storm impacts and effects of mitigation. \nSignificant challenges exist, however, such as the development of repeatable paradigms for \nusing these visualizations, and in the extent of coordinated expertise and engagement necessary to create and apply them. We are continuing development of these visualizations in this \nsite and an additional site on Cape Cod, Massachusetts USA with the intention of doing more \ncomprehensive experimental testing of these visualizations. \n\nA Discrete Choice Experiment to Elicit People’s \nPreferences for Semi-Arid Riparian Corridors: \nA Multinomial Logit Model\n\nAbstract: The aim of this study is to examine public preferences for urban riparian corridors in arid \nregions using simulation and visual quality analysis scenarios. Ecological landscapes are often subject \nto  trade-offs  with  aesthetic  landscapes  that  include  micro  and  macro  environmental  factors  such  as \nmanicured landscapes. It is suggested that there is a preference for aesthetics in landscape design; however,  it  is  unclear  how  laypeople  prioritize  aesthetics  over  different  ecological  factors  in  landscape \nscenes.  This  study  uses  a  Discrete  Choice Experiment  (DCE)  to elicit  the preferences  of  current  or \nformer residents of Jeddah City, Saudi Arabia, for multiple landscape scenes. The method combines \necological landscape characteristics (adopted from the QBR index) found in the study area in Jeddah \nand  aesthetic  characteristics  commonly  suggested  in  landscape  design  projects.  Participants  in  this \nstudy were exposed to a set of illustrated landscape scenes, including various aesthetic and ecological \nelements configurations. Participants’ choices revealed the influence of their ecological and aesthetic \nvalues.  Results  show  that  people  may  prefer  unmaintained  ecological  landscapes  if  minimal  design \ninterventions were provided. This will prevent trading off the ecological unmaintained landscape with \naesthetically maintained landscapes within the study area. This study will help researchers and landscape  architects  advance  visual  preference  research  further  into  the  domain  of  empirical  studies.  It \npresents a new powerful technique to elicit the preference of an individual element in landscape scenes, \nwhich improves the precision of community-based decision-making.   \n\nIntroduction \n\nThe aim of this study is to examine public preferences for urban riparian corridors in arid \nregions  by  testing  to  what  extent  people  are  willing  to  trade  off  unmaintained  ecological \nlandscapes for aesthetics offered by specific micro and macro environmental factors. Landscape design reflects ecological and aesthetic values, and trade-offs are often made between \nthe two in practice. In arid regions, water scarcity means riparian corridors are the richest \nlandscape typology and the only blue-green links for hundreds of miles (BOGIS et al. 2021, \nHOU et al. 2021). Pressure from urbanization and lack of eco-literacy contribute to negative \nfeedback loops, which present dire challenges for migrating avifauna and regional wildlife. \nRiparian systems with high biomass are more desirable when natural resources and biodiversity are prioritized, in which multiple deliverable ecosystem services rely on the quality \nand  health  of  that  ecosystem.  Although  this  can  be  achieved  with  low  or  no  maintenance \nriparian buffers, these unmaintained ecological landscapes play an intrinsic role in sustaining \nthe global ecosystem services and are important for the survival of the avifauna (BOGIS & \nKIM 2021, BIAMONTE et al. 2011, MILLENNIUM ECOSYSTEM ASSESSMENT 2005). Ecological \nlandscapes are often subjected to trade-offs with aesthetic landscapes that include micro and \nmacro environmental  factors  such as  manicured landscapes. It is suggested that there is a \npreference for aesthetics in landscape design (KAPLAN 1977a, KAPLAN 1977b); however, it \nis unclear how laypeople prioritize aesthetics over different ecological factors in landscape \nscenes (ZHAO et al. 2017(. Learning how people make choices between alternatives will allow decision-makers to meet the best design option with the least ecological impact; thus, \nthey can avoid trading off (replacing) ecologically unmaintained landscapes for aesthetically \nmaintained landscapes. Therefore, this study utilizes the Discrete Choice Experiment (DCE), \nalso known as Choice-Based Conjoint Analysis, to elicit the preferences of the residents of \nJeddah City, Saudi Arabia for multiple landscape scenes. DCE is a widely used method in \nmarketing to  reveal preferences by analyzing the trade-offs people make between alternatives. Unlike the Visual Preference Survey (VPS) studies that provide the least useful information (EWING 2001), DCE helps elicit the part-worth utility assigned to each element in that \nscene and provides a list of the most and least preferred elements, which helps improve the \nprecision  of  the  decision-making  (HILL  2017).  This  study  combines  ecological  landscape \ncharacteristics adapted from the QBR1 index that are found in the study area and aesthetic \ncharacteristics, such as micro and macro environmental factors that are commonly suggested \nin landscape design projects adapted from relevant visual preference studies (KENWICK et al. \n2009, KUPER 2017, ZHAO et al. 2017). The combined DCE and VPS method and techniques \nused in this study established a clear connection to the “Visualization, Animation and Mixed \nReality Landscapes (VR, AR)” theme. The aim of this study also contributes to the “Resilient \nLandscape, Global Change, and Hazard Response” theme. \n\nResearch Design \n\nThe one-time self-administered online survey was available from January 1, 2020, to January \n31, 2020. The survey instrument consists of three parts; First, a visual choice set survey of \nthe urban riparian corridor (DCE). Second, a survey questionnaire on attitudes toward several \nfactors related to a) public parks, b) physical activity, c) ecology knowledge & attitude, d) \nthe constructed concrete channels, and e) riparian corridors. Third, a series of socio-demographic questions. The demographic and attitudinal questions are designed to be used as explanatory variables to understand patterns of ecological preferences, perception, and knowledge among the participants.  \n\nThe DCE in this study involves visual representations of several developed choice tasks of \ndifferent design alternatives for the urban riparian corridor. Each design profile represents an \nindependent and unique design alternative (profile) of itself. All the profiles were manipulated using the selected design attributes of the urban riparian corridor that exists in Jeddah. \nThe essential parts of performing a DCE are the creation and arrangement of the profiles into \nchoice sets to be evaluated by respondents. The study was structured as follows: \n\n1) Operationalizing riparian corridor attributes \n\n2) Designing the choice sets: Test the efficiency of the design \n\n3) Data collection and analysis \n\nOperationalizing Urban Riparian Corridor Attributes \n\nTo conduct a discrete choice experiment (DCE), several steps must be completed including \na) selecting the design attributes of interest, b) operationalizing them, and c) selecting their \nlevels. These steps are essential because they construct the design alternatives to be assessed \nin the visual experiment. According to LOUVIERE & TIMMERMANS (1990), in the stated preference experiments like DCE, the selection of attributes should only i) include the salient \nfeatures, ii) be simplified through combining similar attributes to reduce redundancy or confusion between them, and iii) be related to not only potential users but also to the decision-makers. It should be noted that the increase of an attribute or the levels of an attribute will \nexponentially increase the  number of possibilities. What is  most important, though, is the \nrange of the levels. It is recommended for reliability reasons that the number of attributes \nshould not exceed “a maximum of ten attributes with at most 15 levels per attribute” (HILL \n2017, 4); however, according to some research in DCE (SUGIYAMA et al. 2008, 436-437), the \nnumber of attributes and their levels can reach 15 and 40, respectively, with no effect on the \nvalidity of the test results.  \n\nIn this study, the salient attributes of the urban riparian corridor are the micro-environmental \nfactors of the riparian buffer (vegetation cover, water’s edge, ground texture, and cover, and \nwaterscape) and the macro-environmental factors that act as land use policy adjacent to the \nriparian buffer zone (urban density and diversity). The selected design attributes for this study \nwere compiled as seven main attributes and divided into three sub-attributes for five of the \nattributes and two sub-attributes for the other two, which bring them to a total of 19 levels of \nattributes (See Table 1). Next, the attributes were operationalized, meaning an explanation of \nwhat each attribute represents in this study was assigned an ecological quality (EQ) score. \nThe EQ Score is a dummy score used to provide brief data on the ecosystem condition or the \nhabitat quality. The researcher evaluates four main parts and sub-parts in the QBR index that \nrepresent attributes of riparian landscapes. The evaluation method includes a subjective assessment where the observer assigns a score that represents the ecological evaluation for a \nparticular riparian landscape attribute. In this study, a 25 EQ score is the highest score that \ncould be assigned in the evaluation process but unlike the QBR index, a negative score of no \nless than -25 was used to represent the possible impact of an added design attribute (design \nsolution) in this study. This allows the researcher to evaluate the total ecological qualities of \neach possible design alternative that may appear in the choices task and help examine the \ncorrelation  between  the  increase  and  decrease  of  an  EQ  score  and  people’s  preferences. \nLastly, photograph illustrations with all possible combinations between the levels of attributes were created, yielding 972 design concepts. \n\nDesigning the Choice Sets: Test the Efficiency of the Design \n\nTwo important decisions to make when constructing choice tasks are 1) how many concepts \n(unique design alternatives) to present per task, and 2) how many total choice tasks to ask \nper participant. Showing more concepts per screen increases the information content of each \ntask and the task difficulty. Research has shown that respondents are efficient at processing \ninformation about many concepts. To answer choice tasks with four concepts, it takes a respondent  considerably  less  than  twice  the  time  to  answer  two  concepts.  When  a  large \nproportion  of  the  respondents  are  expected  to  take  the  DCE  survey  on  smartphones  with \nrelatively small screens, this also calls for showing perhaps just two or three concepts per \ntask (ORME & CHRZAN 2017).  This study used Sawtooth Software to generate all the possible \ndesign alternatives (concepts). Each participant was randomly assigned to a group of visual \nchoice tasks that were controlled and organized by the software. All the attributes and the \nlevels were equally presented in the visual choice tasks in this empirical study. To achieve \nthe minimum standard error (≤ 0.05) and equally test all the possible design alternatives, the \n“Test  Design”  feature  in  the  software  was  performed.  It  was  found  that  this  study  must \ninclude fifteen random tasks; each task includes three concepts (design alternatives) and a \nnone option (I would not choose any of the three options). The study must include at least \n250 qualified respondents with a 15% chance of no response (for minimum standard error \nand to equally test the preference for every single attribute). Next, the 972 possible alternatives were created using the photo that was taken from the study area in Jeddah City to form \nthe base that was manipulated (illustrates the attributes over) using Photoshop 2018. Figure \n1 demonstrates how the choice tasks may appear for the respondents. \n\nData Collection and Analysis  \n\nThe study follows a non-probability purposeful convenience sampling and snowball approach. \nThis study qualified any current and former residents of Jeddah City who are over 18 years \nold, have a formed attitude and ascribed value toward the coastal-arid landscape of Jeddah, \nand have a formed attitude toward public space in the city. The recruitment invitations were \nsent via emails, social media, text messages, and hard copies of flyers to KAU members and \nthe EcoFoci social media platforms. A total of 988 adults participated in the study; after data \nscreening and cleaning, only 285 were qualified participants. The study was approved by the \nBiomedical  Research  Alliance  of  the  New  York  Institutional  Review  Board  (BRANY-IRB). According to the discussion in section 2.2, the 285 participants are enough to test the \ndesign concepts with minimum standard error.  \n\nThe results of people's choices come into utility scores for each attribute. The higher the score \nthe higher the preferences for that attribute (See Table 2). It is assumed that a respondent will \nchoose  the  most  preferred  alternative  that  yields  the  highest  utility  and  the  best  outcome. \nThere are three estimation approaches embedded in the software. This study implemented \nthe most widely used and robust utility estimation approach, the Multinomial Logit Model \n(MNL). MNL has been used for more than three decades in the analysis of DCE data. It is \nuseful as a top-line diagnostic tool, both to assess the quality of the experimental design and \nto estimate the average preferences for the sample (ORME & CHRZAN 2017).  \n\nResults \n\nTable 2 shows the logit utility estimations (N=252) where the least (-) and most (+) preferred \nlevels of attributes (β) are presented. An overall average relative importance2 analysis was \nperformed to reveal the total sample preferences for the attributes of the urban riparian corridor. The average importance for those attributes can be arranged into three groups: 1) high \n(Complexity & Water’s Edge), 2) moderate (TRVC, Trail, & Activities), and 3) low average \nimportances attributes (Density & Diversity) (See Figure 2). Even though people’s preference decreases as the number of unmaintained landscape increases, the results show that they \nwill most likely accept any design alternative that includes any number for the unmaintained \nlandscape than choosing the none option (See Table 2). The findings of this study showed \nthat there are many ways to minimize or even prevent trading off unmaintained ecological \nlandscapes with consideration of meeting people's preferences (See Figure 2). Figure 2 illustrates the  most and least preferred design attributes. It also reveals that urban density and \nurban diversity attributes show no significant effect on people’s choices compared with the \nother  micro-environmental factors (i. e., water’s edge,  vegetation complexity, trail design, \nand type of activities).  \n\nFigure 3 illustrates the most preferred design alternative for the target population. The most \npreferred design profile includes zones with a low number of unmaintained plants, grass water edge that is minimally modified by adding 4 exotic-maintained plants, a boardwalk, and \nis surrounded by a low urban density of mixed land uses (See Figure 3). Even though people \nare culturally accustomed to the maintained landscape over the natural, messy ecosystems \n(NASSAUER 1995) results of this study show that people are willing to accept a highly ecologically unmaintained landscape (natural, messy) if minor landscape design interventions \nare made (See Figure 4) (See Table 2). \n\nResults show that minimal design interventions would prevent trading off the ecological unmaintained landscape with three distinct groups of preferences (high, moderate, and a low \naverage of importances) for the seven selected attributes affecting the appeal for the riparian \ncorridor in Jeddah City (See Figure 2 & Table 2). Table 2 shows more details about the most \nand  least  preferred  attributes.  A  positive  larger  β  score  denotes  a  higher  preference  for  a \nparticular level of an attribute or a design option, and a negative or smaller β score denotes \nless preference in comparison to the other design options under that level of attribute. \n\nDiscussion \n\nThere are two key findings in this study. The first is related to how people make choices and \nhow we (designers and decision-makers) can avoid design options that lead to removing such \nan important ecological landscape to accommodate a highly-urbanized maintained landscape. \n\nThe second key finding in this study is related to the implemented analysis model which does \nnot  differentiate  between  different  socio-demographic  groups  by  only  describing  average \noverall preferences for the design attributes for the target population. Both key findings are \nexplained as follows: \n\nThe first key finding is related to how people’s choices and how avoiding trade-offs in ecological landscapes can be interpreted from the relatively high average importance attributes. \nThese results show how minimal design intervention can influence the choices of people to \naccept high ecological unmaintained landscapes. Such landscapes with ecological characteristics are believed to be the reason why migrant birds and all connected ecosystems exist and \nsuccessfully evolved. For example, the high average importance attributes were attached to \ntwo attributes, which are the vegetation ‘complexity’ attribute (number of exotic maintained \nplants) followed by the ‘water’s edge’ attribute (See Table 2). The vegetation ‘complexity’ \nattribute  was  the  most  preferred  environmental  factor  to  influence  people’s  choices  upon \nchoosing the most preferred riparian corridor design alternative in Jeddah City. The preference increases as the number of exotic plants increase, which supports the findings from the \nprecedent  landscape  assessment  studies  (KUPER  2017,  MAULAN  2006,  NASSAUER  1989, \n1995,  NASSAUER  &  FAUST  2013,  ZHAO  et  al.  2017)  that  suggest  that  people’s  preference \nincreases as the vegetation complexity increases. However, the low number of exotic plants \n(2 species) shows no significant effect on people's preferences; rather, people would choose \nan option with no exotic plants or the highest number of exotic plants in a scene (4 species). \nIn  other  words,  to  increase  the  acceptance  of  the  ecologically  unmaintained  plants,  the \ndecision-makers may have two choices to increase the number of the manicured landscape \nto the maximum number used in this study (4 species) or to leave it as it is (only the ecological \nunmaintained plants with zero exotic plants). However, it should be noted that increasing the \nnumber of exotic manicured plants may potentially increase the net Global Warming Potentials (net GWPs)  due to the required regular upkeep work (i. e., irrigation, mowing, fertilizing and clipping, etc.) (GU et al. 2015). The second most important attribute was attached to \nthe ‘water’s edge’ (See Figure 2). Results show that the lower the ecological score (EQ) for \nthis attribute the higher the chances to improve people’s preferences for a design alternative \n(See Table 2). It seems that people prefer the ‘grass water’s edge’ solution slightly more than \n‘stone water’s edge’. Both grass and stone design solutions (low EQ scores) are significantly \npreferred over the ‘trees  water’s edge’ (high EQ score) that  was assigned  with a negative \nvalue (mostly not preferred) (See Table 2). The reason behind refusing the trees as a water’s edge alternative could be that the local trees (Prosopis Juliflora) are seen as highly enclosed \nscruffy trees that may make people feel less stable and secure (KIM 2015, 132-133) by closing \nthe  visual  access  to  the  trail  from  both  sides  of  the  stream.  Prosopis  Juliflora  is  a  widely \nspread local plant in the study area (well known) with very low transparency that it is almost \nimpossible to see through in most of the spots it was found at. From biological and evolution \ntheories  perspectives,  APPLETON  (1975)  based  his  argument  that  people  tend  to  prefer  an \nenvironment (scene) with a prospect and a refuge. He explained that panoramic (prospect) \nenvironments offer places to see, while enclosed (refuge) environments offer places to hide. \nThis concept is very important because it explains the reason why people prefer environments \nwhere they can see (prospect) without being seen (refuge). From the safety-concern perspective, “A person with the ability to see and hide has great advantages over a person who cannot \ndo so” (MAULAN 2006, 28). People from Jeddah may value dense vegetation from a distance \nbut would not necessarily like to be in a dense landscape due to the unfamiliarity. The visual \nobstruction resulting  from this dense landscape  may  make  people hesitant to be  within it. \n\nAdditionally, the low transparency imposed by the foliage of Prosopis Juliflora at the edge \nof the water would prevent people from enjoying the water scene and the aquatic birds. The \nchances to take a clear photo of the stream or even see through the tree to watch the migrant \nbirds is near zero. Visitors may struggle to find access through the thorns that cover all the \nbranches to get a closer look or access to the water scene. The tree is covering a wide area \nalong the water’s edge, and this blocks the scenery for the visitors. However, these characteristics are the key reasons for migrant birds to inhabit the study area due to the safe habitat \nand perfect hiding spots that these characteristics offer. Even though the Juliflora trees at the \nwater’s edge may decrease people’s preference due to the lack of human made landscapes \n(i. e., maintenance or designed trail), a minimum design intervention (adding exotic plants) \nwould improve the preference for this attribute. Lastly, the stone as a water’s edge solution \nwas  found  to  be  the  second most  preferred  design  solution.  Stone  and  grass  water’s  edge \nsolutions are both preferred over the tree as a water’s edge option. In terms of ecosystem \nservices, grass and especially trees are more valuable than the stone solution. However, accommodating people’s preference for stone as a water’s edge solution may increase the erosion issue, change the stream morphology, increase the sediment deposit to the stream, and \nmost  likely  will  not  perform  as  well  as  the  other  options  in  controlling  urban-washed \npollution (absorb and biotreat). Accordingly, replacing or removing a tree water’s edge may \naffect the ecosystem and the habitat, and thus may affect the existence of the current avifauna \nand the overall biodiversity (BIAMONTE et al. 2011, MENDENHALL et al. 2016). Any influence \non this ecosystem may force the migrant birds that play a very important ecological role to \nchange their traveling route and thus may lengthen the total flying distance to the next stop \n(BIAMONTE et al. 2011). This may lead to endangering the number of traveled species which \neventually  would  affect  the  global  ecosystem  and  the  associated  ecosystem  services \n(MILLENNIUM ECOSYSTEM ASSESSMENT 2005). If there is a need to trade-off the local trees \nat  the  edge  of  the  water  to  provide  better  access  to  the  water  physically  or  visually,  it  is \nrecommended to reduce the density of these trees in a few spots by moving the trees to another area around the site. The majority of the local plants must remain protected from being \ntouched to provide a safe traveling path  for  migrant birds  as  well as a  safe habitat  for all \nfaunas (BIAMONTE et al. 2011). This may benefit both stakeholders (wildlife and human) and \nkeep the trade-off activities at a minimum level. This study also shows that some attributes \n(i. e., Urban density and urban diversity) have no significant effect on people’s choices compared with the other micro-environmental factors (i. e., water’s edge, vegetation complexity, \ntrail design, and type of activities) (See Figure 2). Accordingly for future research, it is suggested to eliminate the insignificant attributes from the choices and focus on increasing the \nnumber of those attributes of a similar scale (i. e., micro-environmental factors). \n\nThe second key finding in this study is related to the implemented analysis model (Multinomial Logit Model (MNL)) that assumes that there are no different groups which can result in \na different group of overall preferences based on socio-demographics. Therefore, different \ndesign solutions and decision-making should be accommodated based on the targeted population.  Achieving  sustainability  requires  addressing  people’s  socio-economic  needs  while \ndemocratically  engaging  them  to  determine  their  fate  (RANDOLPH  2004,  5).  Mismatching \npublic preferences on the restoration of such sensitive landscapes will negatively affect the \nsuccess  of  preservation  or  restoration  efforts.  The  ecological  quality  of  the  unmaintained \nlandscape and its relative attractiveness (public preference or acceptance) are two essential \nobjectives of landscape design. Both are necessary to achieve successful restoration projects \n(ZHAO et al. 2017, 107). Even though MNL has been used for more than three decades in the \nanalysis of Discrete Choice Experiment (DCE) data, it is only useful as a top-line diagnostic \ntool to assess the quality of the experimental design and to estimate the average preferences \nfor the sample. For future research, it is important to test if there are any significant differences between different socio-demographic groups in their preferences or values assigned to \nthe attributes and levels of attributes used in this study. This study suggests resuming the data \ncollection to obtain more comprehensive data of the target population and see whether there \nare any significant differences in the results compared to the current results. \n\nConclusion and Outlook \n\nThe intent of this study was not to draw a general idea about the target population preferences, \nbut rather to test the efficiency of the DCE method and whether people will accept the minimum intervention made for the unmaintained ecological landscape. However, for more accurate results, it is recommended to resume data collection. The study showed that there are \nmany  ways  to  minimize  or  prevent  trading  off  unmaintained  ecological  landscapes  while \nmeeting  people's  preferences  and  protecting  the  natural  habitat. This  replicable  study  will \nhelp researchers and landscape architects advance visual preference research further into the \ndomain of empirical studies. It showcases an example of the hybridization of different methodologies  and  approaches  in  supporting  an  empirical  study.  It  combined  two  research \ntechniques from marketing and visual preference studies. The method and techniques used \nin  this  study  established  a  clear  connection  to  one  of  the  Digital  Landscape  Architecture \nConference themes and provided an example of an approach to protect, restore, or develop a \nresilient infrastructure. \n\nAn Early Look at Applications for Artificial \nIntelligence Visualization Software in Landscape \nArchitecture  \n\nAbstract: Digital design technologies are currently dominating visual communications in the field of \nlandscape architecture. Due to rapid technological changes, artificial intelligence (AI) graphic software \nappears to be the next frontier in creating digital imagery, though little is known on the potential of \nartificial intelligence  for rendering landscape architecture graphics. The objective of this study is to \ndetermine the effectiveness and efficiencies of current Adobe-based rendering techniques, such as digital collaging in Photoshop, compared with the two major types of AI visualization software: text-based \nimage generation (DALL-E 2) and sketch-based image refinement (NVIDIA GauGAN2). Images produced  by  current  methods  and  AI  software  were  evaluated  by  landscape  architecture  professionals. \nProfessionals were also surveyed on their attitudes toward and adoption of AI programs. This study \naims to determine whether AI-based software can compete with visualization methods currently taught \nto landscape architecture students and used by landscape architecture professionals. Understanding and \nadoption of AI-based visualization techniques may lead to significant changes in landscape architectural practice and education.  \n\nIntroduction \n\nDigital design technologies have been of interest to researchers and practitioners of landscape \narchitecture since the mainstream adoption of computers in the late 1990s (DEUSSEN et al. \n1998, DEUSSEN 2003). The widespread adoption of computers and computer-based software \nled the rise of digital methods for rendering landscapes and architectural structures more efficiently than analogue methods, such as digital representation, 3D modelling, algorithmic \ndesign,  parametric  design,  and  geospatial  modelling  (PEDERSEN  2020,  HOCHSCHILD  et  al. \n2021). Artificial intelligence (AI) is the next frontier in digital design technology with the \nemergence of programs which can produce high quality graphics and renders with less time \nand skill required than previous computer design programs (CURETON 2016, XIAO 2021).  \n\nIn addition, most practitioners and students have little familiarity with digital design techniques beyond CAD-based plans and sections (e. g. AutoCAD, Vectorworks), digital collage-based renders (e. g. Adobe Photoshop) and non-parametric 3D modelling (e. g. Rhinoceros \n3D, SketchUp). It is possible that the skillset of today’s landscape architect may be vastly \ndifferent  from  the  landscape  architect  of  tomorrow  as  artificial  intelligence  technologies \ntransform workflows. The impacts of new technologies on landscape architecture practice \nshould be explored to help the field transition in the face of rapid technological change.  \n\nArtificial intelligence is still an emerging technology in landscape architecture as its practical \napplications are not yet solidly established (CURETON 2016, JAAKKOLA et al. 2019). There is \ncurrently limited research on the potential uses for artificial intelligence software in landscape \narchitecture. ZHANG & BOWES (2018) trained an AI program to model and predict groundwater  levels  for  more  effective  stormwater  management  and  ecological  design  outcomes. \nLEACH (2018) and CANTRELL & ZHANG (2018) emphasize that as artificial intelligence advances, there will be a need to redefine the role of the landscape architect from designer to \ncurator, as machines will execute certain tasks far more efficiently than humans.  \n\nNevertheless, AI-based landscape and urban design will require human intervention to varying degrees, as concepts related to the human world will need to be taught to AI, and ultimately humans must select and refine the most viable AI outputs (LEACH 2018), as seen in \nAI  technology  applied  to  architectural  works  (PENA  et  al.  2021).  FERNBERG  et  al.  (2021) \ncreated a framework for AI programs to learn the concept of landscape, which they define as \nan example of “AI ontology”. FERNBERG et al. (2021) ordered landscape elements from Fresh \nKills Park into increasingly simplified categories, creating a hierarchy that started from the \nlargest  concept  of  habitat  type  (morainal  oak  woodland)  to  the  categories  of  plants  (tree, \nshrub), to specific species. A thorough understanding of AI in landscapes has not yet been \naccomplished by any program due to the complexity of landscapes, but the current generation \nof AI visualization may already have utility for landscape architecture. \n\nThis study explored whether the current generation of artificial intelligence technology can \nbe effectively used as a design tool for visualizing landscapes. The larger research question \nto be answered is: “Can artificial intelligence software be used to visualize designed landscapes as an alternative to current rendering methods?” or “Are AI visualizing methods more \neffective  and  efficient  than  current  Photoshop  digital  collaging?”  ‘Effective  and  efficient’ \nrefer  to  faster  image  generation  with  similar  visual  quality  compared  to  Photoshop-based \nrendering.  \n\nBeyond answering the larger research question, other objectives will be addressed: \n\n1) Compare the quality (accuracy, realism, communicativeness) and efficiencies (time and complexity) of AI visualization with current popular rendering techniques.\n\n2) Determine the capabilities and limitations of the current generation of AI visualization software.\n\n3) Determine which parts of the design process would benefit from utilizing AI visualization.\n\n4) Obtain feedback from landscape architecture professionals on the quality and viability of AI renders in practice and education.\n\nDetermining Effectiveness and Efficiencies of AI Visualization in Landscape Architecture \n\nGeneration of Images \n\nFirst, various leading AI visualization software were examined, leading to the identification \nof two major types of AI visualization: “sketch-based” and “text-based” software. Sketch-\nbased visualization requires a base image made up of rudimentary shapes and outlines, which \nprovide a ‘guide’ for the AI to apply textures and forms from its database, resulting in semi-or photorealistic renders. Text-based visualization AI uses text prompts to generate images \nof varying styles, which can then be edited further with the addition of more text. Exploring \neffective sentence structure and use of keywords for text-based AI image visualization is a \nnew area of research (KONARIEVA et al. 2019, LEE et al. 2021). Two programs capable of \neach type of AI image were selected for use in this study. NVIDIA GauGAN2 (released in \n2021, free to use) was selected as the sketch-based option. GauGAN2 claims realistic landscape generation using a base sketch called a “segmentation map” in coded colours which \ncan be combined with text prompts, though a sketch-only approach will be used for the purposes of this study. DALL-E 2 (released 2022) was selected as the text-based AI option, due \nto its low cost (~10 cents per image iteration) and versatility.  \n\nThe  effectiveness  and  efficiencies  of  these  two  AI  programs  were  compared  to  rendering \nmethods currently taught in landscape architecture programs and used by landscape architecture professionals. Five images produced using traditional digital collage methods in Adobe \nPhotoshop were selected for re-creation in DALL-E 2 and GauGAN2, resulting in a total of \n10 AI-generated renders (Fig. 1). These five renders covered a range of environmental conditions: terrestrial versus aquatic, urbanized versus naturalized, as well as including various \noutdoor structures and taking place in different seasons.  \n\nOn  average,  producing  the  Photoshop  collage  renders  required  anywhere  from  3  hours  to \nover 10 hours, depending on  complexity and availability of digital assets.  In contrast, the \nDALL-E 2 text-based renders required roughly 1 minute to generate 4 square images at a \ntime, and an additional roughly 5 minutes to edit the images if necessary. If none of the 4 \ngenerated  images  were  satisfactory,  then  further  images  could  be  generated  from  the  best \nimage of the four. For example, image (C) in Figure 1, was produced in DALL-E 2 using the \ntext prompt “A beach landscape, on the right side is a lake with a sandy shore, on the left are \ndunes with tall grasses and a sand pathway cutting through the dunes that leads to the sandy \nshoreline. Farther on the horizon line is a row of trees that disappears into the distance beyond \nthe lake. The weather is sunny, with a pale blue sky and fluffy white clouds”. The text prompt \nwas written based on the composition of the original Photoshopped render (A) to try to recreate it. One generated image was then selected but needed to be edited from a square into a \nlandscape aspect ratio like the original image. Images can then be edited in DALL-E 2 by \nadding a “generation frame”, which can extend the image in any direction. DALL-E 2’s editing feature also allows the addition of elements into specific parts of the image using a brush \ntool and additional text (e. g., woman walking holding hands with a child). \n\nThe sketch-based render method in GauGAN2 required a longer time commitment (10 to 20 \nminutes) due to the software needing a segmentation map with the landscape elements translated into simpler forms (Fig. 2). The segmentation  map can be produced in the NVIDIA \nprogram directly, or produced in an external program (e. g., Adobe Illustrator) and uploaded \ninto the software, given that correct colours corresponding to landscape elements are used. \nTo produce  image  (B)  in  Figure  1,  a  segmentation  map  (Fig.  2),  was  produced  in  Adobe \nIllustrator and then uploaded into NVIDIA GauGAN2. The segmentation map was created \nby tracing the original image (A), resulting in a map of coloured vectors which could be read \nby the software to produce a landscape. Like DALL-E 2, GauGAN2 is limited to producing \nsquare images; to produce a landscape orientation image the segmentation map was divided \ninto two squares which were rendered separately and pieced back together into one image. \n\nSurvey of Landscape Architecture Professionals \n\nThe images produced by DALL-E 2 and GauGAN2 were then used as part of a 5-minute \nanonymous questionnaire produced in Qualtrics sent out to key informants in the landscape \narchitecture profession. In addition to the comparison images already produced, other AI-generated images were evaluated using the survey. The aim of the questionnaire was to obtain \nan answer to the research question of whether AI visualization programs can produce quality \nlandscape architecture renders, as well as meet the other research objectives relating to the \ncurrent applications for this generation of AI in the landscape architecture practice. An overall image of the value of AI to the field of landscape architecture could thus be obtained.  \n\nKey informants surveyed were selected based on the following criteria: 1) must be a landscape architect, urban designer, and\/or academic in the profession of landscape architecture; \nand 2) must have knowledge and experience  with digital design techniques, as  well as an \ninterest in emerging design technologies. A list of 20 professionals were selected and invited \nvia email to complete the anonymous questionnaire. 17 out of 20 informants completed the \nsurvey.  However,  out  of  the  17  responses,  only  12  responses  were  correct  and  complete, \nresulting in a 60 per cent response rate. \n\nThe first set of questions involved seven  text-based questions. These text-based questions \nwere used to determine practitioner’s level of interest and use of AI programs, including the \nspecific programs used, what stage of the design process they are used in, and the efficiencies \nand effectiveness of these programs compared to traditional methods. \n\nOverall,  responses  indicated  interest  in  AI  for  generating  landscape  visuals  (11  out  of  12 \nrespondents),  though  less  than  half  of  respondents  (5  out  of  12)  currently  use  AI  in  their \nprojects. As a result, AI does not yet impact most firm’s approaches to design, nor does AI \nplay a key role in the design process (8 out of 12 answered No or Not Applicable). However, \nmost respondents believe that AI will change the field of landscape architecture by altering \ndesign processes (11 out of 12, or 91.67%). This indicates a gap between the potential of AI \nfor landscape architecture and its current implementation, which was confirmed by 11 out of \n12 respondents. To address this gap, some early adopters of AI as a digital design tool are \npromoting AI in their educational or professional work (6 out of 12). \n\nOut of the leading artificial intelligence visualization software, respondents favoured Midjourney (7 out of 12), followed by DALL-E 2 (5), Stable Diffusion (2) NVIDIA Canvas\/ \nGauGAN2 (2) (Fig. 2). No respondents reported using an AI program that was not listed. \nOne respondent reported not using AI software at all. Those who do use AI as part of the \ndesign process overwhelmingly use it for Ideation, followed by Concept Development (Fig. \n3). \n\nFurther insight on the value of AI to the field was obtained via statements on AI visualization \nthat were voted on using a 5-point Likert scale (Fig. 4). These statements received largely \nmixed responses without a clear consensus. The approachability and ease of use of AI visualization software was mixed, with 1 Strongly Disagree (SD) rating, 2 Disagree (D), 4 Neutral \n(N),  2  Agree  (A)  and  3  Strongly  Agree  (SA).  For  AI  performance  on  landscape  designs, \nNeutral and Agree both received 5 votes each, while 2 respondents Disagreed, and no respondents Strongly Agreed nor Disagreed. Architectural designs received a similar response \n(0 SD, 2 D, 5 N, 4 A, and 1 SA). Most professionals surveyed do not strongly view AI visualization as a more efficient alternative to non-AI design software (1 D, 7, 4 A, 0 SA or SD). \n\nHowever, more respondents appear to anticipate that AI will replace certain tasks in the design process (1 SD, 1 D, 2 N, 6 A, 3 SA), and that AI may be used to enhance the design \nprocess (0 SD, 1 D, 2 N, 6 A, 3 SA). Respondents as a neutral to positive leaning response \nto whether AI will improve efficiency and time management are as follows (0 SD, 2 D, 5 N, \n4 A, 0 SA), or results in more innovative design (2 SD, 1 D, 5 N, 4, A, 0 SA). \n\nFollowing the text-based questions were image-based questions. First, informants were asked \nto rate four AI-generated images using a 5-point Likert scale (Very Poor, Poor, Fair, Good, \nExcellent). These images were generated using DALL-E 2 as an attempt to re-create deliverables  from  landscape  architectural  projects  and  included  two  perspective  renders  and  two \nplan-view  renders.  Section  elevations  could  not  be  produced  by  the  AI  program  and  thus \nwere not included. Each image was accompanied by a brief text description of what the image \nwas intended to communicate. The distinctive coloured square watermark in the bottom right \ncorner of DALL-E 2 renders was cropped out to ensure objectivity. \n\nFor all four images, responses were also mixed (Fig. 5). The design perspectives received \nmore positive responses compared to the two plan view images. The master plan view render \nfor a biodiversity-focused public park was rated by most participants as Poor (5 out of 12) to \nFair (5 out of 12). A render of part of an Italian formal garden received more mixed results \n(Very Poor and Poor received 1 vote each, whereas 5 respondents voted Fair, 2 voted Good \nand 3 rated the image as Excellent). The English cottage garden received similarly mixed \nresponses but with a positive bent, with 1 Very Poor rating, 5 Fair ratings, 2 Good, and 1 \nExcellent. Lastly, the mixed-used neighbourhood plan in a hand drawn rendering style received the most negative response, with 4 Very Poor votes, 2 Poor votes, 6 Fair votes and 0 \nGood or Excellent ratings.  \n\nThe final section of the survey asked respondents to compare of the AI-generated renders \nfrom section 2.1 with traditional Adobe Photoshop collage renderings. Five questions asked \ninformants to pick between three images, one produced traditionally and two produced by \nDALL-E 2 and GauGAN2, without being told how the image was produced. Like the previous image-based questions, each set of three images was accompanied by a text description \nof the desired scene that the renders are in intended to convey.  \n\nThe  five  comparisons  revealed  that  the  renders  produced  by  DALL-E  2  were  considered  \neffective by respondents and were largely comparable to the human-led Photoshop renders \n(Fig. 6). In contrast, renders produced by GauGAN2 were not received as positively. For one \nscene (producing a pond in an ecological park), the human-made and AI-generated methods \nall  tied  for  effectiveness  (4  votes  each)  (Fig.  7).  DALL-E  2  and  traditional  methods  tied  \nagain for producing a small urban park with outdoor seating in winter (6 votes each, 0 for  \nGauGAN2),  and  platform  seating  in  an  urban  park  (5  votes  for  both  DALL-E  2  and  Human\/Photoshop, with the 2 remaining votes for GauGAN2). For the scene of a dune landscape \nwith walkway (Fig. 1), the traditional Photoshop method was voted more effective than the \nAI  visuals (6  votes), though  DALL-E 2 received 5 votes  and GauGAN2  with 1 vote.  AI, \nspecifically DALL-E 2, had the preferred render for the designed wetland comparison, with \n9 votes  versus 1 (Human) and 2 (GauGAN2). When segmented in total  number of  votes, \nDALL-E 2 received the most (29), followed by the Photoshop collage (21) and GauGAN2(9).  \n\nDiscussion \n\nOverall, the AI programs DALL-E 2 and GauGAN2 were able to generate renders more ef-\nficiently (i. e., less time required per image) than Photoshop collage methods. Images produced by GauGAN2 were not considered effective final renders by landscape architecture \nprofessionals. DALL-E 2 could not synthesize effective plans for a park or a neighbourhood, \nthough the garden designs were rated higher. This result may be due to how the current generation of AI currently functions, as the garden designs were given descriptors tied to specific \nstyles and eras in landscape architecture, rather than concepts like “biodiversity” and “mixed-\nuse development” which are known to landscape architects and related professions but cannot \nbe properly understood by the current AI programs, which is consistent with recent research \nwith DALL-E 2 and urban design (SENEVIRATNE et al. 2022). Professional respondents typically limit their  use of  AI  visualization  for stages in the design process like ideation and \nconcept development rather than final renderings, or inventory and analysis.  \n\nWhen  comparing  traditional  human-led  Photoshop  renders  with  AI-generated  renders,  the \ntext-based DALL-E 2 renders were preferred over both the Photoshopped renders and the \nsketch-based GauGAN2 renders. Despite the higher control in re-producing the shapes and \ncomposition of the Photoshop scenes, GauGAN2 failed to integrate landscape elements using \nthe segmentation  map in a  way that looked realistic. DALL-E 2 excelled at creating  high \nquality, attractive images of landscapes, though there was less control over where the landscape  elements,  people,  and  structures  would  be  placed.  Improved  sketch-based  GAN  AI \nwould be ideal for producing landscape renders with a high level of specificity without the \ntime commitment of finding  and placing digital assets in  Photoshop. While text-based  AI \nvisualization currently dominates in the research and in practice, there is interest in improving \nGAN-based segmentation map technologies and techniques for image synthesis (LEE et al. \n2022).  \n\nCurrently, there are early adopters of AI technology in the field of landscape architecture. \nThe efficiency and effectiveness of text-based AI appears to have value for the field. Though \nrespondents were attracted to the DALL-E 2 renders during image comparison, practitioners \nand educators that were surveyed do not indicate a preference for AI over non-AI software. \nThe prevalence of neutral responses when determining attitudes towards AI may be due to \nAI  software  having  not  yet  established  its  utility  for  design  professions,  thus  a  lack  of \nknowledge may result in fewer strong opinions. Overall, there is uncertainty with how the \ncurrent generation of AI visualization will be integrated into the design process and whether \nthe  use  of  AI  can  result  in  more  efficient,  innovative,  or  efficient  design.  Landscape  \narchitecture  professionals  from  this  study  believe  there  is  currently  a  large  gap  between  \neducation\/training and the potential of AI in practice, and as this gap closes a clearer image \nof the value of AI to the field will likely emerge. \n\nConclusion and Outlook \n\nOverall, the research objectives stated in this research were met. The current generation of \nartificial intelligence can produce attractive images but is still too limited to produce final \ndeliverables, though some landscape architects are beginning to experiment with using AI \nvisualization for ideation and concept development. Typically, rendering requires some background image or context and understanding of detailed design elements and the specific layout of the design. These components are difficult to achieve at via AI visualizations due to \nsoftware limitations. For example, the integration of human and animal subjects in AI images \nis a key limitation. Landscape renders often include humans and\/or animals to create scenes \nthat are more sympathetic and dynamic. This is especially important for designed landscapes \nintended for programmed activities or ecological restoration. NVIDIA rendering programs, \nincluding GauGAN2 used in this study, does not have a way to include humans or animals \nin the segmentation map. DALL-E 2 is able to produce images with humans and animals, \nthough often with incorrect anatomical details (e. g. unnatural-looking faces, correct number \nof appendages). Thus, DALL-E 2 may be limited to producing humanoid figures that can be \nlooked at from behind or at a distance. Another limitation includes the lack of species-specific \nplant, animal or insect knowledge that may be necessary to produce convincing renders. As \na result, until AI programs improve, it may be more effective to take hybrid human-AI approach to rendering, where an image is produced by AI software and then added to or adjusted \nby the designer in Photoshop. Human intervention is still required in the rendering process, \nsuch as for specific customization in the visualizations, which can be achieved using current \ndigital design approaches.  \n\nBy exploring the capabilities of AI in landscape visualization, we aim to delineate between \n“machine work” and “human work” as suggested by LEACH (2018), as well as identify new \nskillsets and workflows of tomorrow’s landscape architect. The delineation between the designer and the machine will become more and more imperative as artificial intelligence continues to advance exponentially and becomes more accessible to the public. Understanding \nthe advantages and limitations of AI in various applications, as well as direct comparisons \nbetween AI rendering and human-only digital rendering are necessary to solidify the current \nadvantages and limitations for AI visualization in landscape architecture.  \n\nNew efficiencies in digital design techniques may have profound impacts on landscape architecture workflows due to the time-billed nature of private practice, which comprises the \nlargest  proportion  of  work  in  the  field  (TAYLOR  2006).  New  approaches  to  design  may \nemerge as aspects of the design process are simplified, accelerated, or rendered obsolete by \nAI  programs.  Indeed,  time  taken  from  calculations,  modelling,  and  rendering  by  AI  may \nleave more human resources for the iterative, creative, and conceptual brainstorming in landscape  architecture  (YU  2018).  More  research  is  required  to  determine  specific  hybrid  approaches for AI in landscape architecture. Rather than replacing the designer, the potential of \nAI programs may be used to strengthen the role of landscape architecture in designing the \nopen spaces of tomorrow. \n\nDesign with Sound: The Relevance of Sound in VR \nas an Immersive Design Tool for Landscape \nArchitecture \n\nAbstract: Sound in landscape architecture commonly focuses on noise. In design or planning process \nsound rarely plays a holistic and essential role. This is partly due to the need for more tools to address \nthe complex topic. This research introduces novel opportunities to explore the importance of sound as \nan immersive design parameter in landscape architecture and planning within Virtual Reality (VR). \nThe project investigates immersive sound experiences by presenting sonic data in audible form. A VR \napp was constructed from multiple on-location spatial sound recordings. Within a test case, the application  was  used  by  participants,  comparing  a  conventional  design  approach  with  the  proposed  VR \nmethodology. The outcome of the test clearly shows the importance of integrating sound in an audible \nform to enhance the comprehensive understanding of space. The paper will focus on the discussion of \nthe technical as well as the design-specific components. \n\nIntroduction \n\nSound profoundly influences our understanding and perception of space, its materials, and \nits environmental conditions. The same effect can also be seen in VR experiences. The field \nof virtual reality is increasingly focusing on sound design because the virtual environment’s \nsounds immerse the user even more in virtual reality. A study by Jeon and Jo in 2020 examined how visual and audio information influences the  satisfaction rate of the environment \nusing  VR.  The  visual  and  audio  information  distribution  was  76%  and  24%,  respectively \n(JEON & JO 2020). Even though this study shows the overwhelming importance of the visual \naspect of the landscape, aural experience has a noticeable part in the whole experience and \nunderstanding of a site.  \n\nResearch on soundscapes in Landscape Design and Planning has been made but establishing \na link between the conducted research and practice is still in its infancy. Nadine Schütz, who \ndid her Ph.D. in landscape acoustics, noticed that even though sound is admitted to being part \nof a landscape, the unity of visual and aural perception in experiencing a landscape is not \ndeeply a part of the practice of landscape architecture (SCHÜTZ 2017). The field commonly \nhas a noise-oriented viewpoint towards sounds in the landscape, which is guided heavily by \nthe known health aspects caused by noise. Today, the World Health Organization (WHO) \nnames  noise  as  one  of  the  top  environmental  risks  to  health,  pointing  out  that  noise  has \nauditory and non-auditory effects on people (WHO 2018). And according to studies, 20 – \n40% of people are sensitive to noise (PESONEN 2005). Due to the given guidelines and laws \non noise, planners have to use data that show estimated sound pressure levels (SPL) from \nnoise emitting sources – noise maps.  \n\nNoise maps are an important way to show the estimated SPL of noise at a given location for \nplanners and designers to follow the guidelines, laws, and restrictions concerning noise. But \nbecause noise maps are commonly the only source of information regarding sound that is \navailable, planners and designers might use those maps for making broader interpretations of \nthe quality of the site's sonic environment. However, noise maps are difficult to interpret as \nit requires in-depth knowledge of noise which landscape architects rarely have (RAIMBAULT \n& DUBOIS 2005). Non-experts thus rely almost solely on the colors used on noise maps for \ninterpreting the data. We demonstrate the effectiveness of the used color scheme in Figure 1. \nFrom the left, the color schemes presented are: the scheme the city of Helsinki uses today \n(HELSINGIN  KAUPUNKI 2019), ISO standard from 1996, European Acoustics Association’s \n(EAA) proposals from 2012 (ALBERTS & ALFÉREZ 2012), and 2015 (WENINGER 2015). The \ncity of Helsinki’s version is roughly based on the ISO standard 1996 version – which is the \ncase in many European countries. EAA proposed 2012 for a new standard, which was updated in 2015, among other things, to consider color vision deficiencies. The colors and the \norder of colors in the color scheme can affect how the data is interpreted.  \n\nBeate Weninger, who has studied the use of colors on noise maps, emphasizes the reasons \nwhy the color design of noise maps is essential by saying, \"color is a physical stimulus that \ncauses physiological as well as psychological reactions\". Furthermore, she outlines, that if \ncolors  are  not  used  properly,  it  can  lead  to  data  misinterpretation  or  even  manipulation \n(WENINGER 2015). Because guidelines and laws are built on the SPL levels, in work-life for \nlandscape planners, there is no high demand from, e. g., cities or municipalities to use soundscape information more holistically in designing or planning. \n\nIn addition to the problem of not having sonic data other than noise maps, the materials used \nand produced in the design and planning workflow are highly visual. Concentrating on visual \npresentations, planners and decision-makers are not encouraged to think about how the designs are perceived in real life with all senses (FRICKER 2019). Experiencing data in forms \nother than visual could result in different ways of designing, planning, and decision-making. \nDesigning and planning for all senses could result in a healthier environment of higher quality,  more  usable,  and  safer  for  users.  Jukka  Jokiniemi  studied  the  accessibility  and  cross-modality in the built environment. Jokiniemi reminds us that accessibility issues concern as \nwell people with sensory impairment. By designing and planning a city to be perceived by \nall senses, the city becomes better for all (JOKINIEMI 2007). \n\nThis work proposes a workflow to create an immersive VR experience to store and perceive \nsound information. Secondly, we establish a test methodology for immersive sound experiences to draw hands-on conclusions on its implementation within a design workflow. The \nmain research question addressed in this work is whether immersive sound experiences can \nsupport the design process by presenting sonic data in an audible form. \n\nThis  work concentrates on existing  soundscapes.  Creating  new soundscapes or  modifying \nexisting ones is outside the scope of this work. The immersive sound experience is not meant \nto work as a tool for designing or planning per se but as a medium that helps integrate sonic \ndata into the design or planning process. Planning tools usually have a slow implementation \nrate (DAVIES et al. 2009), which guided the scope of the work to concentrate on the implementation of sonic data in the planning process. \n\nThe overall goal of the work is to establish a workflow of sound integration in VR, which \ncan be applied to other planning and design tasks. The work supports the field of landscape \narchitecture in the practical implementation of sound as relevant data for design and planning. \n\nCase Study – Sonic VR App Design \n\nThe VR App  \n\nA VR experience is created to provide the planner with audible sonic data collected from the \nsite. The VR experience consists of a test site on which the user can visit 59 locations, the \npoints where the soundscape of the site was recorded. The construction pieces of the VR app \nare shown in Figure 2, which we introduce in this chapter. \n\nThe chosen test site is located in Helsinki due to using Helsinki’s existing 3D model as the \nvisual data. The requirements for selecting the site were the size of the area and providing \nsufficient free and unprogrammed space. The noise map from the site shows some variation, \nand  the  site  should  not  be  well-known  to  most  participants  in  the  design  test  to  ensure  a \nsimilar starting point between the participants. \n\nThe audio data is in ambisonic format and collected by a virtual reality audio recorder (Zoom \nH3-VR). The recordings were recorded for two days (weekdays in the daytime) with similar \nwind conditions. The order in which the recordings were taken was randomized. The SPL \nlevel  (LAeq)  was  measured  at  the  same  time  as  the  recording,  to  calibrate  the  recordings \nafterward, because the gain level of the recorder needed to be changed between the recording \nevents. \n\nThe audio files are in wav (Waveform Audio File) format and have four channels. According \nto Ortolani, ambisonics is a 3D audio technique that enables sounds to be heard from any \ndirection – compared to stereo or surround sound techniques, where the listener can localize \nthe sound sources only on a 2D plane (ORTOLANI 2015). Before using the ambisonic recordings in a VR experience, a binaural decoder needs to turn ambisonics into headphone signals. \nDecoding the files for headphones usually uses a small set of so-called virtual loudspeakers” \n(ZOTTER & FRANK 2019). This allows connecting the spatial sound to the head-tracking abilities  of  the  VR  headset,  leading  to  the  user’s  ability  to  localize  sound  sources  in  the  VR \nexperience, linking the visual and aural landscapes. \n\nThe user moves in the VR experience using teleportation as the virtual locomotion technique, \nwhich enables the user to move instantly to a new location. At the given locations, the user \ncan move around freely – due to the used six degrees of freedom (6 DoF). They refer to the \nposition and rotation along the three world axes, x, y, and z. According to Lang, these six \ndegrees of freedom can express any movement (LANG 2013). For more straightforward navigation at the virtual site, a mini-map was displayed in the VR. The map remains at the user's \nside and shows where they are.  \n\nDesign Test \n\nThe immersive VR experience as a tool in design workflow was evaluated with a design test. \nThe 21 participants were graduates or students in the planning fields. The design task was to \nidentify a suitable location for a small-scale structure within the park perimeters. The emphasis on sound in the study was carefully hidden from the participants before the test. In the \nadvertisement for the test only the use of VR was stated. The idea of the design test was to \nsee how the given data influences the results of the design task. The participants were split \ninto  two  focus  groups.  The  VR  experience  supplied  to  one  focus  group  was  visual  only, \nwhereas it was audio-visual for the second focus group. \n\nIn the design task assignment, the structure (such as a gazebo or a pergola) was described to \nbe for informal meetings and small gatherings. Stating the activity for the structure is essential because it gives a point of view for the task. It is also crucial for evaluating the soundscape. Studies have shown that the listener's activity or intention for an activity influences \nthe  assessment  of  the  appropriateness  of  a  soundscape  (NIELBO,  STEELE  &  GUASTAVINO \n2013). \n\nThe materials for the participants in the design test included basic information about the site \nin the form of maps, text, and pictures. In addition, participants had a 3D model of the site \nand the VR experience. The 3D model was the 3D view of Google Maps, which participants \naccessed through a laptop. \n\nThe design test had the following steps. First, the participants filled out a short background \ninformation questionnaire. Then they had time to familiarize themselves with the site's materials, including the VR experience, after which they gave their answer – the structure's location. Next, the participants assessed the supportiveness of the provided materials, and a \nsemi-structured interview was completed. \n\nEvaluation  \n\nTwenty-one participants took part in the design test, divided into the focus groups with (11) \nand without (10) sound in VR. The participants were all landscape architects, and according \nto the results of the background questionnaire, the different attributes (e. g., level of studies, \nwork experience) were relatively evenly distributed to the two focus groups. \n\nThe answers – the locations participants gave for the new structure – were distributed over \nthe  designated  design  area.  In  a  few  areas,  there  are  clusters  of  answers;  see  Figure  3.  In \nFigure  4,  the  answers  are  presented  over  the  noise  map,  color-coded  into  the  two  focus \ngroups, and divided into four clusters. Inside the area marked as \"1\" are answers from almost \nhalf of the participants, i. e., 9 out of 21. According to the noise map, they placed the structure \non the north side of the pond, which has the quietest spot in the park. Of these nine answers, \nsix came from the focus group that did not have sound in the VR. Their knowledge of sound \nleaned only on the noise map. Inside area \"2\" are three answers from the focus group with no \nsound in VR. These are closer to the road on the river's west side. The road is the primary \nsource of noise in the park. Inside area \"3\" are five answers from the focus group that had \nsound in VR. Of these five answers, three were almost in the exact location, on the south side \nof the pond. This location is marked with yellow on the noise map, which is quieter than most \nof the park. Inside the area \"4\" are four answers, of which three are from the focus group with \nsound in the VR. \n\nAfter the design task, the participants assessed the given materials on a scale of 0 (not at all) \nto 5 (immensely) on their supportiveness for the design task. There were slight differences in \nwhich materials the focus groups found most supportive. The group with audio in their VR \nexperience assessed the VR experience very highly, followed by the 3D model. The other \ngroup saw the map showing contour lines as the most supportive, followed by the VR experience. They assessed the 3D model as relatively low. Noise map as a material was evaluated \nin  both  groups  evenly,  grading  around  3,3. The  difference  between  the  groups  is  that  the \ngroup without sound in the VR considered a few more materials as more supportive than the \nnoise map. \n\nFinally, a semi-structured interview was conducted. VR as a medium was a positive encounter for the majority. The participants with audio in the VR brought up that VR could help \nplanners understand sound pressure levels. The teleportation in the VR experience causes the \neffect of hearing the differentiation of the SPL of different locations due to the sudden change \nof audio file when teleporting. When discussing the use of sound in the field, the majority of \nthe participants were surprised by how little they knew about sound, although they think that \nsound is a significant part of the environment. Sound had not been an evident part of their \nstudies in the field, and concerning work-life, a majority had worked with sound, but commonly it was from the viewpoint of noise. In general, the use of sound has not had a noticeable \npart in the participants’ professional works. Using data on audio in an audible form in design \nor  planning  was  rare  among  the  participants.  Discussing  noise  maps,  it  became  clear  that \ndecibel levels are pretty abstract to many participants. A few participants brought up directly \nthat they do not understand what the decibel levels in the noise map mean. Participants concentrated more on the noise map’s colors than the numeric decibel values. \n\nDiscussion and Outlook \n\nAs expected, there were some differences in the answers between the two focus groups in the \ndesign test. The group with audio in the VR experience avoided noisier areas more, and they \nwere able to find other suitable locations for the structure. The noise map guided the decision-making more for the group without audio in the VR experience, giving limited choices regarding acoustic comfort on the design test site. Based on the self-assessment and the interviews, the audio in VR was an essential source of information for the design task. This shows \nthat the VR experience with audio did not exclude the information the noise map gave – it \nsupported it (FRICKER 2018). \n\nHaving sound materials in an audible form brings a broader variety of aspects from sound \ninto consideration, contrary to noise maps. The presented immersive sound experience with \na design task showed that it has the potential to support planners in the design process by \nbringing the soundscape as an intuitively accessible resource. \n\nIn the interviews, some participants brought up their need for knowledge for interpreting the \ndata on noise maps, which supports the literature stating that planners do not often have the \nexpertise to interpret noise maps. Planners rely on interpreting the noise map’s colors to analyze their data. The group's answers in the design task without audio in VR also seem to \nsupport this claim. Many participants concentrated on the small, darker green area, which \nmight have gotten too much attention because the color scheme Helsinki uses in its noise \nmaps does not have an intuitive order of colors. As mentioned above, the focus group without \naudio in VR had answered near the quietest spot (marked with green color) on the site more \nthan the other group. Their only information about the soundscape came from the noise map, \nwhich guided their answers. In contrast, the group with audio in VR could find other suitable \nlocations, seen in the  small clusters of their answers. The interviews showed the depth  in \nwhich planners consider sound in design and planning. Design and planning with sound lean \nheavily on noise maps and guidelines relating to decibel levels, which planners need help \ninterpreting. The sound layer in the landscape should be taken into deeper consideration. The \nmain results of the discussion are: \n\n1)  VR has the potential as a medium for storing and perceiving soundscape data so that it can be linked to the visual landscape. \n\n2)  Making the noise maps easier to understand for non-experts would benefit planners and the public. The noise map’s color schemes could be changed to a more understandable and accessible version, as discussed in the introduction of this paper.  \n\n3)  The built environment could become more pleasant and usable by planners actively listening to the environment and bringing that data into design and planning. In addition, valuable  sound  environments  could  be  identified,  preserved,  and  created  more  extensively, and environments could improve for the visually impaired. \n\nIn conclusion, the conducted research highlights the impact and importance to further develop the use of VR in landscape architecture from the perspective of sound. As a result, it \nwill support us to design and plan our environments with more “open ears”. \n\nExpert Perceptions of Uncertainty Communication in \n3D Visualizations of Coastal Hazards \n\nAbstract: Guidance for visualizing coastal hazards has discouraged the use of 3D visualizations because of a lack of experimental testing accounting for their effects on audiences and the potential of \nthese visualizations to be misleading by making outcomes appear more certain than they are. Some \nexperts  continue  to use  model-driven  3D  visualizations despite  this  guidance.  We  thus  conducted a \nsurvey to better understand expert perceptions of uncertainty communication related to 3D visualization \nof storm impacts (flooding and damage to structures). The survey included 115 experts drawn from the \nNortheast USA. We selected experts with differing levels of familiarity with the visualizations tested, \nincluding experts engaged in their creation, to explain how their relationship to the process affects perceptions. We found that the experts overwhelmingly support using 3D visualizations for risk communication providing that adequate attribution, labelling, and background is provided to contextualize the \nvisualizations. The evolution of real-world use and practice suggests further research is needed to better \nunderstand the audience interpretations of the visualizations and revise expert guidance. \n\nIntroduction \n\nCoastal Communities face increasing uncertain risks posed by storm surge, and storm surge \ncombined with flooding from rainfall (TRENBERTH et al. 2018, TRENBERTH 2011, ROMERO \n& EMANUEL 2017). Coastal managers and other experts use 3D visualizations that combine \noutputs from hydrodynamic and hydrological models with realistic depictions of recognizable landscapes for public engagement, disaster risk reduction and training and informing decision makers, such as emergency managers. While disciplines such as landscape and urban \nplanning like employing participatory frameworks of science communication to easily accommodate 3D visualizations (e. g. SHEPPARD 2012), dissemination-based frameworks commonly applied to hazard and risk visualization discourage their use (e. g. KOSTELNICK et al. \n2013). This guidance emphasizes the use of 2D visualizations, management of level of detail, \nand knowledge of audiences such that added detail or dimensions do not imply more knowledge than exists (KOSTELNICK et al. 2013, BOSTROM et al. 2008). Scholars rightly worry that \nrealistic 3D visualizations reify physical models by transforming the abstraction of assumptions, equations, and nodes (places where calculations are made) into highly detailed images \nthat imply higher degrees of certainty regarding outcomes than exist (DEITRICK & EDSALL \n2009, KOSTELNICK et al. 2013). We believe that landscape architects should be cognicent of \nthese issues given that tools traditionally associated with the discipline are being applied in \nthis way. \n\nWe conducted a survey that shows that coastal managers and experts perceive 3D visualization to be effective at relating complex information to interested parties. These visualizations \nin themselves, however, do not conform to guidance and flood mapping practices that discourage use of 3D and emphasize the clear expression of technical uncertainty (e. g. PADILLA \net al. 2020, SEIPEL & LIM 2017, BEVEN et al. 2015). The term “technical uncertainty” is used \nto distinguish this form of uncertainty from other forms of uncertainty such as personal uncertainty and public (political) uncertainty that also shape risk perception, but are not discussed here (WALSH & WALKER 2016). All  flood hazard models involve technical  uncertainty that includes both epistemic uncertainty of the model (our ability to know) and aleatory \nuncertainty  associated  with  the  stochastic  nature  of  events  like  storms  (MERZ & THIEKEN \n2009). These issues are made more complex by increasing emphasis on “deep uncertainty” \nwhere decision makers cannot know the likelihood of an outcome (RUCKERT et al. 2019).  \n\nThe  use  of  3D  visualizations  in  diverse  contexts  thus  poses  a  potent  case  to  explore  how \ncoastal managers and other experts employing them in communicating risk from coastal hazards perceive the need for uncertainty communication when using 3D visualizations for risk \ncommunication. We also investigated how proximity to the visualization project affected the \nresults to determine whether being involved or familiar with aspects of the visualization process altered perceptions of their use in addition to observing other factors, such as type of \nexpertise that may influence perceptions. This work is closely related to our other investigations of whether 3D visualizations are perceived as being “scientific” (STEMPEL & BECKER \n2021), and the effects of context on perceptions of 3D visualizations (STEMPEL & BECKER \n2019).  \n\nMethods \n\nWe asked three questions of 115 U.S. experts participating in an online survey evaluating \nfour semi-realistic 3D visualizations of storm surge (STEMPEL & BECKER 2021): \n•  Are visualizations such as the ones you've just seen appropriate tools for risk communication? (Yes \/ No question) \n\n•  What concerns, if any, do you have about using visualizations like you've seen here for risk communication? (Open ended question) \n\n•  Should visualizations of storm surge distributed to the public include labels describing the scientific uncertainty of predictions? Please provide a brief explanation. (Open ended question) \n\nThe survey instrument was approved by the University of Rhode Island Institutional Review \nBoard (1047179–2), and participants provided online consent at the start of the survey. It was \ndistributed to respondents  using email lists  used by experts in coastal resilience in Rhode \nIsland USA. These included: an internal mailing list for the Rhode Island Emergency Management Agency \/ Federal Emergency Management Agency Integrated Emergency Management Course, the Rhode Island Shoreline Change Special Management Plan, and the Department of Homeland Security Center of Excellence at the Coastal Resilience Center at the University of North Carolina that reaches a wider audience of experts. The inclusion of persons \noutside of Rhode Island reflects an interest in exploring the possibility that proximity to the \nvisualization process or locale depicted might influence the appraisal of the visualizations. \nAdditional  data  collected  included  demographic  information  and  experience  with  storm \nsurge. \n\nA total of four visualizations were used in the survey. All surveys included three visualizations made for the Coastal and Environmental Risk Index (CERI), a system developed and \napplied within the State of Rhode Island. These visualizations depicted three communities in \nRhode Island USA and incorporated depictions of storm surge and of projected structural \ndamages (visualizations dynamically updated). Damage estimates were based on functions \ndeveloped by the US Army Corps of Engineers North Atlantic Coast Comprehensive Study \n(NACCS) (COULBOURNE et al. 2015 (https:\/\/www.nad.usace.army.mil\/CompStudy\/)). CERI \nmodels combine models for inundation, wave, and erosion (SPAULDING et al. 2016). Subsequent evolutions of CERI have been modified to depict wind and a more generalized quantification of risk and is now deployed as an app (SPAULDING et al. 2020). The fourth visualization depicted flooding of coastal port infrastructure made for Federal Emergency Management Agency Integrated Emergency Management Training Course (STEMPEL et al. 2018). \n(Figure 1). \n\nResponses to open ended questions were organized into a spreadsheet and inductively coded \nby the research team (THOMAS 2006). To validate the coding, codes were applied to a random \nsubset of the data (n = 100) by an independent coder. That coded sample was then compared \nand found to be 84 % in agreement with the coded data.  \n\n As initially designed, the yes\/no question as to whether the tested visualizations were suitable \nfor risk communication was intended for use in a logistic regression to determine if proximity \nand expertise, among other factors, influenced the perceived acceptability of using the tested \nvisualizations for risk communication. As will be discussed in the results, however, the one-sided nature of the responses made this analysis moot (additional details regarding methods \nof the larger survey project can be found in Stempel and Becker 2021, “Is it Scientific, Viewer \nPerceptions of Storm Surge Visualizations).  \n\nResults \n\nRespondents \n\nHalf of survey respondents were unfamiliar with the visualizations and the other half exhibited varying degrees of proximity to the labs that created the visualizations. Type of expertise and familiarity with the visualizations and visualization team are summarized in Table 1 and \nTable 2. \n\nThe cohort is overwhelmingly white, comparatively wealthy, and well educated. This lack of \ndiversity reflects the underlying condition of the selected expert cohort. 25% of the cohort \nnoted  their  gender  as  female.  Virtually  all  respondents  had  direct  experience  with  storm \nsurge, which is not surprising given the expertise and career focus of respondents. The personal experience of respondents with storm surge is summarized in Table 3. \n\nUse of Visualizations for Risk Communication \n\nThe answer to the question “Are these visualizations appropriate tools for risk communication” was overwhelmingly one-sided. Of 115 respondents, 97 answered Yes, three answered \nNo, and 15 did not answer the question (87 % response rate). The sentiments expressed in \nthe question “what comments do you have regarding these visualizations” provided insight \ninto  the  positive  assessment,  mostly  emphasizing  the  ability  of  the  images  to  place  surge \ninformation in recognizable contexts and the ability to convey complex information concisely \nin an easy-to-understand format. Examples of comments include: \n\n• “The oblique view of a 3D representation of each community is similar to images that \nthe public sees in the media following a storm surge event. This visualization choice puts \nthis information in a context familiar to the public.” \n• “It helps to know the area being shown to really understand the effect.” \n• “I am impressed as to the synthesis of very complex scientific data that these visualzations are able to express in a relatively easy-to-understand presentation.” \n\nThere was, however, a consistent sentiment that more context was required in the form of \ntext, supplemental images to provide a means of interpreting images (e. g. what every level of \ndamage represents in real terms), and background information.  \n\nConcerns Regarding Use for Risk Communication \n\nThe answer to “what concerns you regarding the use of these images for risk communication” \nwas revealing. 85 respondents answered the question (74 % response rate). 29 respondents \nwere concerned about the potential that the visualizations could mislead the public by being \ninaccurate, overstating or understating risk, or being used in misleading ways. Of those respondents, 15 were concerned about understating risk, and the remainder (14) expressed concern about overstatement or inaccuracy. 15 respondents expressed concern with adequately \ncommunicating the scientific basis for the visualizations and providing adequate background. \n15 respondents expressed concerns with representational choices. There was an expressed \npreference for a yellow-rust color palette, the way results were binned (the color choices were \nthe result of a need to make the visualization color-blind accessible), and the clarity of the \nfeatures represented. The least favourite visualization was the Misquamicut visualization that \nused a tan color to mark the surge zone.  \n\nOther issues raised included concern for the public’s understanding of probability and statistics (5 respondents), Accessibility of the visualizations to lay audiences (4 respondents) and \nthe inclusion of scientific uncertainty or quantification of risk (4 respondents). Four respondents stated that they had no concerns. The main themes are shown in Table 4. \n\nLabels Describing “Scientific Uncertainty”  \n\nThe answer to the question “Should visualizations of storm surge distributed to the public \ninclude labels describing the scientific uncertainty of predictions? Please provide a brief explanation.” Yielded a diverse response. Of 91 responses (79 % response rate), 61 answered \nyes (67 % of those who answered). Six respondents suggested that representations of uncertainty should be simplified, and 14 indicated that it should not be included. The remaining \nten responses discussed the issue without a clear indication of yes or no (Table 5).  \n\nIn discussing uncertainty, several respondents made comments regarding the improbability \nof the depicted storm event; two respondents, for instance, indicated that a 1 % storm was \ntoo unlikely. Most of the comments, however, appeared to reference aleatory uncertainty (the \npredictability of the depicted event), and fifteen respondents explicitly distinguished aleatory \nuncertainty from epistemic uncertainty; for instance, suggesting that uncertainty regarding \nmodels be excluded. One respondent referenced compounding uncertainties. 23 respondents \nexpressed concern for the public’s understanding of statistics and probability. 14 suggested \ncommunicating uncertainty was best done using extended background and supporting information. 13 suggested that disclosure of uncertainty was essential to establish the credibility \nof the visualizations. \n\nOnly  one  respondent  suggested  that  including  uncertainty  would  undermine  efficacy.  Respondents expressed other concerns in the response blank. For instance, six respondents expressed concern that the visualizations could cause panic, cause people to have misplaced \nfeelings of safety, or be misused. Concerns related to the expression of uncertainty are summarized in Table 6. \n\nEffects of Familiarity \n\nThere was a correlation between concerns that risks could be understated and those persons \nwho reported working with or near the science and visualization team. Conversely, persons \nwho had seen the visualizations but not otherwise engaged with the team or trainings using \nthe visualizations were concerned about overstatement. Whether this is a result of familiarity \nwith the data or investment in the process of creating it cannot be determined. Other considerations, such as political leaning, gender, experience with storm surge and type of expertise \nwere examined with no clear correlations explaining this difference. \n\nThere  was  a  strong  correlation  between  persons  who  were  familiar  with  the  work  and  an \nindication of the need to communicate uncertainty. Although it would seem logical that there \nmight be other correlations in the data, few if any other strong signals emerged across types \nof expertise.  \n\nDiscussion \n\nUse of 3D Visualizations for Risk Communication \n\nConventional 2D Visualizations of storm surge and sea level rise are among the most common visualizations of climate related hazards, and clear guidance has emerged for their use. \nThe overwhelmingly positive response to the question “Are visualizations such as the ones \nyou've just seen appropriate tools for risk communication?” would seem to contradict this \nguidance in the literature favoring 2D, rather than 3D, representations (KOSTELNICK et al. \n2013). This guidance, however, partly stems from a lack of experimental testing to account \nfor effects of 3D visualization on risk perception. Use of 3D visualizations is discouraged, in \npart,  because  we  do  not  fully  understand  their  effects  on  audience  perceptions  of  risk \n(BOSTROM et al. 2008, SHEPPARD & CIZEK 2009, KOSTELNICK et al. 2013).  \n\n\nResponses to this survey indicate that expert respondents are aware of the pitfalls and limitations of these visualizations elaborated by frameworks but see a role for 3D visualizations \nproviding that adequate qualification is provided. Issues of numeracy, concerns about perceived overstatement or understatement of risk, all align with factors referenced in exiting \nframeworks (KOSTELNICK et al. 2013). Respondents are also aware that visualizations might \nbackfire and make people feel safer because they only show effects in flooded areas. This \naligns with findings elsewhere in the literature and speaks to the limitations of localized flood \nvisualizations during multi-hazard events (MILDENBERGER et al. 2019, SCHULDT et al. 2018, \nRETCHLESS 2018).  \n\nRespondents envision  visualizations like those being tested as being  used  with contextual \ninformation that elaborates a breadth of information that spans from the technical underpinnings of the visualizations to tangible examples of the damages symbolized. This makes the \nvisualizations part of a larger information portfolio that approximates the effects attributed \nto participatory processes. This tracks closely with our findings that suggest audiences perceive 3D landscape visualizations as “scientific” products based on the presence of appropriate attribution, labeling, and background (STEMPEL & BECKER 2021).  \n\nFamiliarity \n\nExperts unfamiliar with the visualizations or team were concerned about overstatement of \nrisks and those more familiar with the visualizations or participated in the team were concerned  about  understatement.  This  suggests  that  there  may  be  a  relationship  between  the \nproximity of an expert to a process and perceptions of scenarios. We can speculate, for instance, that those familiar with the place feel more urgency in communicating risks based on \ntheir experience or investment in the process (or place), but it is impossible to know from the \ndata collected why this was observed. It is the mirror of known heuristic of risk perception, \n“local optimism bias” that describes how those closest to a risk are most likely to discount it \n(RETCHLESS 2018). Could it be that local experts feel increased urgency to communicate risk \nin situations where audiences discount it? This warrants investigation. \n\nIt is also notable that those same familiar experts were also most concerned with the communication of uncertainty—likely reflecting intimate knowledge of the models and their limitations and the need for qualification to prevent audiences from being misled. This also supports the use of labeling, background, and supporting material to ensure transparency, mitigate bias, and foster perceptions of legitimacy as previously discussed, and found in our other \nresearch (STEMPEL & BECKER 2021). Where preferences were expressed, they supported visualizations  that  most  closely  followed  conventions,  with  clearly  binned  outcomes  and  the \nmost legible color ramps, further reinforcing the application of conventions and standards \nfamiliar to experts and audiences alike. \n\nConclusion \n\nComments such as “It helps to know the area being shown to really understand the effect”, \nreflects experts’ perception of the unique capacity of these localized visualizations to orient \naudiences and communicate impacts. The insight expressed by one respondent that the visualizations were not necessarily communicating risk also draws attention to the extent to which \nthe visualizations emphasize the depiction or indication of impacts in context. This, taken \nwith the discussion suggests to us that it may be possible to realize these benefits and clarify \nusage of 3D visualizations for hazard and risk communication by appropriately qualifying \nthem as visualizations of impacts. Flood visualizations are among the most common climate \nvisualizations in use, and audiences are accustomed to seeing them. We conclude that experts \nare comfortable with using these visualizations providing that recognizable conventions of \nflood visualization are applied, and sufficient context is provided. More testing is required to \ndetermine what, if any differences exist between perceptions of 2D and 3D visualizations and \nto address the lack of knowledge that have led experts in risk communication to discourage \ntheir use. Pursuing this is the next step to refining guidance. \n\nHow Virtual Reality Renderings Impact Scale and \nDistance Perception Compared to Traditional  \nRepresentation \n\nAbstract: Spatial scale and distance are essential attributes of physical space in landscape design. Individuals’ perceptions of spatial scale and distance reflect how well they understand a space, and decides how they design the space. This research studies how scale and distance perception in landscape \ndesign projects using Virtual Reality (VR) renderings can differ from traditional design representations. \nThis  study  examines  perception of  space  using  three design  representation  methods: VR  simple  3D \nmodel, VR realistic rendered model, and traditional representation with the illustrative plan. Fifty-four \nindividuals with design education and practice experience participate in this research. Participants were \ndivided into 3 groups, and every group used one design representation method to estimate the spatial \nscale  of  selected  space  and  distance  to  selected  objects.  Participants’  perceptions  are  investigated \nthrough survey and statistically analysed. This research enriches VR-related studies from the perspective of spatial perception and awareness. It inspires diverse possibilities of future design representation \nin the design industry and education. \n\nIntroduction \n\nDesign Representation and People’s Perception of Space \n\nThe design disciplines depend on graphics, drawing, and multi-types of visual experiences \nas design representation and visualization to demonstrate the work (TWOSE 2017). Design \nprojects involve multi-sectors interests, where design communication is important to design \nengagement, development, and implementation (AAKHUS 2007). The multisensory communication achieved by design representation extensively affects design development and communication among stakeholders and designers (BROWN 2002). Design representation aims at \nmediating the relationship between designers and their products; among designers in a group; \nand between designers and users (BODKER 1998). However, widely-used rendering technology cannot easily realize the same realism as a picture of the actual scene (SKULMOWSKI et \nal. 2021). Many design representation methods focus more on demonstration than engagement  and  communication,  which  makes  the  visualization  less  interactive  and  immersive. \nThese  limitations  could  affect  people’s  spatial  perception,  including  scale  and  distance \n(WANG et al. 2022). Therefore, traditional design representation methods may prevent design \nparticipants, especially those without professional backgrounds, from efficiently understanding the spatial scale and distance in the designed space.  \n\nVirtual Reality (VR) Representation and Design Studies \n\nSince  the  1990s,  digital  landscape  representation  techniques  have  rapidly  increased,  with \nmore  realistic  3D  visualizations  and  interactive  participation  capabilities  (LOVETT  et  al. \n2015). VR is one of these advanced digital techniques. VR can be defined as a computer-generated environment that people can interact with as if this simulated environment was real \n(VAN KREVELEN & POELMAN 2010). In other words, VR technology provides a way of transporting  someone  to  a  digital  environment  where  they  are  not  physically  there  but  feel  as \nthough they are (REBELO et al. 2012). As an advanced visualization technology with interactive  and  immersive  visual  experience,  VR  technology  can  bring  design  participants  vivid \nspatial experiences (ALIZADEHSALEHI et al. 2019) and possibly help them have a better sense \nof scale and distance perception.  \n\nThe existing research on VR-assisted representation primarily focuses on four aspects: design \nvisualization, design education, design construction, and design collaboration. Regarding design visualization, researchers have investigated the impacts of VR technology on improving \npeople's perception of design by comparing VR with traditional design representation methods.  From  the  perspective  of  design  construction  and  collaboration,  existing  studies  have \nexplored how VR and mixed reality could help with the installation and development of landscape construction (KIM et al. 2022), and guide engineers’ collaboration in structure and electricity construction (CHALHOUB & AYER 2018). In design education, VR technology is researched to discuss its impacts on online learning, students' participation, performance, and \ncross-disciplinary education (MILOVANOVIC et al. 2017).  \n\nDesign perception is a core domain closely connected with design visualization, design education, design construction, and design collaboration (CUI et al. 2022, DUPONT et al. 2014). \nIn the product design industry, existing studies have found that immersive VR can reduce the \ntime stakeholders use to comprehend the furniture design options and improve the designers' \ndesign perception in teamwork (PRABHAKARAN et al. 2021). In the Architecture, Engineering \nand Construction (AEC) industry, VR combined with game design engines and Building Information Modelling (BIM) are also found to be helpful to spatial perception. They can provide better perception environments by creating realistic and immersive design representations,  which  increases  communication  quality  and  construction  accuracy  (CHALHOUB  & \nAYER 2018, WEN & GHEISARI 2020). In the field of urban planning, landscape architecture, \nand environmental planning, VR integrated with Geographic Information System (GIS), public open access data, new rendering engines, and geo-design theory are regarded as important \ntechnologies assisting design perception and communication (PORTMAN et al. 2015). \n\nChallenge and Gap \n\nThere are many existing studies related to VR technology and design perception. However, \nthey focus more on general design perception and comprehension. Although some studies \nhave investigated VR and spatial perception of design projects particularly, the discussions \nare still relatively broad. These studies focus more on spatial objects such as buildings, vegetation, streets, water features, and installations. Some studies have involved essential spatial \nelements of spatial perception, including color, sound, weather, and temperature. However, \nfundamental elements of spatial perception, such as the sense of scale and distance, still need \nfurther and in-depth research.  \n\nFrom the perspective of disciplines, such studies examining the impacts of VR representation \non people’s scale and distance perception of design projects in the field of landscape architecture are relatively limited. Relevant topics have been involved in architecture, planning, \nand construction discipline, so similar exploration should be done in the landscape architecture field. Overall, considering the gaps and challenges of current studies about VR technology and spatial perception, exploring the impacts of VR representation on people’s scale and \ndistance perception of design projects will be a meaningful research topic in the landscape \narchitecture discipline. \n\nMethod \n\nStudy Project and Representation Methods \n\nThis study uses a 5-acre conceptual park design made by the research team and ZDG group \nas the study project. The project includes diverse types, scales, and functions of space which \nare common in landscape projects. The illustrative plan, simple 3D model, and realistic rendered model of this project are prepared for the following spatial perception study. How these \ndesign representations are generated and how they will be viewed are explained in Table 1. \nThe VR headsets adopted in this study are two sets of HP windows mixed reality. They are \nconnected  to  a ThinkPad  P1workstation  and  are  used  to  view  the  simple  3D  and  realistic \nrendered models. \n\nFor illustrative plan, design elements are visualized with the correct color and basic texture \n(Figure 1). For simple 3D model, the fundamental shape and geometry of design elements \nare modeled, and abstract plants are included. No material or texture is added to the simple \n3D model, and different materials are represented with different colors (Fig. 2). For realistic \nrendered model, the correct material, texture, and realistic plants are included. At the same \ntime, the environmental context, such as skylight, shadow, and wind, are involved (Fig. 3).  \n\nStudy Participants \n\nThis  study  recruits  54  participants  with  design  study  or  practice  experience  (in  landscape \narchitecture, architecture, urban design, urban planning, and environmental design) from Virginia Tech and several design firms in the U.S. and China. The participants are between 18 \nand 50 years old, 28 of whom are females and 26 of whom are males. Considering that the \nfamiliarity and expertise  with spatial scale and distance estimation  might vary greatly between people with and without design-related backgrounds. In the research, the comparison \nbetween  VR  renderings  and  traditional  design  representation  methods  is  the  focus,  so  the \nimpacts of design experience should be controlled. Therefore, the final selected study participants have 2 to 30 years of experience in design education and practice. \n\nStudy Design \n\nTo examine participants’ spatial scale perception, 3 types of landscape space are selected in \nthe study project (S-test A, S-test B, and S-test C). The areas of these 3 spaces are from large \nto  small,  representing  small-scale,  medium-scale,  and  relatively  large-scale  space,  respectively (Table 2). \n\nTo investigate participants' spatial distance perception, 3 different landscape objects are chosen in the study project (D-test A, D-test B, and D-test C), and their distances to the participants' viewpoint are different (from near to far) (Table 3). \n\nThis study divides participants into 3 groups according to their preference and availability to \nuse  different  design  representation  methods.  Every  group  uses  one  design  representation \nmethod to estimate the spatial scale of selected space and distance to selected objects. For \ndifferent groups, participants are asked to either check the illustrative plan or move around \nthe space using VR headsets and controllers to estimate the approximate area of 3 selected \nspaces. Meanwhile, they are required to either look at the illustrative plan or around the space \nfrom a fixed viewpoint to guess the distance between the 3 chosen objects and themselves. \nTo avoid difficulty in judging scale or distance due to the lack of understanding of the project, \nparticipants are allowed to know the approximate canopy diameter of the nearest trees. For \ngroups using VR techniques, people models are added as a reference, too. During the experiment, every participant has up to 5 minutes to check the plan or 10 minutes to view the space \nusing VR. After the experiment, participants' answers are investigated through the survey. \nTable 4 summarizes the study design explained above.  \n\nResults \n\nThe participants' answers are collected through survey and statistically analyzed with SPSS. \nThe  overall  descriptive  statistics  show  that  people's  perceptions  of  small-scale,  medium-scale, and large-scale landscape spaces are different. Participants make fewer errors in estimating the area for smaller spaces than for larger spaces. However, for spatial distance perception, participants make fewer mistakes in estimating the medium distance. Compared to \nthe long distance, more errors are made in short-distance estimation (Table 5). \n\nSpatial Scale Perception \n\nTable  6.1  and 6.2  shows  the  ANOVA  and  Post  Hoc  analysis  of  participants'  spatial  scale \nperception and 3 different design representation methods. The results indicate the significant \nimpacts  of  design  representation  methods  on  people's  spatial  scale  perception,  including \nsmall-scale, medium-scale, and large-scale landscape space. It shows that traditional illustrative plan, VR simple rendered model, and VR realistic rendered model can affect people's \nscale perception differently. Table 6.2 shows that, for S-test A (small-scale space), VR-based \nrendering  does  not  show  a  significant  difference  from  the  illustrative  plan.  However,  VR \nsimple rendered model and VR realistic rendered model have significantly different impacts on people's spatial scale perception. For S-test B (medium-scale space) and S-test C (large-\nscale space), there is no apparent difference between the traditional illustrative plan and VR \nrealistic rendered model, but VR simple rendered model shows a significant difference from \nthe other two methods.  \n\nSpatial Distance Perception \n\nTable 7 shows the ANOVA analysis of participants’ spatial distance perception and 3 different design representation methods. The results do not indicate any significant difference between the traditional illustrative plan, VR simple rendered model, and VR realistic rendered \nmodel. However, for VR simple rendered model and VR realistic rendered model, the accuracy of distance estimation is significantly higher than that of scale estimation according to \nthe survey data. \n\nDiscussion\n\nSome arguments are worthy of further discussion based on the study results and related data \nanalysis.  The  results  show  that  participants  make  fewer  errors  in  estimating  the  area  for \nsmaller spaces than larger spaces. Compared to the traditional illustrative plan, this difference \nis more evident for both VR simple rendered model and VR realistic rendered model (Table \n8.1). This finding implies that, when dealing with the scale estimation of large-scale landscape design or planning, adopting VR rendering might not be very useful, and illustrative \nplan could be a better choice. On the contrary, VR rendering works better for the area estimation of small-scale landscape designs such as garden, plaza, playground, and small park. \nIn  terms  of  spatial  distance  perception,  the  ANOVA  analysis  fails  to  find  any  significant \ndifference between the 3 design representation methods’ impacts on distance estimation but \nthe descriptive statistics shown in Table 8.2 might indicate more information. All design representation methods studied in this research show limitations in estimating short distances. \nHowever, for short distances, both VR simple rendered model and VR realistic rendered model \ngive a more correct answer than the traditional plan, while realistic rendering works slightly better. \nThis also indicates the advantages that VR rendering has in small-scale landscape projects. \n\nExisting research has found that VR technology can achieve more immersive and realistic \nvisual experiences. Nevertheless, for people with experience in design, using VR rendering \nto  estimate  distance  and  area  is  not  significantly  more  accurate  than  using  the  traditional \nillustrative plan in this research. The reason might be that estimating the spatial scale and \ndistance  from  the  plan  is  an  important  skill  of  professional  designers.  For  people  without \ndesign backgrounds, the different performance between traditional illustrative plan and VR \nrendering might be more obvious. Therefore, “with or without design experience” as another \nresearch variable should be considered in future relevant research.  \n\nMeanwhile, when comparing VR simple rendered model with VR realistic rendered model, \nthe  realistic  model  tends  to  work  better  than  the  simple  model  in  spatial  scale  perception \nwhile the distance perception keeps approximately the same. It indicates that realistic material, vegetation, people, and environmental context could help people develop a better sense \nof scale. Moreover, if VR rendering is required for designers in the design process, using it \nfor distance estimation is more helpful than area estimation. In addition, compared with the \nillustrative  plan,  the  area  estimations  using  VR  renderings  are  relatively  smaller  than  the \nproper answers, especially for medium-scale and large-scale landscape space. The current \nexperiment done in this study cannot thoroughly answer this question. This might be related \nto the difference between the VR perspective and the human eye perspective, which requires \nmore research in the future. \n\nConclusion and Outlook \n\nThis research examines the role of VR technology in establishing a sense of scale and distance for design participants in landscape projects. It also explores the differences between \nVR  technology  and  traditional  design  representation  in  design  perceptions.  This  research \nfinds that participants make fewer errors in estimating the area for smaller spaces than larger \nspaces, especially using VR rendering. Among all 3 design representation methods, VR realistic rendered model is most suitable for scale and distance perception in small-scale landscape projects. Among participants with design experience, using VR rendering to estimate \ndistance and area is not significantly more accurate than using traditional illustrative plan. \nHowever, the realistic rendered model works better than simple 3D model in spatial scale \nperception, while the distance perception keeps approximately the same. Therefore, VR realistic rendered model is more suggested for the area estimation in the design process. Overall, this research enriches VR-related studies from the perspectives of spatial perception and \nawareness. It inspires the diverse possibilities of future design representation in the design \nindustry and education.  \n\nThis study has only explored a limited scope of VR rendering and spatial scale or distance \nperception. Future research needs to consider more factors such as participants' background, \nview perspective or angle, and rendering accuracy. Meanwhile, other design representation \nmethods and VR techniques, such as VR with mobile devices, could be adopted for the next \nstep. In addition, the rapid development of gaming design software and associated technologies is potential to visualize more realistic and immersive 3D scenes. Compared with landscape design  modelling and rendering tools, how these  gaming design techniques can enhance  and  affect  the  spatial  perception  of  design  work  is  worth  of  further  exploration.  In \nterms of investigating and evaluating participants’  spatial  perception, this  study only  uses \nsurvey with statistical analysis, and the sample size is not large enough. On the other hand, \nqualitative approaches like in-depth interview and experiment-based approach like visual impact assessment, eye tracking, and vision research could be involved to extensively understand participants’ thoughts and obtain more comprehensive findings. \n\nSensing River and Floodplain Biodiversity – \nDeveloping a Prototype \n\nAbstract: Freshwaters, such as rivers and floodplains, are among the world’s most diverse ecosystems, \nbut they  are  losing  biodiversity  faster  than  any  other  ecosystem,  mainly  due  to  human  activities.  A \nmajor  problem  is  the  low  awareness  of  biodiversity  loss.  Triggering  emotions  and  amazement  may \nincrease people’s biodiversity perception in a more holistic way. Therefore, with an immersive audio-visual VR-simulation prototype based on 3D point clouds and sound recordings above and below water \ndeveloped in the Unity game engine, we want to allow for sensing river biodiversity. Feedback from a \nuser study demonstrates that the prototype can promote laypersons’ awareness of biodiversity loss and \nprovides insights for its further enhancement. \n\nIntroduction \n\nThe functioning of the biosphere is recognized as an important prerequisite for nature’s contributions to people and in turn human well-being, making it a core aspect of the 2030 Agenda \nfor Sustainable Development’s 17 goals (UNITED NATIONS 2015, IPBES 2019, PHAM-TRUFFERT  et  al.  2020).  Particularly  the  biodiversity-focused  Sustainable  Development  Goals \n(SDGs) 14 “Life Below Water” and 15 “Life on Land” have been identified as valuable systemic  multipliers  of  positive  influences  across  all  goals  (PHAM-TRUFFERT  et  al.  2020, \nOBRECHT et al. 2021). However, biodiversity, defined as the diversity within and between \nspecies, habitats, and of ecosystems, is declining globally at an unprecedented rate (IPBES \n2019). Primary causes are human population growth, accompanied by increasing demand for \nenergy and material goods, and human activities such as dam construction, water abstraction, \nagriculture, and urbanization (IPBES 2019). Lack of awareness among the public and other \nstakeholders is considered to be a critical aspect of poor implementation of measures to address biodiversity loss (DARWALL et al. 2018). Hence, the Convention on Biological Diversity (CBD 2021) and IPBES (2022) call on science and educational organizations to promote \nawareness raising. In this paper, we explore the quality of an audio-visual simulation of a \nriver floodplain in a user study for raising people’s awareness of biodiversity loss. \n\nAwareness Raising of Biodiversity Loss in Rivers \/ Flood-plains \n\nFreshwaters such as rivers and floodplains are among the most diverse ecosystems worldwide \nbut are facing a decrease in biodiversity faster than any other ecosystem, mainly due to human \nactivities such as drainage, agricultural or industrial water pollution, or construction of dams (TOCKNER & STANFORD 2002, DESJONQUÈRES et al. 2019). The recent Living Planet Index \n(WWF 2022) reports an average global decline of freshwater species populations of 83 % in \nthe last 50 years, compared to a global average decline in all monitored species populations \nof 69 %. Therefore, there are not only increasing efforts to measure the losses (LINKE et al. \n2018, DESJONQUÈRES et al. 2019), but also to raise awareness of this problem among nonspecialists and the larger public (DARWALL et al. 2018, MONACCHI & KRAUSE 2017). \n\nA major challenge of awareness building of biodiversity loss in  general and of freshwater \nhabitats in particular, is that many people do not directly perceive it (DARWALL et al. 2018). \nIn turn, they are less emotionally involved and less motivated to take actions against the loss \n(GEHLBACH et al. 2022), an effect that is well recognized in landscape perception research \n(KING et  al. 2017).  In  this  context,  rather  than  acquiring  knowledge  and  understanding  of \nbiodiversity,  it  seems  that  the  triggering  of  emotional,  intuitive  links  between  people  and \nspecies has a greater impact on the value people place on biodiversity (KING et al. 2017). For \nexample, sensory stimuli such as photos and people’s amazement may increase engagement \nwith the environment and a perception of biodiversity in a more holistic way as an interconnected part of nature rather than as a separate aspect (KING et al. 2017). \n\nGEHLBACH et al. (2022) show how photos depicting the costs of biodiversity loss can enhance \nthe recognition of the problem by fostering the viewers’ valuing of biodiversity and evoke \npositive motivational behaviour to make greater donations to biodiversity-supporting charities. In that study, the implemented photos were chosen to trigger negative emotions. As such \nphotos might also cause a sense of helplessness of the viewer preventing further actions, the \nresearchers recommend investigating effects of stimuli that foster positive emotions on the \nmotivational behaviour to help enhance biodiversity. Furthermore, LINDQUIST et al. (2020) \ndemonstrate that multisensory simulations providing not only depictions of a landscape with \na lot of varied vegetation, but also realistic auralizations of the environmental sound enhanced \nthe perceived biodiversity experience leading to higher biodiversity ratings. Congruent audio-visual stimuli increased the perceived realism of as well as the preference for the depicted \nlandscapes and even more, when these were shown in a head mounted display (LINDQUIST et \nal. 2020). As such immersive audio-visual stimuli have high potential to evoke feelings of \nconnection with nature, to foster people’s biodiversity perception in a depicted landscape, \nand  might  help  raising  support  for  actions  against  biodiversity  loss  (KING  et  al.  2017, \nDERINGER & HANLEY 2021), their targeted use needs further investigation. \n\nConcerning sound, audio recordings are a valuable source for gathering ecological information and for ecoacoustic monitoring because environmental sounds not only reflect the behaviour of animals but also the structure and functioning of their habitats (LINKE et al. 2018, \nDESJONQUÈRES et al. 2019, STOWELL & SUEUR 2020, PARSONS et al. 2022). Recent efforts \naim to increase the use of ecoacoustics as a tool for river and floodplain restoration and management (STOWELL & SUEUR 2020, PARSONS et al. 2022). This includes also communicating \nto researchers from other disciplines and to the broader public what happens to the natural \nsoundscape, and what this means regarding biodiversity (MONACCHI & KRAUSE 2017). Audio-visual combinations of the spectrogram and the audio records are frequently used to provide expert users with good understanding of a sound (PARSONS et al. 2022). However, an \nimmersive audio-visual simulation of the soundscape in combination with associated visual \nelements is regarded more supportive for laypeople due to a direct and intuitive experience \n(MONACCHI & KRAUSE 2017). \n\nMethod \n\nOur objective  was to provide people a sensual experience  of a section of the  Sarine river \nfloodplain  (Canton  of  Fribourg,  Western  Switzerland)  through  an  immersive  audio-visual \nVR simulation that users can freely explore (Fig. 1). \n\nDespite the impacts of hydropower production with residual flow management, the assessed \nstretch of the Sarine floodplain represents a complex, dynamic mosaic of contrasting aquatic-terrestrial habitats (i. e., floodplain forest, islands, gravel, large wood accumulations and various channels and ponds). Occasional artificial flooding from the upstream dam maintains \nhabitat connectivity and dynamics. This coupling of aquatic-terrestrial habitats still promotes \nand  maintains  high  biodiversity  (see  TONOLLA  et  al. 2020  and  DOERING et  al. 2021  for  a \ndetailed description of the Sarine). For this reason, the Sarine floodplain is listed as a floodplain of national importance that is even slightly affected (FOEN 2020). In the following, the \ndevelopment of the audio-visual simulation focussed on the representation of the aquatic-terrestrial habitat diversity in general and how its quality was tested in a user study. \n\nAudio-Visual Simulation \n\nAn audio-visual simulation prototype was created in the Unity game engine version 2022.1.0.f1 \n(www.unity.com) for presentation with the HTC Vive Pro Eye equipped with a wireless adapter \n(www.vive.com). LiDAR data from terrestrial laser scanning in the floodplain with a RIEGL \nVZ-1000 was used for realistic above ground visualization. This point cloud data was combined \nwith a digital surface model (DSM) mesh acquired with a WingtraOne drone (www.wingtra.com). \nThe  water  area  was  animated  using  the  Unity  plugin  River  Auto  Material  2019  package \n(NATUREMANUFACTURE 2022) and the MS UnderWater Effect asset (SCHULTZ 2020) made the \nunderwater effects more realistic. A big challenge was the visualization of the massive point \nclouds (> 330 mio points, 50 GB) with high density for realistic representation of the environment. With the PotreeConverter the point cloud was transformed into an octree data structure \n(https:\/\/github.com\/potree\/potree). Further, a shader was applied that dynamically loads the point \nclouds in a hierarchical level of detail structure with frustum culling (FRAISS 2017). When loading the point cloud at game start, the algorithm first analyses the point cloud hierarchy and then \ncontinuously checks, which points need to be loaded. Thus, only the points that are actually \nvisible as pixels on the screen are rendered. Sounds were recorded simultaneously above and below the water surface at different positions in the river using a Zoom F4 field recorder with a \nSennheiser Ambeo VR microphone and two DolphinEar Pro  hydrophones (https:\/\/dolphinearglobal.com). The VR mic was mounted on top of a monopod. The hydrophones were attached to \neach end of a 20 cm long rod and hung over the top of the monopod about 60 cm below the VR \nmic (Fig. 2). \n\nThe recordings were processed in the digital audio workstation REAPER (www.reaper.fm) \nand  integrated  into  the  Unity  scene  using  the  plug-in  Steam  Audio  (https:\/\/valvesoftware. \ngithub.io\/steam-audio\/doc\/unity). In addition, user interaction with the virtual environment \nwas enabled through features such as walking on the ground, ducking to see underwater, or \nteleporting to another position, including the use of a minimap (see Fig. 1 (1)). \n\nUser Study \n\nThe aim of the user study  was to get feedback on the quality of the prototype in terms of \nengaging people with the environment and increasing their awareness of biodiversity through \nexperiencing environmental sounds above and below water. The main question was whether \nthe VR simulation experience had an effect on how  much participants  were aware of and \nwould support action against biodiversity loss. We also wanted to identify ways in which the \nprototype could be improved. \n\nProcedure: After being informed about the overall procedure and giving their written consent, participants stated their current feelings of pleasure, arousal and control of the situation \nusing the 9-point Self-Assessment Manikin scale (SAM, BRADLEY & LANG 1994) and answered questions on their personal characteristics. They were then given controllers and instructed on how to navigate the environment. Then, they put on the VR goggles. Participants \nsaw the virtual floodplain environment and heard environmental sounds. They were asked to \ngo to two locations marked on the minimap and perceive the environment above and below \nthe water surface. They were asked to look for differences and think about which location \nmight provide richer structures for insects, fish and plants on land and in the water. After 5 \nminutes of exploration, they proceeded with filling out a second questionnaire. They were \nagain asked to rate their current emotions using the SAM as well as questions related to their \nperceptions during the VR experience. The entire process took about 20 minutes. \n\nQuestionnaire: Before perceiving the VR simulation, participants were asked to answer (i) \nPersonal questions on a 5 item Likert-type scale (1: not at all, 2: a little, 3: moderately, 4: quite \na bit, 5: very much) on how frequently they experience real rivers or other water landscapes, \nindicating the participants’ familiarity with the presented environmental system (WILLIAMS \net al. 2007). Further, using the same Likert-type scale, they stated how concerned they are \nabout biodiversity loss in rivers, providing feedback on their level of awareness (DARWALL \net al. 2018), and answered how much they would support actions against biodiversity loss \n(DERINGER & HANLEY 2021, GEHLBACH et al. 2022). To reveal if participants, e. g., due to \ntheir job or leisure activities have high familiarity with the perceived environment type, they \nwere also asked whether they have a special relation to rivers or other water landscapes and \nif yes, why. Finally, participants were asked to provide (ii) Sociodemographic information \n(gender, age, education, occupation) and indicate with yes or no whether they work with VR \ntechnology professionally, often play computer games or frequently work with virtual landscapes, or if their daily occupation handles biodiversity. \n\nAfter  perceiving  the  VR  simulation,  participants  were  first  asked  again  to  state  how  concerned they are about the biodiversity loss in rivers and how much they would support actions \nagainst biodiversity loss in order to see whether a change occurred. Then, they were asked to \nindicate on the 5 item Likert-type scale (see above) their ratings for: (iii) Landscape perception  (“How  amazed  did  you  feel  exploring  the  perceived  landscape?”  (KING et  al. 2017), \n“How much did you like this perceived landscape” (GEHLBACH et al. 2022), “How realistic \nwas your experience of the perceived landscape?”, “How vivid did you find the perceived \nlandscape?”, “How realistic did you find the perceived environmental sound?”, “How congruent  did  you  find  the  visual  and  aural  landscape  experience?”  (LINDQUIST  et  al. 2020), \n“How much did you feel emotionally connected with the perceived landscape?” (DERINGER \n& HANLEY 2021), “How much did you like location 1 [resp. 2]” (GEHLBACH et al. 2022)). \n\nThis was followed by questions on (iv) Experienced biodiversity (“How rich in species \/ wild \n\/ natural in character \/ varied do you rate the experienced landscape?” (LINDQUIST et al. 2020, \nGYLLIN & GRAHN 2005), “How much did you experience differences between the two locations concerning perceived species richness, wilderness, naturalness and variedness?”). The \nrating was performed again with the same 5 item Likert-type scale. For analysis, an index of \nthe perceived biodiversity was calculated by averaging the participants’ ratings for the first \nfour questions (cv. LINDQUIST et al. 2020). \n\nThen, participants answered questions related to their  (v) Immersion (“I had troubles with \nVR (cyber sickness) before.” (yes\/no), “Perceiving the VR environment,… I felt nauseated \/ \ndizzy \/ had a dry mouth \/ had problems coordinating \/ was fully involved with the environment \/ felt distracted from daily problems \/ felt sad \/ happy \/ stimulated \/ was bored \/ felt \nstressed \/ was surprised \/ felt in control of the situation \/ was captivated by the simulation” \n(1: strongly disagree, 2: disagree, 3: neither agree nor disagree, 4: agree, 5: strongly agree). \nBased on these questions, two indices were calculated. For the “immersion” index the participants’ average ratings of the questions asking if they felt involved \/ distracted from daily \nproblems \/ happy \/ stimulated \/ in control of the situation; were surprised \/ captivated by the \nsimulation (the higher the value, the higher the immersion). The “no-immersion” index takes \nthe average ratings for the questions if they felt nauseated \/ dizzy \/ sad \/ stressed; had a dry \nmouth \/ problems coordinating; were bored (the higher the value, the less the immersion). \n\nConcerning (vi) Navigation, participants who used the teleportation, the minimap, and \/ or \nthe ducking feature, were asked to rate using a Likert-type scale (1: not at all, 2: a little, 3: \nmoderately, 4: quite a bit, 5: very much) how comfortable they perceived the respective feature for navigating through the environment, and how much it broke their immersion in the \nVR environment. \n\n(vii) Perception of landscape elements and of environmental sound was retrieved using bipolar adjective ratings on a 5-point scale from “very” to “neutral” to “very” of the opposite \nadjective,  as  they  can  help  adjusting  specific  characteristics  of  the  VR  environment \n(MANYOKY  et  al.  2016):  Scenery  lighting  (dark\/bright);  atmosphere  (un-\/pleasant);  entire \nscenery’s  as  well  as  tree\/water  animation  (static\/dynamic);  entire  scenery’s  as  well  as \ntree\/water\/ground  colouring  (in-\/coherent);  tree\/water\/ground  realism  (un-\/realistic);  environmental  sound  (in-\/accurate;  from  one  direction\/spatially  surrounding;  change  in  sound \nwhen dipping head below water surface (in-\/accurate; dull\/clear). In an open question format, \nparticipants could also respond to what disturbed them, and what they expected and missed \nto hear in this floodplain. Finally, they could comment on the study. \n\nStatistical analysis: The data was statistically analysed using the software IBM SPSS Statistics, version 28.0.1.1 (14). First, two groups were generated based on the statement whether \nthe users were professionally involved in biodiversity. Descriptive analyses, and due to nonnormal data distribution, non-parametric tests were then used to assess the change in concern \nabout biodiversity loss as well as the change of the level of support of actions against biodiversity  loss.  Further,  the  users’  emotions,  experienced  biodiversity,  landscape  perception, \nand immersion were analysed. \n\nResults of the User Study \n\nIn total 33 people (11 women, 22 men, aged between 18 – 64 years, 53% 25 – 34 years; 26 \n% 35 – 44 years) participated in the study. All but two participants had an academic background (12% BSc, 53 % MSc, 3% MAS, 26% PhD). Most of the participants stated that they \nexperience real rivers or other  water landscapes  with a  moderate frequency (Mean = M = \n3.39, SD = 1.09, n = 33). 21 % of the participants indicated that they work with VR technol\nogy  professionally,  39 %  had  experience  with  virtual  landscapes.  Half  of  the  participants \nwere professionally involved in biodiversity and will be referred to as “experts”, while the \nremaining participants will be referred to as “laypersons” (i. e., other academics or students). \n\nConcernment about biodiversity loss: Overall, participants’ concernment about biodiversity \nloss in rivers and floodplains before the VR experience was quite high (Median = Md = 4, \nSD = 1.21), as was their rating of how much they would support actions to address biodiversity loss (Md = 4, SD = .87). A Mann-Whitney U test indicated that experts almost do not \nchange their level of concern (M = -0.06, SD = .243, n = 17), while 44% of the laypersons \nindicated that their concern has increased (M = .38, SD = .155, n = 16), U (n1=17, n2=16) = \n191.5, z = 2.568, p = .045. The effect size after COHEN (1992) is r = .45 and corresponds to \na large effect according to GIGNAC & SZODORAI (2016). An asymptotic Wilcoxon test revealed that laypersons’ concern ratings were significantly higher after the VR experience, z \n= -2.13, p < .033, n = 16 with r = .53 (large effect). A Mann-Whitney U test showed that the \nmean differences in the rated level of support for actions against biodiversity loss of experts \nand laypersons are not significantly different, U (n1=17, n2=16) = 168, z =.085, p = .26. \n\nEmotions: The participants’ happiness was generally quite high before the VR experience (M \n= 6.91, SD = 1.04, n = 33). After the VR experience it was significantly higher (M = 7.48, \nSD = 1.46, asymptotic Wilcoxon test: z = -2.64, p = .008, with r = .46 (large effect). The \nparticipants were quite amazed (M = 4.12, SD = .82) and they quite liked the perceived landscape (M = 4.06, SD = .90). They felt moderately connected with the perceived landscape (M \n= 3.64, SD = 1.03). The liking of the two locations, which should be compared, differs slightly \n(location 1: M = 3.55, SD = .94; location 2: M = 4.12, SD = .70). Experts and laypersons do \nnot differ significantly in the scores of the emotional landscape perception ratings. \n\nBiodiversity perception: Overall, the perceived biodiversity was rated to be little to moderate \n(M = 2.48, Md = 2.5, SD = .69). The participants perceived only little differences between \nthe  two  locations  (M  =  1.48,  Md  =  2.0, SD  =  .83). 41 %  of  the  participants  perceived  no \ndifferences at all. \n\nLandscape perception: Overall, the participants rated the visual realism as moderate (M = \n3.42, Md = 3, SD = .97), as was their rating regarding the perceived vividness of the landscape \n(M = 3.70, Md = 4, SD = .88). In contrast, they rated the environmental sound as quite a bit \nto very realistic (M = 4.36, Md = 5, SD = .74), and the perceived audio-visual congruency as \nquite high (M = 4.09, Md = 4, SD = .72). According to the mean bi-polar adjective ratings, \nthe participants perceived the scene and the landscape elements overall  not to be extreme \nconcerning lightning, animation, and colouring. However, the scene was found a little and \nthe trees very static. The latter were also found a little unrealistic. The bi-polar adjectives of \nthe sound characteristics show mostly neutral ratings. \n\nImmersion: Nearly all participants stated that they did not have troubles with VR (cyber sickness) before. The immersion index shows that the participants were moderately to quite a bit \nimmersed (M = 3.62, Md = 3.57, SD = .46). According to the no-immersion index, the participants had only low general signs of distraction from immersion (M = 1.66, Md = 1.57, SD \n= .58). The participants rated the three different features similarly as moderately to quite a \nbit comfortable for navigation: teleportation (M = 3.91, SD = 1.01), minimap (M = 3.73, SD \n=.88), and ducking (M = 3.76, SD =.90). According to the mean responses, teleportation (M \n= 2.24, SD = .79) broke the immersion a bit more than the other two features. However, the \nbreaking of immersion for all features was rated to be only a little, whereby the ducking broke \nit the least of all three (ducking: M = 1.88, SD = 1.05; minimap: M = 2.00, SD = 1.12). \n\nAspects for enhancement: The nature of the point clouds, which become less dense the closer \none gets, disturbed several participants and according to their comments reduced their sense \nof immersion. In single feedbacks, the lack of detail in the trees and the underwater area was \ncriticized. Further, artefacts such as the overlapping river and river bank as well as offsets \nbetween the mesh of the DSM and the point cloud layer were reported as distracting. Regarding environmental sound, the majority of the participants expected to hear water and birds. \nSome mentioned also wind or rustling leaves and fish as well as the floating debris. Further, \nthey expected maybe frogs, nearby urban sounds and the own footsteps. Other aspects address sound characteristics that should be improved, e. g., that the sound of water could be \nmore intense, and that the underwater sound was one-dimensional and did not change over \ntime. Moreover, there was not much change in volume and sounds between above and below \nwater or between the two locations being compared for differences. \n\nDiscussion and Conclusion \n\nThe results show that the prototype raised laypersons’ awareness of biodiversity loss in floodplains, supporting the hypothesis that high fidelity audio-visual simulations that evoke positive emotions may enhance recognition of biodiversity loss (KING et al. 2017, LINDQUIST et \nal. 2020, GEHLBACH et al. 2022). This finding needs to be interpreted with caution,  as the \nsample size of 33 is rather small and the \"laypersons\" in our study do not represent the general \npublic. Further, the simulation quality could be improved, especially with regard to the point \ncloud rendering. It works very well for medium to long range details, but at close range the \nlimited  point  resolution  was  perceived  as  disturbing.  Point  rendering  techniques  such  as \nEWA splatting (ZWICKER et al. 2002) might be useful to overcome this problem. \n\nThe representation of the environmental sound above and below water in the prototype is a \nfirst attempt to integrate this aspect into the VR simulation. At least the audio-visual simulation was perceived as congruent. Further development is necessary to provide a scientifically \nbased  relationship  between  the  sounds  and  the  actual  habitat  quality.  This  may  foster  the \nmuch-needed better understanding of river and floodplain biodiversity among both the general public and experts (MONACCHI & KRAUSE 2017, DARWALL et al. 2018). \n\nWe see the application of further advanced immersive audio-visual VR simulations in particular in the extensive river restoration measures. In Switzerland, e. g., about a quarter of the \n16000 km of degraded rivers and floodplains are planned to be restored by 2090, which requires intensive and diverse participatory processes. Audio-visual VR simulations will be an \nexcellent participatory tool to visualize and assess the changes due to restoration measures. \n\nTransferring Spatial Data from Unity to ArcGIS \n\nAbstract: Research in landscape architecture and adjacent fields uses game engines to create serious \ngames and experimental environments, but precise quantification and analysis of those environments \ncan  be  problematic  when  the  visualization  and  analysis  software  are  not  directly  compatible.  In  response, ESRI and Unity have developed a pipeline to port ArcGIS data into Unity, but the process of \nmoving data from a Unity-based program back into ArcGIS has received considerably less attention. \nThis descriptive paper, therefore, addresses the Unity-to-GIS pipeline by developing an open-source \nworkflow for extracting spatial data from a virtual environment in Unity and transferring the same into \nArcGIS Pro for spatial analysis. \n\nIntroduction \n\nIn this paper we report the results of a new method for integrating game development engine \nand their products with relevant spatial analytics. This is a descriptive paper of a complex \nprocess. Gaming engines have been used for a wide variety of applications and research platforms across landscape studies (CARBONELL CARRERA & BERMEJO ASENSIO 2017, LAKSONO \n& ADITYA 2019). One of the significant advantages offered by these tools is a graphic user \ninterface for crafting cinematic or interactive virtual worlds and experiences, which does not \nrequire extensive coding experience. Two of the most popular and publicly accessible gaming \nengines are Unity and Unreal Engine. Both offer a highly customizable platform with integration across a range of other software. The industry growth, publicly available support and \ntutorials and diversity of uses for gaming engines, including Unreal and Unity, hint at a future \nof continued use and likely growth in landscape architecture. \n\nDisciplinary uses of the technology are currently focused on high quality representation, 3D \ndesign, and creating controlled experiments. However, these powerful tools could also benefit from integrated analytical frameworks for site and spatial analysis. Fortunately, gaming \nengines already have an explicit hierarchical and spatial organization. Further, the real-time \nuser experience can be precisely documented so it would seem reasonable that an analytical \nframework could be developed. However, despite the game engine’s robust capabilities to \ngenerate experiences, few open-access tools exist to analyze the immersive experiences or \nenvironments using real world metrics. Those tools that do provide such options are rather \nlimited in their analytic capabilities (ESRI 2022b) and are not integrated bi-directionally between  software.  In  landscape  research,  this  limitation  leaves  a  gap  between  the  ability  to \nvisualize or interact with proposed landscapes and the ability to substantively understand the \nspatial metrics of those same environments.  \n\nThis research project is one part to a much larger project, the latter that uses Unity to create \nimmersive virtual environments and collect data about users’ interactions and perceptions of \nthose environments. We chose Unity because of custom work produced in prior iterations of \nthe project making migration and comparison of our techniques across other gaming engines \ncost prohibitive. This sub-project examines how we can integrate ArcGIS Pro and Unity as \nan analytical tool to understand those interactions. These programs, however, have only recently begun evolving integration capabilities. Currently, there are no fluid techniques to run \nthe gamut of advanced spatial analytics available in GIS software using data generated from \nimmersive virtual environments. Therefore, this project aims to address the lack of integration from Unity back to ArcGIS Pro. Our goal of developing integration from the gaming \nengine to GIS leads to the primary question of this sub-project: how can 3D mesh and point \ndata in Unity be transferred to ArcGIS Pro for analysis while maintaining spatial fidelity? \n\nIn the physical world, spatial analyses provide valuable information about how environments \ndrive various human behaviors (KWAN 2000). For example, for understanding the layout of \nan urban fabric or the spatial functioning of an ecosystem. Spatial analyses can provide information on the density of structures or populations, distributions of structures’ sizes and \nshapes, trends of spatial relationships between environmental elements, or numerical counts. \nUnity does not intrinsically provide these kinds of information, but the data still exists or can \nbe generated and then embedded in the virtual environments themselves. However, this analytical capacity already exists in Geographic Information Systems (GIS). We recognize there \nare various relevant tools for some of these analytics available within gaming engines, or that \ncould be created within these engines, but these are often one-offs and require extensive technical capabilities. Therefore, significant advantages exist for integrating both GIS and gaming engines (e. g. Unity) together into an analytical framework. \n\nThere  are  several  disciplinary  applications  where  this  integration  could  be  beneficial.  For \ninstance, this integration provides significant advantages over traditional visual preference \nand scenic quality studies. Several previous studies involving self-reported, subjective ratings \nof visual scenes or landscapes  were run as quasi-experiments  with qualitative analyses  of \nsubjects’ focal points and gaze paths (AL MUSHAYT et al. 2021, GUO et al. 2021, LI et al. \n2022),  and  SIMPSON  (2018)  found  results  suggesting  differences  in  gaze  behaviors  across \ndemographic, environmental, and task context variables but lacked additional analysis or environmental data to quantify the spatial variables in the study. Further, SPIELHOFER (2021) \nfound differences in gaze behavior and stated landscape preferences across similar development scenarios in different ecological settings. Each of these studies relied on real or virtual \nenvironments to elicit gaze behaviors or preference ratings, but none of the studies quantified \nthe independent environmental variables potentially shaping the dependent behavioral outcomes. By creating a pipeline from Unity to ArcGIS, we can quantitatively relate environmental spatial variables to viewers’ behavioral responses. For example, we may relate gaze \nbehaviour patterns to scenes’ ecological contexts using hotspot analysis or compare the spatial  distribution  of  elements  in  a  viewshed  along  a  route  to  the  distribution  of  a  viewer’s \nfixations. Additionally, scene-based perspective saliency maps (DUPONT et al. 2016) can be \nproduced and related to a viewshed’s visual magnitude (CHAMBERLAIN & MEITNER 2013) to \nvalidate these studies. \n\nAdditional studies have explored preference and realism ratings alongside physiological and \nbehavioral variables using game engines or immersive environments. SIMPSON (2018) used \na  qualitative  analysis  of  gaze  behaviors  in  real-world  environments  to  suggest  variations \nacross individuals, street settings, and task contexts. This suggestion supports further exploration of controlled virtual environments as potentially faster, cheaper, and more convenient \nto using real world environments to study gaze behavior. In that same vein, SCHROTH et al. \n(2015) found that Unity provided a highly accessible development platform for a “serious \ngame” in environmental education, and in SORIA & ROTH (2018), participants demonstrated \na greater spatial understanding of a proposed design after viewing an augmented reality (AR) \nsimulation allowing them to move freely through the space when compared to an AR simulation only allowing rotational exploration of a view from a single location. \n\nIn addition to visual-psychological studies, gaming engines offer several other opportunities \nto study the relationship between humans and our environment. For instance, BISHOP et al. \n(2011) used CryENGINE2 Sandbox2 editor to create a bushfire simulator to demonstrate fire \nhazards to Australian homeowners. The CryENGINE2 Sandbox2 editor supported a highly \ninteractive virtual environment with a broad range of control over environmental variables. \nIn the game, users were able to prepare their homes for a bushfire in ways that directly connected behaviors to fire outcomes. MANYOKY et al. (2016) also used CryENGINE to develop \nwind park simulations for comparison with real-world, on-site recordings of built wind parks. \nThe comparison indicated high validity for the park simulations, and both conditions were \npresented to subjects using a cave-projection-like system. LINDQUIST et al. (2016) found that \naudio recordings significantly impacted subjects' realism and preference ratings for digital \nlandscape representations. LINDQUIST et al. (2016) presented subjects with aural-visual representations generated from Google Earth imagery and audio recordings of the site. Participants rated the realism and their preferences for combinations of visual and aural components. In all these instances, having a direct pipeline to geospatial data to build the environment and then assess generated data or simulations could have offered time savings and potentially new insights. \n\nTransferring the environmental spatial data  (e. g., structural footprints, heightmaps, or behavioural point data) to a GIS is necessary since Unity does not provide the necessary analytic \ncapabilities. This transfer raises two primary issues: compatibility between software and accuracy  of  data  during  transfer. The  first  issue  arises  from  incompatible  file  formats  when \nattempting to  work directly from Unity  to ArcGIS. Our Unity environments are primarily \ncomposed of 3D assets with mesh and texture data. These data are not readily imported directly to ArcGIS from Unity using previously integrated tools. Although recent collaborations between ESRI and Unity have facilitated the transfer of data from ArcGIS to Unity for \nvisualization, relatively limited options are available for moving data in the opposite direction. The second challenge – maintaining spatial accuracy – comes from manipulating the \nvirtual  environment’s  spatial  data  through  various  formats  to  reach  one  compatible  with \nArcGIS. The data transformations must maintain the original environment’s spatial qualities, \nand each transformation introduces threats to that validity. Consequently, we explored two \nsub-questions  to  answer  our  primary  research  question:  1)  What  processes  and  tools  are \nneeded to generate an integrated framework between Unity and GIS, and 2) How can spatial \ndata about landscapes, objects and user interactions be bridged across the identified software? \n\nMethods \n\nStep-by-Step Process \n\nTo transfer our virtual environment (Figure 1) from Unity to ArcGIS for spatial analysis, we \nstarted from Unity 2021.3.15f1 (LTS) with access to the FBX Exporter package, moved into \nBlender  3.2  with  the  BlenderGIS  addon,  cleaned  raster  outputs  in  Adobe  Photoshop,  and \nfinally reconsolidated the various data transfers into ESRI’s ArcGIS v. 2.9. This section describes the full process in detail. \n\nTo begin in Unity, we organized the environment for export. “Prefabs”, or pre-organized sets \nof  individual  3D  assets,  are  a  convenient  unit  for  export  at  this  stage.  We  subdivided  the \nvirtual environment for export into predefined prefabs with an appropriate internal hierarchy \nthat  maintains  the  desired  spatial  units  for  later  analysis.  To  export,  we  installed  Unity’s \n“FBX Exporter” package from the Package Manager, then used that extension to export each \nprefab to a .fbx in a “Binary” format. \n\nWe then used Blender to convert the .fbx files to shapefiles and to extract a single-channel \nraster heightmap of the environment. We began by importing the .fbx files in Blender. Since \nenvironment was subdivided into multiple prefabs, each imported prefab loaded at the project’s origin, and manual rearrangement was necessary to reconstruct the environment. Using \nthe “Snapping” feature to move the individually imported files into place helped maintain \ntheir spatial validity. Additionally, locking certain transformation axes (such as the z-axis) \nfor all objects when moving the imported prefabs simplified the reassembly. Setting up other \nenvironmental  elements  (e. g.  increasing  the  viewport  clipping  distance,  adding  a  ground \nplane) streamlined the process.  \n\nOnce the environment model was reconstructed in Blender, we began extracting spatial data \n(building footprints and heightmaps) for transfer to ArcGIS. The following process extracts \ntwo types of data: a raster heightmap and an environmental shapefile. To create the height-map, there are two basic phases: setting up the virtual camera as well as compositing and \nrendering. The virtual camera must have an orthographic, top-down view of the entire environment and have an orthographic scale equal to the smaller value of the final render’s pixel \nresolution.  We  set  the  output  file  format  to  “Targa  Raw”  and  color  to  “BW”;  finally,  we \nincluded  “Z”  data  under  passes.  When  compositing  and  rendering,  we  used  nodes  in \nBlender’s compositing tab and added a “Normalize” and an “Invert” node to the resulting \nconsole. We connected depth data from the Render Layers node to the Normalize node, and \nsent that output to the Invert node. We then connected the Invert node’s output to the Composite Node to finalize the process. Using Cycles and an appropriate GPU, we rendered the \noutput heightmap. The heightmap may have background noise on the otherwise flat ground \nplane. If necessary, this noise can be removed with digital imaging processing software, such \nas Gimp or Adobe Photoshop. \n\nBlender can also produce shapefiles of the virtual environment using the BlenderGIS addon. \nThis addon makes exporting shapefiles incredibly easy. To prepare the export, we first embedded additional data to attached to the mesh data; the BlenderGIS addon creates attribute \nfields from each object’s custom properties in Blender. For each value that would become a \nfield in the shapefile’s attribute table, we created a custom property named for each desired \nattribute. For a full analysis, we created each custom property for a single object then copied \nthe properties to all other objects using the “Copy Custom Properties” addon from the “object_copy_custom_properties_1_08” package. Blender’s Python Script tab allows for procedurally assigning values to these custom properties, such as object names or object parent \nnames. These data can then be exported to a shapefile through the BlenderGIS addon. Simple \nenvironments may be exported as polygons, but for more complex environments or models, \nwe recommend exporting as points due to a 2GB file size limitation on the export. With the \nshapefiles  and  heightmaps  completed,  all  files  were  ready  for  ArcGIS.  In  our  project,  we \ncreated custom properties for “Object Name”, “Parent Name”, and “Block Name” to allow \nfor grouping objects at the building and block-level for analysis in GIS. \n\nNext, we created a new project in ArcGIS, established the appropriate folder connections, \nand added the heightmap and shapefiles to the map. Since the environment was not previously geolocated, our data loaded at <0,0>, and the shapefile and heightmap required some \ngeoreferencing  for proper alignment. We defined a projection for each file, and since our \nshapefile was of points, we applied a minimum bounding geometry using the object name \nand object parent name as join fields. Dissolving the output by the desired field achieved a \nnear finished set of building footprints (Figure 2). These footprints may be further refined \nusing “Raster to Polygons” on the Heightmap and erasing the vector footprints by a selection \nof near-0 height polygons. The footprints may then be spatially joined to the maximum intersecting heightmap polygon. At this point, the Unity-to-GIS pipeline is complete. Additional data may be extracted from the experimental environment and joined to environmental \ncomponents using the object name field as a join field. \n\nProcess Overview \n\nTo answer our research questions, we explored a range of processes and software in order to \ndevelop a method to transfer both the structural elements (simple and complex objects) as \nwell as data generated during a user experience from Unity to ArcGIS Pro. Due to popular \ndemand, the GIS-to-Unity pipeline has been well documented and recently formalized in the \nArcGIS plugin for Unity (ESRI 2022a). This tool allows for accurate and interactive visualization of geospatial data, but relatively little documentation exists for the reverse workflow \n(Unity-to-GIS) and analysis of virtual environments.  \n\nTo transfer our virtual environment  from Unity to  ArcGIS for spatial analysis,  we started \nfrom  Unity  with  access  to  the  FBX  Exporter  package,  moved  into  Blender  3.2  with  the \nBlenderGIS addon, exported the necessary spatial data as raster data and shapefiles, cleaned \nraster outputs in Adobe Photoshop, and finally reconsolidated the various data transfers into \nESRI’s ArcGIS v. 2.9. More specifically, the Unity-based virtual environment and its components are exported to .fbx formats for working in Blender. Next, the .fbx files are imported \nto Blender – and rearranged to match their experimental composition, if necessary – from \nwhich a single-band raster heightmap can be rendered. Some additional denoising of the raster output may be necessary before using the file in ArcGIS. To extract vector data from the \nenvironment, the BlenderGIS addon (Domlysz 2014\/2022) provides tools for exporting the \nenvironment in various shapefile types. In our case, we exported the experimental environment as a set of building footprints. These vector and raster data can then be aggregated for \nanalysis in ArcGIS, as in Figure 3. This process can also be used during the design phase for \nnew experimental environments by providing spatial metrics on the environmental components, which can then be sorted into categories for creating districts with controlled and distinct spatial properties. Figure 3 outlines the  workflow  we  developed, and its components \n(software, tasks, and data) to address this gap. Each box represents a specific piece of software with its respective initial data or processing tasks. Each oval indicates the format needed \nto transfer data between programs. \n\nDiscussion and Conclusion \n\nUltimately, analyzing virtual environments offers the opportunity to empirically assess user \ninteractions with virtual space. Preceding research has recognized this advantage and called \nfor greater integration of critical spatial analysis with advanced visualization (MACEACHREN \n&  KRAAK  2001),  especially  as  research  agendas  turn  towards  examining  dynamic  spatial \nphenomena like visual experiences (LIN et al. 2015). By combining Unity with virtual reality \nand eye-tracking software and hardware, we can quantify subjects’ visual exploration of an \nenvironment, and through ArcGIS, we are then able to quantify the virtual environment. By \nusing both types of software, we can explore quantitative relationships between an environment’s spatial statistics and viewers’ behavioral and physiological responses. Furthermore, \nwe can tightly and easily control the environmental variables to explore relationships between \nbehavioral responses and specific environmental parameters. \n\nOur  work  demonstrates  one  method  for  achieving  this  integration  between  environmental \nvisualization and spatial analysis while maintaining spatial accuracy across software. While \nArcGIS’s  “3D  Analyst”  license can circumvent some of this  workflow, our  work demonstrates a workaround using Blender as a free, opensource replacement for extracting data \nfrom immersive virtual environments. This project opens possibilities for further and more \naccessible  research  examining  relationships  between  quantified  environmental  parameters \nand human behavior, which could provide much needed empirical data for evidence-based \ndesign strategies.  \n

Application of Geographic Information System (GIS) \nin Digitizing Hand-Drawn Mental Maps: \nAn Exploratory Study \n\nAbstract: Geographic Information System (GIS) is a common and powerful tool for analyzing quantitative  data  with  locational  information  and  visualizing  digital  maps,  but  its  potential  for  qualitative \nstudies has not yet been fully explored. Based on a case study  from Seoul, South Korea, this paper \ninvestigated how hand-drawn mental maps can be converted into GIS-created maps to gain a deeper \nunderstanding of people's perceptions of the ‘imageability’ of their neighborhood. The findings of this \nprocess also provide insights into the implications and limitations of Seoullo 7017, a regenerated public \nspace in the area. \n\nIntroduction \n\nAs an exploratory study, this paper utilized GIS to read the ‘fuzziness’ of hand-drawn mental \nmaps. Rather than selecting an existing research method and testing a specific hypothesis, it \npresents the steps taken and the findings obtained: understanding hand-drawn mental maps, \nextracting the map elements’ keywords, digitizing the maps in GIS, and analyzing results of \nmap collection in relation to the urban regeneration project of Seoullo 7017. \n\nThis  study  focuses  on  the  Sogong-Hoehyeon  neighborhood  in  Seoul,  South  Korea.  This \nneighborhood is composed of four smaller administrative units called dongs (Sogong-dong, \nHoehyeon-dong, Jungnim-dong, Myeong-dong) with an area of 326 hectares and approximately 21,000 residents. Located in the middle of Seoul, it is the city’s oldest Central Business  District  and  is  known  for  its  high  proportion  of  commercial  districts  (55.9%  of  total \nzoning). The neighborhood is an important traffic center and contains significant cultural and \nhistoric buildings and areas. The concept and boundaries of the ‘neighborhood’ are borrowed \nfrom Seoul Metropolitan Government's 2030 Community Plan, which divided the city into \n115 neighborhoods to create more practical plans based on citizens' daily living spaces. \n\nThe Sogong-Hoehyeon neighborhood was selected as the study area since it is full of ‘imageable’ elements, including the Seoullo 7017 Skygarden, a large-scale public space that was \nrecently regenerated and may have impacted the whole neighborhood imageability. Seoullo \n7017  is  an  elevated  linear  park  built  on  a  former  highway  overpass  constructed  in  1970. \nOpened in 2017, the regenerated park is one kilometer long with 17 walkways. One of the \nmain goals of the Seoullo 7017 regeneration project was to connect the eastern and western \nsides of the neighborhood divided by Seoul Station. The eastern side of Seoullo 7017 (Hoehyeon-dong) is home to many commercial and business high-rise buildings with active pedestrian flows while the western side (Jungnim-dong) is a relatively inactive area. The park \nhas seen an increase in visitors in the five years since it opened, but its effect on the perceived \n‘connection’ of the neighborhood has not been thoroughly studied. \n\nThis study aimed to understand how neighborhood imageability is perceived and whether the \nregeneration project has achieved its intended effect. Imageability, a concept introduced by \nLYNCH (1960) refers to the quality of a physical object that makes it likely to evoke a strong \nimage with an observer. NASAR (1998) argued that imageability is influenced not only by the \nlevel of imageability of a region’s features but also by their qualitative likeability. Based on \nthese thoughts, this research assumed that higher imageability, characterized by more imageable elements that are liked by people, leads to better outcomes. \n\nUnderstanding Hand-Drawn Mental Maps \n\nThis research used mental maps drawn by people to study their perceptions of the neighborhood more directly. Ten participants from diverse groups of gender, age, and profession were \nrecruited, and the mapping processes involved four steps: \n\n1)  Present mental mapping (black): Following the guideline of LYNCH (1960), participants \nwere asked to draw a current map of the entire neighborhood with physical and visual \nfigures they could think of immediately and write down the elements’ names as if they \ntake around the neighborhood or give an explanation to a stranger. Only the names of \nfour dongs were provided as a hint for the neighborhood boundary.  \n\n2)  Present evaluative mapping (red): Borrowing the method of NASAR (1998), participants \nwere asked to rate the likeability of elements that provoke certain feelings on a five-point \nLikert scale from 1 (very dislikeable) to 5 (very likable). \n\n3)  Past mental mapping (blue): Participants were asked to imagine they went back in time \nbefore 2015, not informed that it is when the Seoullo 7017 project started, and to mark \nor add any elements on their map they might have differently recognized in the past. \n\n4)  Follow-up  interview:  Participants  were  interviewed  about  their  experiences,  feelings, \nand opinions of the neighborhood and the Seoullo 7017 project and asked to provide \nreasons for their drawings. \n\nThe collection demonstrated how different people perceive and map out the neighborhood in \nvarious ways: some participants recalled the neighborhood with visually detailed figures (A) \nwhile others in words and names of elements in the notional grid (although the actual area is \nnot in a strict grid form) (B). Some focused on a central landmark and drew its surroundings \n– Namdaemun (Sungnyemun) Gate (C), Namsan Tower (H), and City Hall Plaza (I) – while \nothers used subway stations as their reference points in the form of connected circles (D) or \ndouble circles (G), depending on their personal experiences in the neighborhood. \n\nA few landmarks were depicted in the map using icons rather than basic shapes, including \nNamdaemun Gate (Sungnyemun Gate), the old gate of the Joseon dynasty’s fortress wall and \nthe  first  national treasure of  South Korea (A, D), or Namsan Tower (N Seoul Tower),  an \nobservation tower located at the highest point in the city (F, H). Hatched areas often indicated \na cluster of similar functions, such as food or shopping areas (D, E, J). \n\nKeyword Categorization \n\nCommonly imageable elements in the neighborhood were identified by collecting all the texts \nof ten maps. Informal, inaccurate, or vague names were taken intactly to deliver the original \nperceptions as possible. Among 470 texts, ‘Seoul Station’ was the most frequently occurring \ntext, included in all ten maps. Most maps even included the elements that broke the station \ndown with more details, such as different train lines, exits, and attached buildings. \n\nThe regional context of the Sogong-Heohyeon neighborhood could be inferred by calculating \nthe  frequency of elements on the  maps (Figure 3). The presence of the  Seoul  Station and \nother subway ‘stations’, as well as the mention of ‘Seoul’ in the names of institutions such \nas the Seoul City Council and Seoul Museum of Art, and private buildings like Seoul Plaza \nand Seoul Finance Center, indicate that the neighborhood is a transportation hub located in \nthe heart of the city. The presence of ‘Korea’ in governmental buildings, such as the Bank of \nKorea, Korea Press Center, the Korea Chamber of Commerce and Industry, and the National \nTheater Company of Korea further confirmed the neighborhood's characteristics as Central \nBusiness District (CBD). \n\nLynch's study identified five types of imageable elements: paths, nodes, edges, districts, and \nlandmarks. However, after reviewing the elements and their frequency, this study devised \nseven categories: transit, direction, road, element, building, space, and area. The number of \ncategories was limited to seven, in accordance with the principle by MILLER. (1956). \n\nThis study found that most participants primarily relied on public transit facilities, such as \nsubway or bus stations, as landmarks for orienting themselves, even when these underground \nstations  were  not  easily  visible  from  the  surface.  This  led  to  the  creation  of  the  category \n'Transit'. Additionally, many elements actually located outside the boundaries of the target \narea  were  often  used  as  reference  points  for  direction,  leading  to  the  category  'Direction'. \nFurthermore,  many roads  were drawn as lines  without a  name tag but  still showed  which \nroads were imageable to the participants, leading to the creation of the category 'Road'. \n\nDrawings that did not belong to the directional categories were sorted into four categories: \nelement, building, space, and area. ‘Element’ refers to non-building or movable items that \nmake  up the urban landscape, such as fountains, statues, or groups of pigeons. ‘Building’ \nrefers to structures that people can enter and conduct activities. The functions of frequently \nmentioned buildings were restaurants, department stores, newspaper companies, and cultural \ninstitutions, implying the diverse functions of this CBD. ‘Space’ refers to a distinctly defined \noutdoor open area, such as parks or palaces. ‘Area’ refers to a cluster of elements with similar \nfunctions, subjectively defined by mental mappers, such as food alleys. \n\nElements drawn on each hand-drawn map were converted in GIS into elements of different \nshapes, based on their seven categories (Table 1). \n\nThe elements were marked on the corresponding positions in a real map based on Open Street \nMap (Figure 4). Then, all  elements  were consolidated from  ten different  maps to create a \ncomprehensive map (Figure 5). The final map considered variations in thickness and color, \nas outlined by BERTIN (1983). The thickness of each element was determined based on its \nfrequency across the ten maps, while the color was determined by averaging the likeability \nvalues. The map was created using the QGIS program. \n\nDiscussion \n\nBy  overlaying  hand-drawn  elements  on  the  actual  map  and  consolidating  ten  maps,  areas \nemphasized or overlooked in people's perceptions were identified. For instance, parks and \ngreen spaces within the neighborhood, even Namsan (Nam Mountain), were not frequently \nmentioned despite their considerable area on the actual scale, except Deoksugung Palace and \nCity Hall Plaza. The perceived boundaries of the neighborhood were found to differ from the \nactual  boundaries  in  a  way  that  mappers  tended  to  zoom  in  on  certain  areas  that  they  are \nparticularly interested in while cropping the remaining areas. For example, surrounding area \nof Seoul Station  had the  most elements concentrated.  Although being  given the names  of \nadministratively defined neighborhood, people often mentioned elements that does actually \nnot situate in the actual boundaries, indicating that the perceived boundaries were blurry and \nthat people perceived the neighborhood only in relation to its surroundings. \n\nFinally,  it  was  possible  to  infer  from  the  agglomerated  map  whether  the  aimed  effects  of \nSeoullo 7017 project were achieved, while 8 out of 10 respondents recognized the existence \nof the linear park in their mental maps: \n\n1)  Extension of the neighborhood boundary: The west side of the Sogong-Hoehyeon neighborhood used to be relatively unnoticeable in the past mental maps, however, existing \nelements  in  this  area  such  as  Yeomcheon-dong  handmade  shoe  street  and  Seosomun \nHistory Park became more imageable in the present maps. This even boosted the increase \nof elements’ likeability in this area. Likewise, some landmarks in the east side of Seoullo \n7017 started to be perceived than before. In other words, the perceived boundary of the \ntarget neighborhood was extended by the regenerated public space. Several respondents \nsaid that they started to try a new walking route through many branches of the space. \n\n2)  Emphasis on existing elements near the regenerated public space: Seoullo 7017 made a \nlot of existing imageable elements near itself be rediscovered. Many responded that they \nused to not stay around the Seoullo 7017 or Seoul Station but move to other areas for \ntheir lunch or business meetings, assuming that there are few places worthy a visit. After \nthe regeneration, however, they could recall more elements including decent restaurants, \ncoffee stalls, and conference rooms. \n\nConclusion and Outlook \n\nUtilizing GIS techniques in qualitative research has been under discovered until now and can \nexpand the usefulness of the GIS tool. To this end, converting qualitative data into numerical \ndata without altering its original nuances is the challenging part. Thus, this study aimed to \ninfer implications and make suggestions with the help of GIS, rather than verifying validity. \nHowever, the limitation of this study remains that  its categories and  symbols  used in this \nstudy were specific to the case and may vary in other regions and contexts. Future research \ncan explore additional methods of map projection, such as rubber-sheeting map projection. \n\nAssessment of Ecosystem Service Urban Recreation – \nA Case Study of Leipzig, Germany \n\nAbstract: The World Health Organization (WHO) notes that outdoor recreation prevents diseases and \nimproves urban residents' health. Recent rapid population growth in industrial cities, coupled with high \ndemand for housing and amenities, negatively impacts green spaces and indirectly impedes outdoor \nrecreation. Green spaces are the key factor in improving the quality of life in urban settlements, and in \nthe  spirit  of  social  justice,  everyone  should have  the  right  to  access  them.  In  this  paper, the  city  of \nLeipzig serves as a case study to analyze urban green spaces with recreational functions and explore \nways to achieve sustainable urban development. This study relies on GIS as an analytical instrument \nand ATKIS Basic DLM (Digital Landscape Model) as a base dataset. \n\nIntroduction \n\nThe German city of Leipzig, a growing municipality with high green ambitions, ranks third \nnationally in the European Community Quality of Life Report 2020, with an overall satisfaction rating of 96.3%. In Germany, one-third of the population lives in cities with over 100,000 \ninhabitants, while three-quarters live in urban areas (GRUNEWALD et al. 2017). Urbanization \nsignificantly increases the pressure on remaining urban open spaces. Therefore, parks and \ngreen spaces are crucial for stable and balanced urban growth. “Outdoor recreation” refers to \nrecreational activities in a natural setting. Several research studies have shown  how it enhances the quality of life and contributes to the well-being of urban residents. In public residential areas, recreational opportunities are significant for residents with limited mobility and \nwithout cars (GODBEY 2009). In addition, recreational areas should also be accessible to all \nsocial groups in the interest of social equity (ARTMANN et al. 2019). Ecosystem services (ES) \ncomprise the conditions and processes by which natural ecosystems sustain and fulfil human \nlife. They can be provisioning, regulating, or, as in our case, cultural services (BISE 2022). \nCulture refers to the non-material benefits humans derive from ecosystems (FAO 2022). \n\nUrban green spaces (UGS) are essential for cultural ecosystem services such as outdoor recreation  opportunities.  Residents,  especially  the  elderly  and  children,  of  urban  areas  need \nequal access to green recreational spaces, (ARTMANN et al. 2019). Sufficient open and green \nspaces set higher standards for sustainable urban development. German cities measure public \nopen spaces with different orientation values ranging from 6 to 15 m2 per inhabitant. Sports \nand play areas represent unique UGS uses for which accurate values or ranges of deals are \navailable, while exact values for other functions are challenging to obtain. Based on the German Association of Towns and Municipalities guidelines from the 1970s, many cities still \nassume a minimum size of 0.5 hectares for open or green spaces close to settlements and ten \nhectares for landscape-oriented open spaces (CHRISTA et al. 2016).  \n\nBoth national and international studies, and the WHO, propose various indicators that can \naid in assessing housing quality, leisure quality, and health promotion. Therefore, indicators \nfor  recording  and  evaluating  ES  are  essential  for  operationalizing  and  measuring  success. \nThe evaluation parameters chosen for this study were the accessibility of green spaces from \nresidential areas, the supply of urban residents, and the ratio of supply to total area. ES “Urban Recreation” is quantified using three key parameters: the percentage of green space in \nurban areas  with recreational potential, availability, and accessibility. This study aimed to \nanswer the following questions: Are sufficient UGS available, and are they accessible to residents? Where are there deficits or need for action?  \n\nThe assessment utilized GIS tools to identify the most suitable open spaces for recreation and \nproximity analysis to measure the degree of accessibility. \n\nMethodology \n\nThe chosen approach is in line with the recommendations of WHO (2016) on the design and \nmeasurement of health indicators for public health services and in line with the proposal for \nnational-level indicators to describe and evaluate public health services (GRUNEWALD et al. \n2017).  \n\nSupply (indicator one) identifies the extent of greening within the study area and calculates \nthe ratio to the entire study area. Method of data collection: ratio of green space to the total \nstudy area (GRUNEWALD et al. 2017). \n\nProvision (indicator two) calculates the percentage of green space in an urban area and determines its compatibility with the total population of the study area. Method of data collection: the quotient of total green area and people within a defined commuting belt (m2\/inh.) \n(GRUNEWALD et al. 2017). \n\nAccessibility (indicator three) indicates the proportion of people with access to UGS compared  to  the  total  population  of  the  study  area.  Method  of  data  collection:  ratio  of  served \nresidential areas\/population within parameters (commuter area, minimum size) to total residential areas\/population. (GRUNEWALD et al. 2017). \n\nBased on the national benchmarks (CHRISTA et al. 2016), two classes of size and proximity \nof urban recreation areas (Ur) were defined to analyse the level of accessibility as follows:  \n• 500 m walking distance for less than 10 Ha and greater than 1 ha = Ur.1 \n• 1000 m walking distance for UGS greater than 10 ha = Ur.2. \n\nThe second step determined the ratio of the population\/residential areas served  within the \nparameters  (catchment  area,  minimum  size)  to  the  total  population.  (GRUNEWALD  et  al. \n2017). Finally, the degree of accessibility  was  measured on three scales: (a) Accessibility \nwithin 500 m walking distance; (b) Accessibility within 1000 m walking distance; (c) Accessibility within 500m and 1000m walking distance. \n\nRecreation-relevant UGS Land Uses \n\nUGSs with recreational functions open to the public and do not charge fees are defined as \nurban  recreation  areas  (Ur)  for  this  study.  The  land  types  representing  the  stock  of  green \nspaces  were  selected  regarding  available  recreational  opportunities  and  public  access.  In \nterms of land use class, these ATKIS Basic DLM features are suitable for everyday outdoor \nrecreational activities (Fig. 1). \n\nPark, Green area, and Forestry categories are uncontroversial, as they contribute significantly \nto the recreation of city residents. “Water bodies” can provide a variety of recreational uses \non, in, and around water, including green riparian areas, and can be accessed by the public. \nGrassland (meadows or pastures with natural elements) has a renaturation effect, among other \nthings (GRUNEWALD et al. 2017). Natura 2000 network (N2K) areas have become increasingly crucial as recreation possibilities promote physical and mental health through natural \nexperiences (ROCCHI et al. 2020). Due to entrance fees, membership, or fences, these landuse  functions  do  not  apply:  41008  – allotments,  equestrian  sports,  golf,  outdoor  theatres, \nwildlife parks, swimming pools, traffic practice areas, weekend home areas, zoos, amusement \nparks, open-air museums, and campgrounds (GRUNEWALD et al. 2017). \n\nAssessment Models Flowcharts \n\nTwo  models arise  from the  first selection:  As baseline information  for indicators one and \ntwo, the first model unifies the land use selections and subtracts them from the study area. \n\nAs the basis for assessment indicator three, a second model combines the land use selections \ninto two new classes based on defined parameters. Unlike conventional practices, the proposed  methodology  includes  accessible  green  spaces  beyond  the  study  area.  Ur.1  also  includes UGS outside the study area up to 500 m aerial proximity, and Ur.2 areas include UGS \nup  to  1000  m.  In  the  absence  of  a  formal  database  of  Ur-pedestrian  access  points,  one  is \ncreated by inverting the features into points and representing the linkage between Ur pathway \nentrances and pedestrian-friendly streets. Then the outcome was subjected to proximity analysis (“Create Time Areas” tool), and as a result, we got planes of the primary service areas \nbased on the specified distances. Finally, the generated layers were pruned with residential \nland use to get the served neighborhoods. At this point, were created three new layers: \na) Served residential areas at 500 m walking distance.  \nb) Served residential areas at 1000 m walking distance.  \nc) Served residential areas at 500 and 1000 m walking distance.  \n\nThe served residential areas are expressed as a percentage of the total residential area. The \noutcome was augmented with population information using Data Enrich with Census 2021 \nNexiga GmbH variable to obtain the population served. \n\nResults \n\nLeipzig has an area of approximately 8195 ha of UGS used for recreation, which corresponds \nto about 28 % of the city's total area. The proportion is highest in the South district, with 987 \nha (58%), and lowest in the North district, with only 340 ha (9%). The average is 29%; the \nmedian is 28%. \n\nAs a result of the essential provision at the city level, the per capita value is 134 m2, which is \nabout 893% above the German guideline value (15 m2\/inh). The highest value is in the northwestern district with 397 m2 per capita, 2646 % above the guideline value. The lowest value \nis 49 m2 per capita in the north, still 327 % above the value. The average is 151 m2\/inh; the \nmedian is 120 m2\/inh. \n\nRegarding accessibility to Ur, only about 52% (318847 residents) of the total population can \nreach  both  distances.  About  67%  (410140 residents)  can  get  within  500  meters,  and  86% \n(527256 residents) within 1000 meters. Regarding residential development, only 59% (3219 \nha) are within both distances. About 80% (4271 ha) have access within 500 meters, and 71% \n(3803 ha) within 1000 meters. \n\nThe  results  show  that  the  provision  of  green  space  in  our  case  study  is  indeed  above  the \nnational quantitative standards: \n\nAbout 48% of the population and 41% of the residential areas need direct access to green \nspaces within walking distance of 500 and 1000 meters. On average, 49% and 41%, while on \na median, 52% and 43%. About 33% of the population and 20% of residential areas need \ndirect access within 500 meters. On average, 33% and 22%, while on a median, 36% and \n27%. About 14% of the population and 29% of the residential areas need to be within 1000 \nmeters. On average, 14% and 27%, while on a median, 14% and 23%. \n\nDiscussion \n\nPolicymakers and planners need objective and comparable measures and indicators that reflect the provision of urban green space in terms of its type, size, quality, and functions. The \nmethodology should allow for a reproducible assessment of urban green areas. It is based on \nreliable and quantifiable measurements to provide measurable reference values comparable \nto  national  and  international  standards.  The  indicators  are  operationalized  through  a  GIS-based working method. They include several dimensions whereby errors and uncertainties \ndue to the approximation of natural phenomena are an inevitable part of spatial data. According to GRUNEWALD et al. (2017), a fundamental distinction must be made in the Basis-DLM \nbetween areas with a high or low proportion of green space, i. e., modelling rules prevent the \nmapping of greenspace between row developments or perimeter block developments, which \nmakes it impossible to examine private green areas or spacing green in the context of residential accessibility to recreation-relevant areas. The accuracy of the GIS tools in processing \nthe input data should also not be negligible, as the machines’ inside calculations are unknown \nto the user. Analyzing public health statistics in areas with direct access to recreation and \nthose without would be interesting to highlight the importance of distance between residences \nand recreation areas. \n\nConclusions \n\nPrimary indicators of green space quality in urban areas are essential to show reference values \nfor decision-making processes. In this regard, the methodology provides a basis for pursuing \nmore  sustainable  urban  development,  considering  that  green  infrastructure  is  an  essential \nhealth determinant in city life quality. Furthermore, adopting this analytical approach to various cities can provide a general approach to analysing the cultural functions of green spaces. \nTraditionally, study areas have been limited to city limits, ignoring peripheral green spaces \nthat are of practical use to city inhabitants. As a buffer, green space outside the study area \nwith defined access distances is considered to broaden this methodology. Given the disparity \nin residential accessibility and population shown in Figure 6, this analysis can also help in \nplanning for new residential development to ensure fair and controlled coverage  with this \nservice. \n\nEmotion Sensing for (E-)Bicycle Safety and Mobility \nComfort \n\nAbstract:  The “Emotion Sensing  for (E-)Bicycle Safety and Mobility Comfort”  approach, in short, \n“ESSEM”, funded by the mFUND program of the Federal Ministry of Digital Affairs and Transport \n(BMDV), investigates the subjective safety perception of cyclists in urban traffic. Identifying Moments \nof Stress in the bicycle network in Ludwigsburg and Osnabrück is done by collecting biophysiological \ndata using sensor technologies and surveys. Besides developing a practical tool for evaluating bicycle \ninfrastructures with emotion-sensing data, bicycle components are designed and  assessed within the \nproject. \n\nIntroduction \n\nCommuting to work or school, travelling to leisure activities, and in everyday life – mobility \nshapes our social lives and thus reflects individual needs. As an alternative to private cars \nand as a supplement to public transport and walking, bicycle mobility is becoming increasingly relevant. Due to the Covid pandemic and the rising development of pedelecs, the bicycle is taking on a key role in mobility, especially in urban traffic. Furthermore, cycling  is \nenvironmentally friendly and can improve personal well-being and health. These arguments \nfor bicycle use are still not reflected in the modal split of many cities. One reason that restricts \nwidespread bicycle use is the limited subjective perception of safety. Factors such as travel \ntime, costs, and mode choice also depend on the perceived safety of the mobility form. The \n“National Cycling Plan 2020” states that cyclists who feel particularly unsafe also cycle less \noften  (BMDV  2022).  In  urban  planning,  the  difficulty  in  identifying  stress-triggering  and \ndangerous spatial situations, such as critical intersections and forms of guidance, often exists. \nWhile infrastructure improvements are implemented based on statistically relevant road crash \nhotspots, so-called near misses are often unrecognized. These events are not included in accident statistics, preventing undecided cyclists from using bicycles.  \n\nMobility design develops people-friendly mobility that should enable a smooth and safe transition from one form of mobility to another. Accordingly, using different individual, shared, \nor public transport modes on one route – the intermodal transport chains – should be comfortable and easily provide people with a positive mobility experience (ECKART & VÖCKLER \n2021). The (e-)bicycle plays a key element here on the “last mile.” Research in the context \nof design and bicycle traffic identifies design elements such as materials, water, green spaces, \nand  markings,  but  also  guidance  systems  or  icons  in  projects  that  can  increase  subjective \nsafety (ALBRECHT & ECKART 2020). In addition, detecting stress points within the city can \nhelp identify issues of planning deficits, reflect the infrastructures on site, and upgrade them \naccording to new design findings. \n\nState of Research \n\nObjective and Subjective Safety Perception of Cyclists \n\nIn general, bicycle safety comprises objective and subjective dimensions and their correlation \n(JOHANNSEN 2013). Objective safety describes a quantitative view of road crashes. The basis \nfor this is the publication of police road crash reports Subjective safety, on the other hand, is \nan  emotional  view  of  the  threat  posed  by  a  traffic  situation  by  the  road  users  themselves \n(FULLER 2005). Critical conditions, near misses, or obstructions in traffic primarily influence \nthis subjective perception of safety among cyclists. Cycling experts, therefore, consider the \n“reduction of stress” in cycling as an essential factor for increasing the modal share of cycling \n(GRAF 2016). In everyday life, cyclists use bicycles as their primary means of transportation \nonly if they feel safe while cycling. This circumstance significantly affects people who are \nnon-users or occasional users of bicycles. Individual mobility behavior thus depends not only \non structural or interactive factors but also on “exogenous” factors. The effect of exogenous \ninfluencing factors also varies depending on personal characteristics. These include gender, \nage,  mobility  profile  (people  with  or  without  disabilities),  trip  purposes,  habit  (means  of \ntransport, local knowledge), and psychological dispositions (SCHMIDT-HAMBURGER 2022). \n\nClassification into Cycling Types \n\nIt seems evident that the subjective safety perception in road traffic can vary between people \ndue to individual characteristics and circumstances. One way to compare the subjective safety \nperception  is  by  categorizing  cyclists  into  previously  defined  cyclist  types.  Besides  other \nmodels for distinguishing cyclist types, Geller's categorization allows four groups to be distinguished  based  on  their  cycling  behavior  (GELLER  2009).  The  ESSEM  project  uses  this \napproach  as  a  basis.  The  affiliation  to  a  group  can  change  dynamically  –  depending  on \nchanges in mobility behavior and personal situation – and is not assignable for some individuals. Nevertheless, the classification into Geller's four groups provides a basis for analyzing \nthe abilities, desires, and needs of different cyclist types.  \n\nOn  the  one  hand,  regarding  cycling  promotion,  and  thus  also  for  the  ESSEM  project,  the \nfocus is on the “Interested Cyclists” group. These people are generally open to cycling but \nare concerned about their safety and therefore do not yet use the bicycle for their daily trips. \nOn the other hand, the groups “Fearless Cyclists” and “Everyday Cyclists” are only marginal \ntarget groups, as they already use bicycles regularly for everyday mobility. Here the motivation is to continue encouraging them to use bikes. A particular case is the group “No chance, \nno matter how!”, for whom cycling as a mobility alternative is generally not an option, even \nthough they would physically be able to. \n\nEmoCycling \n\nChristian Nold introduced his “Emotional Cartography” methodology in 2009, which serves \nas the inspiration and foundation for the “Emocycling” method. A core element of this was \na specially developed “bio-mapping” device that geolocates biostatistical data and visualizes \nit as a map (NOLD 2009). This approach allowed humans and their physiological responses \nto act as a sensor for the first time, recording the level of stress or arousal in an urban context. \nFollowing several research papers, ZEILE et al. (2016) revealed the most common triggers \nfor stress responses from cyclists using wearables, cameras, and smartphone-based applications. One finding of this research highlighted traffic as a significant stress trigger. \n\nBesides measuring the test person's vital data (e. g., skin conductivity and temperature) during bicycle use, the sensor wristband synchronizes these data points with the corresponding \nGPS data using a smartphone. A stress reaction (Moment of Stress) occurs when the skin \nconductivity increases and the skin temperature decreases simultaneously. Combining this \nmethod with a (personal) interview makes more detailed results possible for the Moments of \nStress. When researching stress phenomena and their harmful effects on the human body, the \nextent of individual perception of stress is highly relevant.  Further specification of the assessed subjective stress level is possible by adding the mobility profile as well as sociodemographic and sociopsychological assumptions. The endogenous influencing factors refer to \nindividuals' personal demographic, socioeconomic, and sociocultural attributes and their social  environment,  significantly  affecting  perception  (WERMUTH  2005).  Examples  include \ngender, age, physical constitution, local knowledge, and familiarity with the mode of transportation.  \n\nFurthermore,  from  a  biopsychological  point  of  view,  there  are  indications  that  genetic  or \npsychological predispositions can intensify or weaken stress reactions. Here, personality, locus of control, and risk-taking are particularly relevant (SCHANDRY 2016, KOVALEVA et al. \n2012). The information is collected in standardized questionnaires and complements sensor \nmeasurements as a component of data analysis. Overall, the aim is to identify vulnerable user \ngroups in stress perception and learn about barriers to equality for cyclists on the road. The \nmain advantage of mixed-method approaches is the combination of quantitative data analyses \nand qualitative surveys, leading to more reliable and comprehensive results than singularly \napplied measurement techniques. Moreover, the methods used complement each other and \nthus partially compensate for their shortcomings. (RESCH et al. 2020). \n\nESSEM – Project Goals \n\nESSEM deals with identifying impact factors on the subjectively perceived safety of cyclists. \nThe aim is to increase the comfort and safety of cyclists and thus contribute to sustainable \nand climate-neutral mobility. The EmoCycling mixed-method approach helps to identify and \nanalyze Moments of Stress in bicycle infrastructure in Ludwigsburg and Osnabrück. \n\nThe data collected in the project will be used to develop mechanisms for measuring safety, \nthe perception of safety, and mobility comfort in cycling. The infrastructure, environmental \ninfluences, the bicycle used itself, and the cycling accessories are tested as influencing factors. The insights will help identify the need to optimize urban bicycle infrastructures and \nsafety products. The participating model cities promote user-oriented and data-based cycling \nplanning. Further, in Osnabrück, the environmentally sensitive traffic management system \n(UVM) will be further advanced. The long-term goal is to develop an innovative tool that \nholistically assesses urban cycling infrastructure and helps planners optimize it. \n\nAdditionally, the project consortium initiated an innovation network in cooperation with industry partners, existing networks, city representatives, and associations to support the development of data-driven solutions for bicycle-related products and services.  \n\nProcedure and First Results of the Data Collection Phases \n\nPrevious project approaches use the Body Monitor Smartband (ENGELNIEDERHAMMER et al. \n2019), Galvanic Skin Response (GSR) (OSBORNE & JONES 2019, SHOVAL et al. 2018, BIRENBOIM et al. 2019), or other variables such as Heart Rate Variability (PAÜL I AGUSTÍ et al. \n2019).  In  contrast,  the  presented  approach  allows  an  independent  implementation  of  the \nmeasurement routine by the test persons.  \n\n4.1  Phase 1: Initial Data Collection \n\nInitial data collection includes the acquisition of participants by project partner Bike Citizens \n(BIKE CITIZENS 2023) via their same-named application “Bike Citizens,” and the Bike Citizens Analytics tool “BCA” delivers the analysis (CYCLINGDATA 2023). \n\nA link to the ESSEM website then provides further information on data collection. As soon \nas  users  agree  to  participate  in  the  data  collection,  the  application  records  routes  used  by \napproximately 350 subjects. Participants collected data daily during a determined period. The \nanalysis of the anonymized datasets happens in the last step and maps a combined graphic \nshowing the main infrastructure networks and the frequency of bicycle use. Data from the \ninitial data collection phase forms the basis for the multi-phase EmoCycling measurements \ncarried out during the summer of 2022. \n\nResults of Initial Data Collection in Ludwigsburg \n\nThe user data collection happens in the BCA's analysis portal,  where it calculates various \nbike-specific conclusions based on the trajectories. For example, it is possible to determine \nthe intensity concerning the number of cyclists in the network to show the average speed per \nroad segment.  Another feature is to identify (forced) waiting zones about their frequency, \ndetermine the action radius of the participants, and calculate the attractiveness of the road \nsegment compared to the whole network. The attractiveness function identifies cyclists preferred and avoided paths. These segments are displayed in red if cyclists take a detour and \navoid the shortest route. Popular detours appear in green. The line thickness indicates the \nintensity.  \n\nFigure  1  shows  the  results  of  the  analysis.  On  the  left,  the  main  road  network  shows  the \nintensity of all tracks driven in the form of a heat map of the road network. The thicker the \nblue line, the more frequent the road use. The particularly intensive main routes have a red \nmarking. The identification of the heavy frequency of the central axis in the north-south direction, along the baroque palace park of Ludwigsburg, can be seen well. \n\nFurthermore, the road towards the Neckar in the eastern direction and the arterial roads towards the west can be identified as essential axes. On the right, in the focus “city center”, the \nattractiveness analysis shows that many cyclists from the city center avoid the direct route to \nthe station and prefer to use the orthogonal streets. Despite the high traffic intensity on both \nstreets,  this  is  an  excellent  example  of  how  good  bicycle  infrastructure,  such  as  along \nStuttgarter Straße, can motivate or discourage cyclists from using the route. \n\nResults of Initial Data Collection in Osnabrück \n\nIn Osnabrück, the intensive use of paths in the city center is a special characteristic (Fig. 2). \nThe western areas (especially Katharinenstraße,  which is designated as a bicycle lane), as \nwell as the 30 km\/h zones in the Wüste district, have a high level of bicycle use. In contrast, \nmany cyclists avoid Martinistraße, which runs parallel to Katharinenstraße in the south with \na strong MPT dominance, and Lotter Straße in the north. \n\nCyclists  also  avoid  the  southwestern  inner  city  ring  road.  In  the  south  in  Kalkhügel,  Sutthauser Straße is bypassed in favor of Burenkamp. In the southeast, the participating cyclists \nprefer Meller Straße to the four-lane Hannoversche Straße.  \n\nInterview Process and Subject Selection Phase 2 \n\nStandardized surveys will supplement the sensor-based data from the survey phases. The aim \nis to obtain further information about the participants that provide insight into their mobility \nbehavior and personal dispositions. This information supports the observations on stress perception. The survey is accessible via LimeSurvey, which ensures a quick creation of a first \nstatistical analysis. The data is collected anonymized. To comply with data protection regulations and also to correlate data from the survey with data from the sensor-based measurement, each participant creates a pseudonym. Simultaneously, the survey results provide information for ensuring that the sample for the sensor-based measurements is as representative \nof a cross-section of the population as possible. Therefore, the participants can voluntarily \nprovide their email addresses as a contact option.  \n\nPhase 2: Emo-Cycling Data Collection in Ludwigsburg and Osnabrück \n\nThe second data collection phase began in Ludwigsburg in July 2022 and in Osnabrück in \nSeptember 2022. For this purpose, the 30 test persons selected through the initial data collection and the standardized questionnaires were divided into two groups of 15 test persons each \nfor each city and equipped with the measuring instruments.  \n\nDuring the collection, the physiological stress reactions of the participants are recorded, located, and mapped according to the EmoCycling method. Empatica E4 Smartbands are used \nto record near-body data, locate it via GPS using a smartphone, and collect them in the E-Diary app. A unique feature of the ESSEM project is that, after a brief introduction, the participants could take the data collection equipment home with them and independently connect \nthe devices before each trip in their everyday life. Data acquisition with the smartphone provides a packed Spatial-Lite database. The outcome data with the identified Moments of Stress was  stored  in  machine-readable  CSV  format  (cf.  KYRIAKOU  et  al.  2019,  TEIXEIRA  et  al. 2019).  Further  collected  attributes  besides  geocoordinates  longitude  and  latitude  are  Unix timestamp, MOS score, raw GSR and ST values, time_iso, speed, and acceleration. \n\nDuring the survey phase Ia, we could already gain the aimed-for results from the technical \nside by the first test persons in Ludwigsburg. Therefore, the detection of the MOS can be \nevaluated as expected and show first indications of situations in the road network where cyclists perceive stress during their daily rides. In the evaluation, it is noticeable that the participants showed an increased stress reaction on the diagonal from the city center to the train \nstation, like in the initial phase. Another stress hotspot for cyclists is a large construction site.  \n\nThe results from the data collection in Osnabrück after partly completed analyses look promising, too. A concentration of Moments of Stress is detected along Natruper Straße in the \nnorth,  at  the  section  at  Westlicher  Wall,  Burkenkamp,  Lotter  Straße\/Lineschweg,  Magdalenenweg, and further. Further, it shows in contrast to the preferred routes according to the \nattractiveness analysis that, for example, many Moments of Stress occur on the popular road \nof  the  Burkenkamp.  All  these  hotspots  are  relevant  areas  for  investigation  in  the  ESSEM \nproject. In further experiments and analyses, these are the focus areas for studying infrastructure conditions and their design to increase the perceived safety of cyclists through targeted \ninterventions. From a planning point of view, the speed limit planned for the middle of the \nyear 2023 on the southern access road (Iburger Straße) into the city center will increase the \nattractiveness for cyclists. This then can lead to a reduction of perceived stress on the route. \n\nConclusion and Outlook \n\nThe setup presented and the initial results are promising, so it seems realistic to use mixed-method approaches to actively integrate new perspectives in the context of subjective perception of safety and comfort, when cycling, into planning processes and demonstrator development. \n\nThe interim conclusion of the data collection process in the ESSEM project is that without \ndirect contact with the municipality, such a long-term study with the participant acquisition \nis not feasible. On the one hand, this may be due to the framework conditions of a city: The \ncycling-specific DNA of society, the existing modal split, or topography. Thus, it seems evident that the acquisition of participants in the cities of Ludwigsburg and Osnabrück started \nwith different preconditions. On the one hand, an active and comprehensive press campaign \nin  Osnabrück  recruited  almost  300  participants.  On  the  other  hand,  despite  the  direct  approach by the German Bicycle Club (ADFC) to its members in Ludwigsburg, the proportion \nof potentially interested persons was only 10% of the participants compared to Osnabrück. \nConsequently, the active participation of communities in exploratory processes and the political legitimization of projects are crucial for their success. This support also succeeds in \nfurther integrating relevant institutions such as schools, churches, and disability associations \nto acquire a diverse user group early. Nevertheless, the project presented here impressively \nhighlights  the  potential  of  cooperation  projects  between  science,  business,  municipalities, \nand associations. In addition to “classic” location-based services and trajectory detection of \ncyclists, the project attempts to collect biostatistical data on stress detection for the first time \non a large scale over a long period to a) identify potentially existing stress hotspots, b) evaluate and optimize bicycle components with the help of sensor technology c) detect potential \npositive effects of road closures for cars in the context of environmentally sensitive traffic \nmanagement on the stress perception of cyclists, and d) create alternative route suggestions \nfor bicycle navigation with the help of stress hotspots (emo-routing).  \n\nIn addition to formal criteria for maintaining the objective safety of infrastructures, the identified stress hotspots can also be used as a basis for further research to improve the design of \nroads in the context of mobility design. Routing, pavements, and the general design of (bicycle) roads help promote cyclists' well-being. At the same time, reducing stress in bicycle and \npedestrian traffic increases subjective safety. This way, good design can also increase safety \nand improve traffic turnaround. \n\nGeodesign for Trail-Based Tourism: A Regional Case \nStudy in Post-Industrial Pennsylvania \n\nAbstract: This study uses the geodesign framework to design regional and local trail systems, using \nmaterial culture as a sustainable economic driver.  The economic  decline of post-industrial towns in \nSouthwestern Pennsylvania is well documented. The communities comprising this region must reckon \nwith changing landscapes and infrastructure, issues geodesign is well suited to address. However, the \nregion is defined by the organization of life resulting from the material relations of the once-prosperous \nAmerican steel industry.  These hauntological properties are identified and exploited as catalysts  for \npost-industrial heritage and trail-based tourism using the geodesign process.  \n\nIntroduction \n\nThe Monongahela Valley’s  economic decline in the Pittsburgh region since the  American \nsteel industry’s collapse is well documented (DEITRICK 1999, DURYEA 2014). Between 1980 \nand 1986, the region lost over 115,500 jobs, with a decrease in manufacturing employment \ngreater than 40% (DETRICK 1999). The steel industry alone accounted for 50% of the decline \n(DETRICK 1999). Once considered the Monongahela Valley’s “heart”, Monessen fell from a \npopulation of over 20,000 in 1940 to just 7500 people today (CITY OF MONESSEN 2020). The \nresult is a declining population and a lack of investment. Still, these communities must deal \nwith aging infrastructure, landscapes, and an overreliance on outside capital investments.  \n\nCompounding  economic  issues,  in  2014,  the  Pennsylvania  Department  of  Transportation \ncited  over  20%  of  the  roads  in  Southwestern  Pennsylvania  as  poor  and  rated  25%  of  the \nbridges as structurally deficient (UNIVERSITY OF PITTSBURGH INSTITUTE OF POLITICS AND \nINFRASTRUCTURE POLICY COMMITTEE 2014).  \n\nStudy Area \n\nThe study area captured a five-county region, including Allegheny, Fayette, Greene, Washington, and Westmoreland counties through which the Monongahela River flows. The river \nserved as transportation for the steel industry  in the 19th  and 20th centuries and attracted \nresidents to this industrial setting. Correspondingly, communities developed to support the \nindustries, specializing in areas contributing to steel output. \n\nThe infrastructure reflects the industry’s demands and the residential populations supplying \nthe workforce. Much industrial infrastructure remains, lining both sides of the river but is \ninactive  or  used  for  light  industry.  Railroad  lines  divide  it  from  residential  areas  creating \nample waterfront development space. Surrounding areas are rural and forested. \n\nThe Youghiogheny River is a major tributary of the Monongahela River. The Great Allegheny Passage Trail (GAP) parallels the Youghiogheny River and drives tourism to the east \nof the Monongahela. Along this route is Perryopolis, ideal for a case study as it represents a \nsmall population community along the GAP Trail with industrial heritage resources that can \nbe adapted for trail-based tourism.  \n\nStakeholders \n\nStakeholders were chosen by identifying boundaries, landowners, and land stewardship groups, \nincluding the region’s residents and communities. Periodically,  multiple  governing bodies \nintersected physical and geographical boundaries. The Pennsylvania Department of Environmental Protection, the Pennsylvania Department of Conservation and Natural Resources, and \nthe Department of Transportation are three statewide stakeholders with regional considerations. The Mon Valley Regional Chamber of Commerce exists to support the commercial \ninterests of thirty communities. Historical and heritage societies exist at varying scales, sometimes with multiple organizations in a single locale. \n\nThe Allegheny Trail Association supports the GAP Trail through multi-regional and multi-state scales. The Steel Valley Trail Council (SVTC) manages a stretch of trail between Homestead and Clairton, connecting the Montour Trail and the GAP Trail. Furthermore, Rivers of \nSteel is a National Heritage Area organization supporting industrial heritage development in \nthe Southwestern Pennsylvania region.  \n\nAddressing stakeholders allowed for consideration of cultural touchpoints in the Monongahela Valley. These touchpoints were used to inform the organization of society as a result of \nthe  industrial  activity  in  the  area  at  its  peak.  Rather  than  simply  “ruin-gaze”,  a  cohesive, \neducational portrait of the area’s history could be created. An abandoned industrial structure \nis then used as a node or waypoint, defining proximity to surrounding housing and commercial buildings. Main streets and workforce housing areas, such as “company houses” or Donora’s “Cement City” become noteworthy because of distinct features and materials that inform the trail user of their relationship to industry.  \n\nWith stakeholder input, buildings, homes, historic districts, and open spaces and corridors \nthat are disconnected in the post-industrial landscape become connected, or reconnected, as \nnodes via a trail. \n\nPrecedents \n\nBOWNS & STEVENSON (2010) argue, “material culture should be viewed as a strategic and \ngenerative  force  to  promote  sustainable  economic  development  in  the  region”.  GARNER \n(2013) thoroughly examines the connection between historic preservation, economic development,  and  bike  trails,  suggesting  that  longer  trails  and  trail  connections  encourage  increased user activity.  \n\nMACLEOD (2017) takes this a step further describing the creation of the trail as the “cultural \nturn”, but the way in which the user perceives the trail as the “performative turn.” In that \nway, the trail can highlight specific features of an area or suggest alternative views or story \nlines. Trail user engagement can be captured through immersive experiences, capitalizing on \ntechnology to create a trail-based museum. SWENSEN & NOMEIKAITE (2019) echo this concept and point out the increasing demand for trail-based tourism results in demand for new \nand more immersive experiences which can be steered by internet technology. \n\nHere, we can rely on the concept of hauntology and the application of critical-creative preservation to maintain authenticity. Hauntology does not refer to paranormal activity or ghosts, \nbut to the way we live that allows the past to remain a presence in our lives today (DERRIDA \n1994). STERLING (2022) builds upon this and applies it to critical-creative heritage practice; \nhe emphasizes the importance of retaining the intention of the site, to not wash away or conceal its meaning. Applying this to the Monongahela Valley region, we can preserve its heritage as both a steel powerhouse and a relic of a former America, a breathing example of how \nlife was organized by the steel industry and reorganized by its departure. \n\nMethods \n\nThe  plan  establishes  a  model  development  plan  for  trail  towns  at  multiple  scales  using  a \ngeodesign framework. Stakeholder input was considered from a community level to influence a regional plan by identifying congruent goals and challenges. The geodesign framework supports input from various stakeholder groups across multiple scales where multiple \nscenarios, circumstances, and boundaries are considered. Using tools such as ArcGeoplanner \nand City Engine provided real-time responses to scenario modeling, allowing for route and \ninfrastructure adjustment based on stakeholder input (STEINITZ 2012).  \n\nFirst, a regional plan is developed for the Monongahela Valley, utilizing its industrial heritage \nas a catalyst for trail creation and linking it to the GAP Trail, a successful and significant trail \ntourism example. \n\nSecond, a small-scale plan is established for Perryopolis, a 1700-resident town located along \nthe GAP Trail. Significance is placed on discontinuing reliance on aging “car-centric” infrastructure  to  a  multi-modal,  bike  and  pedestrian-focused  model.  In  each  case,  emphasis  is \nplaced on post-industrial heritage tourism and placemaking to create an authentic, trail-based \ntourism experience.  \n\nAnalysis \n\nAt  the  regional  scale,  geographic  and  historic  importance  points  highlighting  the  material \nculture, the historic buildings, sites, and infrastructure representing that culture, were considered in identifying the trail location. To capture these points, a half-mile buffer representing \nthe GAP Trail “Trail Impact Zone” was established surrounding the 56 miles of the Monongahela River included in the Mon Valley Region. The Trail Impact Zone was then enriched \nwith 2020 U.S. Census data within Esri ArcPro, including approximately 52,000 residents.  \n\nPennsylvania Historical & Museum Commission (PHMC) sites within the half-mile buffer \nwere selected to identify potential secondary trail locations. The result revealed 19 interest \npoints  across  four  counties  and  included  items  relating  to  the  region's  industrial  heritage. \nAdditionally, existing trails and access points were considered to provide connectivity across \nthe region and within individual communities. Using the Monongahela Valley communities \nto establish the regional trail system’s primary nodes and emphasize the region’s post-industrial heritage, the 19 PHMC interest points identified potential nodes along secondary and \ntertiary local thematic trails and loops. \n\nEsri’s ArcGIS Geoplanner provides ideal locations for trail placement, maintaining an average grade of 10% or less, following the Pennsylvania Trail Design and Development Principles (PENNSYLVANIA DEPARTMENT OF CONSERVATION AND NATURAL RESOURCES  2022). \nWhile slope is important in trail design, specifically ADA-compliant trails, it is not the only \nfactor to consider. This model determined ideal trail placement using weighted criteria: aspect, land cover, landforms, human modification, and elevation as shown in Figure 2. Slope \nwas given highest priority while elevation and human modification received the next highest \nweights to reveal areas with flat, accessible terrain near existing infrastructure and amenities. \nHuman modified land prioritized either low or elevated levels of modification to discern between rural, pastoral trail areas and commercial or industrial clusters to help identify nodes \nversus pathways. However, land cover prioritized open grassland and forested areas while \nopen water and developed land were deprecated to avoid high traffic areas. Landforms considered plains and tablelands while forgoing mountains and surface water locations.  \n\nLocally, the Trail Model weighted overlay within Geoplanner reveals multiple pathways directly connecting Perryopolis’s downtown area to the Great Allegheny Passage Trail. In addition, the model supports the design of a two-mile trail loop, making this connection. The \nmodel also proved useful in establishing other potential trail routes.  \n\nA five-minute analysis, chosen to accommodate the town's small-scale determined walking \nroutes from the center point of Perryopolis. In this case, a five-minute walk covered the central area of the town, extending from Sampey Park near GAP trail access and Frazier High \nSchool  at  the  opposing  terminus.  Frazier  High  School  campus  extends  to  Rt.  51,  a  major \nhighway. \n\nWithin this five-minute walk study area, two main streets transecting north-to-south and east-to-west  feature  street  parking  on  both  sides  of  the  street  with  lined,  non-metered  parking \nspaces bordering two traffic lanes with a central roundabout. These streets both possessed \n60-foot  rights-of-way.  All  available  off-street  parking  was  digitized  to  estimate  the  total \navailable off-street parking spaces within a five-minute walk of the town center. \n\nAnalysis revealed that a substantial portion of Perryopolis was accessible by a five-to-ten-\nminute walk, linking developed parkland with historic nodes to the Frazier School District \ncampus.  However,  the  town  lacked  infrastructure  supporting  this  accessibility,  including \nsidewalks and bike lanes. Available off-street parking totaled 170,000 square feet or 1,062 \n160-square-foot parking spaces (Fig. 3). This calculation equated to a rate of .59 off-street \nparking spaces per resident in the study area. These parking lots interrupted walkability and \nopportunities for path connections and sidewalks. \n\nFinally, a 3D model (Fig. 4) of Perryopolis was constructed using City Engine. Using the 60-foot right-of-way footprint, a new street cross-section was designed to emphasize shared-use \nfacilities  and  connectivity,  reducing  vehicular  dependency.  Building  density  evaluation \nwithin the immediate town center provided insight into the capacity to support the anticipated \nretail  and  residential  needs  with  an  increased  GAP  user  rate.  Secondary  and  tertiary  bike \nroutes using local thematic elements were then established to support the transportation transformation of the main route. The intention here is to reorient the town towards the economic \ndriver of the GAP Trail, develop the infrastructure required of a Trail Town environment, \nand provide a focal point for placemaking.  \n\nResults and Discussion \n\nResults  \n\nThis analysis provided information to design a 56-mile regional trail connecting five Monongahela Valley counties to the GAP Trail via the Youghiogheny River Trail and the Montour \nTrail. This connection impacts 52,000 residents in the half-mile “Trail Impact Zone” buffer \nacross Allegheny, Westmoreland, Washington, Fayette, and Greene counties. As the GAP \nTrail  generates  about  $800,000  per  trail  mile,  or  $78k  per  1000  people  in  the  region,  we \nestimate the proposed 56-mile Monongahela Valley Trail would generate $45 million in total \nrevenue for the region (GREAT ALLEGHENY PASSAGE ECONOMIC IMPACT REPORT 2021). The trail \nwould generate $22,500 per 1000 people in the region, although this number rises to $53,000 \nwhen the 7.5 miles of trail located in Allegheny County, which includes the City of Pittsburgh \nand the corresponding population, is excluded from the calculation.  \n\nAt the local scale of Perryopolis, redistributing the 60-feet road rights-of-way towards a bike- \nand pedestrian-centric form reorients the town towards the economic highway that is the GAP \nTrail. To increase walkability, a 2.2-mile trail loop connecting developed parkland directly \nto the GAP Trail reduced dependency on automotive infrastructure. Second, street parking \nwas removed from the main street bisecting the town in an east-to-west direction, connecting \n\nSampey Park and the school campus, and replaced it with a central human scale, bike, and \npedestrian path. This shared-use infrastructure creates one main artery linking the town to \nthe GAP Trail, entirely accessible via a ten-minute walk.  \n\nThe replacement of off-street parking locations with increased downtown building density, \nalong with bike and pedestrian infrastructure, will improve walkability and retail opportunities. These factors support trail town guidelines by improving lighting, increasing the number \nof crosswalks, sidewalks, and bike lane lengths, and improving odds for increased economic \nactivity and home values with a reduction of demand on aging infrastructure.  \n\nSecondary and tertiary trails link to historic site nodes, creating a richer experience for trail \nusers. Route 1 approaches the town’s pre-Revolutionary War history by connecting the Ghrist \nMill site located near Sampey Park to Searight’s Fulling Mill, and The Gue House. An alternative trail focuses on the industrial heritage of the town, connecting beehive coke ovens, rail \nstops,  company  housing,  and  stores  weaving  through  multiple  ethnic  and  residential \n“patches” that organized life in Perryopolis near the turn of the 20th century. \n\nThe redesign of Perryopolis toward a Trail Town  model is  workable. Excess parking and \nroad infrastructure designed to handle higher levels of past automotive traffic can be converted to bike and pedestrian infrastructure. \n\nDiscussion \n\nTrail nodes identified at the local scale were established from focused research, inventory, \nand stakeholder consultation. Identifying and inventorying the Perryopolis material culture \nrequired an intimate understanding of the town due to the lack of available data. Lacking data \ncan be anticipated in communities within the region, obstructing placemaking themes. \n\nGeodesign tools, such as Geoplanner and City Engine, allow for rapid responses to add information and stakeholder input. For example, as local histories shaped trail nodes at both \nlocal and regional levels, feedback provided further insight into sites, trail nodes and alignment not discernible from available GIS data. This information enhanced, but also intensified \nthe iterative process. Multiple routes, future scenarios, or trail ideas could be evaluated digitally with little time spent as opposed to more traditional, analog design methods. As additional information is gleaned, tangible results can be reviewed increasing the impact of public \ninput in real-time.  \n\nThis advantage favors small communities and economically distressed regions as it reduces \nthe time to identify alternate routes and calculating change impacts. With the cost of engaging \ntraditional methods a potential barrier, a proposed next step is expanding geodesign as a cost-effective (and time-effective) alternative for areas with limited financial resources or other \nconstraining factors. \n\nEstablishing the proper criteria and weights for the weighted overlay models proved to limit \nthe project. For this project's purposes, the model provided a general analysis for best trail \nplacement. Further investigation into the weighting of criteria and its practical implications \nis a potential next step to be explored. This model could be expanded to include proximity \nanalysis of amenities and other opportunities. \n\nAdditionally, identifying zoning and ordinance info across multiple scales and jurisdictions \nproved to be difficult. In some scenarios, multiple authorities were responsible for an asset \nor area in lieu of a local government entity. The next step for this project would be to compile \naccurate  zoning  and  ordinance  maps  for  the  region  to  properly  identify  opportunities  and \nroadblocks and incorporate them into the design. \n\nConclusion  \n\nThe architecture, historic sites, and artifacts of the Monongahela Valley provide contextual \nplacemaking themes and trail nodes and act as a catalyst for economic activity. Reliance on \nstakeholder input alleviates pain points caused by the lack of available cultural data in economically depressed areas. Thus, the geodesign framework and tools consider stakeholder \ninput and address local issues in a regional context using real-time, scenario-based responses, \nsupporting post-industrial heritage and trail-based tourism in Southwestern Pennsylvania. \n\nTrails, loops, and pathways designed by themes of local and regional histories rather than the \nnumber  of  sites  located  along  the  trail  route  present  a  cohesive  and  consistent  message. \nRoutes were designed to focus on industrial heritage and historical nodes, intersecting and \nconnecting to create a network or trails of varying length in place of a single, longer trail. \nThe overall number of trails, loops, or pathways was preferable to the length of a single trail, \nas users are more likely to visit an area with a higher number of trails. The construction of a \nregional identity and connection directly resulted from this focused placemaking centered on \npost-industrial  heritage.  Using  the  region's  industrial  heritage  as  a  catalyst,  local  themes \nemerged, providing more detail to the sense of place and organization of life at peak economic levels. \n\nFurther research is needed to resolve zoning and land use challenges, specifically the required \nretail and parking ratios. New questions emerged regarding establishing a weighted overlay \nmodel  to  identify  trail  routes  and  nodes  and  methods  to  include  material  culture  in  those \nmodels. \n\nGeodesign in the Strategic Planning Track: \nA Participatory Itinerary  \n\nAbstract: The paper focuses on the importance of participation in strategic planning activities, by integrating traditional and innovative tools, such as Geodesign. The objective is to structure the adoption \nof participatory practices in ordinary procedures of public decision-making for territorial planning, describing an experience in the Medio Agri area (Basilicata, Italy). A so-called 'engaged research' has \nbeen launched that attempts to promote a more resilient and responsible territorial development process, \nbased on the Sustainable Development Goals (SDGs) and that can be funded by the exceptional public \ninvestments currently available in Italy and Europe. \n\nIntroduction \n\nTerritorial governance as a component of the Public Administrations’ responsibilities is becoming increasingly complex due to the socio-economic transformations taking place in our \nsociety. Some key issues gaining increasing relevance in the current debate are urban growth \n(DOMINGO, PALKA & HERSPERGER 2021), Industry 4.0 (MARTINELLI et al. 2019), the energy \ncrisis  (VOGEL et  al. 2021),  and  effects  of  life  expectancy  increase  (BELTRÁN-SÁNCHEZ  & \nSUBRAMANIAN 2019). New and articulated integrated and negotiated planning strategies between public and private actors, centered on people (DALEY AND ANGULO 2009), have become  widespread,  coincident  with  technological  developments,  particularly  digital  ones \n(VENTURA 1995, KANDT & BATTY 2021), and the growing dynamics of globalization. Such \na planning culture is directed towards more equity of choice, greater efficiency, and a greater \nability to preserve resources (TURA & OJANEN 2022, LAS CASAS & SCORZA 2016). \n\nStrategic planning (ARCHIBUGI 2004, 2005) can still be the methodological framework for \naddressing urban and territorial challenges (OLIVEIRA et al. 2018). Among the elements that \ncharacterize  the  strategic  planning  process  are  participation,  leadership  and  partnership \n(TANESE et al. 2006). Strong leadership on the part of local politicians, as well as renewed \nauthority on the part of public administrations, is required to launch and successfully complete strategic planning processes. Partnership, negotiation and public-private agreements are \nimportant aspects of governance models. Citizen participation in strategic public decision-making is both a challenge, a goal and a condition for the success of strategic planning itself \n(NANZ & FRITSCHE 2014).  \n\nGeodesign, facing the growing demand for participatory processes in planning, offers a well-structured framework (STEINITZ 2012, MILLER 2012) continuously improved by case study \napplications (FISCHER et al. 2020, LI & MILBURN 2016) with remarkable benefits. On the one \nhand, in training specialists from disciplines dealing with the land at different levels (environment, social participation, sustainability); on the other hand, in promoting innovative digital approaches to spatial planning issues. Geodesign, as an approach, can be a key element \nin catching up with chronic delays and bringing innovation to enable regions and local comunities  to  meet  today's  urgent challenges (CAMPAGNA 2022). Geodesign  has been defined \nby Wheller (2020) as the tool that ”combines the art of design with the science of geography. \nIt  uses  stakeholder  input,  creative  design  techniques,  evaluation,  rigorous  methodologies, \nspatial analysis, modeling, and mapping to find the most suitable, environmentally friendly, \nand sustainable options for how to use space. The space may be used for wildlife conservation, development, infrastructure, flood control systems, or transportation projects [...]”. \n\nThe paper deals with the planning concept in current local legislation, how a participatory \nprocess is structured, which techniques were used in the experience in the Medio Agri area \n(Basilicata, Italy) according to the participation objectives. Finally, the integration of the strategic planning process with Geodesign is described and the benefits and limitations of this \nexperience are discussed. \n\nThe Participation Concept in Local Planning \n\nThe research interest is focused on the Agri Valley, which is located in the south – western \nsection of the Basilicata region, between the Tyrrhenian and Ionian, taking into consideration \nonly six municipalities of the entire Agri river basin. \n\nThe study area, referred to as the Medio Agri (Fig. 1), is characterised by an evident migratory phenomenon leading to progressive depopulation. There is a widespread lack of services, which are mostly concentrated in the single centre of Sant'Arcangelo, and a fragmentation in land governance actions. However, the Medio Agri area has significant potential, \nespecially in terms of cultural and natural heritage, with a high ecosystem ecological value \n(DASTOLI & PONTRANDOLFI 2022).  \n\nTo fully understand how innovative tools are needed in planning that incorporate participation as a key element, it is necessary to know that National Urban Law No. 1150 adopted in \n1942 currently still frames the Italian planning system. However, since the 1990s, the regions \nhave begun to issue a new series of so-called “second-generation” town planning laws because they introduce new forms and content into the regional body of law. In 1999, the Basilicata Region – the place where the Geodesign experience took place – also approved its \nown urban planning law on territory use and government (LUR 23\/1999). This legislative act \nwas considered innovative with respect to some of the issues at the centre of scientific and \ndisciplinary debate at the time. Among these themes is the citizens' and private stakeholders' \nparticipation (Article 9) in the planning choices definition and in urban tools management \nand implementation. \n\nIt is understood that participation has become part of the legislative corpus affecting urban \nplanning, but not in an organic manner. Participation in public decisions represents, on the \none hand, a discouragement to possible legal disputes between the users and administration, \nand on the other, the possibility of arriving at more conscious and effective decisions in the \nperspective of 'result' planning.  \n\nIn particular, it is important not to underestimate the need to revisit, reorganise and rationalise \nthe many forms of agreement (protocols, understandings, tables...) that have been built up \nover time, especially with local autonomies.  \n\nA substantial adaptation is needed especially after the Urban Agenda for the EU (EUROPEAN \nCOMMISSION 2016, CAPROTTI et al. 2017, LAS CASAS et al. 2019) in which the participation \nand multi-level governance principles are emphasised. Indeed, this Agenda established a city \nparticipation process for the European policy-making, with an open discussion on the best \nsolutions to be proposed to address current and future challenges. \n\nNumerous experiences have been conducted over time that have fully included participation, \nboth at the local level with the C.A.S.T. project (PONTRANDOLFI 2017) and at the European \nlevel with LEADER and CLLD as well as at the international scale with Geodesign (ORLAND \n& STEINITZ 2019). The C.A.S.T. project aimed to develop participatory processes based on \nthe widespread use of new ICT technologies, supporting and not replacing more traditional \nparticipation forms. The project was a first step towards the creation of permanent territorial \nlaboratories and competence and observation centres for territorial processes. The European \nUnion promotes and supports actions in this application domain through international cooperation projects and good practice exchange. Moreover, participation composes the bottom-up approach that animates European regional convergence programming and the 'new cohesion policy' for the next European programming period 2021-2027. \n\nParticipatory Process Methods and Application \n\nAnyone who deals with participatory activities must be clear about some basic elements that \nappear cyclically in the participatory process, so questions such as: Why involve? When to \ninvolve? Who? How?  \n\nThere are currently numerous tools and techniques for promoting participation at the public \nlevel, some set out in national legislation and that of the most virtuous regions, others of a \nvoluntary nature (TAMBURINI 2009). In both types, then, it is possible to distinguish between \ninstruments:  \n•  Formal or informal \n•  Technically complex or simple \n•  With limited, sectorial and consolidated or multi-sectorial actors.  \n\nIn strategic planning processes, it is difficult to achieve effective and involved participation \nat  all  stages.  For  this  reason,  participatory  actions  often  represent  marginal  aspects  of  the \nentire process and are mainly organised at the beginning (to suggest preliminary planning \nhypotheses) or at the end (as a final scenario presentation). A structured participatory process, \nsuch as the Medio Agri one, involves the following steps (Fig. 2), in which the techniques \nused may change, but the internal dynamics, steps and goals to be achieved are the same. \n\n\nIn this case, it was decided to hold each meeting of the participatory process in a different \nmunicipality, in order to create a participatory itinerary involving the citizens of the six communities.  The  choice  of  this  modality  stemmed  from  the  area's  territorial  characteristics, \nwhere there are small municipalities in close proximity, where it was necessary to build a \nsynergic area development strategy. This approach made it possible to carry out  “heritage \nwalks” to make the inhabitants recognise their territory, to increase their direct knowledge of \nit, and to make them more aware of the use of space and its problems, especially for project \nproposals. The participatory process carried out by the research team in the Medio Agri area \ninvolved a mixed approach, which combined different techniques and integrated Geodesign \n(via the Geodesign Hub online platform) into the participatory design phase (Tab. 1).  \n\nThe early stages of the Medio Agri participation process closely follow the Participatory Impact Pathways Analysis (PIPA) (ALVAREZ et al. 2010) approach; it is an effort that allows \nproject staff and stakeholders to jointly describe the project's theories of action, develop logic \nmodels and use them for project planning and evaluation. The participation process included \nthe  organisation  of  a  context-specific  strategic  plan.  The  initial  participatory  analysis  involved the mapping of stakeholders, and the evaluation of the context through an internal and \nexternal diagnosis, carried out using the SWOT analysis technique. In the third part, scenario \nbuilding began with the definition of strategic objectives as strong ideas on which to build \nthe intervention plan, using the objective tree technique. Next, the detailed organisation of \nthe objectives for each strategic theme and the identification of the actions to be implemented \nwere carried out. In this last phase, for the participatory planning activities, the Geodesign \nplatform was used as a tool to support the identification of actions. \n\nGeodesign represents a methodological approach to decision-making that integrates spatial \ninformation science tools to support physical spatial development planning, which can help \naddress  many  of  the  current  problems  found  in  urban  and  regional  planning  practices \n(CAMPAGNA & DI CESARE 2016). The Geodesign methodology is based on the constructive \ninteraction of working groups that develop the plan in parallel and then reach a synthesis. \nThere is a preliminary phase in which the research group draws up a set of thematic maps \nrepresenting a snapshot of the territory from different points of view (environmental, cultural \nheritage, economic sectors, mobility, etc.). \n\nThe maps show which territories, according to their specific vocation, are most attractive\/vulnerable for a specific land use. In this case, seven maps (industry-trade, agriculture, heritage-nature, cultural heritage, institutional services, infrastructure and mobility, reception) are colour-coded from red (high vulnerability) to green (low vulnerability). \n\nThirty-eight participants from different backgrounds took part in the participatory design activities using Geodesign: local administrators, freelancers, researchers, university students, \nmembers of associations and the local community. The participants were divided into two \ngroups: tourism promotion and territorial protection, on the one hand, and local development \nand institutional and reception services, on the other. \n\nAfter a brief overview of the study area and the objectives of the meeting, each participant, \nusing a sketch-planning tool available within the platform, drew up a set of georeferenced \nconceptual ideas (diagrams) representing specific policies or projects for each system.  \n\nIn particular, all participants were asked to place on a map the interventions or policies that \ncould  realise  the  objectives,  which  had  been  previously  prioritised  with  the  multi-criteria \nanalysis. Each intervention or policy had to have a title, a location, a description, an estimated \ntotal amount and had to belong to one of the seven systems into which the context analysis \nwas summarised. Eighty-six actions to be carried out were localised and described in this \nphase, divided into interventions and policies. The two groups' participants selected a series \nof project proposals in order to build a shared scenario, which was subjected to evaluation in \norder to know the impacts generated by transformations. The two groups were then asked to \nnegotiate the two scenarios until a single strategic development scenario shared by all participants was reached. \n\nResults and Discussion \n\nA  participatory  process's  success  depends  on  numerous  organisations,  political,  technical, \ncultural and relational  factors. The research team conducted a crucial initial phase,  which \ninvolved activity organisation and the involvement of stakeholders selected to carry out participatory activities; a phase that was deemed necessary for the success of the experience.  \nThe participation process results were fundamental in guiding the research team in the definition of the integrated strategy, which was defined considering the overall framework of the \nproject (GALDERISI 2023). In particular, the preliminary knowledge phase, the participatory \nanalysis, the simulation of priorities and, above all, the Geodesign experience were taken into \naccount.  The  concrete  incorporation  of  the  suggestions,  indications  and  proposals,  which \nemerged in all the discussion-debate activities of the participative process, allowed the area \nstrategy to effectively address the needs and requirements of the communities living there, in \na medium-long term vision.  \nA participatory process like this has contributed to investing in local communities' human-social capital, promoting greater information, education, training and resulting awareness of \nthe actors involved on problems and possible solutions from a sustainable development perspective. Comparing with the PIPA approach, a feeling of common purpose and better programmatic integration (when more than one project is represented in the workshop) was also \ngenerated in the Medio Agri case study. However, work still needs to be done to empower \nlocal stakeholders and participants themselves to take the strategy forward. In addition, it has \nhelped to bring to life the 'think globally, act locally' approach, which broadens knowledge \nof the issues under discussion and their practical implications, especially with regard to innovative and digital participation techniques. On a professional level, experimentation of this \nkind has introduced methods for professional updating.  \nThe itinerant Geodesign attempted to restore the area's fragmentation, activating a virtuous \nparticipation process that involved citizens from different municipalities and made them interact. The itinerary was a learning process as it aimed to generate reflections to stimulate \npeople's awareness of the area in which they live, favouring attentive and at the same time \ncritical looks. \nA great deal of emphasis was placed on stimulating exercises in observing the territory and \nsocial practices in places, including 'heritage walks' in municipalities at each meeting. In line \nwith the most valid Italian and European experiences, the intent of the project was to translate \nthese actions into sustainable services useful to the reference context. It was an opportunity \nfor analysis and reflection on the issues of territorial government and urban transformations.  \nPublic administrations are direct beneficiaries of the project results, which can be included \nin the elaboration of local development programmes and projects. After the project ended, \nthis area was submitted as a 'pilot area' within the National Strategy for Inner Areas (SNAI) \nwith national relevance.  \nHowever, the Geodesign experience in the Agri Valley has its limits. In fact, although the \nworkshop represented a turning point in the multi-municipal planning of this territory, it did \nnot  succeed  in  bringing  urgent  global  issues  into  the  participatory  process,  but  rather  the \ncriticalities detected at the local scale prevailed. One of Geodesign's characteristics is that it \naddresses issues  at an international level (climate change,  globalisation, global population \nincrease), the so-called Megatrend phenomena. This limitation must necessarily be overcome \nin future Geodesign experiences because local communities must also collaborate in the development of strategic and sustainable planning. \n\nConclusion \n\nWhile the topicality and interest in Geodesign research is already producing promising results \nand case studies (PETTIT 2019, SCORZA 2020) there is not yet a wide diffusion of its application in planning, especially in Italy. There has been an advancement with the introduction of \nEsri's annual Geodesign Summit (ESRI’S GEODESIGN SUMMIT 2022), where the most up-to-date application, technology, education and theory of Geodesign meet, providing information \non the principles, methods and tools that shape this field. \nIt is a priority to consolidate and standardise the participatory processes currently practised \nat the various institutional levels, giving them greater clarity through common criteria. \nThe Geodesign approach can enhance the strategic and sustainable planning process, as sustainable development is considered in the modelling and evaluation of planning alternatives, \nwhich pays attention to issues of climate change, population growth, urbanisation and threats \nto biodiversity. Moreover, the multidisciplinary contribution and the stakeholder participation represent the fundamental point for a territorial co-design that, following a well-defined \nprocess, allows for a scenario shared by the participants.  \n\nGreen Space Intensity, Land Surface Temperature \nand Green Canopy Top Mapping: A Case Study in \nthe Suburban Settlement of Törökbálint, Hungary \n\n\nAbstract: Our research aimed to generate maps for an online decision support platform of the municipal \nwebsite of the town Törökbálint, in the Agglomeration of Budapest, Hungary. The town is rapidly developing, thus green space management, and urban heat island effect mitigation have become important \nissues. We used vegetation index, land surface temperature calculation method and image classification \nmethods. We processed satellite images, orthophotos and digital surface models to generate map layers \nshowing the vitality of green space, the surface heat average, and the height of green canopy top. The \nmaps were requested to be understandable for lay people and local decision-makers but relevant for \nexperts in spatial development, urban planning and town management. These requirements meet with \nseveral points of geodesign framework but there are further steps recommended based on the feedback \nof users. \n\nIntroduction \n\nThe moderate urban development and outward expansion in the agglomeration of Budapest \nresults in green space loss and growing urban heat island effect in small towns. The green \nspace changes, thanks to the moderate spatial development, appear sometimes in the heart \nbut mostly on the edge of the city (BMM 2017). The urban fringe meets with the suburban \nsettlements that are rapidly growing in population, intensifying in artificial land use types, \nand enhancing services and transport functions. \n\nMost of the studies mapping vegetation coverage use the well-known NDVI index (GIBSON \n& POWER 2000) while the heat island mapping studies use Land Surface Temperature (LST) \ncalculation (SOBRINO et al. 2004). The majority of the studies use both methods to document \nthe strong relation between vegetation coverage and surface temperature (BOKAIE et al. \n2016,  PARK & CHO  2016,  RANAGALAGE  et  al.  2017).  Many  studies use  these  methods  to \nanalyse land use or green space changes based on spatial development (SANNIGRAHI et al. \n2018, NASAR-U-MINALLAH 2020). Related to medium resolution of 30-100 meters, the satellite image-based indices are mostly used for bigger cities or metropolitan regions of more \nthan five hundred square kilometres by territory. Only a few of the studies use these for single \nsettlements, which are usually a few dozen square kilometres in size. This scale sometimes \nrequires high number of details, that needs a higher resolution, and higher accuracy of \nVHR images or LIDAR data (FLOHR et al. 2022). Some of the publications show that researchers are using surface models for mapping tree canopy cover (PARMEHR et al. 2016) or \nfor surveying development of potential green roofs (SANTOS et al. 2016). \n\nThe “VHR demand” leads to the use of occasional data representing coincidental situations \nduring the year, instead of representing the whole year or the vegetation period. The research \nmade at settlement level usually faces the dilemma of whether to use a high number of satellite  images  with  medium  and  high  resolution  for  low  price,  or  to  use  a  few  VRH  images \n(orthophotographs,  UAV  images)  with  a  higher  resolution,  high  costs  and  snapshot  situations. Some research combines data from different sources and resolutions like OSM and \nSentinel image for differentiating public and non-public green spaces (LUDWIG et al. 2021), \nor Sentinel and Landsat images for higher resolution surface temperature maps (ONAČILLOVÁ \net al. 2022). The research that applies spectral indices (e. g. NDVI, EVI etc.) combined with \nsupervised classification (KWAN et al. 2020) or that which uses thresholds to separate vegetation and non-vegetation sites (ARYAL et al. 2022) have also interesting results in mapping \ngreen space. \n\nPilot Site, Aims, Methods and Materials \n\nThe pilot site, the town of Törökbálint, has a territory of 29 km2 and 14 thousand inhabitants, \ngrowing yearly by 180 people in the last decade. The new residential and logistical areas have \nbeen  using  almost  1%  of  the  town’s  total  territory  annually  in  the  last  12  years  based  on \nUrban Atlas dataset. This is a typical suburban settlement in the Budapest Agglomeration \n(Fig. 1).  \n\nThe  municipal  government  and  the  mayor’s  office  run  a  website  (MUNICIPALITY  OF \nTÖRÖKBÁLINT 2022) with map server. It uses different layers for mapping natural capacity, \nsocial services and future developments of the town. The website is openly available for any \nuser, but it is mostly applied by architects, town planners and town management. It supports \ndecision-making simply through its geographic layers. There was a need for an easily understandable layer showing the intensity of green space in a range from 0 to 100%. The decision-makers and lay people prefer the simple range instead of the NDVI values from -1 to +1 with \nthe green cover range only from 0 minimum to 0.7 maximum. Additionally, the NDVI has \nnot been convincing for green space change studies in towns of Hungary in the last decades. \n\nOur research used vegetation index and thermal bands of satellite images to generate a 30m \nand a 10m resolution Green Space Intensity (GSI) map of the year 2022, and a 30m resolution \nLand Surface Temperature (LST) map of the years 2021-2022. The analysis was based on \nmany images of frequently used satellites (Landsat 7, Landsat 8, Landsat 9 and Sentinel 2) \nfocused on the vegetation period from 1 May to 31 September using “Collection2 Level2” \ndata from Earth Explorer website (U.S. GEOLOGICAL SURVEY 2022). All together 45 satellite \nimages were used to prepare the mean NDVI map that was the base map of green space \nintensity  generation.  Some  of  the  images  had  small  cloud  coverage  with  shadows.  These \nneeded special processing based on automatic and manual cloud and shadow removal actions.  \n\nThe mean NDVI map was adjusted geographically to VHR images prepared by the institute \nresponsible  for  remote  sensing  and  geodesy  in  Hungary  (LECHNER  KNOWLEDGE  CENTRE \n2022). The orthophotos, used for the geocorrection of satellite analysis results, were prepared \nin the years of 2015, 2016 and 2019 with very high geographic accuracy (+\/-0.5m), and very \nhigh spatial resolution (0.4m). The NDVI mean map prepared from satellites was calibrated \nto the infrared orthophotos and other real colour imagery of various sources (VHR images of \nGoogle Earth from 2019 to 2022). This calibration is based on the adjustment of NDVI values \nto the green  minimum and green  maximum using  visual  interpretation of VHR images in \ncontrol sites. Based on this process, the Green Space Intensity (GSI) map was prepared. \n\nThe Land Surface Temperature (LST) map was processed based on the most frequently \nused method (SOBRINO et al. 2004) using Landsat 7, 8 and 9 satellites of two years: 2021 \nand 2022. The reasons why we used images of two years (from May to September) were: \n1)  The summer of 2022 was extremely dry. For more than 50 days there was no precipitation in the town. The summer of 2021 was close to the average. \n\n2)  The number of usable images reached the required minimum (Nr. 10) only in the two years (2021 and 2022). In this case, the “usable image” means “cloud free” or “non-disturbingly cloudy (less than 40% cloud and shadow affected) images. \n\nAdditionally, a map that shows the height of “Green Canopy Top” was generated based on \nan orthophotograph and a surface model. Supervised image classification method based on \ntraining areas was used on infrared ortho imagery to generate green area coverage  maps \nwith the resolution of 0.4m. The classification used multiple training areas to identify green \nand non-green areas (built-up, paved, bare soil and water). The Normalized Digital Surface \nModel (NDSM) is prepared every three  years by the remote sensing institute of Hungary \n(LECHNER KNOWLEDGE CENTRE 2019) for the territory of the country. It is a practical side \nproduct of orthophoto generation based on stereophotographic analysis. It  shows only the \nheight of the landscape elements above ground (buildings, vegetation and built structures), \nas the terrain model was deducted from the surface model. That is why it is called “normalized” surface model. With the coupled use of green space (classified image) and height of \nlandscape elements (NDSM) the height of Green Canopy Top map was generated (Fig. 3). \n\nResults \n\nThe maps, as a result, became part of the municipal website of the town Törökbálint, openly \navailable through the municipal online platform (MUNICIPALITY OF TÖRÖKBÁLINT 2022): \n1)  Green Space Intensity (GSI) map 2022 (30m res., based on 13 Landsat images) \n2)  Green Space Intensity (GSI) map 2022 (10m res., based on 32 Sentinel images) \n3)  Land Surface Temperature (LST) map (30m res., 33 Landsat im., years: 2021-22) \n4)  Green Canopy Top (GCT) map 06.15.2019 (1m res., based on orthoph. and NDSM) \n\nThe Green Space Intensity (GSI) is generated, as described in method chapter, by calibrating \nNDVI values. Green Space Intensity (GSI) shows what is the territorial ratio and vitality of \nvegetation in the site. The numeric values were described as percentage (%). The zero percent \nGSI is representing areas without green coverage. Hundred percent GSI means the existence \nof total green coverage in good health condition during the vegetation period (mostly tree \ncovered parks, woodlands or forest patches). The validation of GSI values was done at 70 \ndifferent locations in 210 sampling boxes (Fig. 2). The visual interpretation was based on \nsix different images using infrared and real colour, summer and early spring VHR images \nfrom the years 2019-2022). The average difference between the visual interpretation of green \ncoverage and the GSI calculations was less than 5%. \n\nThe Green Space Intensity maps summed up 64% and 65% (GSI) for the whole territory of \nthe town. Thus, these maps based on different sources and resolution (Landsat and Sentinel) \nhad only a minor (1%) difference. Based on the Green Space Intensity maps with zonal statistics function of QGIS software, it was possible to describe and illustrate that: \n• the forests and the orchards or vineyards (here mostly abandoned) have the highest GSI values (95% and 86%) based on the polygons of Urban Atlas dataset, \n\n• the GSI value of residential area types (medium, dense, continuous) range from 55% to 62% while the industrial and commercial areas have only 39% GSI, \n\n• the average GSI of rural areas (mostly grasslands and few plough lands, which are partly abandoned) vary from 66% to 73%, \n\n• there is surprisingly high ratio of green space along railroads (69%) but it is moderate for roads (49%) and is especially low in case of highway roads (34%). \n\nMost of the “action areas” defined by the settlement development strategy have 40-44% GSI, \nbut the action area Nr. 8 had 74% GSI in 2022. This is very high value for a future industrial \npark  development  site.  Based  on  this  result,  the  reconsideration  of  future  use  was  recommended. Luckily the centre of the town, and the sites of historical character, defined by the \ntown’s guidebook, are located in a small valley, and have good GSI values (55-60%). The \naverage GSI in the 30m neighbourhood of factory buildings is very low (27%) and it is the \nhighest nearby  holiday dwellings (63%). The residential building  surroundings (within 30 \nmeter) have 54% while the public buildings have only 46% GSI. \n\nBased on the Land Surface Temperature map used as dataset with zonal statistics function \nof QGIS software, it was possible to describe and illustrate that: \n\n• the coldest surface is the only lake of Törökbálint (21°C, 70°F) and the green corridors along the creeks (23-24°C, 74-75°F), that is “warm”, considering that this is an average value of 5 months in the middle of the year (recorded at 11:33 am); \n\n\n• the warmest surface is the so-called DEPO area which is the biggest warehouse area in the settlement, and the average value is 34°C, (93°F), while the extreme single values in the middle of the summer are higher than 40°C (104°F); \n\n\n• the former military site with an average of 30°C, (86°F) is among the warmest areas; \n\n• the action area Nr. 4. is planned to be an intermodal transport hub but is currently an arable land crossed by a creek with linear forest and shrubland providing moderate GSI (44%) and low temperature average (25°C, 76°F) for the total action area. \n\nThe  supervised  classification  results  (of  VHR  infrared  ortho  imagery)  generated  a  40  cm \nresolution layer showing green surface (and non-green surface). This dataset was suitable: \n1) to map green spaces in the town with high spatial resolution and accuracy, \n2) to measure the ratio of green space for districts, residential blocks and plots, \n3) to combine with surface model and to prepare the “Green Canopy Top” (Fig. 3). \n\nIt is necessary to mention that the classified green space map, prepared from a single VHR \northo image, provided high spatial details but did not offer temporal average and thus it did \nnot represent the complete vegetation period. It is a snapshot from 15 June 2019, which is \nusually the most vital part of the year considering the chlorophyll content of the vegetation. \n\nA strong correlation was illustrated between the green spaces and the areas represented by \nlow Land Surface Temperature. Most of the forests had an average value of 23°C (73°F), but \nmost  of  the  hill-land  grasslands’  averages  were  already  30°C  (86°F),  because  of  the  extremely dry summer and very thin layer of the soil. Compared to the wetlands and lake area \nof 21°C (70°F), the industrial and commercial  sites had 33°C (91°F) (Fig 4). The LST is \nusually higher in cases where the Green Canopy Top (GCT) is lower, and it is lower where \nthe canopy top is high. In case of grasslands and plough lands, GSI is high in average (65-70%) but the Green Canopy Top is very low (0-2 meters) and the LST values are outstandingly high (27-28°C, 81-82°F). This suggests that the GCT can have more influence on LST \nthan GSI. \n\nDiscussion \n\nThe Green Space Intensity maps and the Land Surface Temperature map were overlaid in \nGoogle Earth Pro software as a top layer to provide a perspective 2.5-dimensional view with \nvisual graphic effects to make it comparable with photorealistic elements (Fig. 5).  \n\nFive years ago, a complete country scale map was prepared with 10m resolution using NDVI \nand Leaf Area Index (LAI) data of Sentinel satellite transformed to a Greenness Indicator \nfor Hungary based on 2017 images (KOLLÁNYI et al. 2019). That research used only few \nimages prepared on three different days within the year. The map is good for country level \nbut needs to be enriched in the number of processed images for the settlement level. Our \nactual research in Törökbálint shows the best practice of using mean value maps for green \nspace and surface temperature analysis. This enhanced method was tested and elaborated in \nother cities and towns of Hungary in the last decade (Budapest, Debrecen and Szeged). Our \nresearch is innovative in Hungary in preparation of the Green Canopy Top map, that applies \na combined use of two data sources and big amount of VHR data. The future development \nof this analysis is highly recommended as green infrastructure plans must be prepared for 81 \ntowns of Hungary in the near future. The type of analysis we made could be a crucial part of \nthese plans. \n\nIn our research, the application of remote sensing methods contributed to the general town \nplanning process in Törökbálint and is linked to most of the topics related and published \nwith geodesign (DEBNATH et al. 2022). It was decided to protect and partly enhance the old \ntown character area’s moderate green space intensity. The further use of two action areas (Nr. \n4 and 8 on Fig. 5) defined by the settlement plan started to be reconsidered and revised as \nthese areas have strong roles in green system, and an outstanding role in urban climate mitigation. \n\nWe conducted to fill the website of the town Törökbálint with map layers, illustrations and \nwritten explanations for decision support purposes. Our research generally fits to the Geodesign framework of STEINITZ (2012), even if some of the approaches are not completely covered. In our research, the stakeholders and locals as the “people of the place” are free to read \nor view the results. The data was converted to be understood easily by the decision-makers \nbut there were only a few occasions when the opinions turned into direct feedback. \n\nOur research is  missing the purpose of  modelling and simulation. The users, stakeholders \nneed to point out the risks, threats and opportunities themselves that should be integrated in \nthe geodesign process ideally from a technological viewpoint (ERVIN 2016) e. g. with scenario modelling. Based on the scheme of our maps a worst-case future scenario and a more \nheat-resilient scenario with different suburban residential types could be prepared and lead \nto a more complex and interactive decisions-support system. \n\nEven if there was limited feedback it was clear that the research should be enhanced further \nwith: \n1) green space per capita calculations \n2) shaded area calculation, or a special calculation for under-canopy layer volume\n3) canopy volume calculation, with related field survey. \n\nConclusion and Outlook \n\nBased on the maps, derived from satellite images, the Green Space Intensity and Land Surface Temperature were determined for different land use types, building surroundings, character areas and action sites. The supervised classification of VHR image and surface model \nprocessing was used for mapping the height of a Green Canopy Top. In our future research \nthis could be a good base map for per capita calculation, for shaded area measurement, for \nunder canopy volume or canopy volume analysis. We illustrated the relations of green space, \nsurface temperature and canopy height in a diagram of cross section and in perspective visualisations. All maps were integrated in the municipal website of town development. In the \ntown of Törökbálint the research results are can support decisions about land use and green \ninfrastructure planning, considering urban heat island mitigating goals as well. \n\nLandscape Performance Related Factor Correlations \nin Small Public Spaces: Structural Modeling Applied \nto Nanjing Subway Entrances \n\nAbstract: Landscape performance is an important topic in contemporary landscape architecture. The \nrenewal of small-scale public spaces has a positive effect on community vitality and the sustainable \ndevelopment of landscape resources. To improve the quality of small-scale public spaces, a clear understanding of landscape performance and its correlative factors is necessary. The structural equation \nmodel (SEM) provides a way of objectively evaluating the influence of public space on landscape performance, which could be useful in the design decision-making process. This study develops a methodological framework that implements SEM to assess landscape performance and provides a case study \nof typical small-scale public spaces represented by subway entrances in Nanjing. Five latent factors and \n11  observed  factors  were  selected  to  construct  a  landscape performance  assessment  model. Various \nmethods, including investigation, image recognition, and modeling, are exploited to analyze the characteristics of the subway entrances quantitatively. An SEM-based calculation is conducted to obtain the \ncorrelation coefficient of latent factors and the explanatory degree of observed factors. Statistical analysis indicates that the green space and traffic capacity affect the landscape performance of the subway \nstation entrances. Moreover, the fluctuation of any latent factor may cause the decay or enhancement \nof related factors. Therefore, we suggest that design strategies for subway station entrances should balance traffic, visibility, plants, and facilities. More importantly, this study works as a technical support \nand reference point for future empirical research to obtain an in-depth understanding of the ideal landscape construction forms in small-scale public spaces making it possible to predict the level of landscape performance when a new study area is given. \n\nIntroduction \n\nContemporary landscape architecture research has gradually shifted from the intuitive perception  and  empiricism  of  traditional  research  to  the  scientific  research  and  judgment  of \nquantitative analysis. Moreover, many researches focus on the sustainable utilization and renewal of urban public landscape resources (BROWN & CORRY 2011). Increasing attention has \nbeen paid to the scientific cognition and quantitative assessment of landscape renewal and \nthe transformation of small-scale public spaces. Therefore, landscape architecture must address the challenges of urbanization and improve the technology of design decision making \n(PATTACINI 2021). \n\nAn urban subway station entrance is a typical example of a small-scale public space. It maintains coordination with urban public spaces and provides a generous landscape experience \nfor residents. However, the landscape potential of small-scale public spaces has been critically  ignored  in  previous  constructions.  Many  small-scale  public  spaces  are  isolated,  restrained, and pedestrian-unfriendly due to the lack of overall design and planning (DOULET \net al. 2017). It is important to scientifically analyze the landscape composition forms of small-scale public spaces to propose universal design principles  and strategies for landscape renewal. \n\nLandscape performance is a measure of the effectiveness of landscape practices in achieving \nthe expected objectives under the premise of sustainable development (LUO et al. 2021). Previous studies have focused on the selection of performance evaluation factors and measurement methods and the description and comparison of these factors. However, some of these \nfactors are theoretically abstract. Few studies have systematically and objectively covered \nthe construction mechanism of landscape space. In factor analysis, structural simplification \nof these latent factors is an indispensable procedure for solving estimation problems.  \n\nAs an effective multivariate statistical method, the structural equation model (SEM) can measure factors that cannot be directly observed by transforming them into observable factors and \ncan verify the path loading between latent factors and observed factors by harnessing a covariance matrix (LIVOTE & WYKA 2009). Traditional methods, such as the analytic hierarchy \nprocess, fuzzy comprehensive evaluation, and entropy weight method, suffer from over-reliance on expert opinions and low accuracy of the factor weight calculation. Compared to these \ntraditional methods, a factor weight calculation based on SEM is more accurate because it \ncan calculate the factor structure and factor relationship at the same time. Moreover, an indicator is allowed to be affected by one or more factors. In addition, this approach improves \nmodel production efficiency by providing a verifiable theoretical framework and graphical \nmodel (HUANG et al. 2021).  \n\nTherefore, to address the aforementioned challenges, we propose a novel approach to achieving the quantitative evaluation of landscape performance in small-scale public spaces. We \nutilize multi-source data to extract the features of public spaces and evaluate the influence of \nthe observed and unobserved factors on landscape performance using SEM. The urban subway station entrances of Nanjing are selected as a research case. The results, including recognizing the importance of  correlative factors and proposing design strategies for subway \nstation entrances, provide technical support and a reference point for the renovation of similar \nsmall-scale public spaces, such as streets, pocket gardens, and public squares. \n\nMethods \n\nThis study proposes a method for assessing landscape performance based on SEM. It can \nanalyze  the  weight  of  each  factor  related  to  landscape  performance  of  small-scale  public \nspaces in a quantifiable way and can reveal the relationship among latent factors. The methodology used consists of four steps (Fig. 1): (1) establishing the theoretical model, (2) data \ncollection, (3) parameter estimation, and (4) correlation analysis. \n\nEstablish the Theoretical Framework \n\nThrough the literature review and the physical properties of subway entrance, the influencing \nfactors on the landscape performance can be summarized into two categories: space character \nand construction factor. Specifically, traffic capacity is the main function of subway entrance, \nand visual openness reflects the spatial form of subway entrance. They jointly illustrate the \norganizational relationship of the landscape space (AZIZ 2020). Greening and facilities, as \nthe  physical  components  of  subway  entrances  (CATHERINE  et  al.  2013,  PESCHARDT  & \nSTIGSDOTTER 2013), mainly reflect the service capability.  \n\nNext, these latent factors were subdivided into ten observation factors. As shown in Figure \n2, the observed and latent factors are represented by rectangular and circular boxes, respectively. The “dependency” relationship between factors is represented by a one-way arrow. \nThis  model  is  the  basis  for  all  subsequent  data  analysis.  The  meanings  and  measurement \nmethods of each observation factor are shown in Table 1. \n\nNote that landscape performance exists as a “high-order factor” in the model, which can fully \nexpress the relationship between the first-order factors, namely, the four latent factors. Thus, \nit is assumed that all observation factors can be combined into a total score for landscape \nperformance. In the process of data analysis, a hierarchical and structural evaluation model \nis established on the basis of measuring real environmental data, objective weighting, and \nstructural characteristics of factors. As a result, it can reflect the operational logic of landscape performance in a subway entrance. \n\nData Collection and Preparation \n\nConsidering the construction situation of the existing subway entrances in Nanjing, we selected 131 entrances as survey samples, such as, Gulou subway station and Jimingsi subway \nstation (Fig. 3). The data collection methods can be classified into three categories: mapping \ndata based on computer-assisted auditing, environmental data based on image recognition, \nand environmental audits based on field investigations (Fig. 4). \n\n(1) Traffic capacity \n\nThe observed factors of “traffic capacity” and “visual openness” are connectivity and visibility level, respectively, both of  which reflect the spatial organization efficiency of subway \nstation entrances. Space syntax is an important tool for simulating the structure and function \nof space. It is exploited to reflect the features and trends of walking behavior (HILLIER & \nHANSON 1984). Therefore, we harnessed Depthmap to build an axis model to obtain the average connectivity data (Fig. 5).  \n\n(2) Visual openness \n\nThe acquisition of visibility was achieved in a similar way to that of the previous factor. The \ninvisible occlusion area and landscape elements obstructing the view, such as wind pavilions \nand plants, are defined. Then, Depthmap is used to obtain a visibility graph (Fig. 6). \n\n(3) Greening quality \n\nThe latent factor “greening quality” is composed of four observed factors. Data collection for \nthese factors relies on photography and measurement. The photographic equipment includes \na SLR camera, wide-angle lens, and fish-eye lens. First, a panoramic picture was shot using \nthree observation points. A Canon wide-angle lens was used to simulate the visual field of \nthe human eye. In the shooting process, the shooting position of the camera was set at the \nmiddle line of the road, and a visual height of 1.5 meters  was taken as the benchmark for \nshooting.  \n\nAfter taking photos on the site, both the visual green quantity and canopy rate use semantic \nsegmentation  technology  to  determine  the  proportion  of  green  space  pixel  values  in  each \nphoto (Fig. 7). We used Photoshop to correct the distortion and semantic segmentation technology, that is, the SegNet recognition model (BADRINARAYANAN et al. 2017), to identify \nimage composition. The pixel points in the pictures were identified as elements such as sky, \nroad, buildings, and greening. Next, the percentage of pixel values of plant communities in \neach picture was calculated, and the visual green quantity data were obtained by calculating \nthe average value of the observation points. \n\nFor the measurement of the canopy rate, three observation points in the path of pedestrians \nentering the subway station were selected, and a fish-eye camera with a 180° angle was used \nto shoot fish-eye images at a height of 1.5m from the observation point (Fig. 8). Image semantic segmentation was also carried out to calculate the proportion of tree crown pixels at \neach measurement point. \n\n\nAs for the measurement of variety richness, we counted plant varieties and measured the area \nvolume of the sample. This index was calculated using the Gleason (1922) index. The shading rate of the walking space was obtained by calculating the ratio of the projected area of \ntrees to the total area of pedestrian space. \n\n(4) Service capacity \n\nTo measure the service capacity of landscape facilities, this study investigated the number \nand distribution of seat facilities, barrier-free facilities, and parking spaces by field investigation (Fig. 9). To measure the pavement integrity of the pedestrian space, the ratio of the \nwell-preserved pavement area of the pedestrian space (excluding damage, cracks, warping, \nfracture, etc.) to the total pedestrian area was calculated. \n\nParameter Estimation and Fitness Evaluation \n\nAfter collecting all samples, we imported the original data into the SPSS software for normalization to eliminate the influence of different dimensions. Then, 131 samples were randomly divided in half by SPSS for exploratory factor analysis and SEM analysis.  \n\nWe imported the normalized data into SPSS 24.0 software for the exploratory factor analysis. \nSince traffic capacity and visual openness contain only a single observed variable, we assume \nthat  there  is  no  unexplained  measurement  error  in  these  two  latent  factors.  Therefore,  we \nfocus on exploratory factor analysis among service capacity and greening quality, and their \ncorresponding observed factors. \n\nThe results of the Kaiser-Meyer-Olkin test and the significance test (< 0.05) showed that the \nobserved factors were correlated. As shown in Table 2, the component extraction results of \n“service capacity” and “greening quality” were consistent with the theoretical model. Since \nthe coefficients were higher than 0.9 and the construct reliability result was acceptable, we \nregard  the  latent  factors  as  interrelated  with  the  corresponding  observed  factor  group. \nThrough exploratory factor analysis, the hierarchical relationship of the index system  was \npreliminarily determined. \n\nThen, AMOS software was used to run SEM analysis to verify the index system. In the evaluation of absolute fit indices, the model passed the confirmatory analysis. Specifically, the \nCFI was 0.927, which meets the standard requirement that it should be greater than 0.9. The \nRMSEA was 0.075, which was slightly less than the critical value of 0.08. Among the relative \nfit indices, both the IFI and PNFI met the adaptation standard of 0.9. \n\nThe model fit results show that landscape performance was positively correlated with service \ncapacity, greening quality, visual level, and connectivity to varying degrees. Following the \npath coefficient, the effect intensity of the observation factors on landscape performance was \ndetermined (Fig.10). \n\nFinally, the path coefficient can be used for the weight of each factor to predict the level of \nlandscape performance  when a new study area is given. Furthermore, researchers can improve landscape performance and put forward targeted design  strategies  from the  four dimensions of space, vision, green space, and facilities. \n\nResults and Discussion \n\n(1) Greening quality significantly affects landscape performance. \n\nThe model fit results show that the factors had a positive effect on landscape performance: \ngreening  quality  (standard  path  coefficient  0.93),  connectivity  (standard  path  coefficient \n0.82), and service capacity (standard path coefficient 0.71). This finding confirms the importance of greening quality and transportation in the landscape design of subway entrances. \nThe visual level of plant communities is the most important influence on the landscape performance of subway entrances. \n\n(2) Correlation among the latent factors of subway entrances \n\nThere is an interaction among the latent  factors. When one landscape factor is  weakened, \nother landscape factors may change accordingly. For example, traffic capacity had a negative \neffect on service capacity (path coefficient -0.33). The smaller the proportion of impassable \nareas (green space, non-motorized parking lot) in the landscape space, the more balanced the \nsegmentation of the traffic path to the overall space. As a result, the better the connectivity \nof the entrance, the more ideal is the traffic capacity. \n\nThe permeability of the external observed variable along the street interface inhibits “greening quality” (path coefficient -0.50). This demonstrates that the main factors affecting the \npermeability  of  the  subway  entrance  are  not  only  the  landscape  conditions  of  the  subway \nentrance itself but also the external urban spatial forms, such as road greening. This finding \nsuggests that the relationship within the outer urban green space should be considered in the \ndesign of subway entrances. \n\n(3) High-level interpretation of observed and associated latent factors \n\nIn  the  measurement  model,  the  observed  factors  were  able  to  comprehensively  depict  the \nlatent factors. Visual green quantity (path coefficient 0.92) and canopy rate (path coefficient \n0.91) had the largest contribution to the greening quality of the subway entrances. This result \ndemonstrates that the three-dimensional green quantity indexes, namely, “visual green quantity” and “canopy rate,” can better reflect the performance of the green space in subway entrances than a two-dimensional quantity index can. \n\nThe factors that dominated the service facilities were the pavement integrity of pedestrian \nspaces and the capacity of parking facilities. First, by investigating at subway entrances, we \nfound that the high-frequency activities in the entrances were shown to be traffic and transferring, while the activity frequency of recreation behaviors were relatively low. This means \nthat the rest facilities were not the main facilities. Second, the main activity scenes for users \nwere the squares and roads, which reflects the importance of pavement integrity in subway \nentrances. Through the survey, it was found that the main way for people to arrive at a station \nwas by walking or cycling, which gives rise to the high-density non-motorized parking demand. \nTherefore, improving pavement quality and parking design can promote traffic efficiency. \n\nConclusion and Outlook \n\nIn this work, we proposed a methodological framework that implemented SEM to analyze \nthe correlation between landscape performance and four latent factors in small-scale public \nspaces. Using the path diagram in the SEM model, we developed a landscape performance \nevaluation system and discussed the design strategies of subway station entrances. With the \nsupport of multi-source sample data, we summarized the regular characteristics and formed \nan intuitive description of the landscape performance of subway station entrances in Nanjing. \n\nIn comparison with previous studies, this study offers improvements in three respects. First, \nit allows researchers to objectively measure the latent factors in small-scale public spaces by \nanalyzing physical environment  factors. Second, our  use of  SEM technology to avoid  the \nsubjective  and  descriptive  defects  of  traditional  performance  evaluation  provides  a  novel \nquantitative way for landscape performance research. Finally, our framework can be used to \ncarry out retrospective analyses of landscape design and help designers identify design defects. \n\nThis study has certain limitations. Because the case study comprises subway station entrances \nin Nanjing, it cannot be guaranteed that the statistical results are generalizable to other small-scale public spaces. The factor sets of landscape performance still need to be expanded by \nadding indicators related to the behavior patterns of pedestrians. Notwithstanding its limitations,  this  study  provides  a  scientific  basis  for  proposing  design  strategies  for  small-scale \npublic spaces, especially subway station entrances. Our ongoing work is to propose evaluation thresholds according to the data distribution to determine whether a design performs well \nor not, and to  apply the results to more projects and to analyze  more  types of small-scale \npublic spaces. \n\nApplying digital technology in landscape performance research is an important symbol of \ncontemporary  development.  To  improve  the  landscape  environment,  researchers  should \nadopt mathematical models with parameter systems, strengthen logical analysis, and improve \nestimation methods. This study provides an example and a basis for landscape performance \nevaluation using SEM. By applying our method to other small-scale public spaces, the analysis and measurement of the rules of landscape performance can be achieved. The study also \nprovides strategies for landscape optimization design in small-scale public spaces. \n\nWalking in the City: An Experimental Pedestrian \nStress Test for Marienplatz in Stuttgart, Germany \n\nAbstract: In an experimental study with 15 participants, stress measurements of pedestrian traffic were \nconducted in the urban environment of Stuttgart's Marienplatz. Stress was measured with sensor wristbands.  In  addition,  standardized  questionnaires  were  used  to  incorporate  personal  dispositions  into \nstress generation. Heat maps, group comparisons, and exploratory cluster analyses were used to obtain \ncircumstantial evidence of stress emergence. Four stress factors were identified (lack of space, interruption of the desired line, noise, and quality of infrastructure). Individual factors associated with stress \nhere include gender, knowledge of the place, and psychological factors. Cluster analysis also suggests \nthree groups that are homogeneous in their experience of stress. The results of the study will be used to \nbetter understand stress as an inhibiting factor for choosing walking as a mode of transportation. In the \nfuture, the findings can help to better integrate the human scale into the data collection and implementation of (digital) urban planning projects. \n\nIntroduction \n\nWhen looking at the use of road space by road users, it quickly becomes apparent that pedestrian traffic remains a residual factor and has so far received rather scant attention (Umweltbundesamt 2018). However, walking has not only become more important in leisure time, \nbut walking is also becoming more common as a means of transport (KNIE et al. 2021). This \nraises the question of the relevance of good pedestrian infrastructure. The positive effects are \nmanifold. Walking is  healthy (UMWELTBUNDESAMT 2018) and does not contain financial, \nlinguistic or cultural obstacles. Walking promotes the revitalisation of public space and ensures participation in public life (AUSSERER et al. 2013). Strengthening walking as a means \nof transportation can also contribute to a reduction in air and noise emissions. The increase \nin walking is inhibited by factors such as air quality, urban planning obstacles, accident risks \nwith other modes of transport or even old habits in the choice of transport (UMWELTBUNDESAMT  2018).  The  role  of  walking  is  particularly  present  in  cities.  Dense  development, \nhigher noise and pollutant emissions, and increased traffic volumes impact the everyday life \nof pedestrians (GEHL 2018).  \n\nAnother prominent dynamic in the urban context is the steadily increasing incidence of stress-related diseases. In addition to the prevalence of cardiovascular diseases, mental illness such \nas  depression  are  increasingly  observed  (ADLI  2017).  Subjective  factors  related  to  stress \nsometimes influence, even  unconsciously, the choice of transport (ZEILE et al. 2021). The \nincreasing  global  urbanization  underlines  the  relevance  of  knowledge  generation  about \nstressors in urban space. Against this background, the focus will be on pedestrian traffic. The \npromotion of pedestrian traffic is a central component of future urban and transport planning. \nTherefore, the aim of this thesis is to answer two research questions: \n\n1. Which structural and social factors lead to the emergence of stress among pedestrians in \nurban spaces? \n\n2. What role do social and psychological characteristics play? \n\nThe practical part of the measurement of pedestrians’ stress was conducted at Marienplatz in \nStuttgart. This location is interesting for answering the research questions because it is characterized by a high mix of uses and its status as a transportation hub, making it well-suited \nfor the study. The study can be considered a prototype for further research in the area of urban \npedestrian stress. \n\nThis study is embedded in the (NRVP-) project “Cape Reviso” which investigates the causes \nof conflicts and stress in pedestrian and bicycle traffic in order to be able to address them in \n(digital) traffic planning (WÖSSNER et al. 2020). \n\nTheoretical Background \n\nStress, an Emotional Construct  \n\nStress theories differ in terms of adaptability and operationalizability. Stress always arises \nwhen, depending on the theoretical underpinning, the physique (stress-as-a-reaction) or psyche (stress-as-a-stimulus or stress-as-a-transaction) has to muster resources to process environmental stimuli. The most prominent is the transactional stress model according to Lazarus (1999), but at the same time, it is also the most complex, since stress always arises situationally in the interaction of people and the environment. On the other hand, there are stress-as-a-reaction models (Selye 1956, Cannon 1932), which examine the physical reactions to an external stimulus (BERCHT 2013). Critical here is the assumption that a stimulus “stresses” \nall people equally (LYON 2005). The third group of theories, which understands stress as a \nstimulus, focuses on the psychological effects. Here, it is assumed that there are “critical life \nevents” (HOLMES & RAHE 1967) that objectively trigger stress to some extent (BERCHT 2013). \nDue to developments in emotion research in this field, the stress-theoretical basis of this paper \nis more in line with models from the stress-as-a-reaction perspective. However, subjective \ncomponents are included in the data collection and analysis.  \n\nThe Mobility Behaviour of Pedestrians \n\nHuman perception, and thus also (mobility) behavior, is determined by several factors. These \ncan be divided into exogenous and endogenous influences. If the influencing factors originate \nfrom the environment, we speak of exogenous factors. In this case, the built or natural environment stimulates human sensory perception (ŚLESZYŃSKI 2012). Based on empirical findings, in this work, these are divided into lack of space (confinement), resistance (barriers), \nthe quality of the environment and noise. These directly form the basis for the working hypotheses. \n\nLack of space: Insufficient width of sidewalks leads to insufficient distances during encounters and a high density of people. The distances to the roadway or the presence of obstacles \ncan also trigger a feeling of crowding (ŚLESZYŃSKI 2012). In addition, so-called near misses \ncan influence pedestrians' subjective perception of safety. The critical value is 150 cm, which \ncorresponds to the legal minimum distance for overtaking (ZEILE et al. 2021). The lack of \nspace can thus trigger stress in pedestrians if the situation is assessed individually (SPIEGEL \n1992). (H1): Lack of space leads to stress among pedestrians. \n\nBarriers: Pedestrians, aim for a route that connects their starting point to their destination as \ndirectly  and  briefly  as  possible.  The  preference  for  short  and  direct  routes  often  leads  to \nshortcuts where convenience is more important than safety. Pedestrians use this imaginary \nroute or “desire line” (ŚLESZYŃSKI 2012) to get around. The response to a disruption in the \nindividual's desire line is to increase speed,  wait, or swerve to  maintain personal distance \n(LEE & KIM 2017). Barriers change the perception of space and influence individual desire \nlines. This can trigger stress (SPIEGEL 1992). (H2): The interruption of the desire line leads \nto stress among pedestrians. \n\nNoise emissions: The effect of noise can be equated with a stress reaction from a volume of \n65 decibels. Stress hormones are released and blood pressure and heart rate increase. Continuous noise can trigger illnesses such as hearing damage, cardiovascular disease and psychological problems (ADLI 2017). (H3): Noise emissions lead to stress among pedestrians. \n\nQuality of environment: The condition of the environment correlates with pedestrian stress \nresponses. Environments characterized by a high mix of uses or industries may be associated \nwith the experience of stress (LAJEUNESSE et al. 2021). Furthermore, unclear routing\/guidance systems (AUSSERER et al. 2013), slopes, suboptimal road surfaces or poor visibility also \ncontribute to a devaluation of the quality of the walking infrastructure, which can discourage \npeople from walking and, in extreme cases, cause negative emotions. The demands on the \nquality of the environment can vary depending on age, mobility impairment, knowledge of \nthe place, or the purpose of the trip (ŚLESZYŃSKI 2012). (H4): Low quality of infrastructure \nleads to stress among pedestrians. \n\nEndogenous factors, “the inner milieu”, also determine the strength of the effect of stressors \nby mitigating or increasing the likelihood of stress occurring. In terms of gender, women are \nthought to experience stress more quickly than men. A walking or visual impairment can also \npromote stress (DÖRRZAPF et al. 2014). Furthermore, an advanced age tends to favor stress. \nRegarding the purpose of travel, commuters are assumed to be more stressed because they \nprefer a fast and direct connection to their destination (SCHOON 2010). If a mode of transportation is used on a daily basis, it is assumed that stress is less likely to be induced if the mode \nof transportation is chosen voluntarily. Not only the familiar mode of transportation but also \nthe  familiar  environment  can  influence  the  occurrence  of  stress.  Stress  is  triggered  more \nquickly in unfamiliar environments than in places that are frequented more often (AUSSERER \net al. 2013). Finally, psychological predispositions may have a supporting or reducing effect \non the stress response (SCHANDRY 2016). These psychological factors include personality, \ncontrol beliefs, and risk tolerance. The relevance of including psychological characteristics \nis also based on the importance of individual evaluations of stressors (KOVALEVA et al. 2012). \n\nMethodology \n\nThis paper attempts to draw as comprehensive a picture as possible in answering the question \nby using triangulation between methods (FLICK 2008). The subject of the study, the stress of \npedestrians,  is  investigated  qualitatively  and  quantitatively.  Triangulation  was  used  in  the \nstudy for data collection (standardized as well as open questionnaire, sensor data), analysis \n(spatial and statistical) and interpretation.  \n\nThe dependent variable of this empirical study is the emotional construct stress. Measuring \nstress in terms of response is feasible, although theoretical limitations must be accepted. Biological indicators are used to identify moments of stress. When confronted with a stressor, \nthe human organism regulates endogenous stress responses to establish homeostasis. These \nresponses are detectable through a variety of body-related parameters and are recognized as \na proven method for measuring stress from external stressors. These include the increase in \nelectrodermal activity (EDA) and the decrease in skin temperature (KYRIAKOU et al. 2019, \nSCHANDRY 2016). Based on the functioning of these biosignals, Kyriakou et al. (2019) developed an algorithm that can detect people's moments of stress (MOS) using wearable biosensors. The biosensor wristband “E4” from the company Empatica was used to measure the \nbiosignals. The data is collected in an app (e-diary) on a smartphone. The result is a database \nin which one line corresponds to one second of the measurement period and provides information about a MOS (yes\/no) and its geographic coordinates, which can thus be read and \nvisualized in a geographic information system (GIS). To check whether the exogenous stressors  influence  the  MOS,  a  mixed-methods  approach  was  chosen.  Camera  recordings  were \nused to examine causal factors for the occurrence of a MOS. \n\nA standardized questionnaire based on validated scales of mobility in Germany (2019) from \nthe BMVI (NOBIS & KUHNIMHOF 2018) and the Leibniz Institute for Social Sciences (GESIS) \nwas developed to record endogenous factors. The questionnaire was used to collect information about the person and his or her sociodemographic background as well as psychological characteristics in addition to (pedestrian) traffic behavior. The personality of individuals \nis traditionally determined on the basis of the so-called Big Five, which consists of the characteristics  extraversion,  neuroticism,  openness,  conscientiousness  and  agreeableness.  The \nBig Five are considered to have good predictive power for certain aspects of life. The level \nof control beliefs describes a person's belief that he or she has control over various situations \nand that these are the result of his or her actions (internal) or that fate, coincidences, or powerful others are responsible for the occurrence of certain events (external) (RAMMSTEDT et al. \n2012, KOVALEVA et al. 2012, BEIERLEIN et al. 2014). The degree of control belief is a relevant \nfactor in the evaluation of a stress reaction (BROSSCHOT et al. 1994). \n\nThe study to  measure pedestrians'  stress around Stuttgart's  Marienplatz took place in July \n2021. According to conflict points between road users and an analysis of accident sites involving pedestrians, a route of three kilometers  was defined for the measurement. The 15 \nparticipants were asked to complete the questionnaire in advance. After the test run with the \nwristband and the cameras, participants were subsequently given a questionnaire ex-post on \nwhich they could mark subjectively perceived stressful or relaxed locations. \n\nFor the data analysis, a so-called global heat map of all MOS was first created. Subsequently, \nvarious further heat maps were created according to hypotheses and the endogenous influencing factors in order to obtain a more differentiated picture. This makes it possible to gain \ninsight into the locations where a large number of MOSs of different individuals occurred. \nFinally, these were also compared ex-post with the information from the questionnaires. Further analyses focusing on the endogenous factors of the participants were performed by appropriate statistical analyses. In the first step, an overview of the distribution of the expressions was obtained. For this purpose, the distributions of the expressions and the MOS were \nexamined (cf. Table 1). The table shows extreme values in parts. These should be read against \nthe background that this is not a representative sample and that the homogeneity of the group \ncan be classified as quite high. \n\nFor further analysis of the relationships between the endogenous factors and the development \nof MOS overall and subdivided by exogenous factors, mainly group comparisons were performed. For this purpose, the percentage of MOS is compared  with the percentage of the \nsample. The sign of the difference can be read as an indication of a disproportionate or disproportionate susceptibility to a stress response. It is hypothesized that this is an indication \nof subjective appraisal mechanisms that play a role in stress generation. In a further exploratory step, a hierarchical cluster analysis was performed according to Ward's method in order \nto obtain a more accurate picture of different “stress groups” (BACHER et al. 2010) within the \nsample. To ensure the comparability of the variables, they were z-standardised. \n\nDiscussion of the Results \n\nThe 15 data sets comprised a total of 379 MOS, which corresponds to an average of 25 MOS \nper respondent. The actual range of MOS values was between 18 and 36. After the spatial \nanalysis of the MOS and their causes, statements could be made about places perceived as \n“stressful” on Stuttgart's Marienplatz according to established hypotheses (cf. Fig.1). As exogenous factors, lack of space, interruption of the desire line, noise and quality of infrastructure were identified and reviewed as causes for the development of MOS. It turns out that all these factors individually, but also in combination, can indeed be causal for stress. In addition, information about places subjectively perceived as stressful was extracted from questionnaires ex-post. \n\nIt was found that lack of space was the most common stressor related to the built environment \nfor pedestrians, resulting in an MOS (68%). This was followed by noise (40%) and interruption of the desire line (32%), and finally the quality of infrastructure was also identified as a \ncause for some MOS (12%)1. Thus, this seems to play a lesser role in causing stress compared \nto the other exogenous influencing factors. The study situation may have played a role here, \nwith participants possibly paying less attention to their surroundings than to completing their \ntask. For the majority (66%) of MOS, a combination of multiple stressors was decisive. In \nsummary, Figure 2 gives an overview of the locations with the highest concentration of stress \npoints, which can be assigned to the individual hypotheses. It was particularly noticeable that \nmost of the MOS were located at major intersections around Marienplatz). Due to their separating effect, these correspond to the stress factor interruption of the desire line, since the \npedestrians were interrupted on their route there. Noise also closely linked to large and therefore busy intersections. The lack of space also has an effect at large intersections, e. g. due to \ncars passing closely, as well as in areas of side streets where the sidewalks are very narrow \nand especially when there is a confrontation with other pedestrians, some of whom are also \npushing strollers or similar. MOS also occurred in areas of “shared space” of pedestrian and \nbicycle traffic due to lack of space.  \n\nIn a more in-depth analysis consisting of descriptive statistics and cluster analysis, the sociopsychological characteristics of the participants were compared with the MOS (cf. Tab. 1). \nThe aim  was to attribute a regularity of stress reactions to them in order to obtain a more \naccurate picture of stress evaluation. Some suspected influencing factors proved to be irrelevant for further analysis, as there was no variance in the participants' response behavior or no \nfluctuations in the MOS could be attributed to them. Interesting findings emerged about gender  and  habit  (knowledge  of  the  place),  which,  according  to  theory,  proportionally  led  to \nmore or less MOS, but also about psychological characteristics. Individuals with higher than \naverage levels of agreeableness, higher than average levels of internal locus of control beliefs, \nand higher than average levels of neuroticism were found to have relatively more MOS. The \nlatter finding is consistent with research findings that neurotic people are highly susceptible \nto stress (MEDICAL TRIBUNAL VERLAGSGESELLSCHAFT 2021). People with a higher internal \nlocus of control may have a moderating effect on MOS because they are more likely to believe  that  they  can  control  the  consequences  of  their  actions.  Presumably,  this  could  also \nincrease  feelings  of  stress,  as  these  individuals  may  also  see  responsibility  in  themselves. \nConversely, the results are similar for the components of above-average extraversion, openness,  conscientiousness,  external  locus  of  control,  and  risk-taking.  Individuals  with  these \ncharacteristics recorded less MOS. The strongest effect was seen here for extraversion, which \nseems plausible, as does the influence of the other factors. People who are strongly extravert \ntend to also believe control is external and this allows them to externalise stress feelings. \n\nFinally, a cluster analysis was performed to form any groups according to socio-psychological characteristics that show statistical similarities in terms of the emergence of stress. Three \nclusters could be formed, described in terms of their characteristics as hesitant loners with a \npioneering spirit (cluster 1), security adventurers (cluster 2), and insecure loners (cluster 3): \n\nCluster  1:  Six  female  participants  who  are  moderately  knowledgeable  about  the  location. \nThey are on average more extraverted and neurotic, rather less open, rather less conscientious, as well as rather less compatible than average. They have above-average internal control beliefs, below-average external control beliefs, and tend to be more risk averse. On average, the cluster members had more MOS (28) than participants from the other clusters and \nthe global average. \n\nCluster 2: Six participants who are familiar with the location. Members of this cluster are less \nextraverted, more neurotic, more open, less conscientious, and rather less compatible than \nthe average. They have rather below-average internal control beliefs, rather above-average \nexternal control beliefs, and are risk averse. The participants have an average number of MOS \n(25) compared to the other clusters and the total score. \n\nCluster  3:  Three  male  participants  who  are  unfamiliar  with  the  location.  Members  of  this \ncluster tend to be more extraverted, less neurotic, more open and conscientious, and more \ncompatible than average. They tend to have above average internal and average external locus of control beliefs and are more risk averse. Participants in this cluster have lower average \nMOS compared to the other clusters and the overall score (20).  \n\nThere are a few critical points to note at this point. The homogeneity of the group is very \nhigh. To obtain a more diversified picture, a broader coverage  of participants  would have \nbeen necessary, especially regarding age, people with mobility impairments and more socio-economic\/cultural diversity. The choice of Stuttgart's Marienplatz should also not be viewed \nuncritically. The choice of the square is certainly justified, but it is already well-designed and \ncentrally located. However, the question arises as to whether it makes sense in the context of \nthe city as a whole to improve a suitable location based on a large number of studies or to \nexamine more peripheral locations that often escape general attention but are of central importance for many groups. Especially for groups that often do not participate in the urban \ndiscourse and thus do not call science, politics and administration onto the scene.  \n\nConclusion and Outlook \n\nTo generate more knowledge about the interaction of stress and walking, a triangulating process was used to collect measured stress data along with information on participants' socio-demographics, mobility behavior, and psyche to conduct a comprehensive analysis of stressors in the field. Heat maps were used to show that the most relevant stressors in this study \nwere lack of space, interruption of desire line, and noise. The analysis of the supporting hypotheses showed that, firstly, women were more likely to feel stressed in this study. People who are knowledgeable about the location tend to feel less MOS. In terms of psychological characteristics,  the  factors  neuroticism,  agreeableness,  and  internal  locus  of  control  were \nfound to be more likely to promote the development of stress. If people are more extraverted, \nopen, conscientious, willing to take risks, and have a higher degree of external control, they \ntend to have less MOS. In addition, hierarchical cluster analysis allowed us to calculate three \ncluster groups (hesitant loners with a pioneering spirit, safety adventurers, and safety-conscious loners). These clusters suggest a more detailed examination of the impact of planning projects related to the built environment on specific groups. The goal of this subdivision was to allow for a more nuanced consideration of the “human factor” in the measurement of stress, while using grouping to avoid working with individual case studies. \n\nThe  results  of  the  study  shed  light  on  the  development  of  stress  among  pedestrians  on \nStuttgart's Marienplatz. Not only were infrastructural stressors identified, but the inner milieu \nof people was also included in order to be able to examine the perception of stress in more \ndetail. In addition, the mix of methods proved to be appropriate for capturing, mapping and \ninterpreting these different levels of information and relationships. Although this study cannot be considered representative due to its sample, it can serve as a stimulus for further research of this kind and contribute to a diversified database. As mentioned above, walking can be made more attractive by reducing stress, thus contributing to more sustainable, healthier \nand socially just cities.  \n

A Collaborative Augmented Reality Decision Support \nSystem for Crowdsourcing Urban Designs \n\nAbstract: Globally, cities and their infrastructures, people, and ecology systems are experiencing unprecedented  changes  due  to  environmental  change  and  anthropogenic  pressures.  To  create  the  most \nsustainable approaches for mitigating and adapting to environmental changes, urban designs require a \nradical rethink that accounts for the needs of local citizens and stakeholders. Decision support systems \n(DSS) can be utilized to engage with members of the public to elicit their opinions on proposed designs. \nTo better engage citizens, DSSs have started to include the use of virtual reality and augmented reality \nto demonstrate designs, however, these systems are often only applied to smaller stakeholder engagement  events.  Here,  we  created  a  theory-informed  augmented  reality  application  for  collaborative \ncrowdsourcing of urban designs. The designed system allows users to manipulate a two-dimensional \nmap with targets representing different objects (e. g., trees, lakes, infrastructure). The changes made to \nthe 2D map are then visualized through a mobile application that displays a 3D AR visualization of the \nchanges the user  makes in real time. The resulting application can be used to engage a diversity  of \nparticipants in a range of urban and environmental planning contexts. \n\nIntroduction \n\nUrban areas are facing new challenges due to climate change, including, adaptation to new \nclimate  regimes,  damage  to  infrastructure  and  health,  well-being  impacts  from  extreme \nweather events, and increased demand for housing for climate migrants (HUNT & WATKISS \n2011, BAI et al. 2018, HOBBIE & GRIMM 2020). However, globally, there are no “one size fits \nall” solutions for increasing urban resilience as each city or region faces unique challenges \ndue to its environmental conditions and varying capacities to respond to climate stressors. \nPractitioners have pursued a range of interventions to increase urban resilience with varying \nsuccess, including green infrastructure and building social capital. Failure of such interventions is often related to inadequately addressing social equity issues (MEEROW et al. 2019), \nand there are increased calls for tools that chart just and equitable approaches to planning for \nclimate change (VAN BERKEL et al. 2022). \n\nUrban design and planning are traditionally expert-driven and top-down, which may not consider the needs and desires of local communities and stakeholders. This is particularly prob-\nlematic as climate change hazards disproportionately impact low-income and minority communities, whose voices are often under-represented in the planning process (MEEROW et al. \n2019).  Numerous  studies  have  tested  approaches  for  more  inclusive  design  and  planning, \nexamining  the  efficacy  of  stakeholder  workshops  (KUSTER  et  al. 2020),  as  well  as  digital \nparticipatory platforms or decision support tools (HASLER et al. 2017). Their result indicates \nthat decision support systems (DSSs) that involve stakeholders in the planning and design \nprocesses can result in more just and equitable outcomes (CAMPBELL‐ARVAI & LINDQUIST \n2021, LINDQUIST & CAMPBELL‐ARVAI 2021, VAN BERKEL et al. 2022). \n\nDSSs are tools used to enable meaningful citizen participation in landscape and urban planning. DSSs include a range of media or “boundary objects” (WHITE et al. 2010) that help \nstructure discussions around a topic. In the case of urban planning, they can include photomontages of proposed developments, participatory GIS, or highly immersive experiences us-\ning 3D videogame (FOX et al. 2022a). While less technologically advanced approaches using \ndescriptive narratives or 2D designs are effective for engaging citizens in planning scenarios, \nproviding DSSs with high levels of realism may be more beneficial for long-term engagement \n(GNAT et al. 2016). For example, providing players with a realistic 3D model of the proposed \ndesigns may increase spatial awareness and orientation for legitimacy and accuracy of discussion, as well as a sense of place and connection to depicted locations (GNAT et al. 2016). \nThis sense of connection is likely related to the credibility of depicted experience, and legitimacy often requires realism and applicability to real‐world scenarios (FOX et al. 2022a).  \n\nAugmented reality (AR) is an interactive experience that integrates real-world visualizations \nwith computer-generated content, for example, overlaying textual descriptions and icons on \na  mobile  phone  camera's  live  feed.  AR  can  allow  for  real-world  visualization  of  in  situ \nchanges to urban areas in real-time (IMOTTESJO & KAIN 2018), e. g. to visualize flooding on \na real site (HAYNES et al. 2018) and provide an additional layer of immersion and realism \n(CIRULIS & BRIGMANIS 2013), and may affect feelings of connectedness to the site related to \nits perceived realism (GNAT et al. 2016, OLSZEWSKI et al. 2017). However, DSSs are often \nbuilt as stand-alone software, applicable to a single context, and unable to be easily updated \nto incorporate any changes to the environment (FOX et al. 2022b). Here, we aim to provide a \nflexible DSS that can harness real-world geographic information system (GIS) data to generate an interactive AR DSS for landscape and urban environments. \n\nApplication Creation \n\nApplication Design \n\nTo ensure that our application aligns with the most innovative approaches to engage citizens \nin urban planning we designed our game to follow the framework for gamified DSSs introduced by FOX et al. (2022a). The framework for gamified DSSs provides guidance on the \nthree dimensions needed for success: engagement, education, and application. First, the use \nof innovative game technologies such as 3D models displayed in VR and AR provides more \nimmersive experiences for users than games using text, or 2D models (GNAT et al. 2016, VAN \nLEEUWEN et al. 2018). Furthermore, static DSSs that do not allow users to interact in a meaningful manner does not provide an engaging experience for users to provide feedback (FOX \net al. 2022a). We, therefore, designed our game to provide users with interactable 3D models \ndisplayed in AR to provide the most immersive experience. Second, without an educational \nelement, the long-term engagement of citizens may be limited (DEVISCH et al. 2016, FOX et \nal. 2022a). We have therefore designed the application to provide feedback about the environmental and economic impacts of their choices. Third, for a DSSs to be successful, the \noutcomes of the engagement process should provide real-world benefits for local people. As \na DSSs, should be grounded in a real-world context, we, therefore, built out DSSs to be flexible with the location displayed. \n\nAugmented Reality Development \n\nThe AR DSS application was developed using unity3D, with a module of Vuforia Engine \nLibrary SDK as a main component of the architecture. We include image targets as tracking \nfeatures for this project. This specific  module allows us to add advanced computer vision \nfunctionality to our DSS, to create an AR experience that users can realistically interact with \nobjects in the environment to provide opinions on a proposed design. Vuforia supports the \nmajority of phones, tablets, and eyewear. The software makes it possible to create augment \nrealistic 3D model from the use case by pointing the camera of the device over a main target \n(paper map) on any of these devices. Once the environment is displayed, the user can overlay \nother small targets over the 3D model in order to add corresponding elements to the environment. The DSS records the decision, of each user of the session and send this data to Firebase, \nrecording the objects selected and its location, and the use case. Currently, the application \ndoes not allow user to 3D elements, and add them to the map (i. e., all paper cut-outs are \npredefined).  The  models  that  we  are  using  for  this  use  case  are  manually  loaded  into  the \napplication by the developer. \n\nApplication Development \n\n3D models for AR urban areas can be created manually by human designers or through computer-aided designs (CAD) methods, however, these methods are often time-intensive and \nexpensive (GNAT et al. 2016). 3D model data for the augmented reality model can be generated using a range of GIS data sources, including pretended buildings available from GIS \nsoftware such as ArcPro, or generated through LiDAR point cloud data (FOX et al. 2022B). \nHowever, automatically generated models do not offer the same levels of realism as manually \ncreated 3D models (ZHANG & MOORE 2014). Therefore, the choice of data for developing \nthe 3D worlds is context dependent. Here, we utilize ‘RenderDoc’ to automatically generate \nmodels from Google Maps of the case study area. For the additional elements added via the \nAR app (e. g., tress and ponds), these models can be found through existing libraries such as \n‘Speedtree’ or the asset store on ‘Unity’, or manually created for specific use cases. In the \ncurrent beta version of the application we are using fixed locations that we have designed. \nFor future releases policy makers will be able to upload their own study sites to ‘Unity’ via \n‘SketchUp’ to push them to the application. \n\nDescription of Application \n\nOur AR mobile application DSS displays interactive 3D models of real-world locations and \nhas been developed to work in conjunction with paper maps (Figure 1). The AR application \ninterprets the paper map as a target and then displays a pre-made augmented environment on \nthe phone screen at the location of the paper map. This enables users interacting with a map \nof a proposed site to view additional information on their phone such as 3D models of the \nbuildings present in the map through their devices. Viewing 3D models of the environment \nwithin the study site (e. g. models of buildings and trees) enhances the user's experience by \nproviding additional information about the proposed design. Multiple participants can be engaged in augmented reality as the application works simultaneously across multiple mobile \ndevices. This also allows users to take different perspectives of the same augmented environment at the same time. \n\nThe AR mobile application also allows users to interact with and change the AR visualization \nin real time. The AR application recognizes multiple targets and is able to display them concurrently in the augmented environment. The additional targets are provided to users in the \nform of paper cut-outs that the users can place on the map. These paper cut-outs can be used \nto represent different natural and artificial features (e. g. trees, ponds, buildings, and benches) \nthat are then displayed as 3D models in the augmented environment at the location of the \npaper cut-out. To demonstrate this application Figure 2 shows Heilman Park in Detroit, Michigan, USA as an augmented reality environment with additional 3D models of trees and a \npond dictated by the locations of paper cut-outs. \n\nWe replicated the functionality of the paper maps on the touch table (Figure 3) for demonstration purposes. This allows for a higher level of interactivity with users able to easily zoom \nin and out of the map, while the AR 3D models scale with the movement. \n\nDiscussion \n\nThe resulting system offers different levels of complexity with both 2D and 3D depictions \nproviding an easy-to-use interactive experience allowing  for the real-time  visualization of \ndesign  and  planning  decisions.  This  flexible  AR  DSS  allows  policymakers  to  send  paper \nmaps and plans to stakeholders so they can develop their own scenarios remotely. This has \nthe potential to catalyze inclusive planning processes from start to finish, by reducing barriers \nto  use.  First,  policymakers  can  send  unedited  maps  to  citizens  and  stakeholders  to \ncrowdsource potential ideas. The AR application provides functionality for participants to \nscreenshot their designs and return them to the policymakers. Second, we envisage that these \ncrowdsourced designs could be hosted in a virtual gallery open to the public to assess and \nprovide additional comments. Third, policymakers can use these crowdsource designs in their \ndevelopment planning. These final design proposals can then be distributed to participants \nwho can then view these models in augmented reality and provide additional feedback. By \nallowing for citizens to engage at a time and location that is convenient for them, this methodology allows for policy makers to better engage with wider audiences than traditional DSS \n(Figure 4).  \n\nThe AR app will be distributed to users using a downloadable application for mobile phones, \nwhile  the  physical  maps  and  targets  can  be  sent  to  citizens  in  the  mail.  However,  we \nacknowledge that not all residents may have access to a mobile smartphone capable of supporting the AR required to partake in the design process. We also developed the application \nto be used with maps displayed on a digital touch table or interactive screen. Though this \nmethod does not allow participants to create designs remotely, it provides the ability for policymakers to hold interactive stakeholder engagement sessions that allow interactive citizen \nfeedback. During these stakeholder sessions, the planners can provide participants with mobile devices already set up with the application allowing users to easily engage with creating \nurban  designs.  Furthermore,  where  stakeholder  engagement  teams  do  not  have  access  to \ntouchscreen technology, they can still host successful interactive and engaging feedback sessions using large print out maps. \n\nWe have also designed the application to include additional functionality so that the system \nprovides real-time performance metrics of decisions made e. g. the impact on air quality or \nstormwater  management.  Adding  new  trees  that  trigger  the  visualization  of  sustainability \nmeasures  would bring a greater understanding of the local environmental impacts of their \nchoice (FOX et al. 2022a). While the beta version of the tool only has rudimentary functionality for editing, in the future we envisage a wide range of modifiable elements that will serve \nin diverse spatial planning applications. For example, modification of the height or features \nof a building via the interface could be reflected in real-time to those viewing in augmented \nreality for understanding shadows and site impact. \n\nConclusion and Outlook \n\nRealistic AR DSS provides an engaging and immersive method for residents to participate in \nurban planning decisions. Though AR has previously been implemented into planning DSSs, \nthey are often only used to engage with smaller audiences. Our mobile application design \nprovides the advantage of allowing planners to collaboratively crowdsource designs. By being able to send paper maps and targets to all residents and stakeholders, our method allows \nfor wider audiences to be reached compared to traditional AR DSS methods. Furthermore, \nby developing a method that is accessible to audiences via remotely asynchronous engagement as well as focused workshop events, our DSS can both reduce barriers to participation \nand is applicable to a wide range of planning and engagement contexts. \n\nA Collaborative Augmented Reality Decision Support \nSystem for Crowdsourcing Urban Designs \n\nAbstract: Globally, cities and their infrastructures, people, and ecology systems are experiencing unprecedented  changes  due  to  environmental  change  and  anthropogenic  pressures.  To  create  the  most \nsustainable approaches for mitigating and adapting to environmental changes, urban designs require a \nradical rethink that accounts for the needs of local citizens and stakeholders. Decision support systems \n(DSS) can be utilized to engage with members of the public to elicit their opinions on proposed designs. \nTo better engage citizens, DSSs have started to include the use of virtual reality and augmented reality \nto demonstrate designs, however, these systems are often only applied to smaller stakeholder engagement  events.  Here,  we  created  a  theory-informed  augmented  reality  application  for  collaborative \ncrowdsourcing of urban designs. The designed system allows users to manipulate a two-dimensional \nmap with targets representing different objects (e. g., trees, lakes, infrastructure). The changes made to \nthe 2D map are then visualized through a mobile application that displays a 3D AR visualization of the \nchanges the user  makes in real time. The resulting application can be used to engage a diversity  of \nparticipants in a range of urban and environmental planning contexts. \n\nIntroduction \n\nUrban areas are facing new challenges due to climate change, including, adaptation to new \nclimate  regimes,  damage  to  infrastructure  and  health,  well-being  impacts  from  extreme \nweather events, and increased demand for housing for climate migrants (HUNT & WATKISS \n2011, BAI et al. 2018, HOBBIE & GRIMM 2020). However, globally, there are no “one size fits \nall” solutions for increasing urban resilience as each city or region faces unique challenges \ndue to its environmental conditions and varying capacities to respond to climate stressors. \nPractitioners have pursued a range of interventions to increase urban resilience with varying \nsuccess, including green infrastructure and building social capital. Failure of such interventions is often related to inadequately addressing social equity issues (MEEROW et al. 2019), \nand there are increased calls for tools that chart just and equitable approaches to planning for \nclimate change (VAN BERKEL et al. 2022). \n\nUrban design and planning are traditionally expert-driven and top-down, which may not consider the needs and desires of local communities and stakeholders. This is particularly prob-\nlematic as climate change hazards disproportionately impact low-income and minority communities, whose voices are often under-represented in the planning process (MEEROW et al. \n2019).  Numerous  studies  have  tested  approaches  for  more  inclusive  design  and  planning, \nexamining  the  efficacy  of  stakeholder  workshops  (KUSTER  et  al. 2020),  as  well  as  digital \nparticipatory platforms or decision support tools (HASLER et al. 2017). Their result indicates \nthat decision support systems (DSSs) that involve stakeholders in the planning and design \nprocesses can result in more just and equitable outcomes (CAMPBELL‐ARVAI & LINDQUIST \n2021, LINDQUIST & CAMPBELL‐ARVAI 2021, VAN BERKEL et al. 2022). \n\nDSSs are tools used to enable meaningful citizen participation in landscape and urban planning. DSSs include a range of media or “boundary objects” (WHITE et al. 2010) that help \nstructure discussions around a topic. In the case of urban planning, they can include photomontages of proposed developments, participatory GIS, or highly immersive experiences us-\ning 3D videogame (FOX et al. 2022a). While less technologically advanced approaches using \ndescriptive narratives or 2D designs are effective for engaging citizens in planning scenarios, \nproviding DSSs with high levels of realism may be more beneficial for long-term engagement \n(GNAT et al. 2016). For example, providing players with a realistic 3D model of the proposed \ndesigns may increase spatial awareness and orientation for legitimacy and accuracy of discussion, as well as a sense of place and connection to depicted locations (GNAT et al. 2016). \nThis sense of connection is likely related to the credibility of depicted experience, and legitimacy often requires realism and applicability to real‐world scenarios (FOX et al. 2022a).  \n\nAugmented reality (AR) is an interactive experience that integrates real-world visualizations \nwith computer-generated content, for example, overlaying textual descriptions and icons on \na  mobile  phone  camera's  live  feed.  AR  can  allow  for  real-world  visualization  of  in  situ \nchanges to urban areas in real-time (IMOTTESJO & KAIN 2018), e. g. to visualize flooding on \na real site (HAYNES et al. 2018) and provide an additional layer of immersion and realism \n(CIRULIS & BRIGMANIS 2013), and may affect feelings of connectedness to the site related to \nits perceived realism (GNAT et al. 2016, OLSZEWSKI et al. 2017). However, DSSs are often \nbuilt as stand-alone software, applicable to a single context, and unable to be easily updated \nto incorporate any changes to the environment (FOX et al. 2022b). Here, we aim to provide a \nflexible DSS that can harness real-world geographic information system (GIS) data to generate an interactive AR DSS for landscape and urban environments. \n\nApplication Creation \n\nApplication Design \n\nTo ensure that our application aligns with the most innovative approaches to engage citizens \nin urban planning we designed our game to follow the framework for gamified DSSs introduced by FOX et al. (2022a). The framework for gamified DSSs provides guidance on the \nthree dimensions needed for success: engagement, education, and application. First, the use \nof innovative game technologies such as 3D models displayed in VR and AR provides more \nimmersive experiences for users than games using text, or 2D models (GNAT et al. 2016, VAN \nLEEUWEN et al. 2018). Furthermore, static DSSs that do not allow users to interact in a meaningful manner does not provide an engaging experience for users to provide feedback (FOX \net al. 2022a). We, therefore, designed our game to provide users with interactable 3D models \ndisplayed in AR to provide the most immersive experience. Second, without an educational \nelement, the long-term engagement of citizens may be limited (DEVISCH et al. 2016, FOX et \nal. 2022a). We have therefore designed the application to provide feedback about the environmental and economic impacts of their choices. Third, for a DSSs to be successful, the \noutcomes of the engagement process should provide real-world benefits for local people. As \na DSSs, should be grounded in a real-world context, we, therefore, built out DSSs to be flexible with the location displayed. \n\nAugmented Reality Development \n\nThe AR DSS application was developed using unity3D, with a module of Vuforia Engine \nLibrary SDK as a main component of the architecture. We include image targets as tracking \nfeatures for this project. This specific  module allows us to add advanced computer vision \nfunctionality to our DSS, to create an AR experience that users can realistically interact with \nobjects in the environment to provide opinions on a proposed design. Vuforia supports the \nmajority of phones, tablets, and eyewear. The software makes it possible to create augment \nrealistic 3D model from the use case by pointing the camera of the device over a main target \n(paper map) on any of these devices. Once the environment is displayed, the user can overlay \nother small targets over the 3D model in order to add corresponding elements to the environment. The DSS records the decision, of each user of the session and send this data to Firebase, \nrecording the objects selected and its location, and the use case. Currently, the application \ndoes not allow user to 3D elements, and add them to the map (i. e., all paper cut-outs are \npredefined).  The  models  that  we  are  using  for  this  use  case  are  manually  loaded  into  the \napplication by the developer. \n\nApplication Development \n\n3D models for AR urban areas can be created manually by human designers or through computer-aided designs (CAD) methods, however, these methods are often time-intensive and \nexpensive (GNAT et al. 2016). 3D model data for the augmented reality model can be generated using a range of GIS data sources, including pretended buildings available from GIS \nsoftware such as ArcPro, or generated through LiDAR point cloud data (FOX et al. 2022B). \nHowever, automatically generated models do not offer the same levels of realism as manually \ncreated 3D models (ZHANG & MOORE 2014). Therefore, the choice of data for developing \nthe 3D worlds is context dependent. Here, we utilize ‘RenderDoc’ to automatically generate \nmodels from Google Maps of the case study area. For the additional elements added via the \nAR app (e. g., tress and ponds), these models can be found through existing libraries such as \n‘Speedtree’ or the asset store on ‘Unity’, or manually created for specific use cases. In the \ncurrent beta version of the application we are using fixed locations that we have designed. \nFor future releases policy makers will be able to upload their own study sites to ‘Unity’ via \n‘SketchUp’ to push them to the application. \n\nDescription of Application \n\nOur AR mobile application DSS displays interactive 3D models of real-world locations and \nhas been developed to work in conjunction with paper maps (Figure 1). The AR application \ninterprets the paper map as a target and then displays a pre-made augmented environment on \nthe phone screen at the location of the paper map. This enables users interacting with a map \nof a proposed site to view additional information on their phone such as 3D models of the \nbuildings present in the map through their devices. Viewing 3D models of the environment \nwithin the study site (e. g. models of buildings and trees) enhances the user's experience by \nproviding additional information about the proposed design. Multiple participants can be engaged in augmented reality as the application works simultaneously across multiple mobile \ndevices. This also allows users to take different perspectives of the same augmented environment at the same time. \n\nThe AR mobile application also allows users to interact with and change the AR visualization \nin real time. The AR application recognizes multiple targets and is able to display them concurrently in the augmented environment. The additional targets are provided to users in the \nform of paper cut-outs that the users can place on the map. These paper cut-outs can be used \nto represent different natural and artificial features (e. g. trees, ponds, buildings, and benches) \nthat are then displayed as 3D models in the augmented environment at the location of the \npaper cut-out. To demonstrate this application Figure 2 shows Heilman Park in Detroit, Michigan, USA as an augmented reality environment with additional 3D models of trees and a \npond dictated by the locations of paper cut-outs. \n\nWe replicated the functionality of the paper maps on the touch table (Figure 3) for demonstration purposes. This allows for a higher level of interactivity with users able to easily zoom \nin and out of the map, while the AR 3D models scale with the movement. \n\nDiscussion \n\nThe resulting system offers different levels of complexity with both 2D and 3D depictions \nproviding an easy-to-use interactive experience allowing  for the real-time  visualization of \ndesign  and  planning  decisions.  This  flexible  AR  DSS  allows  policymakers  to  send  paper \nmaps and plans to stakeholders so they can develop their own scenarios remotely. This has \nthe potential to catalyze inclusive planning processes from start to finish, by reducing barriers \nto  use.  First,  policymakers  can  send  unedited  maps  to  citizens  and  stakeholders  to \ncrowdsource potential ideas. The AR application provides functionality for participants to \nscreenshot their designs and return them to the policymakers. Second, we envisage that these \ncrowdsourced designs could be hosted in a virtual gallery open to the public to assess and \nprovide additional comments. Third, policymakers can use these crowdsource designs in their \ndevelopment planning. These final design proposals can then be distributed to participants \nwho can then view these models in augmented reality and provide additional feedback. By \nallowing for citizens to engage at a time and location that is convenient for them, this methodology allows for policy makers to better engage with wider audiences than traditional DSS \n(Figure 4).  \n\nThe AR app will be distributed to users using a downloadable application for mobile phones, \nwhile  the  physical  maps  and  targets  can  be  sent  to  citizens  in  the  mail.  However,  we \nacknowledge that not all residents may have access to a mobile smartphone capable of supporting the AR required to partake in the design process. We also developed the application \nto be used with maps displayed on a digital touch table or interactive screen. Though this \nmethod does not allow participants to create designs remotely, it provides the ability for policymakers to hold interactive stakeholder engagement sessions that allow interactive citizen \nfeedback. During these stakeholder sessions, the planners can provide participants with mobile devices already set up with the application allowing users to easily engage with creating \nurban  designs.  Furthermore,  where  stakeholder  engagement  teams  do  not  have  access  to \ntouchscreen technology, they can still host successful interactive and engaging feedback sessions using large print out maps. \n\nWe have also designed the application to include additional functionality so that the system \nprovides real-time performance metrics of decisions made e. g. the impact on air quality or \nstormwater  management.  Adding  new  trees  that  trigger  the  visualization  of  sustainability \nmeasures  would bring a greater understanding of the local environmental impacts of their \nchoice (FOX et al. 2022a). While the beta version of the tool only has rudimentary functionality for editing, in the future we envisage a wide range of modifiable elements that will serve \nin diverse spatial planning applications. For example, modification of the height or features \nof a building via the interface could be reflected in real-time to those viewing in augmented \nreality for understanding shadows and site impact. \n\nConclusion and Outlook \n\nRealistic AR DSS provides an engaging and immersive method for residents to participate in \nurban planning decisions. Though AR has previously been implemented into planning DSSs, \nthey are often only used to engage with smaller audiences. Our mobile application design \nprovides the advantage of allowing planners to collaboratively crowdsource designs. By being able to send paper maps and targets to all residents and stakeholders, our method allows \nfor wider audiences to be reached compared to traditional AR DSS methods. Furthermore, \nby developing a method that is accessible to audiences via remotely asynchronous engagement as well as focused workshop events, our DSS can both reduce barriers to participation \nand is applicable to a wide range of planning and engagement contexts. \n\nA Collaborative Augmented Reality Decision Support \nSystem for Crowdsourcing Urban Designs \n\nAbstract: Globally, cities and their infrastructures, people, and ecology systems are experiencing unprecedented  changes  due  to  environmental  change  and  anthropogenic  pressures.  To  create  the  most \nsustainable approaches for mitigating and adapting to environmental changes, urban designs require a \nradical rethink that accounts for the needs of local citizens and stakeholders. Decision support systems \n(DSS) can be utilized to engage with members of the public to elicit their opinions on proposed designs. \nTo better engage citizens, DSSs have started to include the use of virtual reality and augmented reality \nto demonstrate designs, however, these systems are often only applied to smaller stakeholder engagement  events.  Here,  we  created  a  theory-informed  augmented  reality  application  for  collaborative \ncrowdsourcing of urban designs. The designed system allows users to manipulate a two-dimensional \nmap with targets representing different objects (e. g., trees, lakes, infrastructure). The changes made to \nthe 2D map are then visualized through a mobile application that displays a 3D AR visualization of the \nchanges the user  makes in real time. The resulting application can be used to engage a diversity  of \nparticipants in a range of urban and environmental planning contexts. \n\nIntroduction \n\nUrban areas are facing new challenges due to climate change, including, adaptation to new \nclimate  regimes,  damage  to  infrastructure  and  health,  well-being  impacts  from  extreme \nweather events, and increased demand for housing for climate migrants (HUNT & WATKISS \n2011, BAI et al. 2018, HOBBIE & GRIMM 2020). However, globally, there are no “one size fits \nall” solutions for increasing urban resilience as each city or region faces unique challenges \ndue to its environmental conditions and varying capacities to respond to climate stressors. \nPractitioners have pursued a range of interventions to increase urban resilience with varying \nsuccess, including green infrastructure and building social capital. Failure of such interventions is often related to inadequately addressing social equity issues (MEEROW et al. 2019), \nand there are increased calls for tools that chart just and equitable approaches to planning for \nclimate change (VAN BERKEL et al. 2022). \n\nUrban design and planning are traditionally expert-driven and top-down, which may not consider the needs and desires of local communities and stakeholders. This is particularly prob-\nlematic as climate change hazards disproportionately impact low-income and minority communities, whose voices are often under-represented in the planning process (MEEROW et al. \n2019).  Numerous  studies  have  tested  approaches  for  more  inclusive  design  and  planning, \nexamining  the  efficacy  of  stakeholder  workshops  (KUSTER  et  al. 2020),  as  well  as  digital \nparticipatory platforms or decision support tools (HASLER et al. 2017). Their result indicates \nthat decision support systems (DSSs) that involve stakeholders in the planning and design \nprocesses can result in more just and equitable outcomes (CAMPBELL‐ARVAI & LINDQUIST \n2021, LINDQUIST & CAMPBELL‐ARVAI 2021, VAN BERKEL et al. 2022). \n\nDSSs are tools used to enable meaningful citizen participation in landscape and urban planning. DSSs include a range of media or “boundary objects” (WHITE et al. 2010) that help \nstructure discussions around a topic. In the case of urban planning, they can include photomontages of proposed developments, participatory GIS, or highly immersive experiences us-\ning 3D videogame (FOX et al. 2022a). While less technologically advanced approaches using \ndescriptive narratives or 2D designs are effective for engaging citizens in planning scenarios, \nproviding DSSs with high levels of realism may be more beneficial for long-term engagement \n(GNAT et al. 2016). For example, providing players with a realistic 3D model of the proposed \ndesigns may increase spatial awareness and orientation for legitimacy and accuracy of discussion, as well as a sense of place and connection to depicted locations (GNAT et al. 2016). \nThis sense of connection is likely related to the credibility of depicted experience, and legitimacy often requires realism and applicability to real‐world scenarios (FOX et al. 2022a).  \n\nAugmented reality (AR) is an interactive experience that integrates real-world visualizations \nwith computer-generated content, for example, overlaying textual descriptions and icons on \na  mobile  phone  camera's  live  feed.  AR  can  allow  for  real-world  visualization  of  in  situ \nchanges to urban areas in real-time (IMOTTESJO & KAIN 2018), e. g. to visualize flooding on \na real site (HAYNES et al. 2018) and provide an additional layer of immersion and realism \n(CIRULIS & BRIGMANIS 2013), and may affect feelings of connectedness to the site related to \nits perceived realism (GNAT et al. 2016, OLSZEWSKI et al. 2017). However, DSSs are often \nbuilt as stand-alone software, applicable to a single context, and unable to be easily updated \nto incorporate any changes to the environment (FOX et al. 2022b). Here, we aim to provide a \nflexible DSS that can harness real-world geographic information system (GIS) data to generate an interactive AR DSS for landscape and urban environments. \n\nApplication Creation \n\nApplication Design \n\nTo ensure that our application aligns with the most innovative approaches to engage citizens \nin urban planning we designed our game to follow the framework for gamified DSSs introduced by FOX et al. (2022a). The framework for gamified DSSs provides guidance on the \nthree dimensions needed for success: engagement, education, and application. First, the use \nof innovative game technologies such as 3D models displayed in VR and AR provides more \nimmersive experiences for users than games using text, or 2D models (GNAT et al. 2016, VAN \nLEEUWEN et al. 2018). Furthermore, static DSSs that do not allow users to interact in a meaningful manner does not provide an engaging experience for users to provide feedback (FOX \net al. 2022a). We, therefore, designed our game to provide users with interactable 3D models \ndisplayed in AR to provide the most immersive experience. Second, without an educational \nelement, the long-term engagement of citizens may be limited (DEVISCH et al. 2016, FOX et \nal. 2022a). We have therefore designed the application to provide feedback about the environmental and economic impacts of their choices. Third, for a DSSs to be successful, the \noutcomes of the engagement process should provide real-world benefits for local people. As \na DSSs, should be grounded in a real-world context, we, therefore, built out DSSs to be flexible with the location displayed. \n\nAugmented Reality Development \n\nThe AR DSS application was developed using unity3D, with a module of Vuforia Engine \nLibrary SDK as a main component of the architecture. We include image targets as tracking \nfeatures for this project. This specific  module allows us to add advanced computer vision \nfunctionality to our DSS, to create an AR experience that users can realistically interact with \nobjects in the environment to provide opinions on a proposed design. Vuforia supports the \nmajority of phones, tablets, and eyewear. The software makes it possible to create augment \nrealistic 3D model from the use case by pointing the camera of the device over a main target \n(paper map) on any of these devices. Once the environment is displayed, the user can overlay \nother small targets over the 3D model in order to add corresponding elements to the environment. The DSS records the decision, of each user of the session and send this data to Firebase, \nrecording the objects selected and its location, and the use case. Currently, the application \ndoes not allow user to 3D elements, and add them to the map (i. e., all paper cut-outs are \npredefined).  The  models  that  we  are  using  for  this  use  case  are  manually  loaded  into  the \napplication by the developer. \n\nApplication Development \n\n3D models for AR urban areas can be created manually by human designers or through computer-aided designs (CAD) methods, however, these methods are often time-intensive and \nexpensive (GNAT et al. 2016). 3D model data for the augmented reality model can be generated using a range of GIS data sources, including pretended buildings available from GIS \nsoftware such as ArcPro, or generated through LiDAR point cloud data (FOX et al. 2022B). \nHowever, automatically generated models do not offer the same levels of realism as manually \ncreated 3D models (ZHANG & MOORE 2014). Therefore, the choice of data for developing \nthe 3D worlds is context dependent. Here, we utilize ‘RenderDoc’ to automatically generate \nmodels from Google Maps of the case study area. For the additional elements added via the \nAR app (e. g., tress and ponds), these models can be found through existing libraries such as \n‘Speedtree’ or the asset store on ‘Unity’, or manually created for specific use cases. In the \ncurrent beta version of the application we are using fixed locations that we have designed. \nFor future releases policy makers will be able to upload their own study sites to ‘Unity’ via \n‘SketchUp’ to push them to the application. \n\nDescription of Application \n\nOur AR mobile application DSS displays interactive 3D models of real-world locations and \nhas been developed to work in conjunction with paper maps (Figure 1). The AR application \ninterprets the paper map as a target and then displays a pre-made augmented environment on \nthe phone screen at the location of the paper map. This enables users interacting with a map \nof a proposed site to view additional information on their phone such as 3D models of the \nbuildings present in the map through their devices. Viewing 3D models of the environment \nwithin the study site (e. g. models of buildings and trees) enhances the user's experience by \nproviding additional information about the proposed design. Multiple participants can be engaged in augmented reality as the application works simultaneously across multiple mobile \ndevices. This also allows users to take different perspectives of the same augmented environment at the same time. \n\nThe AR mobile application also allows users to interact with and change the AR visualization \nin real time. The AR application recognizes multiple targets and is able to display them concurrently in the augmented environment. The additional targets are provided to users in the \nform of paper cut-outs that the users can place on the map. These paper cut-outs can be used \nto represent different natural and artificial features (e. g. trees, ponds, buildings, and benches) \nthat are then displayed as 3D models in the augmented environment at the location of the \npaper cut-out. To demonstrate this application Figure 2 shows Heilman Park in Detroit, Michigan, USA as an augmented reality environment with additional 3D models of trees and a \npond dictated by the locations of paper cut-outs. \n\nWe replicated the functionality of the paper maps on the touch table (Figure 3) for demonstration purposes. This allows for a higher level of interactivity with users able to easily zoom \nin and out of the map, while the AR 3D models scale with the movement. \n\nDiscussion \n\nThe resulting system offers different levels of complexity with both 2D and 3D depictions \nproviding an easy-to-use interactive experience allowing  for the real-time  visualization of \ndesign  and  planning  decisions.  This  flexible  AR  DSS  allows  policymakers  to  send  paper \nmaps and plans to stakeholders so they can develop their own scenarios remotely. This has \nthe potential to catalyze inclusive planning processes from start to finish, by reducing barriers \nto  use.  First,  policymakers  can  send  unedited  maps  to  citizens  and  stakeholders  to \ncrowdsource potential ideas. The AR application provides functionality for participants to \nscreenshot their designs and return them to the policymakers. Second, we envisage that these \ncrowdsourced designs could be hosted in a virtual gallery open to the public to assess and \nprovide additional comments. Third, policymakers can use these crowdsource designs in their \ndevelopment planning. These final design proposals can then be distributed to participants \nwho can then view these models in augmented reality and provide additional feedback. By \nallowing for citizens to engage at a time and location that is convenient for them, this methodology allows for policy makers to better engage with wider audiences than traditional DSS \n(Figure 4).  \n\nThe AR app will be distributed to users using a downloadable application for mobile phones, \nwhile  the  physical  maps  and  targets  can  be  sent  to  citizens  in  the  mail.  However,  we \nacknowledge that not all residents may have access to a mobile smartphone capable of supporting the AR required to partake in the design process. We also developed the application \nto be used with maps displayed on a digital touch table or interactive screen. Though this \nmethod does not allow participants to create designs remotely, it provides the ability for policymakers to hold interactive stakeholder engagement sessions that allow interactive citizen \nfeedback. During these stakeholder sessions, the planners can provide participants with mobile devices already set up with the application allowing users to easily engage with creating \nurban  designs.  Furthermore,  where  stakeholder  engagement  teams  do  not  have  access  to \ntouchscreen technology, they can still host successful interactive and engaging feedback sessions using large print out maps. \n\nWe have also designed the application to include additional functionality so that the system \nprovides real-time performance metrics of decisions made e. g. the impact on air quality or \nstormwater  management.  Adding  new  trees  that  trigger  the  visualization  of  sustainability \nmeasures  would bring a greater understanding of the local environmental impacts of their \nchoice (FOX et al. 2022a). While the beta version of the tool only has rudimentary functionality for editing, in the future we envisage a wide range of modifiable elements that will serve \nin diverse spatial planning applications. For example, modification of the height or features \nof a building via the interface could be reflected in real-time to those viewing in augmented \nreality for understanding shadows and site impact. \n\nConclusion and Outlook \n\nRealistic AR DSS provides an engaging and immersive method for residents to participate in \nurban planning decisions. Though AR has previously been implemented into planning DSSs, \nthey are often only used to engage with smaller audiences. Our mobile application design \nprovides the advantage of allowing planners to collaboratively crowdsource designs. By being able to send paper maps and targets to all residents and stakeholders, our method allows \nfor wider audiences to be reached compared to traditional AR DSS methods. Furthermore, \nby developing a method that is accessible to audiences via remotely asynchronous engagement as well as focused workshop events, our DSS can both reduce barriers to participation \nand is applicable to a wide range of planning and engagement contexts. \n\nA Target-driven Tree Planting and Maintenance \nApproach for Next Generation Urban Green \nInfrastructure (UGI) \n\nAbstract: Enhancing ecosystem services (ESS) has become a major concern in planning urban green \ninfrastructure (UGI). The keys to this are urban trees and their dynamic growth in both overall biomass \nand specific canopy geometry. However, such dynamic growth interacts not only with the abiotic context but also with tree manipulations such as pruning. To address these aspects, this paper proposes a \nnovel theoretical workflow for designing with urban trees by applying a 3D voxel approach. It aims at \nachieving a quantitative target such as higher leaf coverage without conflicting with other urban functionalities such as traffic and adequate light. We then illustrate how this target guides strategic planning \nof tree planting and management. Finally, we highlight  how this conceptual approach can stimulate \nfurther research and technology development to design and manage next generation UGI. \n\nIntroduction \n\nAs  a  response  to  increasing  urbanization  and  climate  change,  urban  green  infrastructure \n(UGI) emerged as a concept to improve human health and well-being within urban boundaries.  Trees,  as a  primary  component  of  UGI,  are  not  static  elements  but  follow  dynamic \ngrowth through their lifecycle. Therefore, the capacity of UGI relies on adequate design and \nmanagement methods that allow for long-term development of trees.  \n\nEcosystem  Services  (ESS)  describe  all  functions  of  urban green  infrastructures  (UGI)  for \nhuman well-being. In the field of urban forestry, ESS have been evaluated specifically for \nevery  individual  tree,  based  on  factors  such  as  diameter  in  breast  height, age ,  and crow n \nshape. Local environmental conditions affect tree growth (Eleonora Franceschi et al. 2022), \nwhich in turn result in different ESS such as carbon storage, shading, and cooling (MOSER et \nal. 2015). According to Nastran et al. (2022), Palliwoda et al. (2020) and Xu & Zhao (2021) \nthe ESS of UGI increase with biodiversity and total biomass in cities. This could be achieved \nby the following concept, where a tree’s canopy volume is set as a design target. In an example of a natural environment with young trees (see Fig. 1a), around 50% of the theoretically \npossible canopy volume is achieved. When the same vegetation is simply copied and pasted \nto a fictive built-up area, human living space and shade is squeezed in (such as shown in Fig. \n1b), but further functions are not met. For enabling these functions (i. e., open space, mobility, underground infrastructure), some canopy areas are excluded from the allowed growth \nspace (forbidden areas) for trees. In this approach, it allows a maximum leaf area or canopy \nvolume of trees without impairing other functions and requirements such as exposure or ventilation. To define such a target in a design perspective, human comfort, and aesthetic aspects \nsuch as viewpoints are highly  relevant as well. A 3D target-driven workflow (section 2.1) \ncan be a solution for meeting these diverse needs. To evaluate trees’ performances, a range \nof decision-support tools for green infrastructure and ecosystem services have been developed in recent years (VAN OIJSTAEIJEN et al. 2020). The performance of trees, after they have \nreached the desired size, can be assessed for feedback on tree planting and maintenance. In \nthis way, tree planting design (i. e., species, position, density) and any common thinning and \npruning could contribute to a higher ESS (i. e., thermal comfort and visual aesthetics) (CHEN \net al. 2021, KRAYENHOFF et al. 2020, LANGENHEIM & WHITE 2022, Wang et al. 2022).  \n\nHowever,  currently  such  a  performance  goal  cannot  be  fully  met  because  of  the  lack  of \nknowledge about the dynamic growth of trees interacting with the abiotic context and the \ngrowth of the branches after pruning. For example, without knowing about the effect of the \nadjacent building on the form of a canopy, targeted ESS cannot be achieved. In this context, \ndeveloping a visionary tree planting and maintenance design workflow is crucial for increasing the value of the tree’s ESS. The specific focus of this workflow will be on urban trees \nand their integration into the built environment. \n\nSteps and Research Topics for Completing the Workflow \n\nIn the following section, we describe the most important aspects and steps of our approach \nto illustrate how the digital workflow is supposed to look like. The proposed workflow has \nthree consecutive steps which are structured by the planning and implementation procedure \n(before planting, planting, and after planting): 1) A tool for setting the target leaf voxel (TLV) \nto achieve optimal spatial occupation of the tree canopy in a dense urban environment. 2) A \ntree planting design tool to increase the chance of achieving the TLV. And 3) a tree maintenance and management tool for best approaching the TLV. \n\nFor defining volumes for intended tree growth in voxels we will mainly build on CityGML \nfiles. The CityGML databases of cities are novel datasets for analyzing and studying urban \nareas with a data-driven approach. CityGML files provide the materials for voxelizing the \nnon-built above-ground space in urban areas and that are filled as “potential leaf volume”. \nThe next step is finding and estimating the quantitative and qualitative limitation factors for \ngrowing leaves in cities which are called the forbidden voxels. Some examples of the limiting \nfactors are building openings, street areas, urban objects such as traffic lights or bus stations, \nurban ventilation requirements,  and building  entrances.  The final  voxel model  will  be the \nmaximum  possible  potential  leaf  volume  with  a  consideration  of  the  limiting  factors  for \ngrowing leaves. For a conceptual illustration of this approach (see Figure 2), after filling the \nmodel with a 100% TLV, the forbidden area for occupations and different functions are removed to find the maximum possible voxel space for leaves and to determine the necessary \ngrowth space for roots accordingly. Like TLV, the necessary root volume can also be determined and represented by voxels. For these root voxels, the limitations, e. g. by foundations \nor technical infrastructures, are excluded from the potential root space.  \n\nTarget-driven Tree Planting \n\nThe shape and growth rate of tree canopies varies in different locations of cities. For example, \nthe influence of urban microclimate (MOSER-REISCHL et al. 2019), shadow of buildings, and \nlight competition in trees (KOTHARI et al. 2021) are well documented. Different natural and \nhuman-made factors affect tree growth in cities. Therefore, it is a semi-natural process that \ncould be digitalized and predicted like other semi-natural processes. A visionary tree planting \ndesign workflow requires a reliable prediction of the growth of the tree canopies. Based on \nthese observations, in urban forestry several growth models such as CityTree (RÖTZER et al. \n2019, 2021) have been developed to understand the impact of the environmental and surroundings. For such models, the location, where trees are planted plays a crucial role. However, the quantitative impact of such factors on the development of the trees canopy is not \nprecisely evaluated yet. For instance, adjacent buildings change the canopy shape in different \ndirections which is not considered in these tree growth models. A planning strategy without \nassessing these impacts will fail in ESS target achievement. \n\nHere we propose to use machine learning models to identify specific patterns between the \nadjacent object and the tree growth rate changes. Geometric features of buildings and trees \naround can be extracted from the CityGML file together with the accurate geolocation data \nof the trees. For further analysis, a dataset of the specific tree can be gathered that represents \nthe growth changes in previous years and the adjacent object around it. Data analysis methods \nshow the correlation between the different geometric features of adjacent objects (i. e., direction, distance, dimension, height) and growth rate changes. All in all, a tree canopy growth \nmodel can be developed based on abiotic environmental factors. \n\nA tree planting design model can be developed based on this canopy shape growth model. \nChanging the tree planting position, species, and the number of trees can change the prediction factors for final canopy shape and target achievement. Current practices without concerning the final 3D targets lower the chance of ESS achievement in both underground and \nsubaerial targets (Figure 3). For example, current standardised practices (i. e., linear planted \ntrees in equal distance and age) decrease the ESS target achievement, increase maintenance \ncost and invade the forbidden areas (i. e., human spaces, mobility spaces, underground infrastructure). \n\nDetailed Tree Design and Management \n\nFrom a design and engineering perspective, ESS can be delivered through detailed tree geometry such as the position of branches. For example, branches with dense leaves should \navoid being in front of windows to allow light coming through; facades facing the south and \nwest could be isolated with tall branches from the radiation of the late-afternoon sun that \nheats buildings intensively (ZHAO et al. 2022); opening a canopy at certain points on a street \ncould enhance ventilation for better thermal comfort for pedestrians (SANTIAGO et al. 2019). \nAchieving these goals requires a tailor-made design for tree branches that sprout at different \nplaces to increase ESS. \n\nThis design workflow needs to  integrate two scales of time: in a short time (1 year), it is \nnecessary to make decisions of how to prune or bend branches to keep them growing in the \ntarget voxels; in the long term perspective (10-60 years), it needs to be decided which shoots \nshould be preserved to best support the long term development towards the TLV. The short-term simulation can be achieved through either empirical statistics, functional-structural plant \nmodels (FSPMs) (LOUARN & SONG 2020), or machine learning (JORDAN & MITCHELL 2015). \nThe long-term simulation is in parallel to a chain reaction of multiple rounds of short-term \ngrowths, which can be evaluated with Markov chain models (i. e., HAJIAGHA et al. 2022). \nEven with all these simulation tools, the prediction of the tree’s behavior in both time scales \ncannot achieve 100% accuracy. Therefore, an iterative tree monitoring and manipulation procedure is required to annually revise the simulation through the data of actual growth (Figure \n4) (SHU et al. 2022). With this approach, the designed TLV for delivering ESS through specific geometry of branches can be best approached. \n\nResult and Conclusion \n\nThe combination of the steps “setting TLV”, “tree planting”, and “tree management” descibes \na semi-automated tree planning workflow. It is a method for enabling a maximum leaf coverage in cities to increase biomass and ESS without conflicting with grey infrastructures and \nother functionalities in cities. \n\nThe approach is very different from the established way of design thinking, which is thinking \nin objects. Thinking and designing with voxels is something very abstract, partly even counterintuitive. That could be a hurdle to establishing the approach in landscape architecture and \nurban  planning.  In  addition,  developing  the  3D  voxel  model  approach  further  to  consider \nmore qualitative and quantitative factors such as social and visual comfort can be a motivation for future research. A 3D voxel model contains quantitative data with geometric features \nbut considering qualitative features such as visual comfort through the 3D voxel is a knowledge gap that still needs to be closed. Therefore, a current limitation is how to translate qualitative factors to a 3D geometry dataset like a voxel model. Moreover, currently, the approach \nsolely builds on a volumetric description of the growth space. Very important factors like \nsoil compaction or events like droughts are not adequately represented. Furthermore, root \nspace influences tree growth and ESS delivery. Fleckenstein et al. (2022) investigated the \neffect of the different root space conditions on DBH growth, the total number of leaves, and \nleaf transpiration in some specific species (FLECKENSTEIN et al. 2022). Limited root space \nrestricts root development and tree growth. These problems cause further difficulties of stability and lack of water and nutrients, which could result in xylem embolism and ultimately \nin tree death (HITCHMOUGH 1994). To include some of these aspects, the approach could be \ncoupled with plant growth simulation tools such as CityTree (RÖTZER et al. 2019, 2021). \n\nHowever, the advantage of our approach compared to other possible approaches is the ability \nto predict the chance of ESS achievement by designing a target-driven UGI. To provide the \nbest ESS with precise tree geometry, one option is that an urban tree designer directly draws \nthe desired geometry of branches with polylines in 3d modeling software; only branches that \ngrow  within  a  certain  distance  from  these  planned  polylines  are  kept,  and  others  will  be \npruned away every year. But the tree design in this way requires in-depth knowledge about \ntree growth and manipulation. A bad “design” can easily lead to the failure of branches in \nrealizing the desired geometry. To avoid this, targeted leaf voxel (TLV) is a suitable media \nto transfer tree design to commands for tree manipulations. By transferring the real condition \nof the trees to a voxel model and comparing it with the TLV, urban designers can plan to \nprune and manipulation. For example, they can prune the branches in forbidden voxels or \nbend other branches to lead them to target voxels. Figure 5 illustrates a proposed approach \nfrom an existing urban tree to a voxel model. Furthermore, leaf area density (LAD) (BÉLAND \net al. 2014) could be an extra feature in voxels. Urban designers can input the desired LAD \nfor every voxel by coloring them. A darker voxel means desiring a denser canopy in this \ncubic space. When such a design represented with LAD in voxels is set, it is fed to a decision-making mechanism for approaching a minimum of the deviation between the real LAD in \nspace under potential tree growth and the ideal target LAD. Therefore, the proposed workflow in this research is a target-driven leaf and root voxel design for maximizing the leaf \nvolume and designing a LAD target. \n\nConstruction of Pocket Park Network Systems in  \nUrban Built-Up Areas: A Case Study of Harbin City \n\nAbstract: Pocket park planning is an important tool for urban habitat enhancement, yet previous studies \nhave not often considered pocket park network construction. In order to determine the spatial differences in the demand for pocket parks, this article adopts a systematic perspective and uses the built-up \narea of Harbin city as an example. It does this by extracting the existing green space network, evaluating \nthe spatial suitability of pocket parks, and combining the two previous steps. The results show that the \ndensity of buildings and roads in the city are negatively correlated with the supply of ecological resources; there are differences in the demand for pocket parks among spaces in the built-up area. By \ncomparing the current situation with the simulation, the designed network of 88 pocket park corridors \nwill effectively alleviate the problem of green space shortage. In addition, from the perspective of the \nnetwork system, we propose three strategies for constructing pocket parks for different spaces, providing a new approach to enhance the urban living environment and promote the harmonious development \nof humans and nature. \n\nIntroduction \n\nAn urban  green  space built-up area is a large densely populated human  settlement. Rapid \nurbanization has resulted in the disorderly expansion of the city, the reduction of the green \nspace area of the urban built-up area, the serious fragmentation of the ecological environment, and the inefficient use of urban construction land (ZHANG & HAN 2021). Pocket parks \nare characterized by high flexibility, small scale, human dimension, diverse functions and \nhigh accessibility. At first, addressing the problem of the shortage of urban green space resources, relevant studies show that successive pocket parks become places for leisure activities of urban residents (WU 2015) and provide ecological corridors for animal migration in \nthe  city  (ALEXANDER  et  al. 1997).  However,  past  studies  seldom  constructed  pocket  park \nnetworks  from  the  perspective  of  landscape  ecological  network  to  solve  the  contradiction \nbetween biodiversity protection and human demand for natural resource in cities. Therefore, \nthe urgent problems to be settled are how to construct pocket park networks, build pocket \nparks according to, and create a harmonious development of humans and nature. \n\nAs for the construction methods of urban green space networks, most of them pay attention \nto a city’s scale and urban agglomeration. For example, the identification of landscape ecological networks based on minimum consumption distance (BEIER et al. 2008); the design of \necological networks with emphasis on species conservation based on electric current theory \nand  random  wander  model  (MCRAE et  al. 2008);  the  identification  of  ecological  networks \nfrom the consideration of the minimum cost path of species migration (BEIER et al. 2011). \nPrevious studies have often been applicable to the scale of cities or urban agglomerations. \nHowever, for urban centers with dense buildings and insufficient green areas, relevant studies \nare not enough to achieve the goal of building micro-green space networks. For pocket parks, \nprevious studies mainly focused on how to design and construct them, but did not integrate \nurban pocket park space from the perspective of network to form the network of pocket park \nspace. For instance, design pocket parks from the difficulty of maintenance (NORDH et al. \n2009); design the remaining spaces in the city as plant-related pocket parks (NAGHIBI et al. \n2021); put forward the design strategy of traffic-oriented pocket park from the perspective of \ndisplaying  urban  culture  and  large  flow  (LIU 2019).  Therefore,  it’s  difficult  to  intuitively \nquantify  the  spatial  demand  difference  of  pocket  park  space  system  and  construct  urban \npocket park space pointedly. \n\nTaking the built-up area of Harbin as an example, the article analyses the spatial demand of \npocket parks and constructs a pocket park network system through methods such as landscape \nform spatial pattern analysis (MSPA) and hierarchical analysis (AHP). The specific objectives of this study are 1) to identify the green space network in the built-up area of the city \nusing  the  MSPA  method;  2)  to  construct  a  pocket  park  suitability  assessment  system  and \nobtain a pocket park suitability assessment map through the AHP method; 3) to integrate the \nabove  assessments  to  obtain  a  pocket  park  spatial  demand  assessment  map  and  design  a \npocket park network. \n\nMaterials and Methods \n\nStudy Region \n\nGuidelines for the Preparation of Municipal Territorial Spatial Master Plans (for Trial Implementation), published in 2020 by Chinese government, emphasize the balance and accessibility of green spaces in urban centres and encourage the construction of more green spaces \nand open spaces for residents' activities in urban centres. The built-up area of Harbin city is \nlocated in the western part of Harbin, with a total area of about 495.02 km2 accounting for \n0.93%  of  the  area  of  Harbin  city,  but  only  78.13%  of  the  total  population  of  Harbin  city \n(2021). The built-up area has a small land area and a high population density. According to \nHarbin  Yearbook  2018,  Harbin's  urban  gardening  coverage  rate  is  33.7%,  which  is  lower \nthan  the  average  of  non-garden  cities.  2021,  to  implement  territorial  spatial  planning,  the \nconstruction of a park city is proposed, and the formation of a “city park – community park \n– pocket park” three-type park system is improved to enhance the quality of the human living \nenvironment. \n\nData Sources \n\nThe following data were used in the research for this article: \n1) The 2020 Landsat8 OLI_TIRS satellite digital map, road network map, 250m×250m resolution Normalised Vegetation Index (NDVI) data and 30m×30m resolution Shuttle Radar Topography Mission (SRTM) data were  obtained  from  the  Resource  and  Environmental  Science  Data  Centre  of  the  Chinese \nAcademy  of  Sciences  (https:\/\/  www.gscloud.cn),  the  satellite  digital  maps  were  pre-processed using the maximum likelihood tool of ENVI 5.2 to obtain the site classification maps. \n\n2)  2020  night  time  light  data  images  (NPP-VIIRS)  data  from  (https:\/\/www.mines.edu). \n\n3)  2020  1km  x  1km  resolution  World  POP  population  density  map  data  from \n(https:\/\/hub.worldpop.org), which was calculated and mapped at the initiative of the University of Southampton. 4) The full version of the bus stop and park attraction data has yet to be officially released. In order to analyse these data, the location coordinates of the stops and attractions were obtained by batch query in Gaode Map via Python, and the spatial coordinate system was corrected by a geographic coordinate system conversion tool. \n\nMethods \n\nThe method can be divided into three steps (Figure 1). The first step was to extract the green \nspace network pattern of the urban built-up area using the MSPA method and the connectivity \nindex (PC). The second step is to establish the pocket park suitability evaluation system using \nAHP and obtain the pocket park suitability spatial assessment map. Based on the results obtained from the above two steps, pocket park network system are designed. \n\n\nGreenspace Network Pattern Analysis Based on the MSPA Approach \n\nGreenfield Core Identification \n\nEcological sources are identified, and data for resistance surface construction are obtained \nthrough MSPA, a technique for processing raster images based on the mathematical and morphological principles of erosion expansion, open and closed operations (GAO et al. 2019). \nThe data is widely used in the ecological network planning to identify study area's structure \nand type of landscape. The article identifies habitat core areas (e. g. landscape areas, wetland \nparks, etc.) through MSPA. In the article, the core areas of habitats (e. g. landscape areas, \nwetland  parks,  etc.)  identified  by  MSPA  are  selected  for  more  than  3ha,  the  connectivity \nprobability (PC) of the core area patches are calculated, and the importance of the core area \npatches is determined based on the PC assessment of connectivity. \n\nIn the formula, n denotes the total number of patches in the landscape, ai and aj denote the \ndenotes the maximum probability of direct dis-area of patches i and j, respectively, and \npersal of species in patches i and j. According to this formula, the ranking of the degree of \ncontribution of the patch to the connectivity of the entire green space network is obtained and \nthe relative importance percentage (dPC) of the patch is obtained. The connectivity indicators \nwere calculated using conefor2.6 software, and the ecological sources were screened and the \ncore areas of the green spaces were delineated according to the results of the indicators. \n\nGreenfield Network Resistance Surface Reviews \n\nThe article aims to extract the original green space network, so the resistance surface is constructed  by  selecting  the  original  land  types  in  the  city  as  indicators  and  constructing  the \nindicator system. MSPA identifies the landscape types and selects the core and bridge areas \nas evaluation indicators. The remaining core areas are divided into very important core areas \n(1<dPC≤3), important core areas (0.3<dPC≤1) and general core areas (dPC≤0.3); the bridges \nare  divided  into  very  important  bridges  (dPC>0.8),  important  bridges  (0.2<dPC≤0.8)  and \ngeneral bridges (dPC≤0.2); the land use types are divided into seven categories as evaluation \nindicators. Different resistance values – ranging from 1 to 1000- were assigned to different \nindicators using expert evaluation based on how resistant they were to species migration. \n\nGreenfield Network Building \n\nUrban greenspace networks provide habitat for organisms in cities, while improving the ecological quality of habitats and promoting cities towards species diversity and landscape diversity (KONG & YIN 2008). First, the cost connectivity tool in the Spatial Analyst tool of \nArcGIS 10.8 was used to generate the minimum paths between patches. The corridors connected by greenfield patches with dPC>1 were treated as important ecological corridors. In \ncontrast, the corridors connected by the remaining patches were treated as general corridors \nto obtain the greenfield network map of the study area. The kernel density analysis can obtain \nthe density of corridors in the surrounding space and provide key guidelines for enhancing \nthe green space network. The nuclear density analysis tool in ArcGIS 10.8 was used to obtain \na map of the nuclear density distribution of corridors in the study area. \n\nAHP-based Spatial Suitability Assessment of Pocket Parks \n\nAHP is a simple, flexible and practical multi-criteria decision-making method for quantitative analysis of qualitative problems. Many factors that influence the construction of pocket \nparks, so three groups of variables were considered when establishing the assessment index \nsystem: vegetation suitability, residents' demand and spatial accessibility (Table 1). The suitability  was classified into four levels using the quantile classification  method,  with corresponding scores of 1, 2, 3 and 4, reflecting the suitability of the space. Through the expert \nscoring method, a hierarchical model of the selected factors was constructed on Yaaph software, and the expert scoring data was included to obtain the ranking weights of individual \nfactors. Using the weighted sums in the Spatial Analyst tool in Arcgis 10.8, the spatial suitability distribution map of the pocket park was obtained. \n\nPocket Park Network System Construction \n\nPocket Park Space Needs Assessment \n\nPocket park demand refers to the spatial distribution of demand for pocket parks in built-up \nurban areas based on the services provided by existing green spaces. The service radius of a \npocket park is about 500m. Using the multi-ring buffer tool in Arcgis 10.8, the spatial distribution of green space network services is obtained by assigning values to the corridors and \npatches based on their distances from each other. After obtaining the spatial distribution of \ngreen space network services and the spatial suitability assessment map of pocket parks using \nthe weighted sum tool, the spatial demand assessment map of pocket parks  was obtained. \nFirstly, according to the assessed values, the park is divided into five levels, which are “low”, \n“relatively low”, “medium”, “relatively high” and “high”. \n\nPocket Parks Network \n\nThe pocket park's network are designed following an evaluation of its spatial requirements. \nRefer to the following basis: \n1) To enhance the connectivity between landscape patches and \npromote the efficient flow of materials, energy and information. \n2) As a complex human-nature-society mega-system, the pocket park network should be designed with the needs of \npeople and nature in mind to achieve harmonious development between people and nature. \n3) Respect the existing conditions of the city, rely on the existing urban pattern and green \nspace network, and design a pocket park network between existing green space resources to \nenhance the resilience of the urban built-up area system. The pocket park network system is \neventually obtained. \n\n\nResults \n\nGreenfield Network Patterns \n\nGreenfield Core Distribution \n\nBased on the MSPA, below are the details: 1) the ecological core areas within the built-up \narea of Harbin city are primarily distributed in the southeast and northwest sides of the source \nland with better ecological resources connected by short distance corridors; the remaining \ncore areas (>3ha) are relatively small in area, independent of each other and scattered within \nthe built-up area of the city. 2) Among all land use types, the largest area of residential land \nis mainly distributed in the central-eastern part of the built-up area, with an area of about 25% \nof the total area, but the number of core areas only accounts for 7%; it can be concluded that \nthe core areas of green space within the built-up area of Harbin city are unevenly distributed \nand relatively scattered. The north side of the Songhua River has an aggregated distribution \nwith better ecological resources. In contrast, in the central and western parts of the built-up \narea, the supply of ecological resources decreases with the increase in floor area. \n\nResistance Surface Assessment \n\nThe results of the expert scoring show that the resistance values of buildings, roads, other \nsites and water bodies are more than ten times those of the other elements in the evaluation \nindex. The resistance values of the urban built-up area are shown in figure (Figure 3). 1) The \narea with resistance values higher than 500 is 371.75 km2, accounting for 75.18% of the total \narea, while the area with resistance values lower than 10 is 29.72 km2, accounting for 6% of \nthe total area; 2) The areas with higher resistance values are mainly located in the old urban \nareas of the city, especially the central areas of the built-up areas and the old urban areas of \nDawai,  while  the  areas  with  relatively  low  ecological  resistance  values  are  located  in  the \nbuilt-up areas of the city. The areas with relatively low ecological resistance values are found \nin Jiangbei and the new Qunli district, on the border of the built-up area, and have the propensity to spread out along rivers and roads. In summary, it is concluded that 1) the ecological \nresistance of the existing built-up areas is directly related to the type of urban land use, with \nthe older urban areas suffering from a serious lack of ecological resource supply due to the \nhigh floor area and road area; 2) In the built-up areas of Harbin, areas with low resistance \nvalues are located around areas with high resistance values and are poorly connected. In general, earlier urban planning needed to consider the layout of green space, which led to insufficient green space and limited distribution of construction land in built-up areas, resulting in \nhigh overall resistance values in built-up areas.  \n\nGreenfield Network Building \n\nAs shown in the figure (Figure 2), 1) Harbin's built-up area has 13 significant green space \ncorridors, totalling 48.80 km in length, forming a green space corridor through the north and \nsouth. In addition, there are also 208 general ecological corridors with a total length of 338.57 \nkm, concentrated in the northern and western areas. 2) The green space network in the built-up area of Harbin forms three high-density zones, the northern part of the Songhua River, the \nQunli New Area and part of Xiangfang. Many corridors connect the inner areas with strong \nmaterial circulation, and corridors connect the high-density areas; the central and  western \nareas have a small number of corridors and poor connectivity; the remaining areas have a \nfragmented  distribution  of  corridors  and  patches  (Figure  3).  In  summary,  the  green  space \nnetwork in the built-up area of Harbin could not be more evenly distributed and better connected, with small spatial clusters and sparse distribution in most areas. \n\nPocket Park Spatial Suitability Assessment \n\nThe weighting results of the elements (Figure 4) show that residents' needs are the most important of the three elements and dominate the suitability assessment. The proportion of each \nelement and its sub-elements is shown in the table (Table 1). The first category (vegetation \nsuitability)  accounts  for  only  9.36%  of  the  total,  and  factors  such  as  slope,  elevation  and \nNDVI account for less than 5%, which shows that they have a relatively small impact on the \nsuitability assessment. The second category (residents' demand) accounts for 62.67% of the \ntotal, becoming the dominant variable in the suitability distribution, and the distribution of \npopulation accounts for 35.72%, far exceeding the other sub-elements. The third category \n(spatial accessibility) accounts for 27.97% of the total, with the distance to the bus station \naccounting  for  about  10%  more  than  the  first  category.  In  summary,  the  construction  of \npocket parks is influenced by the accessibility of the surrounding residents and space, while \nthe suitability of the space for planting is less influenced. Pocket parks are suitable in areas \nwith high population density and dense road networks. \n\nThe spatial suitability assessment of pocket parks based on the above table shows that the \nsuitability is higher in the economic centre (Songlei, Zhuozhan Shopping Centre), cultural \ncentre (Central Avenue, Sophia Cathedral) and political centre (Harbin Municipal Government) of the city, while the suitability is relatively low in scenic areas, parks and wetlands.  \n\nIn summary, residential areas and urban centres with high population density and dense buildings are more suitable for pocket parks and are distributed in a faceted manner. Areas with \nlow  suitability  are  those  with  a  good  existing  ecological  environment  and  low  density  of \nbuildings and road networks. Based on the definition and characteristics of pocket parks, the \nsmall size and flexible layout of pocket parks, which are mostly built on the streets of cities, \nprovide convenient leisure space for residents and support the reliability of the article's results. \n\nPocket park Network System Construction \n\nPocket Park Space Needs Assessment \n\nBy integrating the above data, the spatial distribution map of pocket park demand is obtained \nand  shown  in  figure  (Figure  5). The  demand  for  pocket  parks  in  the  city's  center  and  the \nhistoric district of Dawai is high, accounting for more than 75% of the total high-demand \narea. The demand is distributed in a face-to-face aggregation, and the demand on the west \nside is distributed in a face-to-face pattern, but the demand is lower than that in the central-eastern area. The high demand on the north and south sides is spread sporadically, with large \nspatial differences in demand. To sum up, there are distinct regional variances in the demand \nfor pocket parks, with the demand being highest in old urban areas with a high population \ndensity, densely populated roads, and limited green space. \n\n\nPocket Parks Network \n\nThe roads are used as a backbone to connect small green spaces, creating a green vein that \ncan provide a place for leisure activities for the city's residents. A total of 88 pocket park \nnetwork were designed according to the demand, with a total length of 298.66 km (Figure 6). \nBy comparing the spatial demand of the pocket parks before and after the design, it was found \nthat the area of the high demand area decreased from 53.631 km2 to 1.07 km2, the area of the \nlow and lower demand areas increased from 63.6% to 90.6%, and the connectivity of the \noriginal  green  space  network  improved.  In  summary,  the  pocket  park  network  effectively \ncomplements the existing urban green space network, satisfies residents' demand for green \nspace, promotes the flow of material and energy, and confirms the accuracy of the study's \nfindings. \n\n\nDiscussion  \n\nThis paper takes the built-up areas of Harbin as an example. The research results show that \nthe lack of green space in built-up areas is mainly due to the high density of buildings and \nroad network in early urban planning: Because of the concentration of population the demand \nis not proportional to the supply and results in a poor quality of human settlements. Consequently, this paper increases corridors according to the specific needs of space and improves \nthe living environment in the planning of pocket park network. This paper develops a method \nof constructing pocket parks by using the graph theory of minimum cost distance and landscape suitability. This study constructs the pocket park network system based on the source-sink theory and landscape suitability theory. The process follows two main objectives of extracting the green space network through MSPA and minimum cost distance, maintaining the \nconnectivity of landscape. The second is to analyze the space demand of pocket park from \nthe perspective of space suitability. This study proposes a network construction method based \non different scales and multi-party requirements. It’s beneficial to the restoration of urban \necology and the long-term harmonious coexistence between man and nature in practice. This \nstudy provides a reference for the improvement of pocket park construction in terms of methods and results. Firstly, based on green space  network, this paper supplements the  microgreen space virtual network and further integrates landscape ecology and space suitability \nprinciple. Secondly, different from the design and construction of the existing pocket parks, \nthis study incorporates the space network, and also refers to the needs of residents, animals \nand plants. Such pocket park networks can maintain the harmonious development of human \nand nature in urban built-up areas, maintain the connectivity of landscape in urban built-up \nareas,  meet  urban  residents'  yearning  for  natural  environment,  make  the  construction  of \npocket parks more pointed, and improve the service function of urban green space system.  \n\nConclusion and Outlook \n\nThis paper makes the following conclusions: \n\n1) In 2020, green space resources in Harbin's built-up urban areas were negatively correlated  with  building  and  road network  density.  More  green  space  and  expensive  land \nprices lead to high costs for building large green parks and scenic areas. Building pocket \nparks to meet residents' green space needs is more appropriate. \n\n2)  There are differences in the demand for pocket park networks in different areas of Harbin's built-up urban area. Older urban areas have the most urgent need for pocket parks \nand are suitable for building a high-density pocket park network; newer urban areas are \nsuitable for linking high-demand areas in series to form intersecting pocket park corridors; urban areas are connected by micro-green space wandering lines.  \n\n3) In the upgrading of green spaces in built-up areas of the city, a network of pocket parks \ncan be established by designing network according to local conditions, improving the \neffectiveness of the network and rational pre-planning.  \n\nIn response to the low efficiency of green space in well-developed communities and the imbalance between supply and demand, a scientific method of constructing a pocket park network is proposed, which not only interfaces with the existing greenbelt network of the city, \nbut  also  provides  guidance  for  the  selection  of  sites  for  the  construction  of  pocket  parks. \nDemand  for  recreation  varies  between  residents,  but  the  paper  failed  to  specify  the  target \npopulation, so it needs to be further developed. \n\nExperiences with Volunteered Geographic \nInformation (VGI) on a Small Street Tree Inventory \n\nAbstract: This paper revisits early ideas of the promise of volunteered geographic information (VGI) \nand investigates ways that current VGI tools and methods do or do not support simple VGI projects. \nThe primary lens for this investigation is a community mapping project that has built a geospatial database for an annually updated street tree inventory. While simple in its conception, the project has \nencountered various citizen science and VGI barriers to maintaining its annual progress. \n\nIntroduction \n\nWriting  about  local  government  GIS  in  1995,  Ventura  predicted  that  “As  local  citizens' \ngroups learn more about GIS databases and technologies, they may force local government \nto use GIS more effectively.” A decade later, volunteered geographic information (VGI) had \nemerged as an important counterpoint to public participatory GIS (PPGIS), showing his prescience while raising questions about whether VGI and PPGIS were becoming indistinguishable (TULLOCH 2008). In that moment it seemed that an imminent proliferation of new technologies, then loosely categorized as neogeography, would erase old disciplinary boundaries \nand disrupt  processes  in ways  that  would alter  basic  public-interfacing GIS work.  Today, \nfaced with a myriad of user-friendly handheld technologies and tools, it is worth reflecting \non the changes that have occurred, but also those that have not.  \n\nThis  paper  reflects  on  perspective  of  VGI  collection  by  examining  a  basic  volunteer  GIS \nproject: a community street tree inventory. Many VGI projects have been far-reaching, like \nthe  worldwide  data-collection  and  data-correction  approach  of  the  Open  Street  Map \n(HAKLAY 2010). More recently, attention-grabbing VGI research has included high-volume \noptions like iNaturalist (YAN et al. 2020) demonstrating that VGI is an expansive field with \nscientific applications. The nature of a community street tree inventory is useful for the examination because of its seeming simplicity, but also through its need for updates reflect a \ndynamic set of features. For the example considered in this paper, the project has relied on \ndifferent quantities and quality of volunteers across more than 5 years of a public inventory \nprocess.  Its  progress  as  a  volunteer  project  rather  than  a  consultant-produced  product  has \nforced useful conflicts that still often result in practical decisions about mixing generations \nof technology (e. g., jumping between paper field maps and online interactive maps without \nusing handheld input apps).  \n\nHowever, this also raises questions that, in 2008, seemed like they would be more resolved \nby now. While initial concerns about replicability and reproducibility  may have imagined \nwild hypothetical outcomes, scientific applications have suggested that the usability of contributed  data  can  be  substantial  when  situated  properly  (OSTERMANN  &  GRANELL  2017). \nWith  voluminous  contributions  made  globally  to  citizen-science  databases  like  eBird  and \niNaturalist  (ZHANG 2021),  shouldn’t  a  focused  community  resource  VGI  project  be  fairly \nstraightforward? The volunteered data portion has proved to continue to be difficult. Why? \n\nHighland Park Street Tree Inventory \n\nHighland  Park  Borough  is  a  small  municipality  covering  less  than  5  square  kilometers  in \nCentral New Jersey, USA. With only about 15,000 residents, it is a modest sized community \nwith neighbourhoods where many houses and streets were built around 100 years ago. As \nsuch, it is known in part for some areas with large old street trees and a mature urban forest \nthat  extends  beyond  the  streets.  Responsibility  for  the  street  trees  is  held  by  the  Borough \ngovernment and overseen through an official Street Tree Advisory Committee (STAC) of \nresident-volunteers appointed by, and reporting back to, the Borough government.  \n\nIn the most densely populated state in the US, much of which is either urban or denselypopulated suburban, information about street trees is an important potential piece of information infrastructure. However, for many communities, street tree and urban forestry management and maintenance are largely funded with state or other external grant support. Grant \nproposals often include a scoring mechanism that rewards communities with street tree inventories already in place. This is a challenging problem as New Jersey is divided into over \n500 municipalities, which includes over 300 that have fewer than 10,000 residents. Communities with smaller populations are continually challenged to use mapping technologies in \nways that appropriately reflect and, perhaps, shape their landscapes, hence the opportunity to \nleverage volunteers as a way to improve the work of the STAC and secure more funding. \n\nIn 2016, the Highland Park Street Tree Inventory (HPSTI) was initiated by a resident with \ngeospatial experience using multiple approaches to capture potential locations of trees. Several high school students were trained to use basic GIS tools and taught to identify a few \ncommon species of trees. They went into the field with maps of the potential locations to \nverify or correct the initial dataset. At the end of a second year of fieldwork, with the help of \na few adults and the students, the HPSTI completed a first map of the entire borough and \nover 3,000 verified street trees. \n\nOver the next several years, the techniques had to change as the students graduated and became unavailable and as some potential volunteers had different levels of comfort with technology (tree lovers are not always the most computer savvy community members). However, \nin each subsequent year, he HPSTI has been updated with a re-confirmation of the existing \ntrees, adjustments in species identification or location and additions and losses as trees are \nplanted or removed. Integrating the updates across different geographies, with information \nfrom  multiple  sources  and  technologies,  the  HPSTI has  experienced  inconsistent  methods \nand techniques to develop the best possible easily-captured data that does not use methods \nthat might exclude or alienate potential volunteers. \n\nWhile a few species are uncommon, a small number of species represent the majority of trees. \nThe inventory found that 6 species each had over 100 trees, and combined to represent just \nover half (51%) of the street trees in the borough. The inventory found 16 species with 50 or \nmore trees, which combined for 75% of the Borough’s street trees (Figure 1). This is similar \nto many other cities, but also demonstrates the opportunity to employ volunteers with limited \nplant identification skills. As noted later, however, it is unclear if perceptions of expertise \nlimit volunteer confidence. \n\nCooperation and Compromises \n\nEarly in the project, a common concern was that complex methods or high expectations might \ncause the volunteers to quit without completing a first round of inventory. In order to improve \nthe  chances  of  successful  completion,  the HPSTI volunteers  and  the STAC  cooperated  to \nidentify unnecessarily time-consuming tasks. One early example came after experimentation \nwith different measurement techniques like Biltmore sticks and diameter tape (devices commonly used by arborists to quickly estimate or measure tree size based on the trunk’s diameter)  when  it  was  decided  that  simpler  size  classes  and  estimation  would  suffice  in  many \nsituations. Another was the decision to collect data on tree condition and damage to canopy, \nexcept in extreme situations. \n\nAs the project advanced, mapping compromises emerged. Handheld GPS devices gave way \nto phones. Data collection apps like Survey123 were tried, but novice volunteers balked at \nthe complexity and GIS-educated volunteers wanted more editorial access. Experiments are \nbeing  considered  applying  data  collection  tools  integrating  real  time  kinetics  (RTK)  and \nglobal  navigation  satellite  system  (GNSS),  which  are  high-precision  alternatives  to  traditional GPS. But for much of the work, the inventory streamlined the field workflows by extracting a draft database from multiple years of aerial photography and then using field visits \nto confirm locations, identify errors of omission, and identify specimen size and species. It \nmay reflect on the potential volunteers with interest in trees, but technology did little to draw \nin more volunteers. Instead, simple processes and a feeling of completion were seen as key \nelements. More importantly the particularly narrow, linear nature of the database altered the \nways in which spatial accuracy mattered, shifting the needs in the field. Ultimately, the database reflects a combination of multiple techniques. \n\nWith a completed database and several years of updates, the simply-developed HPSTI has \nalso been used to generate a variety of more-advanced analytical outcomes. Online mapping \nhas been used to generate feedback. The data have been analysed to identify opportunities \nfor new planting. Stretches of homogeneity have been marked as areas of concern for pests \nand disease management. And change over time has been illustrated in animated cartographic \nproducts. \n\nImplications \n\nThere  have  been  much  larger  street  tree  or  urban  canopy  VGI  projects  (e. g.,  FOSTER  & \nDUNHAM 2015). But the promise of VGI included local empowerment and individual applications, so reflections of VGI success and failures should include modest work and diverse \nfunding and training support. In addition, a continuing concern has been the ways that ubiquitous technologies, sensors and feedback might change users (JOHNSON et al. 2020) but the \nHPSTI and its community instead shifted more to ways that technology needed to adjust to \nfit the project and its volunteers.  \n\nFor years this smaller project has relied on motivation in ways that seemed different than \nother citizen science research (JENNETT et al. 2016). Novelty accounted for little, despite its \nprominence elsewhere. Still, new technologies continue to pique the curiosities of potential \nvolunteers. Handheld apps using the phone camera and AI to identify plant species have recently sparked new interest. It may be that novelty can still be leveraged as a means for recruiting participants. \n\nWhile larger cities will remain data-rich environments, this project serves as a reminder that \nthere may be ways to collect data and build databases relying on VGI or other crowdsourced \napproaches. Even as landscape architects organize community workshops and encourage the \npublic to share opinions and local knowledge, there are larger opportunities to structure these \ninputs and collaborate with motivated residents. While these more closely resemble scientific \nground-truthing  citizen  science  than  creative  charettes,  they  may  still  result  in buy-in and \nlasting support. \n\nRevisiting VGI, Participation and Engagement \n\nEarly efforts in citizen science and VGI raised concern with public contributions that purported to be science-based without an appropriate knowledge level. Included in these early \nframings were questions about ‘Why do people do this?’ (GOODCHILD 2007) How should \nthese contributions be vetted or confirmed? Should the contributors be vetted or certified? \nSome of the larger citizen science projects have leveraged VGI to advance science; eBird has \nproduced large VGI datasets used to model seasonal patterns of avian populations (FINK et \nal. 2020). But VGI has also been demonstrated as useful for smaller projects like the community infrastructure and geodesign project by SEEGER et al. 2014. \n\nIn addition to inventory volunteers, the project has also received a modest amount of input \nfrom the general public. The interactive online map assigns each tree a unique ID and asks \nfor submissions about existing trees to use the ID. Additionally, the HPSTI page requests \nsubmissions informing the Borough’s STAC of new street trees. In 5 years, there have only \nbeen a handful of public submissions; all were corrections or modifications rather than potential omissions or removals; twice the submissions have been made by homeowners wanting to be sure their personally acquired unusual or exotic specimen was acknowledged in the \ndatabase). While public engagement is high when verifying street trees in the field, the online \ndatabase may overwhelm even fairly interested parties. In addition, GIS maps carry an implied seriousness that makes them seem complete even when accompanied by requests for \ncommunity-generated updates, corrections and edits.  \n\nEarly examples also raised concerns about whether the novelty of the tools was part of the \nappeal. It was hoped that phone apps and interactive online maps would attract a wider variety \nof volunteers, especially since the initial group of volunteers included several high school \nstudents. Instead, the volunteers of all ages have largely preferred paper maps. After at least \nsix years of trying different technologies and approaches, paper maps are still dominant as \nthe HPSTI tool for annual field work (Figure 2). The hand-marked field maps are referred to \nlater, to edit the database using desktop GIS software. \n\nUltimately, a larger question is simply whether the tool fits its public. While they may have \nbeen focusing on larger applications for complex democratic processes, SIEBER & HAKLAY \n(2015) point out how important context can be for appropriate outcomes: “Framing a civil \nsociety  participation  via  VGI  (and  its  mobile  permutations)  requires  a  conscious  effort  to \nrender the technology and the way that it is used in a specific social context relevant to the \nvalues of an organisation or case study.”  \n\nThe continued efforts to expand engagement with the HPSTI, in a community where many \nresidents appear to be interested in the associated public resource decisions, would suggest \nthat there is a potential misfit between the currently constructed tool and the community. This \nisn’t meant as a highly negative comment as much as a recognition of the remaining potential \nfor VGI to see increased use in a motivated community like Highland Park. Still, maybe the \ngap is the combination of citizen science with community policy. Which one would volunteers be participating in? Without a clear distinction, do other motivating factors get minimized? Fortunately, the inventory continues, with each year’s field confirmation and updates \npresenting a new opportunity for testing new ways to engage and sustain participation from \ncommunity volunteers. \n\nLandscapes between Signal and Data: Formal \nIdentification and Analysis of Forest Clearings in \nOslo through Lidar Data \n\nAbstract: This article investigates how airborne light detection and ranging (lidar) point cloud data and \ntheir embedded data models can serve as the basis for revealing, studying, and manipulating clearings \nin the urban forest situated in the north of Oslo. Furthermore, the article describes the unique high-resolution public and open digital data and infrastructure that surveys these landscapes in Norway. The \nmain research question is concerned with whether the extensive public data available in Norway can \nreveal unseen and undervalued clearings with potential value as remnants of an important and unique \ncultural landscape practice. The methodology involves sampling data models from prevalent publicly \navailable urban planning and forest management datasets and applying them to high-resolution lidar \nscans of the forest. Beyond the initial identification of precise landscape figures, this methodology introduces a creative visual workflow to appreciate, evaluate and work with the previously undervalued \nclearings in the Oslo forest. The article further hypothesizes on how certain morphological conditions \nof the Oslo forest clearings are only thoroughly visible and open to analysis and manipulation through \nhigh-resolution lidar surveys. \n\nThe value of the particular case of Oslo is twofold. First, the databases in Norway for performing such \nanalysis are public, facilitating the necessary accessibility to conduct such research. Second, a history \nof careful urban forest management, reconciling the public use of the forest with the interests of commercial forestry, have developed into unique and delicate ways to perform clear-cutting in the urban \nforest around Oslo. The convergence of these two outlined unique conditions present the need to perform this study in Oslo rather than in other locations; however, the workflow and methodology described here could be applied to other forested areas in the presence of available data and unique aspects \nof clear-cutting that can be re-valued through imaging. \n\nThe nature of lidar data collection, analysis, and applications in a design workflow are generally understood,  and its  disciplinary  implications  are  generally  well  delineated  (URECH  et al.  2020);  however, \nworking creatively with the data opens an under-explored arena for design disciplines, one in which the \naesthetic, poetic and formal evaluation of the data is used not to classify the land and tree cover (BELL \net al. 2018), but to instead scout and reveal landscape entities typically seen as of lesser value or simply \nunder-described. \n\nAs geospatial data becomes raw matter and part of the everyday design workflow of architecture and \nlandscape architecture practice (URECH et al. 2020), designers need new ways to frame data as creative \nmedia in these disciplines. In this article, we describe the base condition of geospatial data as multivariate (ASH et al. 2018) – many definitions and derivatives of itself in dependence on the systems they \nare embedded in. We introduce terminologies such as model sampling and data scouting to give way to \nthe formulation of data models in landscape architecture, with a particular focus on lidar surveys of the \nforest around Oslo. \n\nIntroduction \n\nThe clearing is a preeminent and timeless landscape figure that has been framed in architecture and landscape architecture as archetypal, particularly in the west, going as far as describing the origin of the discipline through this spatial metaphor (DRIPPS 1999, GIROT 2016). In \nthis paper, we interrogate the undervalued existence of the myriad of clearings near Oslo by \nusing contemporary surveying technologies that further reveal what is (morphologically) particular about the clearings around Oslo. We aim to elevate the cultural landscape value of the \nclearings  surrounding  Oslo,  first  by  revealing  them  and  then  carefully  describing  them \nthrough the lidar surveys that depict them with high precision. \n\nThe research, workflow, and images produced by the authors revealing and  analyzing the \nclearings were later tested as two separate yet interrelated experiments with master students \nin order to test the operationality and transmissibility of this methodology with students that \nwere tasked with re-designing clearings: The first was in the context of an advanced master \nstudio at the architecture school at Yale in collaboration with the landscape program at AHO \nin Oslo in 2020 (CALLEJAS 2021). In this first case, students could not visit the sites and only \naccess them through the analysis of data models. In the second case, the methodology was \napplied in the context of the exhibition “arboretum, l’arbre comme architecture” at arc en \nrêve centre d’architecture in Bordeaux, France, in 2021. The studio between Yale and AHO \ntested the transmissibility of this method to master’s students in architecture and landscape \narchitecture, while the exhibition at arc en rêve generated new visuals from the research following the refined methodology. \n\nThe research on the clearings is independent of the experiments with the master’s students; \nhowever, testing the methodology with students exposed how this workflow can empower \ndesigners tasked with intervening in those landscapes where the forest meets the urban fabric. \nIt was of particular importance that the author’s research could be disseminated among students with a clear methodology that they could use for their own manipulations of the clearings. These formal manipulations are not the main output of the research but rather test and \nproof of the operationality of the methodology, which gain significance when shared with \ndesigners, who are mainly focused on propositions rather than observation. \n\nMarka: The Urban Forests of Oslo \n\nThe  city  of  Oslo  is  surrounded  by  dense  forest  regions  –  Marka.  Although  only  a  certain \nportion of Marka belongs to the municipality of Oslo, the entire region of Marka is continuously surveyed by the city of Oslo (NÆSS et al. 2011, TIITU et al. 2021). The border between \nthe city and Marka has remained mostly unchanged since the 1936 Municipal Master Plan of \nOslo and was protected by national law in 2009 (KLIMA- OG MILJØDEPARTEMENTET 2009). \nThis has led to both a densification of the urban fabric approaching the border of Marka, as \nwell as the preservation of the forest areas within Marka (TIITU et al. 2021). As a result, there \nis no transitional space between the city and the forest, but instead, a dense urban fabric that \nabruptly stops at the edge of the forest of Marka. \n\nThe  close spatial  juxtaposition  of  the urban  area  and  the forest has  extended  the reach  of \nfunctional spaces of the city into the forest. Forest clearings typical of forestry practices aggregate the overlapping agencies, we claim, of forest management and production of urban \nspaces. The urban forest is not defined as classification in prevalent datasets for land cover \nclassification on  a global  and  a  local  scale. In  terms  of  their  spectral  signature,  these  two \nclasses relate to polar opposite definitions. \n\nWe trace the distinct shapes of clearings to translate them and the underlying forces of their \nemergence into a design environment. We look at the urban forest in the North of Oslo, in \nparticular,  at  forest  clearings,  to  define  a  new  spatial  typology  that  is  situated  in  between \nurban and natural landscapes. The forest clearings not only are “found” and classified through \nthe study of images arising from manipulating data but also have the potential to be studied \nas voids with different qualities to serve the city of Oslo. These morphological qualities, we \nclaim, are only truly visible in the lidar surveys. Furthermore, the notion of the clearing as a \nvoid could also be questioned by looking at the data at higher resolutions, which we have not \ndone yet, but vegetation in the clearings of considerably smaller scale than mature trees could \nbe evaluated with higher resolution scans of individual clearings. \n\nWe encounter these landscapes through remote sensing technologies, and in particular data \nderived from lidar scanning, which are produced in a standardized environment to support \nurban planning and environmental management practices. We consider how the standardization of geospatial data processing models may constrain the emergence of new, or yet unknown, typologies of landscapes. We compare available datasets (derivative data produced \nfrom large-scale capturing processes) to new potential data compositions that may aid in describing the emerging figure of the forest clearing. \n\nData for Clearings \n\n“Medium resolution satellite images such as Landsat (30 m) and SPOT (10–20 m) have been \napplied to measure forest logging since they can detect forest clearing down to 0.1 ha resolution” (GAO  et  al.  2020)  0.1  ha  are 1000  m².  The  area  covered  by  a  Landsat  pixel  at  30 \nmeters resolution is 900 m². Our selection of clearings ranges anywhere from 5000 m² to \n78000 m², or 5 to 86 Landsat pixels, or a square pixel grid ranging from 2 x 2 to 9 x 9 pixels. \n\nWe understand that there is a difference in whether a spectral signature is identifiable in a set \nof  pixels  and  the  definitions  of  a  spatial  typology.  The  recognition  of  typology  is  always \nindependent of scale; the clearing as a typology cannot be identified as a change in the signal \nreadings. \n\n“Although all these methods that use optical sensors estimate the areas involved in degradation, they do not at the same time produce estimates of the amount of biomass (or carbon) \nlost, nor do they assess the intensity or rate of biomass loss. LiDAR based measurements can \nprovide accurate measurements of features of logging activity such as roads, skiing trails and \ngaps” (GAO et al. 2020). Approaches of lidar for forest attribute mapping have been reviewed \nby (COOPS et al. 2021, LOHANI & GHOSH 2017). \n\nIn our exploration, we consider the emerging figure of the forest clearing as a new spatial \ntypology (the apparent absence of trees) situated between urban and natural landscapes. The \nclearing  is  fleeting  –  an  unstable  entity.  The  occurrence  of  clearings  cannot  be  detected \nthrough sensors and systems as natural cycles, such as the seasonal growth of trees and plants, \nnor  can  they  be  understood  in  relation  to  technological  cycles,  such  as  satellite  or  sensor \nrevisit time. The specific metrics of the clearing necessitate us to define the typology in relation to several available datasets. Different datasets may expose a different set of clearings. \n\nNo Data Without Models \n\nSpatial data models are used to capture a conceptualization of geographic phenomena in a \ncomputer system (KITCHIN & THRIFT 2009). Spatial data are continuously produced as part \nof a spatial data model, although only a fraction of the data may be available to a certain set \nof users. Model and data are entangled and are dependent on the definition of a set of entities: \n• geographic phenomena that are observable and can be abstracted into concepts, \n• sensor technologies that can capture these phenomena, \n• the capacity of a computer system to process and store the resulting data. \n\nThese entities themselves may inherently be unstable and prone to change, either constant or \nover time. Spatial data may be subject to multiple processes that define or transform the data. \nIt is therefore argued that model and data are inextricably linked. “…without models, there \nare no data […] no collection of signals or observations – even from satellites, which can \n“see” the whole planet – becomes global in time and space without first passing through a \nseries of data models” (EDWARDS 2010). Multiple definitions of the forest clearing may be \n“shimmering” through our data. (EDWARDS 2010) uses the term “shimmering” to describe \nthe inherent instability of data that results from continuous processing. \n\nIn our research, we use the term “model” to refer to a system that continuously processes, \ntranslates, or distorts a set of data with the aim of generating object-based spatial variables. \nAssumptions of geometry that are embedded in the models that decode vast point cloud data. \nWe want to give space to the notion that the clearing may have many definitions and depictions that may ultimately be connected to the representation that captures the clearing. The \nexperiments accounted for in the article contribute to the agency of designers by proposing a \nmethodology and workflow to work on different levels of data production and processing. \n\nIn their article (M’CLOSKEY & VANDERSYS 2022) discuss the arbitrary nature of land cover \nclassification, a method for delineating a set of land cover types from remote sensing imagery. Standardization in geospatial data processing models may even constrain the emergence of new, or yet unknown, typologies of landscapes embedded in the visions arising from \nsuch models. As the present model is incapable of dealing with transitional zones, accounting \nmerely for the absence or presence of trees, we can only use it to determine the boundary of \na clearing and determine its dominant shape. \n\nData Stack \n\nNorway has recently completed the largest digital survey in the history of the country, providing lidar laser data for 380,000 km², which will steadily be released through hoydedata.no \n(TV2 2022). Kartverket provides a continuously updated online status map at (KARTVERKET \n2022). The New National Height Model (Nasjonal Detaljert Høydemodell, NDH) produced \nfrom the lidar scans will provide the country with one of the most detailed national elevation \nmodels to date. \n\nThe lidar data were generated with specific aims such as forestry, forest management, and \necosystem surveys; however, this data for the forest might not be collected again unless there \nis a particular case of use; therefore, we operate under the assumption that the clearings as \ncaptured by this dataset even though they will change in time, might not be ever captured \nagain at such high resolution. Recurring cycles of data are higher for photogrammetry; in any \ncase, the clearings as depicted by the survey are in the past, and they will stay in the past; \ntherefore, our methodology perhaps points to the value of the clearings as cultural landscapes \nwith even potential heritage value that might only endure in the lidar captures.  \n\nIn Norway, the Geodata Act (2010) and the Geodata Regulation (2012) implement the panEuropean  INSPIRE  directive  (2007)  for  geospatial  infrastructure.  (KOMMUNAL-  OG MODERNISERINGSDEPARTEMENTET 2018) provides a good record of the Norwegian government’s geodata strategy. Lidar data follows the LAS Specification 1.4 – R15, a global standard set by the American Society for Photogrammetry and Remote Sensing (ASPRS). In Norway, Hoydedata.no gives access to local high-resolution elevation data derived from laser scanning or photogrammetry as raw point cloud data and processed gridded raster elevation \ndata, as well as continuous national elevation data models as gridded raster with a spatial \nresolution of 1 m, 10 m, and 50 m. \n\nIn the transition from urban fabric to forest area, we encounter two datasets that are polar \nopposites  in  terms  of  the  density  of  data  points  and  thematic  focus.  Felles  KartdataBase \n(FKB) data is a standardized set of geographical information used for planning and design \npurposes  that  captures  objects  and  surfaces  as  detailed  vector  data.  The  dataset  and  its \nmetadata products are available through the national geoportal Geonorge (GEONORGE 2023). \nThe dataset has a gradual decrease in the density of data points from urban regions to rural  \nareas, with forests depicted as flat surfaces. Skogressurskart 16 (SR16), the forest resource \nmap for Norway, is a raster-based dataset focusing on detectable spatial patterns in forests.  \n\n(ASTRUP et al. 2019) FKB and SR16 provide the base for developing a model for the clearing \nas formalized urban forest space, even if their dedicated data formats and definitions don’t \nmatch our requirements. Both datasets are at least in part derived from lidar data. FKB has a \nnational dataset with a focus on urban developments, design, and legal matters. SR16 is an \nimportant tool for forest inventory analysis and forest management. \n\nBoth datasets provide national coverage in the framework of their specific data but are opposites  in  the  density  of  their  data  points:  SR16  only  covers  areas  delineated  as  forests  and \nprovides no data for urban spaces, while FKB has the highest resolution in dense urban areas \nand a lower level of data points in forest areas. Both datasets aim to create and maintain a \nnational coverage of data. The datasets are continuously updated to register the present state \nof  the  environment.  They  are  not  designed  to  have  a  wide  temporal  resolution  or  capture \nseveral moments in time. \n\nScouting Digital Clearings \n\nFrom within the forest, there is no overview of it – a clearing may stay hidden to a human \nobserver  until  encountering  it  by  walking  into  it  by  accident.  It  reveals  itself  by  walking \nthrough the forest. For digital datasets that register the forest it may be the same – the clearing \nreveals itself through the dataset, and between datasets none may account for the same set of \nclearings.  \n\nFinding,  detecting,  and  measuring  the  clearing  is  the  starting  point  in  our  process  of  data \ncollection. Considering the unstable nature of the forest clearing, the sites had to be determined through the methods we use to describe them. We utilize the concept of “data scouting” to describe a situation where: \n\n• the metrics for detecting an object of study or spatial typology is closely related to the definitions of geospatial data, and these metrics vary across different datasets or \n\n• the exact location of a site cannot be known other than from a data source. \n\nThe lidar dataset is inherently designed as a high-dimensional data stack. Lidar point cloud \ndata may contain a set of x,y,z coordinates, backscatter intensity, color values derived from \npassive sensors as true-color or infrared RGB color composites, multi- or hyperspectral data, \nor metadata-level information on the acquisition and processing of the data. Point cloud processing focuses on detectable patterns in this data stack to derive certain data conclusions \nthrough the use of specialized algorithms, software packages, or tools. These derivative data \nmay be stored in the same dataset as for point classification codes, or processed as entirely \nnew datasets in vector and gridded raster formats, as for feature vectors derived from object \ndetection or return intensity images. \n\n“Currently, only a small portion of the data in a point cloud scan (less than one percent) is \nused to produce conventional surveys or documents such as contour plans, elevations, and \nsections. The remaining ninety-nine percent of the scanned data is seldom used and usually \ndiscarded.” (GIROT 2019) As for all remote sensing data, resolution, as well as sensor capacities and the process of capturing, may limit or constrain further analysis or processing. \n\nOur  scoping  region  for  the  forest  north  of  Oslo  intersects  with  three  lidar  datasets:  Oslo \nMarka 2010 (1 pt\/m²), Oslo Kommuneskogen 2010 (10 pt\/m²), and NDH Oslo Nordmarka \n2016 (2 pt\/m²). To allow for complexity in testing our model and overcome eventual thresholds that lower-resolution datasets may impose on feature extraction, we decided to use the \nmost  high-resolution  data  available  in  our  study  area  –  Oslo  Kommuneskogen  2010.  The \ndataset was created in six sessions between 08.08.2010 and 01.09.2010 and covers an area in \nthe  Oslo  Marka  amounting  to  approximately  188  km².  A  metadata  document  is  available \nthrough hoydedata.no at (TERRATEC 2010) \n\nModel Sampling – Revealing Clearings \n\nThe forest clearing is one of the figures that is absent from the standardization of classification datasets. Accounting for the specific metrics of the clearing necessitates defining specific \ncharacteristics that are developed in relation to available datasets focused on the forest. We \nhave tested the hypothesis that certain spatial conditions of the clearing can only be evaluated \nand fully understood by working directly with the lidar data. Therefore, we apply it to the \nmost high-resolution lidar data available in our study area, Oslo Kommuneskogen 2010. \n\nThe point cloud and its derivative products – the raster and the vector – allow for high versatility in approaching the clearing as digital data space. However, each derivative dataset may \nhave unique advantages or disadvantages relative to the specified use cases of the model. Our \nmodel for forest clearings is based on sampling the data models of SR16 and FKB and applying their metrics to the Oslo Kommuneskogen 2010 lidar dataset. We made a preliminary \nselection of 40 sites, from which we selected and processed data for a subset of 26 sites. We \nproduced object-oriented feature vectors and high-resolution Canopy Height Models (CHM) \nfor the selection of 26 sites. The sites become the ground from which our model becomes an \nexpression of formal spatial analysis. \n\nThere are numerous methods for feature extraction from lidar data, some of which are proprietary and owned by individual companies or institutions. Delineating individual trees from \nlidar data can be achieved by identifying peaks in high vegetation points as tree top points, \nand  the  downward  sloping  surface  from  these  points  as  the  extent  of  the  tree  crown.  The \npeaks and valleys in the fabric of forest lidar data allow for extracting precise metrics for \nindividual trees. \n\nModel \n\nThe prevalent datasets (FKB, SR16) that are used for registering the peculiarities of the urban \nenvironment and forest areas are developed to capture a specific spectrum of sensible objects \nand surfaces. Through FKB, we understand and study urban structures as objects that can be \nclearly delineated as vectors, while SR16 rasters are based on detectable patterns in forest \nregions that are independent of zoning, access roads, or topography. We sample existing data \nmodels from the local FKB and SR16 datasets and apply them to our new model: we apply \nthe dense vector data structure of FKB data to the forest and sample SR16 data on the level \nof the resolution of dense lidar point cloud data. \n\nCanopy Height Model \n\nDerivative raster data that are commonly produced from lidar point clouds include Digital \nSurface Model (DSM), Normalized Digital Surface Model (nDSM), Digital Elevation Model \n(DEM), and Canopy Height Model (CHM) as well as intensity images. Elevation models are \nproduced as gridded raster data. In contrast to 3D point cloud data, gridded raster data are \nessentially 2.5 dimensional because any grid cell only samples one vertical elevation value \n(DONG & CHEN 2018). The DEM is created from ground points and the DSM from the highest \npoints within each raster cell (DONG & CHEN 2018). \n\nThe Canopy Height Model (CHM) is a model commonly used in fields dealing with forestry \nor forest management. It is “a raster grid that stores the upper surface (i. e., maximum) height \nof vegetation canopy” (DONG & CHEN 2018), the elevation of trees relative to the ground. \nThe CHM is generated by subtracting the DEM from the DSM. The CHM literally removes \nthe ground from underneath the trees, projecting the root point elevation to an apparent 0 in \nthe coordinate reference system. As a result, the digital model of the forest ground becomes \nflat. The complex entanglements that make this environment are removed: elevations, slope, \nand potential drainage flow paths. The CHM, as used for forest management, is identical to \nthe NDSM, used in the context of urban environments. The use of different terminologies for \nthe same datasets produced by the same process indicates how models are context but not \ndata-specific. For our case study, we produced CHM raster datasets at 25cm resolution for \nall sites. \n\nFeature Extraction \n\nFKB data registers objects and surfaces as detailed vector data. The dataset has a high density \nin urban areas and an increasingly lower density in less urban areas. Urban FKB data shows \nthe context and connection of the individual objects and surfaces that are delineated in the \ndata. In the forest, FKB data may show a continuous surface that accounts for the area as \n“forest” but has no distinction of objects within this environment. We trace trees in the forest \nas organisms that are in constant interaction. They become the figure in constructing the urban forest. Starting from the urban, we translate the metrics of FKB to the forest and create \na set of 3D vector objects that account for the forest as a space of complex interactions and \nentanglements. We created vectors for: tree root point, crown top point, radius, buildings, \nand elevation contours for DEM and DSM at a 10 cm interval. A dense collection of vectors \nthat we describe as a “vector cloud”.  \n\nDigital Forest Clearings \n\nThe clearing emerges as an unstable spatial typology resulting from an entanglement of processes  related  to  the  forest.  First,  the  spatially  dependent,  geophysical,  bio-chemical,  and \necological processes that determine the growth of trees in the forest, and second, processes \nof forest management. We encounter the forest clearing through geospatial data. The forest \nbecomes a digital asset as part of extractive industries that generate information on forest \nenvironments to harvest trees. Data on trees precedes the agency of these industries to transform the forest landscape. Data surveying the forest describe the clearings as the absence of \ntrees in the seemingly isotropic fabric of the forest, rather than a space surrounded by trees. \nThe clearings emerge within the space of the survey as a combination of factors such as forest \nmanagement and geophysical parameters including elevation, slope, accessibility, and tree \nheight at the time of capture. The clearings do have value as visible remnants of a cultural \nlandscape practice, they are frozen in time by the lidar survey, and this space within the model \nmight be the only space in which the clearings can be studied in morphological detail. \n\nConclusion \n\nThe value of this particular case in Oslo is twofold; on the one hand, the databases in Norway \nfor performing such analysis are public; on the other hand, a history of careful urban forest \nmanagement, reconciling the public use of the forest with the interests of commercial forestry, have developed into unique and delicate ways to perform clear-cutting in the urban \nforests around Oslo. These two unique conditions converged to present us with the need to \nperform this study in Oslo; however, the workflow and methodology described here could be \nemployed in other forested areas, as long as the data is available and there is a unique aspect \nof clear cutting to be valued through images. \n\nAs previous authors in the field have suggested, working with lidar-derived point clouds aids \nin overcoming “limitations of current digital models that restrict site-specific analysis and \nintegration during design processes. The method tested extends the design vocabulary to any \nformal artifact and interlinks all scales addressed by designers. Digital modeling based on \nlaser-scanning surveys offers new ways to address (urban) landscapes, in which topography \ncan be used as an interface to negotiate ecological, cultural, and spatial design questions.” \n(URECH et al. 2020) To this we add, based on our own research, that the continuous processing of lidar-derived point cloud data does not merely transform, filter, or reproject numbers within a given reference system in order to streamline or derive new design methods, \nbut literally reveals unseen spatial worlds in between signal and data. Our case study in the \nforest near Oslo goes further not only into testing a methodology for design (CALLEJAS 2022), \nbut in fact, contributes to revealing unseen or undervalued landscapes that exist between signal and data, and that might not have been discussed before as meaningful urban artifacts or \npotential civic spaces. \n\nThe clearings as visualized within the lidar surveys are, therefore, landscapes in their own \nterms, with characteristics that can only be understood via the study of the lidar survey as \nopposed to photography, photogrammetry, or field observations. \n\nData may be constrained by their embedded models to represent unknown spatial entities. \nData curation and scouting may be one part of developing the integration of spatial data into \narchitecture  and  landscape  architecture  practice,  but  we  want  to  put  emphasis  on  the  importance of making models that produce and transform data for unique and unknown spatial typologies. Model sampling is our strategy for understanding and extracting the metrics of \ndata models for FKB and SR16 and transferring them to the Oslo Kommuneskogen 2010 \nlidar dataset. \n\nThis work is a precursor to a methodology that allows for a closer creative engagement of \narchitects and landscape architects with the models that are built around forest data. The case \nstudy of forest clearing around Oslo could be applied to other locations where there is an \nassumption of the presence of undervalued landscape entities, particularly remnants of cultural  landscapes,  as  long  as  high-resolution  data  is  available.  This  methodology  has  great \npotential in the emerging field of cultural landscape preservation or landscape heritage, especially when there is a need for delicate intervention. \n\nAs the clearings are in constant transformation due to forestry and natural processes such as \nsuccession, the clearings that exist in the survey are always in the past. The methodology \nexposes the limitations of performing analysis as the cycles of high-resolution data acquisition are too slow to depict reality. The idea that we always operate with the past rather than \nthe present of a landscape is not only a limitation, as it exposes the need to be able to “manipulate” malleable models when discussing this workflow as projective rather than purely \ndiagnostic. \n\nModels may operate as unidirectional filters, allowing to transform high dimensional data to \nless complex derivate data products. In the case of land cover classification, complex earth \nobservation  images  may  be  reduced  to  images  of  a  predefined  set  of  material  categories, \nwhere it is not possible to derive the former of the latter. Land cover classification data are \nprogrammed to allow for the interpretation of changes in the surface of the earth, within the \nframework  of  a  given  classification  system,  where  each  system  produces  individual  data \nproducts that are not comparable to the ones of another system. It may not be possible to \nextrapolate from the classification data to the remote sensing observations used to produce \nit, or even to the ground truth condition they aim to capture. \n\nThe application of the methodology elsewhere might be limited by the following factors: data \navailability and temporal or spatial resolution of the data. The analysis of the data is in tune \nwith the relative low speed and intensity of forestry in the urban forest north of Oslo; therefore, spaces can be “discovered” and valued in the data space before radical changes occur \nin the real site. Another limitation is that this methodology, however useful for delineating \nand framing clearings, does not offer enough resolution to study natural succession at the \nclearings’ edges, as this phenomenon happens at a different scale and pace compared to the \nmaturing of trees and forestry. This limitation was exposed by some of the experiments with \nmaster students that wanted to go into more detail to address the clearings’ edges subject to \nnatural succession. \n\nDesign disciplines have little agency over how remote sensing systems are constructed and \nhow data is collected, processed, and made accessible, but the constantly increasing amount \nof open data demands designers to develop their own interpretative and operational models \nas a response. This research is an example of a designer´s approach to such interpretations. \n\nLinking Image-based Metrics to 3D Model-based \nMetrics for Assessment of Visual Landscape Quality \n\nAbstract: Visual landscape quality represents a potential attribute of landscapes that affects people’s \nperception  and  psychological  well-being.  With  the  perceived  sensory  dimensions  as  a  conceptual \nframework, this study proposes methods to measure visual quality in both real environments and virtual \nmodels, using image-based metrics from computer vision techniques and 3D model-based metrics from \nparametric modelling techniques. Using the Clementi Woods Park in Singapore as a case study, we \ncompared these two types of metrics using statistical methods and proposed an approach of using a \nregression model from empirical studies to estimate subjective preference for design scenarios and thus \nto evaluate the result of landscape design scenarios. \n\nIntroduction \n\nWith increasing concern regarding urban dwellers’ psychological well-being, studies have \nlooked towards urban green spaces (UGS) as a potential salutogenic means of alleviating this \nissue. In quantifying the spatial attributes of UGS, although the quantity of greenery has been \nadequately studied, research on the objective and standardised quality that reflects perception, however, remains largely insufficient. In attempting to understand the subjective nature \nof  landscape  quality,  visual  landscape  quality  (VLQ)  has  served  as  a  means  of  assessing \nlandscapes  that  affect  people’s  perceptions.  However,  current  approaches  to  assessing the \nVLQ have some limitations. First, the conventional methods are often based on top-down \nmeasurements of the landscapes which are weak in accurately representing the scenes from \nthe human perspective, thus leading to the inaccuracy of measurement of the human-centric \nexperience. Secondly, as GASCON et al. (2015) pointed out, proper metrics to measure VLQ, \nespecially those used in urban contexts and associated with perceptions, still lack exploration. \nThe stated research gaps lead to challenges in assessing real-world environments as well as \nunbuilt landscape designs that link to VLQ. The objective metrics to measure VLQ based on \nproper conceptual and technical instruments thus needs to be further explored. \n\nWith the emerging computer vision techniques based on deep learning, an increasing number \nof studies have demonstrated the use of semantic segmentation and depth estimation to extract metrics such as the green view index (GVI), sky view index (SVI), and depth from real-world photographs. Photographic images can reflect spatial information that mirrors what is \nseen by people. Aside from photographs, 3D models also have the possibility of measuring \nlandscapes at eye level. QI et al. (2022) utilised 3D point clouds based on terrestrial LiDAR \nscans for VLQ evaluation and achieved detailed measurements of various visual and spatial \nfeatures, albeit costly to use. A more convenient tool to use, however, could be a 3D model \nbased on the landscape, which allows the evaluation of designed landscapes virtually instead. \n\nWe also realised that prevalent research focused on evaluating and measuring the VLQ of \nthe as-is environment, but few studies have quantified the visual quality in landscape design \nthat has yet to be built. The results of scientific research are also difficult to apply to guide \ndesign practice. LIU & NIJHUIS (2020) highlighted the importance of mapping the spatial-visual quality in improving landscape design, whereas the measurement methods were not \nbased on evidence-based studies but solely on expert evaluation. In this paper, one of our \naims is to attempt to directly apply the results from empirical studies in improving landscape \ndesign. In addition, it is targeted to utilise more common tools such as Rhinoceros and Grasshopper that can be effectively applied by designers in the practice field, not exclusively limited to academic exploration. Therefore, we consider linking the use of image-based metrics \nand 3D model-based metrics to measure VLQ in different contexts. Given the stated research \ngaps and aims, the underlying research questions are as follows: \n1.  To what extent do image-based and 3D model-based metrics adequately measure VLQ? \n2.  To what extent do image-based and 3D model-based metrics align with each other in VLQ measurement? \n3.  To  what  extent  can  the  metrics  be  applied  in  design  optimisation  using  results  from empirical studies? \n\nMethodology \n\nOur underlying study focuses on developing and measuring metrics for assessing VLQ that \nmirror critical human perceptions based on conceptual frameworks and digital techniques. \nWe consider both photographic images and 3D virtual models as useful tools to assess VLQ \nfor the aims of assessing the real environment and measuring designed landscapes respectively. The usage of the two instruments is integrated into a holistic framework primarily by \napplying the conclusions from empirical studies using images and iteratively testing and improving a hypothetical design. The image-based metrics can be used to assess real environments while the metrics for 3D models can be applied to exploring different design scenarios \nthat have the potential to promote people’s subjective perceptions such as preference. The \nfollowing sections show how the study is conducted to achieve the research targets. \n\nCase Study – Clementi Woods Park in Singapore \n\nWe utilised the Clementi Woods Park in Singapore as a case study to investigate the use of \nimage and 3D model-based metrics. Within the park, we selected ten locations that cover \nmultiple spatial characteristics ranging from open fields to sheltered viewpoints. Onsite panoramic photos were taken at these 10 sites (Fig. 1) and their subsequent image-based metrics \nwere measured (Section 2.2). To measure the 3D model-based metrics, however, we first had \nto reconstruct the park in a 3D environment inside Rhinoceros (Fig. 2). To accurately model \nthe topography and surface elements, we used the digital terrain\/surface models and airborne \nimagery from the Singapore Land Authority (SLA), and an orthophotograph using a drone. \nTrees and shrubs were modelled using adapted low-polygon-count techniques previously developed (LIN et al. 2018, GOBEAWAN et al. 2018, GOBEAWAN et al. 2021). Other elements \nsuch as paths, benches, pavilions, etc., were manually modelled based on the imagery data \nincluding maps and photos (Fig. 3). \n\nImage-based Metrics and Computer Vision \n\nIdentifying the critical landscape characteristics is a prerequisite. The attention restoration \ntheory (ART) and stress reduction theory (SRT) underscore the role of greenery in providing \neffortless attention to people and relieving stress levels by experiencing natural environments \n(KAPLAN & KAPLAN 1989, ULRICH 1983). In addition, the prospect-refuge theory (APPLETON \n1975) proposed that the simultaneous presence of open enclosed space could lead to people’s \nhigh preference for landscapes. The perceived sensory dimensions (PSD) proposed by GRAHN \n& STIGSDOTTER (2010) have been demonstrated as a useful framework that is closely associated with psychological well-being. The PSD includes eight conceptual dimensions namely nature,  prospect,  space,  refuge,  serene,  culture,  and  social  which  indicate  different  spatial \ncharacteristics and are often measured through surveys with respondents after exposing them \nto a variety of landscape scenes (AKPINAR 2021).  \n\nHowever, few studies investigated how to use objective metrics to measure these characteristics digitally. Following the meanings of the updated version of PSD put forward by STOLTZ \n& GRAHN (2021), we proposed eleven metrics that follow the implications of five selected \nand adapted dimensions: natural, open, sheltered, cultivated, and diverse. Those metrics and \ntheir related calculations or interpretations are summarised in Table 1. These eleven metrics \nincluding tree, shrub, depth, diversity of plant groups, etc., aim at quantifying multiple landscape attributes from the visual quality perspective. \n\nRegarding computer vision techniques, semantic segmentation and depth estimation are used \nin this study. In terms of the semantic segmentation model, we chose PSPNet pre-trained \nbased on the ADE20K dataset developed by ZHAO et al. (2017) which has been commonly \nacknowledged as having high performance regarding its accuracy and the number of identifiable  landscape  elements.  As  for  the  depth  estimation,  we  use  the  R-MSFM  model  pretrained based on the KITTI dataset developed by ZHOU et al. (2021). Using these tools, we \ncan measure the above-mentioned metrics and thus evaluate VLQ from multiple dimensions.  \n\nConcerning the “natural” dimension, the visible tree, grass, and shrub can be measured. As \nfor  the  open  dimension,  depth  is  a  critical  feature  that  reflects  the  openness  of  space;  the \nvisible area of the sky (SVI), also serves as an important factor for openness. Concerning the \n“sheltered” dimension, we measure the tree canopy and structure that are overhead serving \nas refuge elements. The visible buildings, service facilities, and paths are regarded as “cultivated” components that refer to man-made and managed features of landscapes. “Diverse” \nin this paper particularly underlines the richness of these types of vegetation. The Shannon \ndiversity index of the visible trees, shrubs, and grass serve as the basic method for calculating \nthis metric. Representative examples of a few metrics to show the spatial characteristics are \ndisplayed in Figure 4. \n\n3D Model-based Metrics and Grasshopper \n\nThe methodology of extracting image-based metrics described above is still in the process of \nrefinement. However, one distinct limitation of this method is the inability to measure hypothetical or unbuilt landscapes such as those still in the design phase. As such, the study attempts to use the 3D model to extract the same metrics as above in anticipation that this can \nlead to a method to measure different design scenarios. In this study, we utilise the visual \nprogramming workflow of Grasshopper within the Rhinoceros environment to measure the \nmodel.  Aligning  this  with  the  image-based  methods  above,  we  measure  the  proportion  of \nvarious landscape elements in view, thus mimicking the image-based segmentation method. \n\nTo simulate the approximate height of a human eye, we begin by setting a viewpoint at 1.6m \nabove the topographical mesh. From this location, a set of rays (61206 rays, each of 500m in \nlength) are emitted spherically via a parallelised 3D IsoVist component (CASCAVAL 2019) \nwhich is typically used for viewshed analysis. These rays are then separated into two sets \n(Fig. 5), the first being a 120-degree (60 degrees above and below the horizon) horizontal \nview representative of the same viewing angle on which the image-based segmentation is \nbased. The second is the overhead\/top view (>60degree above the horizon) which is used to \ncalculate the sheltered metric. \n\nThe  added  benefit  of  using  a  3D  model  is  that  the  landscape  objects  are  already  pre-segmented as different 3D objects unlike those from a photograph which require an additional step of classification. Here, we make use of the IsoVist component to identify the different \nlandscape elements being intersected (Fig. 6) and subsequently measure the proportion or \nrays hitting each of these elements. These resulting proportions eventually provide the various percentage-of-view-based metrics such as GVI, SVI, BVI, and so on. In the image-based metrics, a depth metric is calculated; similarly, here we calculate a depth metric (Fig. 6) by \ncomparing the average distance of each intersected ray and unitise the results between 0 and 1, meaning completely enclosed and completely open respectively.  \n\nData Analysis for Comparison and Application of Metrics \n\nUpon obtaining the results of the two methods of metric measurements, we then set forth to \ncompare them and propose a method of applying these metrics in design practice. First, to \ncompare the quantitative characteristics of the metrics derived from images versus 3D models, we reported descriptive statistics of the calculation results, the indicators of which include \nmean, median, standard deviation, maximum, and minimum values. Second, to investigate \nthe extent to which the two types of metrics align with each other, an analysis of variance \nand  bivariate  correlation  between  the  two  types  of  metrics  was  calculated  for  inspection. \nThird, as these proposed metrics are exclusively founded upon expert-based approaches and \nhave not been verified to determine if they capture subjective perception, the regression result \nof an online survey from another study of ours (in writing) was applied in this paper. The \nonline survey employed 1500 respondents to provide a range of perception-based responses \nto 100 interactive panoramic images of UGS in Singapore. Since “preference” is an important \naspect  that  reflects  subjective  perception  in  general,  we  selected  this  factor  for  analysis. \nLastly, the metrics were used to establish a regression model, the result of which would be \nemployed to measure hypothetical design scenarios by adjusting landscape elements inside \nof Rhinoceros. \n\nResults \n\nComparison Between Image-based and 3D Model-based Metrics \n\nThe result of descriptive statistics of the two types of metrics is shown in Table 2. Overall, \nthe mean, minimum and maximum values of each metric present similar features in quantitative variation between these two types of metrics. Particularly, the values of image-based metrics regarding tree, grass, and sky were largely close to the corresponding results of 3D \nmodel-based metrics. Taking grass as an example, the results for 3D models and images were \nrespectively 0.316 and 0.303 for the mean, 0.021 and 0.057 for the minimum, and 0.487 and \n0.494  for  the  maximum.  Furthermore,  to  accurately  examine  the  extent  to  which  the  two \ntypes of metrics aligned with each other, we used analysis of variance (ANOVA) to compare \nthem.  If  the  p-value  of  Welch’s  ANOVA  for  a  metric  is  higher  than  0.05,  it  indicates  no \nsignificant differences in this metric between images and 3D models. The result suggested \nthat all the metrics are largely consistent, except depth. The reason was that the algorithms \nfor depth based on the two instruments were different. Although we could measure the actual \ndistance in 3D models, it was currently not feasible to estimate it from a single image. Thus \nthe image-based depth was merely quantified as a relative distance. To resolve this issue for \ndepth, we investigated the bivariate correlation based on Spearman coefficients; the coefficient of 0.903 (at the significance level of 0.05) indicated a high correlation between the two \ngroups. To summarise, all these metrics showed consistency between the two types based on \nimages and 3D models.  \n\nParametric Design with Metrics \n\nBased on the survey data as mentioned in Section 2.4, a total of six image-based metrics were \nscreened through a stepwise ordinary least square (OLS) regression model. The metrics include sky, tree, diversity of plant groups, service facility, depth, and shrub which are critical \nfactors to predict subjective preference. These corresponding 3D model-based metrics would \nbe  useful  for  optimising  the  design  by  controlling  landscape  elements  within  the  virtual \nmodel. In addition, a linear regression model is applicable in evaluating the design scenarios \nrepresented in 3D models by calculating the estimated preference (EP) as the reference to \njudge the design, which is:  \n\n\n\nEP =  2.285 ∗ [sky] +  1.249 ∗ [tree] +  0.656 ∗ [Diversity of plant groups] +  0.829 ∗ [service facility] +  2.388 ∗ [depth]   +  1.128 ∗ [shrub]   +  2.065  \n\nWe selected viewpoints 3 and 9 as examples to display how to improve the design by adjusting the model with key metrics (Fig. 7). The main design interventions include adjusting the location of the various landscape objects such as trees and service facilities as well as the \ninclusion of  shrubs  to  increase  the diversity  of plant groups  index  all while  attempting  to \nincrease the sky view index simultaneously. After the adjustments, we re-calculated the metrics and the EP (Table 3). The EP value for viewpoint 9 is increased by 0.15 while viewpoint 3 only increased by 0.01. \n\nDiscussion and Limitation \n\nThis study innovatively links image-based metrics to 3D model-based metrics, demonstrating \nthe workflow and its feasibility of employing the proposed metrics and methods to assess \nVLQ of real environments and virtual design scenarios. As image-based metrics are increasingly  applied  in  scientific  studies  that  associate  landscapes  with  various  kinds  of  cultural \nservices, our methods have successfully built a technical approach that allows the application \nof scientific results into a measurable design intervention. On the other hand, there are limitations to the study stemming from two sources, the first being the algorithms and accuracy of the image segmentation and the second being the oversimplification of real-world landscapes into virtual ones resulting in certain metrics (such as BVI, and service facility view \nindex) being not completely aligned with each other, and thus with lower correlation coefficients. There are some potential reasons: First, a lack of local-based training imagery datasets may result in lower accuracy of image segmentation. Second, in 3D models, the location, \nsize,  shape,  and opacity of  trees  and shrubs  differ  from  those  in real  environments  which \nprobably decreases the visible proportions of buildings and other objects since they are occluded by  the opaque vegetation,  in  comparison  to  image  segmentation  can  recognise  the objects behind vegetation with less foliage (Fig. 8).  \n\nFurthermore, one of the limitations of using our proposed metrics is that the visual quality is \nonly measured from the perspective of static viewpoints, of which the metrics can change \njust by adjusting the viewpoint by a few meters such that a building is no longer occluded by \na tree for example. What people experience or perceive when actually visiting such landscapes is as such likely oversimplified with a handful of measurements from viewpoints. That \nsaid, there are two possible ways to alleviate this issue. The first is to use multiple viewpoints \neither by repeatedly photographing the site or by measuring them via the 3D model to form \na map instead (Fig. 9). The map might provide us with an alternative way to visualise the \ndifferent qualities of the site through a first-person view analysis. A second possible solution \nis to calculate 3D metrics instead as proposed by QI et al. (2022). 3D metrics based on point \nclouds such as green volumetric ratio (GVR) and horizontal, vertical, and distance diversity \n(HVDD) of various can quantify 3D spatial attributes considering the volumes rather than \nthe visible areas of landscape elements. This method expands the scope of landscape spatial \nattributes that can be measured purely from a visibility analysis. In future studies, we intend \nto deepen the comparison between different tools including point-cloud-based 3D metrics, \nand integrate these metrics for comprehensive applications in multiple research contexts. Last \nbut not least, although we used 3D models to try to reproduce the actual landscape, it was \nchallenging to achieve identical views derived from both 3D models and images. Some inevitable errors might happen during the modelling process or in identifying the accurate locations of the viewpoints. However, statistical methods have still demonstrated the relative \nconsistency of the two types of metrics. \n\nThese  technical  issues  mentioned  above  would  undoubtedly  resolve  as  improvements  are \nmade to both image segmentation as well as virtual models. This paper illustrates the exploration of linking metrics using different tools as well as linking scientific study to design. We thus encourage any studies in the future to apply quantitative conclusions from research in \ndesign practice through a similar methodology. \n\nConclusion \n\nWe proposed image-based and 3D model-based metrics to measure VLQ of real environments and virtual models and then compared the characteristics of these two types of metrics, \nwhich  allows applying  findings  of  evidence-based  scientific  studies  in parametric  designs \nusing Grasshopper to evaluate the design results. We also demonstrated a method of utilising \nregression models to estimate the potential subjective preference derived from design scenarios. Even though further studies need to optimise the digital techniques for metric calculation, the progress of linking the two types of metrics for research and design use expands \nthe scope of the application of landscape metrics. \n\nProducing 2D Asset Libraries with AI-powered \nImage Generators \n\nAbstract: A proliferation of Artificial Intelligence (AI) applications specific to landscape architecture \nhas revealed potential disruptions to many aspects of the professional design process, including tasks \nthat require creative skills but are very time-consuming. Creating 2D assets for design renderings is an \nexample of one such task, requiring an inordinate amount of time to create just a few image cut-outs \nwith little customization. Generative AI art tools offer the possibility to both reduce production time \nand improve the quality and customizability of asset libraries. In this paper, we present a comparative \nassessment of three image generators’ (Dall-E 2, Midjourney, and Stable Diffusion) abilities to produce \n2D asset libraries. The analysis includes the strengths and weaknesses of each generator in accuracy, \nusability, and artistic style. Recommendations for potential prompts and workflows to achieve desired \nresults  with each  generator are also  provided, along  with a  reflection  on  the  greater  implications  of \ngenerative AI for landscape practice. \n\nIntroduction \n\nWhile Artificial Intelligence (AI) has been growing steadily for many decades, recent advancements in applications specific to the architecture, construction, and engineering (AEC)industry have revealed potential evolutions for many parts of the landscape design process. \nThese include tasks such as site inventory, landform modeling, or conceptual urban design, \namong others (BARBARASH et al. 2022, BRÜTTING 2020, JOHNS et al. 2020, LIU & TIAN 2022, \nRAMAN et al. 2022). Many of these tasks require unique expertise, skills, cognition, and creative agency and can be an expensive, even if vital, portion of design services. On the other hand, there are many production tasks that are less impactful but typically still require the \nskills of a designer to complete. AI may be able to offload some of these secondary activities, \nimproving the trade-off between their costs and quality. At the same time, integrating AI into \ndesign workflows may also be perceived as a disruption of traditional (creative) processes \nsuch as design representation. For instance, the recent release of descriptive image generators \nsuch as DALL-E 2, Midjourney, or Stable Diffusion could dramatically change the way designers obtain and inventory image assets. With descriptive prompts from a human user, these generators utilize machine learning models trained on millions of images to produce two-dimensional renderings in nearly any conceivable artistic style in less than a minute. While the ability to autonomously create entire artistic scenes for design ideation is striking, a more \nintriguing and perhaps pragmatic use for these generators is the production of 2D asset libraries to be used for typical design renderings. \n\nMany design practices build internal entourage libraries to facilitate rapid conceptual rendering production using online resources such as ARTCUTOUT, pngimg.com, or SKALGUBBAR. While useful, these libraries are often limited in materials, styles, and diversity of assets. This is especially evident when trying to create bespoke representations that evoke a localized sense of place, either with native plant materials, textures, implementations, or people. AI image generators offer the potential to overcome these limitations through ongoing, rapid,  and  near-infinite production  of  assets  that  can  be  refined with  increasingly  specific \ncommand prompts – at an increasingly lower price. In this paper, we present a comparative \nassessment of three image generators’ abilities to produce 2D asset libraries. Our analysis \nincludes the strengths and weaknesses of each generator in accuracy, usability, and artistic \nstyle.  We  also  provide  a  reflection  on  potential  prompts  and  workflows  for  most  quickly \nachieving desired results with each generator. \n\nAssessment of Three AI-image Generators \n\nMethods \n\nThe three AI image generators assessed as part of this research were Dall-E 2, Midjourney, \nand Stable Diffusion. These were selected based on their proven high-quality outputs, large \nuser base, and usability factors. Dall-E 2 and Midjourney are paid subscription services that \noperate on cloud-based servers, while Stable Diffusion is run locally. All three support both \ntext-based and image-based prompts, though other features vary, such as inpainting (AI redrawing portions of an image), outpainting (AI generating imagery around the periphery of \nan image), program modifications (creation of supplemental features or AI generation models), and seamless tiling (repeatable patterns).  \n\nAll three services were assessed for their usefulness in aiding the creation of 2D representations  through  the  rapid  creation  of  unique  plants  and  entourage  assets,  which  are  discrete \nimage elements that can be included in a larger image. The researchers attempted to create \nseveral near photo-realistic target assets in the different categories of trees, shrubs, perennials, people, and objects. We also attempted to create several assets in various artistic styles. \nOver 300 total assets in the above categories were generated for this study. We abandoned \nthe process of creating an asset once 15 prompts had been used without a successful asset \nbeing generated, as at this point it typically required between 20-30 minutes and would not \nbe considered feasible from a financial and resource perspective. Once a usable asset had \nbeen generated, there was further evaluation of the ability to reliably produce usable variations, the ease of accessing the image files, and the ease of sharing with limited post-production (<2 minutes of masking or background deletion). Finally, as a result of this work we \nsupply best practices for efficiently generating assets in all three AI image generators. \n\nPrompts are the textual guide for the AI to generate an image and should include specific \ndetails for the desired image. In addition to describing the target subject (ie. a specific plant \nspecies), the prompts can include contextual information to guide the AI in crafting the visual \nstyle of the image. For instance, adding ‘8k’ will encourage the AI to create a highly detailed \nimage, or “3d game asset” will encourage an image with a 3-dimensional, digital feel. Additionally, Midjourney and Stable Diffusion offer different functions within the prompts. For \ninstances,  in  Stable  Diffusion  the  addition  of  ()  bracketing  will  cause  Stable  Diffusion  to \nassign greater weight to that word, and multiple parens can be added to further increase the \nweight of a word. In Midjourney, specific commands can be utilized to alter the AI model, \nfor instance adding – test will invoke Midjourney’s artistic mode. A full list of commands \nfor each service can be found in their respective documentation and users will benefit from \nbecoming familiar with them.  \n\nResults \n\nBased on the image outcomes (see Table 1), Dall-E 2 is overall the most reliable AI image \ngenerator for the creation of high-quality visual assets across all the categories, on average \nproducing the highest quality images with the fewest prompts required (identified as steps in \ntable 1). Additionally, Dall-E 2 reliably produced usable images utilizing very similar prompts \nfor each asset and was excellent at creating variations (see Figure 1). Midjourney required a \nfar greater number of iterations on prompts to produce useable assets and many times failed \nto produce a viable asset. Generally, it was effective at generating trees and objects, but underperformed in production of shrubs, perennials, and people, often returning close-ups of \nleaves and flowers, or overly complex artistic compositions of people. \n\nHowever, Midjourney was excellent at producing stylized assets (e. g. urban sketcher, impressionist, etc.). Once a usable prompt was developed, all three platforms were effective at \ngenerating  reliable  variations,  but  Midjourney  produced  the  widest  variations  in  images. \nHowever, Midjourney also produced the highest quality images with a more effective built-in AI upscaling feature. Finally, it was determined that output images in all three platforms \nwere equally accessible for download and use. \n\nFor  objects,  we  found  that  Dall-E  was  the  most  reliable  at  creating  objects,  but  that  both \nMidjourney and Stable Diffusion struggled to create isolated objects but tended to create the \nobject within a larger artistic composition. For Dall-e, a simple prompt of “photograph of a \n<target object> on white background” reliably produced a usable image asset.  \n\nImage prompts were also found to be useful, especially in the case of Stable Diffusion. For \ninstance, in generating a “Quercus gambelii with autumn color,” the results in Stable Diffusion were significantly improved when given a basic image prompt of an orange circle atop \na brown line on a white background (see Figure 2). While Dall-E 2 and Midjourney support \nsimilar functionality, they do not reliably produce as positive results as Stable Diffusion using \nimage prompts. Stable Diffusion was typically effective at generating trees, and shrubs, but \nstruggled with generating perennials, people, and objects. Notably, Stable Diffusion was the \nleast effective at isolating the assets individually on a white background, which significantly \nhampers the use of the asset in a visualization workflow.  \n\nDiscussion \n\nThe goal of this experiment was to assess the ability of AI image generators to produce reliable 2D assets that could be used for design renderings. While all three generators assessed \nwere eventually able to achieve the desired results, they also exhibited some general distinctions in process and product. Given the above results, our observations are provided in the \nfollowing subsections.  \n\nRecommendations for AI to 2D Workflows \n\nWe used AI image generation as a means of creating a specific, visually-isolated entourage \nthat could be easily integrated into a 2d rendering using photo-editing software. Beyond the \ncreation of an accurate and high-quality asset, the most important feature is that the asset is \nisolated on a clean, solid background color that can easily be removed to isolate the asset. In \ncreating the assets, we tracked our prompts to try and identify particular prompts that would \nreliably create new assets with little modification of the prompts. Based on our research we \nidentified  several  prompts  that  produced  reliable  results,  and  several  keywords  that  could \ncommonly be added to fine-tune a prompt. These prompts are included in Table 1.  \n\nOverall Assessment of Three Generators \n\nDall-E 2: For the pure task of producing isolated, usable images, Dall-E 2 exhibited the most \nresponsiveness to and greatest understanding of prompts. In other words, it was the most apt \nat intuiting the desired result from the language provided and thus the prompts should be \nclear, simple, and segmented for maximum efficacy. Dall-E 2 also seemed to produce the \nmost  usable  photorealistic  versions of  people  assets.  The  products  might  not have  always \nbeen the highest quality of photorealism compared to those of its counterparts, but Dall-E 2 \nwas the best at isolating assets for easy transfer into a 2D rendering task. Where Dall-E struggles is taking the desired products and reiterating them with different aesthetics. It does what \nyou want but will not be initially responsive or flexible with how you want it. Knowing this, \nDall-E 2 seems to be the superior choice as a sort of asset library workhorse. \n\nMidjourney: Midjourney is by far the best generator for artistic representation at the time of \nwriting. Its fluidity in iteration on anything from photorealism, to mid-century modern, to \nrenaissance styles is, so far, ahead of the pack. However, Midjourney’s initial inability to \nstray from the more artistic or illustrative into rawer, isolated, and usable imagery means that \n2D asset creation will take more effort and might still fail to produce desired results. These \npoints seem to suggest that Midjourney is best used for more exploratory or evocative design \nexercises like producing mood boards, ideating conceptual motifs, or storyboarding, activities in which the program renders some uncannily stunning results. \n\nStable Diffusion: Even if Stable Diffusion did not match the overall performance of Dall-E \n2 or Midjourney, it excelled in utilizing image prompts in combination with text prompts, \nwhich could be reliably used to reduce the number of prompts needed. This is an important \nquality, as a hybridized image-text prompt tool that is responsive holds near unbounded potential. There are many design precedents in the world that are not yet textually or linguistically  recorded,  let  alone  integrated  into  a  natural  language  model  like  those  used  by  AI-generators, but that have a clear visual motif that can be replicated. If a user could use the \nprompt  tool  consistently  and  with  effective  description,  Stable  Diffusion  could  help  them \ngenerate new, heretofore “unnamed” design vernaculars and then integrate them into their \npractice’s asset library. \n\nPossibilities (and Challenges) for Pedagogy and Practice \n\nAs we were conducting this research, the authors simultaneously experimented with integrating generative AI in the classroom. Combining the more structured approach of our study \nwith anecdotal information from parallel teaching efforts offers some insights for issues, possibilities, or challenges with using AI-driven applications in both design pedagogy and practice. The results of both efforts suggest that AI image generators are a viable medium through \nwhich students and young practitioners can achieve rapid generation of large asset libraries \nin a comparatively shorter time than traditional methods and in many instances with higher \nquality results. Typical workflows include drawn out web searches and perusing social media \nsites like Pinterest for pre-made .png images that do not always fit aesthetic or functional \nneeds of a project (e. g. specific plant species, objects, views or actions of people, etc.). A \nlack of fit would then cause students to grab “next best” images and spend extra time masking \nor isolating desired elements in post-production. AI image generators, Dall-E 2 in particular, \nseem to drastically reduce the disjointedness of these workflows allowing greater customizability, more rapid iteration if image needs are not immediately met, and fewer post-production needs  so  long  as  the outputs properly  isolate  elements.  The process  of  creating  these \nlibraries also has the potential to reinforce student knowledge of design details such as materials,  plant  species\/genus\/common  name\/structure\/etc.  or  stylization  (see  Figure  3),  and \nhelp them know how to describe said details in both broad and succinct “terms” as they iterate \non the semantic abilities of the AI-driven language models that power the text prompters. \n\nAll of these same benefits can apply to professional practice, which for all intents and purposes is an extension of the academic learning environment with the added concreteness of \nusing the image assets created for real-world, consultant-quality deliverables. After all, entry-level designers, often new grads, carry a disproportionate amount of production work that \ninvolves both the creation and implementation of 2D image assets in crafting renderings for \nconceptual or schematic packages. It is they who are at once tasked to learn as much as possible about the office’s approach to design, materials use, project management, and then project those values to clients and the public in the form of two-dimensional imagery. Generative \nAI applications could help the industry’s emerging professionals better achieve this task by \nmitigating  cognitive  load  (ROBINSON  2019)  and  time  expenditure  –  which  in  turn  affords \nthem more mental energy and critical thinking capacity to focus on the more complex design \naspects of a given project. In other words, a dramatically reduced and simplified production \nworkload can breed more energetic, more adept, and therefore happier and impactful young \ndesigners who will be more likely to stay in the landscape architecture workforce.  \n\nHowever, while there are many great possibilities for integrating AI image generators into \nteaching  and  practice,  it  is  also  important  to  highlight  their  possible  limitations  and  challenges. First, the success of these generators in producing desirable and actionable images \nfor  the  specific  task  of  building 2D  libraries  is  largely  dependent  on  the  quality  and  consistency of the input data – i. e. the text prompts and base images. As the programmer George \nFuechsel’s attributed proverb goes, “garbage in, garbage out.” If a designer does not quickly \nget a sense of what verbiage elicits the best response from the AI or does not properly put \nthemselves “on the loop” to help the AI see desired patterns, they may end up spending just \nas much time and energy producing assets as they would using traditional methods. Similarly, \nif the use of the generator is not purposeful or task-oriented, the user could also find themselves distracted from the more meaningful problems of a project by the alluring creative \ncapabilities of the generator, exploring representational possibilities in the same way they \nwould a social media site instead of working to ‘create’ those possibilities through their mastery of the project’s actual design needs. Social media algorithms like those of Pinterest are \nknown  to  induce  this  type  of  “curatorial”  labor  and  distraction  (LABBAN  &  BIZZI  2022, \nSCOLERE & HUMPHREYS 2016). While AI image generators like those produced by OpenAI \nfree users from the attention-seeking nature of recommender algorithms, they should be considered no different in their potential to be a sort of representational Potemkin village. \n\nMoreover, the development of generative AI art tools is nascent, with most applications having come to market in the past two or three years. Such nascence has upsides – such as the \nfact that the underlying computer vision and machine learning models will only improve over \ntime as more users provide feedback and the internet provides more open data – but it also \nhas downsides, such as the potential for the models to build in bias or become lopsided in \ntheir outputs based on who is using them (e. g. if the vast majority of MidJourney users are \nfantasy book illustrators, Midjourney may begin to center its production patterns on aesthetics and functions of those users). Furthermore, the art and design worlds have yet to establish \nthe commercial landscape of AI-produced imagery – which in the case of the generators in \nquestion is not technically original imagery but rather novel composition and stylization of \nopen access images from the web – which leaves the legality of using AI-generated assets for \ndesign consulting contested and could in higher profile cases create problems for the more \naggressive users of the medium (SMITS & BORGHUIS 2022, ZEILINGER 2021). While these \nproblems are currently more abstract relative to the purposes of our study, they raise some \nimportant issues to reflect on as the use of AI in design practice continues to grow. \n\nConclusions and Future Research \n\nAI image generation holds substantial promise for creating improved 2D representations of \ndesign projects. Even though the generators assessed in this study are in their infancy, they \nare already useable in existing projects. As these software advance in learning they may move \nfrom  improving  access  to  assets  for  renderings  to  fundamentally  disrupting  the  creative \nclockworks of landscape design. The oscillations of technology adaptation and abandonment \noften parallel the development of skills needed to use technology. AI image generators represent  a  major  evolution  of  technology  that  could  revolutionize  the  role  of  drawing  and \ngraphics skills. Ubiquitous use could begin to suggest that graphical skills may no longer be \nas critical as they once were, but it would require replacing with a skill to translate a graphical \nvision  into  a  semantic  or  ontological  description  that  can  be  understood  by  an  AI  system \n(FERNBERG et al. 2021). Here human and machine become more intimate and symbiotic than \nwhen a designer simply commands graphical software. The ability to learn from AI and to \nteach AI could fundamentally shift the creative process. \n\nIn each of the instances presented in this paper, industry will certainly reflect on the costs \nand benefits, whether as economic terms, or perhaps, existential terms. The software tested, \nrepresent early movers which are readily useful, turn-key solutions. However, the field of \nrapid image generation is advancing rapidly and only represent one of many potential disruptions to the design process. Future research and discussion are needed to reflect on the topic \nof AI in design representation. Additional topics to explore include artistic blockbusting with \nfeatures like inpainting, linguistics models as design tools, AI generators in participatory processes and community visioning, or ethical considerations such as ownership of images, intellectual property, and AI copyright. We hope this case study helps open and contribute to \nfurther conversations about the role of creative AI in digital landscape architecture. \n\nStandardization  of  Landscape  and  Environmental \nPlanning for 3D\/4D BIM and LIM Projects \n\nAbstract: As in object planning (architecture, civil engineering), standards for attribution in BIM models have now been developed for landscape and environmental planning. These standards, as well as \nfirst applications on real projects, will be presented from 2D to 3D and 4D. \nThe semantic standards will contribute significantly to lossless exchange of data and content on landscape and environment with the BIM collaborators and to a precise and direct digital communication. \nHowever, this cannot only mean – in accordance with the motto of previous mainstream BIM policy – \nfaster and more cost-effective project planning. Rather, the concerns of nature and the environment can \nand must be given decidedly more attention and weight, so that potential environmental degradation \ncan be recognized and evaluated at the earliest possible time, then designated clearly and distinctly and \nthus be resolutely avoided through the next planning steps. \nBIM should help to substantially identify and implement the sustainability aspects of construction in \nthe entire BIM cycle and in the spatial and temporal effects on landscape from the local to the global \nscale. It is in this, not just in the merely greater efficiency and cost savings, that the great opportunity \nof integrated digitization and interdisciplinary collaboration lies. This is the current task of landscape \nand environmental planning as well as of landscape architecture and, due to their expertise, also of civil \nengineering, architecture and urban planning. It is the big chance to make extensive use of the BIM \nmethod. First examples for this are given. \n\nIntroduction \n\nBIM planning is making great progress in Germany (BIM DEUTSCHLAND 2022). For example, major infrastructure projects, especially at DEUTSCHE BAHN (DEUTSCHE BAHN 2022), \ncan now be handled exclusively using the BIM method. This also applies to Landscape and \nEnvironmental  Planning,  which  is  extremely  important,  as  each  infrastructure  or  building \nproject regularly causes impacts upon all environmental factors. The environmental values \nand impacts (being analysed e. g. by the Environmental Impact Assessment (EIA)), should \nno longer be managed just by nearly isolated expert work, but the respective results have to \nbe integrated into the so-called BIM collaboration model and communicated among all participating co-workers as clearly as possible. The decisive chance of the BIM collaboration is \ntherefore a considerably higher intensity and quality in the interdisciplinary exchange and \nhence in the joint effort for the avoidance of further environmental effects upon species, soils, \nwater, air, climate change, health, and the landscapes (see esp. NIKOLOGIANNI et al. 2022). \nThe same is true for LIM projects, where building projects are not in the center of consideration, but landscape and environmental systems analysis and management as such, and, com-\nparable to BIM, with various stakeholders and contributors. \n\nFor the highest quality in interdisciplinary work, however, comprehensive information and \ndata exchange is fundamental and therefore requires overarching technical standards (e. g. \nOULLETTE 2018, VAN LUCKWALD & TEMMEN 2017, LIU et al. 2017). \n\nSo far, the integration efforts for BIM-GIS collaboration (GIS as the fundamental software \nsystem for Landscape and Environmental Planning) have focused mainly on the aspect of \nlossless data exchange, as mentioned above. Here, the advantages of FME (Feature Manipulation Engine) or Open Source Approaches (OSA) in contrast to Data Interoperability Exten-\nsions (DIE), have become evident (e. g. GNÄDINGER & ROTH 2021, HERLE et al. 2020, ZHU \net al. 2019, CARSTENS 2019).  \n\nStandards for Semantics \n\nA  decisive  factor  for  successful  model  coupling  and  BIM  collaboration  is  that  uniformly \nstructured expert models and the underlying data standards for semantics exist. Otherwise, \nthe attribution would remain idiosyncratic in each planning case, although a generalization \nwould be of decisive advantage. WIK et al. (2018) developed a set of definitions and parameters, aiming at a unified landscape object standard for Norway. Similarly, an object cata-\nlogue was recently developed in the German buildingSMART landscape architecture specialist group (Figure 1, Figure 2) and is now published (BUILDINGSMART 2023). The catalogue is currently being adopted and developed further by major German infrastructure institutions.  \n\nThe class catalogue should now serve to carry out the attribution of the BIM environmental \nmodels  uniformly,  whether  in  GIS  or  in  CAD,  so  that  on  the  one  hand  all  environmental \nplanners use this standard. In addition, the other BIM collaborators should always be able to \nread in the attributes in a uniform structure and terminology and, as far as required, understand and interpret these data, not least because contents and terminology are very specialized \nand fundamentally different from those of the technical planners (civil engineers, architects). \n\nFirst Application of the Object Class Catalogue in 2D and 3D \n\nThe availability of standards for the attribution in Landscape and Environmental Planning \n(for analysis and preparation of measures) and as well as in Landscape Architecture (for design and realisation of measures) means that systematic and generally valid requirements of \nthe client can now be applied to the specialist models of landscape and environmental planning. First applications of the catalogue in 2D, in a real “classical“ planning project (Second \nS-Bahn Main Line Munich) are represented by Figure 3, as an example.  \n\nBased on this, the further focus was on 3D application. For an infrastructure project in Hamburg (VET Suburban), GIS point data with attributes were transformed to 3D objects and \nexported in IFC format via FME. All attributes were included as custom property sets (Figure \n4). This approach was the basis for the subsequent application of the object class catalogue \nin 3D models. \n\nThe availability of the catalogue as well as the availability of technology to create landscape \nelements in IFC format now enables Landscape and Environmental Planning to effectively \nparticipate in the real BIM collaboration.  \n\nStandards as a Prerequisite for Decisive Added Value for \nPlanning and Policy \n\nIt can be assumed, that the introduced technological process of standardizing attribution and \nas well as geometric features will substantially help Landscape and Environmental Planning \nas well as Landscape Architecture to better communicate and share their analytical results \nlike ecological and aesthetical insights. Beyond this, our field of expertise might even gain \nmore insights within our own domains of research and practice, as we no longer produce just \nmaps in 2D, but models in 3D and 4D in order to reconstruct more realistic objects of the real \nworld: functioning, and also changing environmental systems of all kinds and on all scales. \nDoing so, we expect an added value to our work and more informative results. \n\nAs an example for this added value, we modeled 3D trees including breeding cavities for \ndetermining  potential  conflicts  with  infrastructure  elements,  respectively  construction  site \nelements. The data were sampled by digital field data collection: locations, dimensions and \nphysical status of the trees as well as of the cavities – for example cavity numbers per tree, \nheights,  expositions  and  respective  further  observations.  The  post-processing  of  these  3D \ntrees  allows  for  a  rich  and  highly  integrated  data  analysis  as  well  as  for  a  rapid  collision \ndetection with potential construction elements and therefore for a quick feedback loop between all planners and stakeholders involved. \n\nSimilarly,  a  GIS-based  tool  for  designing,  planning  and  complementing  of  tree  alleys  in \nMecklenburg-Vorpommern, Germany, was developed. We programmed a form in which the \ndesired tree species, tree qualities, distances to the road and between the trees are to be entered. The result is calculated immediately and the respective tree models become elements \nof the Landscape Information Model (LIM). The growth of the trees over the years, starting \nfrom the time of planting, can be simulated (4D) and the design model is ready to be discussed \namong the stakeholders involved (Figure 4).  \n\nIn this way, for example the change in the aesthetic appearance over the decades as well as \nthe necessary distances to the roadside due to the increasing trunk diameters can be represented. Different tree species as well as their habitus in youth, middle age and old age and \neven the species-specific appearances during the seasons could also be displayed by integration of a software extension with 3D plant models by LAUBWERK (2022). Here, the transition \nor overlap between Landscape Planning and Landscape Architecture becomes obvious, as \naesthetics, design and free space planning get relevant.  \n\nDiscussion \n\nWhat is the state of the development towards BIM in Landscape and Environmental Planning? Initial standards for interdisciplinary data exchange at the semantic level are in place, \nbut further refinement in the attribution of natural goods and more experience based on further projects are still needed, not least at the geometric level. Thus, the technical conditions \nfor BIM collaboration are essentially in place, although there is still much need for optimization in detail towards automation, i. e. the replacement of semi-automated and still necessary manual work (GNÄDINGER & ROTH 2021). \n\nWe are now in a position to analyse the environmental impacts of infrastructural or urban \ndevelopment projects and to exchange information with our planning partners directly, i. e. \nin a collaborative model, and to work together on optimizing solutions and on reducing impacts upon all natural assets. The content and results of Landscape and Environmental Plan-\nning are obviously getting into sharper focus of planning partners, especially engineers and \narchitects, and further stakeholders through BIM-GIS integration (and of Landscape Architecture through the BIM capability of the CAD expert software) than was previously the case. \nFor this, the standardization processes are fundamental and extremely valuable. \n\nHowever, standardization is only the necessary foundation – it is not sufficient for real, even \ngreater  attention  to  environmental  concerns  in  all  planning  and  construction  activities  for \ninfrastructure and urbanism. This is because, despite appropriate analysis, planning and compensation measures, the landscape, soils, ecosystems and climate continue to be stressed by \nconstruction  and  operation beyond  sustainable  capacities. Building  activities  are  therefore \nnot yet sustainable in the comprehensive sense.  \n\nOutlook \n\nWith regard to standards, the existing object catalogues – of which the buidingSMART catalogue is presumably just one – must be further developed and coordinated with each other \nso that unified and non-differentiating standards soon apply. Especially in the international \ncontext, this is certainly a great challenge, since the methods of Landscape and Environmental Planning as well as the professional legislation differ greatly from country to country. \n\nBIM will not just contribute to optimizing time schedules and cost plans, but to all relevant \nsustainability aspects of building activities through the whole life cycles (6D). Beyond the \nspecific infrastructure or urban project, the remaining environmental effects in space and time \n(improvement or additional deterioration) have to be examined. It turns out that extensive \nnew digital tasks emerge, especially for landscape and environmental planning, for the use \nof GIS and for interdisciplinary work. \n\nThere  are  other  “next  steps”  needed  to  support  a  comprehensive,  transformative  effect  of \nlandscape and environmental policy: \n•  The systematic development of methodological workflows in all phases of the BIM cycle, since only selective ones were developed so far \n\n•  The application of GIS also in the area of long-term environmental data management on larger scales, e. g. for infrastructure providers, cities and regions (6D, 7D) \n\n•  CO2-e  balances  for  projects,  considering  all  sectors,  such  as  industry  and  building, transport, land use, energy etc.Implementation of ecological services into BIM- and LIM-models. \n\n• The (increasingly digitally supported) landscape ecological and landscape architectural research as well as applied planning, are challenged to further explore their methods of analysis, their modes of representation, the integration and processing of information from external \nprofessional models, the potentials of 3D to 7D in geometrics and semantics, the possibilities \nof  communicating  content  to  planning  partners, politics  and  the  public  and  to  make  them \nfurther usable with regard to priority in protecting nature and environment. \n\nThree Ways to Assess Reliability in Professional \nVisual Impact Assessment \n\nAbstract:  The  public  expects  that the  services  provided  by  professionals,  such as  physicians  or  accountants are reliable. As the public becomes more concerned about visual impacts, it is to be expected \nthat questions will be raised about the reliability of visual impact assessment methods. This paper presents a case study investigating three types of reliability: rater reliability, test-retest reliability, and firm \nreliability. Reliability is generally found to be good but may not reach the highest professional standard. \nThe comparison of two firms suggests there may be a subtle client bias. \n\nIntroduction \n\nEnvironmental impact assessments are widely required worldwide as a condition for permitting projects (SADLER 1969). Visual impacts are among the public’s top concerns when large \ndevelopment projects are proposed that have the potential to change the landscape and its \nappearance significantly. Examples include wind energy development (BISHOP 2011), high \nvoltage transmission lines (IETPP 1996), and forest management (RIBE 1989). While there \nare decades of research about the public’s perception, it may be surprising that there has been \nlittle or no investigation of the reliability of the professional judgements made in visual impact assessments (VIAs).  \n\n“Reliability” is being used in the scientific sense of whether a method produces consistent \nresults. For instance, if a group of professionals all apply the same method to evaluate the \npotential  visual  impacts  at a  key  observation  point  (KOP),  do  they  all  arrive  at  the  same \nconclusion? If they are all exactly the same, then it is really only necessary that one professional  conduct  the  evaluation.  But  if  there  is  variation  among  their  evaluations,  then  it  is necessary to average the findings across several professionals to obtain a reliable evaluation. \nA related concern is whether the evaluation of a professional is stable over time. For instance, \nif the evaluations are conducted six months or a year apart will the results be the same? Finally, a third concern is whether professionals representing clients with different interests \narrive at the same conclusion or does their evaluation tend to tilt toward their client’s interests \n(BAZERMAN, LOWENSTEIN & MOORE 2002). \n\nThis paper investigates each of these three ways of looking at reliability of professional visual \nimpact evaluations: (1) the reliability within a group of professionals on the same team making their assessments at the same time, (2) the test-retest reliability of the same professionals \nmaking judgements separated by a substantial time interval, and (3) the comparison of results \nfrom two groups of professionals using similar methods but representing different clients.  \n\nThe judgements made in VIAs can have real-world consequences. Therefore the standards \nfor establishing an acceptable reliability coefficient should be higher than for basic research. \nPALMER & HOFFMAN (2001) recognize that while reliabilities of 0.7 are fair and 0.8 are very \ngood for basic research, reliabilities of 0.9 should be expected from professional assessments. \n\nMethods \n\nThis investigation uses data from the VIAs prepared for the approximately 187-mile (306 \nkm)  Northern  Pass  Transmission  Project,  which  would  deliver  1,200  MW  of  hydropower \nelectricity from the Canadian border south through the state of New Hampshire to the Boston, \nMassachusetts metropolitan area. For most of its length, the aboveground portion is collocated  in  the  right-of-way  of  an  existing  115  kV  transmission  line.  The  analysis  primarily \nrelies on data from the VIA prepared for the US Department of Energy by T. J. Boyle Associates (2017) as a requirement to obtain a federal permit (hereafter referred to as DOE). A \nsecond VIA was prepared by T. J. Dewan & Associates (2015) for Eversource Energy for \nsubmission to the Site Evaluation Committee as part of the New Hampshire permitting process (here after referred to as SEC).  \n\nBoth VIAs used a formalistic analysis of simulated views from a number of KOPs selected \nto represent the range of conditions encountered along the proposed project route. The procedure used in the DOE VIA was based on SHEPPARD & NEWMAN (1979), which rated the \nproject’s color (0-9), form (0-6), line (0-3), texture (0-3) and scale (0-6) contrast with the \nsurrounding landscape as well as its scale (0-12) and spatial (0-6) dominance. The sum of \nthese values determines the visual impact as severe (36-45), strong (27-35), moderate (18-\n26), weak (9-17) or negligible (0-8). The SEC VIA adapted a rating system from SMARDON \n&  HUNTER  (1983)  that  was  based  in  part  on  SHEPPARD  &  NEWMAN  (1979).  However,  it \napplied a slightly different approach to weighting the components. For this analysis, the SEC \nratings were adjusted as shown in Table 1 to be equivalent to those used for the DOE VIA. \n\nIn December 2014 six landscape architects who were involved in the NPTP’s field inventory \nfor DOE were trained to conduct the VIA ratings. They evaluated the no change (alternative \n1) and proposed project (alternative 2) photorealistic simulations from 15 KOPs for the DOE \nVIA. The simulations were 11”x17” high-resolution color prints with only a minimum of text \nto identify their location. They were considered in a randomly assigned order; each evaluator’s judgements were made independently without any discussion.  \n\nAfter submission of the VIA report, Eversource Energy proposed a new preferred route (alternative 7) that buried a substantial portion of the route around a scenic National Forest. \n\nSeven new KOPs were added to provide better representation of project impacts. This resulted in 22 KOPs for alternatives 1 and 2, but alternative 7 had only 14 KOPs with above \nground views of the project. The team evaluated all three alternatives in November 2016.  \n\nThe Pearson r is used to measure the inter-rater correlation among the 6 evaluators. Fisher’s \nz  transformation  is  used  to  calculate  the  mean  Pearson  inter-rater  correlation  (COREY, \nDUNLAP & BURKE 1998). In addition, the intraclass correlation coefficient (ICC) is calculated \n(PALMER & HOFFMAN 2001). The Type 2 ICC is used because all six evaluators evaluated \nall of the simulations. It incorporates the variation among both raters and KOPs, and reflects \nthe absolute agreement among raters. ICC(2,1) is the expected reliability for one evaluator; \nICC(2,k) is the reliability for the group of six evaluators. In addition, these data also provide \nan opportunity for a test-retest reliability using the Pearson r to compare the ratings from \n2014 and 2016 for alternatives 1 and 2 at 15 KOPs. \n\nFor the SEC VIA three landscape architects evaluated alternative 7. The SEC VIA included \nsix KOPs that were at the same location as KOPs used for the DOE VIA. This permitted a \ncomparison between the visual impact ratings of the two firms. \n\nResults \n\nRater Reliability \n\nThe ICC and inter-rater (i. e., mean Pearson correlation) reliabilities for the six landscape \narchitects evaluating the three alternatives in 2016 are given in Table 2. The ANOVA analyses to compute the ICC values are all significant at the .001 level.  \n\nTable 3 presents the Pearson correlations between the six individual evaluators. Since alternatives  share  the  same  base  photo,  the  alternatives  are  not  independent  from  each  other. Therefore, their results are presented separately. In general, the Pearson correlations are significant at the 0.001 level. \n\nTest-Retest Reliability \nThere  were  15  KOPs  with  views  of  aboveground  structures that  were rated in both 2014 and 2016. Pearson correlation  is  used  to  determine  the test-retest reliability for the six evaluators and the mean correlation for the group. The test-retest Pearson correlations in Table 4 are generally significant at the 0.001 level. The mean correlation for the group is calculated using  Fisher’s  z  transformation.  These results indicate that even after a year the ratings are very consistent for all raters. \n\nFirm Reliability \nThe mean visual impact ratings for the six  KOPs  that  are  common  between the DOE and SEC VIAs are shown in Table  5.  The  Pearson  correlation  between  the  mean  DOE  and  SEC  VIA ratings for the six common KOPs was 0.842 (p = 0.158). While this correlation is high, the very small sample size means that the p-value is higher than is  normally  acceptable.  In  addition, the Pearson correlation measures consistency rather than absolute agreement, which may not be the most useful way to compare two firms.  \n\nThese ratings are plotted in Fig. 1. The general trend of the lines rises from left to right, which \nis why the correlation is high. However, it also appears that There is also a trend for the SEC \nvalues to be lower than the DOE values. This suggests the possibility of a client bias. \n\nKey Observation Points\n\nThe procedure used for the DOE VIA interprets the ratings into five levels of severity: Severe \n(36-45), Strong (24-35), Moderate (18-26), Weak (9-17), Negligible (0-8). Another way to \ncompare the firms is to look at the probability that the individuals from each firm judged the \nvisual impact to be severe or strong (i. e., a greater impact), or at lower level. Fisher’s exact \ntest is used to determine that the evaluators for the DOE VIA were significantly more likely \nto rate the visual impact as greater than the evaluators for the SEC VIA (p = 0.02). \n\nDiscussion \n\nThe difficulty with determining the reliability of professionally conducted VIAs is that normal practice is to have only one evaluator; highly contested projects may use three or four \nraters, as was done for the VIA submitted to the SEC. The DOE VIA used six independent \nraters, which is very unusual – this author is not aware of another VIA using this many professionals. While more raters would lead to a more robust VIA analysis, six evaluators and \n22 KOPs is adequate to obtain the statistically significant ICC, inter-rater and test-retest results reported here. The group’s results are high, sometimes meeting the 0.90-standard for \nprofessional services, but often falling just short of it. Correlations at this level are statistically significant with only six raters. \n\nThe group’s rater reliability for the contrast ratings is higher than previously reports (PALMER \n2000). One reason for this may be that the VIA professionals are both experienced in using \ncontrast ratings and have field experience throughout the project area. Nonetheless, it is worth \nnoting that the single-rater reliabilities (i. e., ICC(2,1)) were not up to professional standards, \nwhich supports BLM’s (1986) direction to use multiple raters.  \n\nThe question of how many raters are necessary to obtain the group reliability expected of \nprofessionals is difficult to answer. In practice, evaluating a large number of representative \nKOPs is probably more important than a large number of trained raters. SMARDON evaluated \nthe reliability of the more common ratings used in VIAs and recommended a team of ten \nraters (SMARDON et al. 1983, 93). Writing in a medical journal, KOO & LI (2016, 157) suggest \n“as a rule of thumb, researchers should try to obtain at least 30 heterogeneous samples [e. g., \nKOPs] and involve at least 3 raters whenever possible.” BUJANG & BAHARUM (2017) review \nhow to determine the minimum number of raters for a given number of observations (KOPs) \nunder different assumptions and provide tables to guide that determination. The number of \nraters  is  generally  determined  based  on  the  assumption  that  there  is  no  agreement  among \nthem (i. e., reliability is 0). If the rating for 20 KOPs indicate that the ICC is 0.8 or 0.9, then \nthree raters are sufficient for statistical significe (alpha = .05) under that assumption. This \nchanges if it is assumed that the typical reliability is 0.7 for a VIA and a firm wants to demonstrate that their calculated reliability of 0.9 is statistically significant; then 9 to 12 raters are \nrequired. \n\nThe test-retest reliability presented here is a found opportunity made possible because the \ndeveloper changed the project design, which is not an unusual event. It would be a benefit to \nthe profession if the practice reported here is followed by others, and an evaluation of the \noriginal simulations is repeated as well as evaluating the new design. The results indicate \nhigh  reliability  in  applying  the  contrast  ratings  on  the  same  scenes,  even  after  a  year  has \npassed, though sometimes falling short of the 0.90-standard for professional services. \n\nInterpreting the comparison of firms is more difficult. There is a strong correlation between \nhow the firms evaluated six sites, meaning that their ordering of sites for impact severity was \nvery  similar.  However,  there  is  also  a  tendency  of  the  SEC  ratings  to  be  assessed  as  less \nsevere than the DOE ratings. This could be because of client bias, but it might also be a result \nof converting the SEC ratings to be equivalent to the DOE rating scale. Client bias is potentially  a  significant  problem,  since  the  developer  is  normally  responsible  for  preparing  the \ntechnical reports supporting an agency’s environmental impact assessment – the DOE report \nwas an exception. This is an area that deserves further research. \n\nBAZERMAN et al. (2002, 3-4) investigated client bias among accountants doing audits. Like \nVIAs, accounting audits may have the appearance of deterministic objectivity, but actually \nrequire a substantial amount of professional judgement and interpretation resulting in unintended distortions. They identify three opportunities for bias that also apply to VIAs. \n\n•  Ambiguity. Bias thrives wherever there is the possibility of interpreting information in different ways. \n\n•  Attachment. Auditors have strong business reasons to remain in clients’ good graces and are thus highly motivated to approve their clients’ accounts. ... it is well known that client companies fire accounting firms that deliver unfavorable audits. \n\n•  Approval. Research shows that self-serving biases become even stronger when people are endorsing others’ biased judgements – provided those judgements align with their own biases – than when the are making original judgement themselves. \n\nThey suggest that there is a need to provide for auditor independence and removal of the \nthreat of being fired for unfavorable findings. Perhaps VIAs could benefit from similar provisions. \n\nThis study has additional limitation that others interested in this work should consider.  \n\n•  The contrast ratings made for both the DOE and SEC reports were made in the office, not in the field. The BLM (1986) has long stipulated that contrast ratings need to be made at the KOPs in the field.  \n\n•  The two firms used slightly different photosimulations, though the viewpoints are very near to each other. The selection of a simulation’s viewpoint in itself could be a form of client bias (SULLIVAN et al. 2021). \n\n•  The  simulations  used  in  this  study  are  based  on  summer-like  photography;  the  DOE evaluations were done in the winter and the date of the SEC ratings is unknown. There is evidence suggesting that the field evaluations should be conducted in the season represented  in  the  simulations.  PALMER  (1990)  found  that  “when  people  evaluate  scenic quality, they do so within their present seasonal context.” \n\nConclusion \n\nThe public is justified in expecting that VIA professionals produce reliable reports. Three \nways to evaluate the reliability of visual impact judgements for KOPs are demonstrated. In \nthis case study, the ICC(2,k) reliability of the six evaluators is 0.924, 0.888 and 0.894 respectively for the three alternatives. This result is very high, as it should be for professional ser-\nvices. In contrast, the ICC(2,1) reliability for a single rater is 0.655, 0.570 and 0.585, which \nis unacceptable for professional services. The implication is that multiple trained professionals must be used to evaluate the visual impact at each KOP.  \n\nA second approach to reliability is to determine the stability of the evaluations over time. \nThis  case  study  compared  the  same  15  KOPs  evaluated  for  two  alternatives  by  the  same \nindividuals using the same procedures at different times, nearly two years apart. The test-retest mean Pearson correlations are 0.847 and 0.868, which indicates substantial stability. \n\nFinally, the results from two VIAs prepared by different firms for different clients are compared for six KOPs that were evaluated in both VIAs. The Pearson correlation between these \nfirms’ evaluations is 0.842, which shows high consistency comparable to the test-retest reliability. However, when the interpretation thresholds for impact severity are applied to the \nindividuals’ ratings, the firm whose client was the developer was much more likely to assign \na lower-level impact rating than the firm whose client was the government permitting agency. \n\nReliability is an important attribute of professional services and should be required by regulatory agencies and demonstrated as part of VIAs. This requires that a panel of trained evaluators independently rate the same KOPs using the same methods. A rigorous demonstration \nof  reliability  would  include  test-retest  evaluation,  and  a  comparison  of  VIAs  prepared  by \nseparate firms for different clients. \n\nVirtual Landscape Design Patterns in Albion Online \n\nAbstract: The digital game industry has significantly contributed to the global entertainment economy \nin the last twenty years. Designing a virtual landscape consumes the greatest effort and time in game \ndevelopment, but only a limited number of landscape architects are joining the industry. We can expect \nthat future landscape architects will join the industry, as virtual landscape shares similar characteristics \nto physical landscape, and they need to be prepared. Therefore, we need to understand the current status \nof virtual landscape design, which is the goal of this research. We selected the concept of Pattern Language because today’s game industry is unaware of the advantages of design patterns and a common \nvocabulary. Albion Online was selected because the game has various landscapes and relatively easy \naccess. Virtual landscapes in the game were analyzed by reverse-engineering methodology. As a result, \nsixteen design patterns were found throughout a level. This indicates that today’s game industry isn’t \nactively using design patterns. However, we can assume that if well-trained landscape architects join \nthe  game industry,  they  could  efficiently design  richer  landscapes  with  various design  patterns. We \nneed to start the education and training of young landscape architects for this new frontier. \n\nIntroduction \n\nThe global digital game market was valued at 220.79 billion USD in 2022 and is expected to \nexpand (VIDEO GAME MARKET SIZE & SHARE GROWTH REPORT n. d. 2022). Designing virtual landscapes in games is the most critical and time-consuming task in their development \n(KIM 2019). Though the industry suffers from inefficient landscape design without landscape \narchitects’ participation, insufficient inroads have been made into the field of landscape architecture. One of the reasons for this situation is the absence of study or courses on this \nsubject in landscape architecture education. Some courses try to adapt the topic, but most use \nvirtual landscapes to improve the design of physical spaces. However, we can expect that \nfuture landscape architects will join the digital game industry or other industries related to \nmedia as virtual landscape shares similar characteristics to physical landscape (KIM 2019), \nand preparations are needed. Therefore, we should understand the current status of virtual \nlandscape design, which is the goal of this research. \n\nThe reason why designing a virtual landscape is the most consuming part of game development is that it has a unique feature compared to space designs in other media, namely interaction. However, the game industry still adopts methodologies from old and non-interactive \nmedia such as movies. We need landscape architects in this industry to design proper interactive spaces because we have been trained to design interactive spaces more efficiently and \neffectively (KIM 2019). \n\nWe selected the concept of Pattern Language (ALEXANDER 1977) as our target. Design patterns are to overcome the deficiencies that arise due to the non-universal design procedures \nof the digital game industry, with common design patterns that can be systematically provided between designers as a vocabulary (KIM & BAZIN  2018). This shared vocabulary is \nrequired  in  order  for  the  digital  game  design  process  to  be  efficient  (KREIMEIER  2002). \nAdopting existing vocabulary and methodologies taken from physical landscape architecture \ninto  virtual  landscape  design  will  be  beneficial  (KORKMAZ  &  KIM  2022).  If  the  industry \nadopts them, we can expect a more productive design process and better designs in the game \nindustry. Additionally, the findings of this research will be helpful for landscape architects \nto expand into a new domain. \n\nReference Study \n\nDesign Patterns in Digital Game Industry \n\nALEXANDER (1977) pioneered the concept of Pattern Language. He tried to find simple and \nwell-formed  solutions  to  design  problems  of  varying  scales  (DAWES  &  OSTWALD  2017). \nMore than one alternative solution can be found for a problem, but the combination of problem and solution is the crux of the Alexander Model (BARNEY 2020). He sought to identify \ndesign patterns architects could use to design good spaces. With the language of design patterns, a simple building block format has emerged that can help non-professionals to demystify socio-spatial considerations (LEA 1994). We need design patterns in architecture as they \ncan be used to solve specific problems and perform the animation process, which is another \ncritical task. According to ALEXANDER (1977), unless all people build buildings in society \nwith a common language, they will not be able to maintain their vitality. He found 253 patterns and argued that designers should develop this Pattern Language and that they can develop languages completely independently (BARNEY 2020). \n\nALEXANDER's (1977) findings and the virtual landscape design have common features. Patterns are used in software engineering or any field to explain design decisions. Following his \npath, KREIMEIER (2002) identified design patterns in digital games and proposed pattern formalism for digital game design. He focused on four basic elements; Name, problem, solution, \nand consequences overall, called Pattern Template, in digital games. As KIM (2019) mentioned, a shared vocabulary is required in the field because game development is a cooperative  design  process  with  the  participation  of  numerous  designers  from  various  fields.  His \nstudy presents the effect of a design methodology based on the shared design vocabulary. \n\nKim's Virtual Landscape Methodology (2019) \n\nFor a coherent virtual landscape design, effective communication between designers, artists, \nand programmers is critical (KIM 2019). He addressed the lack of efficiency, common language, and method in current game design and proposed a Design Methodology for Virtual \nLandscapes in Digital Games. Before the game designers learned this method, they worked \non scribbling sketches without unified language. This made it difficult to understand each \nother and did not provide consistent information. After learning the methodology, the teams \ncould  present  their  ideas systematically.  KIM  (2019)  could  observe  that  the  shared design \nmethodology can save 14% of the time and 74.3% of the cooperative work and increase the \nresult’s quality. Not only to understand the value of design language in the game industry, \nbut this study also adopted KIM’s (2019) methodology to reverse engineer the target landscape in the game. The methodology suggests designing the landscape as a combination of \ndifferent layers, such as the natural, artificial, and media layers, as shown in figure 1. By \nfollowing the structure of the methodology, we could dismantle the target site systematically \nand analyze it layer by layer. \n\nResearch Methodology \n\nReference Study  \n\nWe  conducted  a  series  of  literature  reviews  to  understand  the  concept  of  design  patterns, \nmainly with the work of ALEXANDER (1977) and KREIMEIER (2002). Based on their studies, \nwe could understand how to adopt the concept of design patterns to evaluate the target landscape  in  the  game.  We  also  studied  Pattern  Language  for  Game  Design  from  BARNEY’S \n(2020). However, the strategy KREIMEIER (2002) followed for finding the design patterns in \nthe games was decided to be followed because he is the initiator of the field. We also conducted a reference study about reverse engineering based on KIM’S (2019) study. ANONYMOUS (2022) studied Reverse Engineering in North Korea's Gaming Economy, but because \ntheir  case  study  was  not  about  reverse  engineering  on  mapping,  we  could  not  adopt  their \ntheory in our research. We could understand that originally KIM’S (2019) methodology was \na design methodology to design and build a virtual landscape. However, we could adopt the \nmethodology in a reversed way to analyze the design of the target space’s design. \n\nReverse Engineering Albion Online \n\nWe selected KIM’S (2019) methodology to reverse engineer Albion Online (SANDBOX INTERACTIVE 2017), a Massive Multiplay Online Role-Playing Game (MMORPG). The game was \nselected because it has a large, open world and is relatively easier to access than other games. \nAdditionally, as an MMORPG game, various players run diverse activities spontaneously, \nsuch as hunting and dueling, and designers focus on emphasizing these activities with the \nlandscape and its assets (Figure 2). \n\nWithin the game, we selected Longmarch Meadow as the target site because it is mainly for \nnew starters and carries every basic element in the game. By following KIM’S (2019) methodology in reverse, we could organize the location and the condition of every asset on the \nsite  in  the  form  of  a  master  diagram,  figure  3.  This  reverse  engineering  is  the  preferred \nmethod to find the locations of the assets that make up the game's map and to find the design \npatterns that ALEXANDER (1977) introduced. \n\nFirst, we found the site’s map from the game's official website (www.albiononline2d.com) \nand transferred it to Autocad, and scaled it according to the dimensions from the website. \nGeomorphological mapping of the area was made on the scaled map, and the levels on this \nmap were calculated as average by taking the avatar's height (1.8m) in the game as a reference. Second, we manually measured the landscape's size, location, and number of assets. \nThe average walking length of an avatar is taken as a reference, which is 0.762 meters. The \nlocations of assets were measured by counting the number of avatars’ steps. Third, we organized the collected information about assets in each layer, such as the story, natural, artificial, \nand media layers. Combining all layers, the master diagram of Longmarch Meadow (Figure \n3) was visualized using CS Photoshop software. With the master diagram, we could find \ndesign patterns in the landscape on various scales. \n\nResult \n\nBy analyzing Longmarch Meadow with the master diagram, we could discover 16 design \npatterns mentioned in A Pattern Language (ALEXANDER 1977) in five fields; Arenas, Dungeons, Environment & Media, Non-playable Characters (NPC), and Circulation. In figure 4, \nthese five design fields are in the main headings. The design patterns from A Pattern Language are included in the subtitles with page numbers. Problem-solution logic of the design \nmodels is at the center of these analyses. We followed KREIMEIER'S (2002) Pattern Template \nfor  the  analysis.  Firstly,  the  definition  of  design  patterns  by  ALEXANDER  (1977)  is  explained. Second, the similarity of the design patterns found with the game's design patterns \nis explained depending on the problem (issue) and solution. We added figure 4 to present the \ndesign patterns. \n\nDesign patterns in Albion Online  \n\n•  Pattern 1.1. The Distribution of Towns. Civilization cannot develop in a place where \nthe population is disproportionately concentrated. Balance in population distribution is \nrequired. Issue: Multiplayer online games may freeze or crash due to overload players. \nSolution: Designers created five isolated areas to minimize the number of players per \nspace.  \n\n•  Pattern 1.2. Carnival. A city needs its fantasies, just as an individual must imagine magnificent events to unleash their inner strength that daily events cannot restrain. Issue: \nPlayers can easily be bored with routine content. Solution: With five arenas placed and \nhelped players to interact with other thus, the routine was avoided.  \n\n•  Pattern 1.3. Connected Play. Kids need each other for their mental health. Issue: In the \ngame, the players are often alone, and their relationship with other players is limited. \nThis makes players drop the game. Solution: In arenas, players have to work together, \nwhich creates an emotional bond, and that helps players to bond more.  \n\n•  Pattern  1.4.  Adventure  Playground.  Adventurous  and  imaginative  play  should  exist. \nIssue: Adventure playgrounds exist all over the game, but these adventure playgrounds \nsometimes do not meet the needs of the player thus, they can search for different adventures. Solution: A different adventure playground is designed with arenas where players \ncan form groups and spend time together. The adventure playground experience in arenas is different from the experience elsewhere in the game. \n\n•  Pattern 1.5. Something Roughly in the Middle. A public area is an open space that can \ncarry out various activities. Issue: The lack of understanding of the purpose of the arena \nand the feeling of emptiness in the space. Solution: NPCs and other assets, such as treasure chests, were placed in the arena to fill this void. Thus, the space became meaningful.  \n\n•  Pattern 1.6. Structure Follows Social Spaces. The physical spaces of a structure must \nmatch the social spaces determined by its occupants' activities and social groups. Issue: \nThe game’s space has a unique style of the middle ages. Therefore, the structure materials should reflect the period. Solution: In the arena design, a design compatible with the \narea and its social environment was achieved using cut stone on the walls and slate on \nthe floor. \n\n•  Pattern 2.1. Carnival. Same contents as Pattern 1.2. Issue: Players can easily be bored \nwith  routine  content.  Solution:  With  different  dungeons  placed,  a  new  entertainment \narea became part of the game and helped players to interact with other thus, the routine \nwas avoided.  \n\n•  Pattern 2.2. Adventure Playground. Same contents as Pattern 1.4. \n•  Pattern 2.3. Secret Place. People must live with a secret place in their homes that is \nused in special ways and moments. Issue: Special areas are needed in the game that can \nexcite  players  with  their  mystery.  Solution:  Different  dungeons  have  been  designed, \ncreating a mystery and providing new experiences, which increases stress and curiosity. \n•  Pattern 2.4. Different Chairs. People come in various sizes and sitting positions. While \npicking out chairs, users should consider choosing a wide range. Issue: Beginner, intermediate and expert players exist in the game at the same time. Dungeons should be usable and experienceable for every level of players. Solution: Separate dungeons are designed for experts and beginners without entry limits. Players can select any options.  \n•  Pattern 3.1. High Places. One of the most fundamental human instincts is to go to a \nhigh  point  where  you  can  survey  the  surrounding.  Issue:  No  landscape  observation \nchance exists at different topography levels makes the game unattractive. Solution: Topography was shaped to make a dynamic emotion. In hills, rocky areas, the coast, and \nthe lake at the lower level can be accessed, and each is located at a different level. \n•  Pattern 3.2. Pools and Streams. We are mostly made of water, it's where we come from, \nand it greatly impacts our mentality. Issue: Players can easily be aggressive in Albion \nOnline, and players' stress levels are consistently high. It is important to offer the player \nrelaxation. Solution: Longmarch Meadow has many water elements for relieving stress.  \n•  Pattern 3.3. Animals. Animals have an equally significant role to deliver a specific atmosphere of the space, as do trees, grass, and flowers. Issue: Players hope to feel various \nspace and their atmosphere in the game. However, it will be a burden to provide for all \nspecies of animals. Solution: Different biomes exist in the game, with the addition of \ndifferent animals in each biome. This lets players travel through various biomes and feel \nthe space with different animals. Also, animals are non-player characters (NPC), but we \ntook them under Environment & Media as they are the biggest part of the environment. \n•  Pattern 3.4. A Place to wait. Conflicts or bored can occur during the waiting process. \nIt's crucial to mix up the waiting time with other activities. Issue: Players enter a waiting \nperiod on the road. Therefore, players will get bored and impatient during the waiting \nperiod.  Solution:  Extra  activities  have been  added  to  make  the waiting process  more \nenjoyable for players. While crossing the road, the player can cut down a tree and dig a \nmine. \n\n•  Pattern 3.5. Ornament. Everyone has an inner urge to decorate their surroundings with \nornaments. However, properly applied decorations will only be effective because decorations serve a specific and clear purpose in a building or space. Issue: The virtual landscape \nseems incomplete without small touches. Solution: The designers decorated the landscape \nto increase realism by placing different abstracted-designed assets without any practical \nfunction. \n\n•  Pattern 3.6. Warm Colors. The warmth of a space's colors influences whether a space \nis comfortable or uncomfortable. Issue: The atmosphere of the game is tough and rapid \ncausing stress to players. Solution: The comfort feeling in the game is provided by landscapes with warm-toned colors.  \n\n•  Pattern 4.1. A Place to wait. Same contents as Pattern 3.4. Issue: Sudden stress is required in the game to let players focus. Solution: Designers placed surprises by placing \nenemies in a random algorithm. \n\n•  Pattern 5.1.  Circulation  Realms.  People are  under  great mental  stress because  they \nhave no idea where they are. We can assume that a person must always have a mental \nmap or set of instructions when they travel. Issue: Players may get lost due to the large \nareas. Solution: A map is placed in the right corner of the game as a user interface (UI) \nthat navigates players. \n\n•  Pattern 5.2. Path and Goals. A path's design is suitable for walking when it appears \nproper  and  comfortable.  Targets  must  be  positioned  in  regions  of  natural  interest  to \norganize paths. The paths are then created by connecting their targets. Issue: The site \nis vast, and players can easily get lost to get out of the site. Solution: Designers placed \nmain axis-based roads, so players can move from one to another with minimum confusion.  \n\n•  Pattern 5.3. A Place to wait. Same contents as Pattern 3.4. Issue: It is challenging for \nplayers to have fun if they all share the same road. Solution: Designers provided enough \nroads with various options, so players can select with their condition and taste. \n\nDesigners used 16 design patterns in Longmarch Meadow, repeated with the same strategy \nin every region. This indicates that only 6% of design patterns from A Pattern Language were \nused. \n\nDiscussion and Conclusion  \n\nIn Albion Online, Longmarch Meadow region, only 16 of ALEXANDER's (1977) 253 design \npatterns were used. These patterns come together differently to form the five design fields of \nthe game. Considering the scale of the level, we can understand that only a limited number \nof patterns were applied. These 16 patterns repeat in the other regions of the game, so we can \nassume that design patterns in the game created a systematical approach to design and helped \nto solve issues over the different regions. We can assume that if more of these 253 design \npatterns that ALEXANDER (1977) can be included in the digital game design with well-trained \nlandscape architects, many limits in the digital game industry can be resolved.  \n\nDigital game development must have a precise design methodology (ROLLINGS & MORRIS \n1999). Landscape architects, a systematically working professional group, should be realized \nby the digital game industry, which is an important value of today and the future. Unfortunately, the current game industry is unaware of these advantages so issues arise in the industry \n(KIM  2019),  such  as  design  taking  longer  and  the  final  result’s  quality  is  not  satisfying \n(PETRILLO et al. 2009). At the same time, landscape architects need to realize the advancing \ntechnology and this developing industry. They need to start investing and working to be involved in this field as they have the potential to take a big role in the industry. Researching \ndesign patterns from the landscape architecture field can be a solution to overcome the limits \nthat arise due to the non-universal design procedures of digital game companies with common design languages (KIM & BAZIN 2018). We need to merge landscape architecture and \nthe game industry.  \n\nThe characteristic of spaces between the physical space’s landscape and the virtual landscape \nexists, and this makes traditional landscape architects to be hesitated to expand their design \ndomain. However, both spaces carry the same characteristic in the root, the interactivity, and \nthe only difference is the depth of those interactions (KIM & BAZIN 2018). This presents that \nthe professionalism of landscape architects is valid in a virtual landscape design. They can \ndesign space more efficiently by imagining an area on a large scale and designing a plan. \nAdditionally, they have information about vast references. It is obvious that landscape architects’ performance will be more efficient and provides better quality result compared to those \nwho currently are designing virtual landscape without any understanding of space.  \n\nAt the same time, this paper carries limits. We only could examine one game and should have \nmeasured the efficiency of design patterns more statistically. Therefore, in future research, \ndesign patterns will be examined through different games, and the contributions of design \npatterns will be explored in depth. However, this paper still carries potential as a study investigating the value of landscape architects and their design methodology in a rising frontier.  \n\nVisualizing and Tagging Trail Experiences \n\nAbstract: Public spaces are changing in importance for people as they seek new opportunities to use \nprotected areas for outdoor recreation. Within these spaces, constructed features including trails, encourage visitors to move throughout and use the landscape in particular ways. In designing these spaces, \npublic land managers and landscape architects play a key role in shaping the kinds of activities and \nrelationships to the environment are encouraged or discouraged. In this paper, we examine trail user \nand maintainer perceptions of the kinds of activities people engage in on trails. This pilot paper shares \nanalysis tools and interactive modes for communicating survey results. We create a domain-specific \nsentiment tagging lexicon for classifying trail activities and experiences. Finally, we develop an interactive digital dashboard to explore possibilities for new forms of public and planner engagement with \nprotected areas and trails through virtual data products.  \n\nIntroduction \n\nPublic interest in protected areas, including hiking and other recreational trails, is on the rise, \nparticularly in the context of the COVID-19 pandemic (CHRISTIANA et al. 2022, POWER et al. \n2021). Park visitation is at an all-time high, causing many iconic trails and sites to implement \nwaiting  lists  and  lotteries  for  entry  (NEW  YORK TIMES 2021, POHLE 2021).  More  broadly, \ntrails of local or regional importance are also seeing increased visitation, including an increased number of visitors who are new to outdoor recreation. Dismayed and concerned visitors documented  these  increased  pressures  in numerous  viral videos on social  media  that \nshowed heavily trafficked trails and overcrowded campsites across National Parks and local \nwilderness areas. Thus, the pandemic has brought a public conversation around trail maintenance, usage, and behavior to the national forefront, and raises important questions about \npublic engagement, access, and stewardship of shared wilderness sites. \n\nGiven the broad-reaching significance of trail spaces, in addition to their growing importance \nas shared common spaces in the public eye, the necessity of further examining how trail users \nengage with these spaces is clear. In the context of landscape architecture, tools for analyzing \nand communicating data related to public trail use, perceptions, and values may help improve \nthe efficacy of user-centered and community-engaged design. In addition to providing spaces \nfor outdoor recreation, trails also support mental and physical health (EMIL et al. 2016) and \nare visible design features on a landscape. Due to the impacts of trails on their surrounding \nnatural  environment  (BALLANTYNE  &  PICKERING  2015),  trail  design  is  a  particularly  important topic in the realm of sustainable land management. Moreover, trails are critical components of urban greenways (LUYMES & TAMMINGA 1995), which, for decades, have been a \ncore emphasis of green infrastructure studies and environmental management.  \n\nDrawing on survey data of trail users and maintainers, we approach two challenges for working with digital data related to trails, outdoor spaces and nature experiences: 1) improving \ndomain-specific text analysis frameworks and 2) creating platforms for public engagement \nwith datasets. In this paper, we develop an open-source trail activity lexicon that can be used \nby planners and scholars to tag terms related to outdoor recreation experiences and spaces. \nIn addition, we designed an interactive digital dashboard that visualizes information about \ntrail use across the United States. Accessibility is emphasized in the design of both outputs \nto ensure their contribution to broader themes of public space management. To this end, we \nalso include further discussion on possible future applications of this framework, as well as \nanalysis of its potential limitations, towards the end of the paper. \n\nIn  landscape  design  and  planning  practice,  communication  plays  a  key  role  in  supporting \npublic engagement. Language-based communication is a common approach for engaging the \npublic and stakeholders through workshops, online\/in-person surveys, and public hearings. \nIncreasingly in the age of big data, incorporating public perception on landscape issues necessitates a more efficient methodology. For example, in terms of landscape design and conservation planning, the Geodesign-based support approach demonstrates that public percep-\ntion and diverse engagement are well communicated (LI & MINLBURN 2016, PERKL 2016, \nRODERICK 2018).  Studies  engaging  with  social  media  also  demonstrate  the  advantages  of \nsynthesizing  public  perception  into  planning  practice  (WANG et  al. 2018).  However,  constructing platforms integrating various forms of information into a single, publicly accessible \nformat  remains  an  emerging  area  (SCHÄPERMEIER  et  al.  2021).  The  popularity  and  ready \navailability  of Python programming  gives  the opportunity  to  establish  dashboards  dealing \nwith information, visualization, spatial configuration, and even social networks. There are \nmany potential applications of user-specific research in the context of landscape architecture. \nFor example, understanding the activities people perceive to be encouraged and discouraged \ncan yield insights into the values and preferences of diverse user groups, which in turn may \ninform more thoughtful, user-centered landscape design.  \n\nThe main purpose of this dashboard is to ensure wider accessibility of this research. Data \nvisualizations are important tools in the field of landscape architecture. Various visual representations of landscape design, including maps and models, are used by landscape designers \nand planners to reach diverse stakeholders. Visual representations have become fundamental \nowing to their pivotal role in the wide range of social, political, and ecological issues faced \nby landscape architects. These visualizations can augment awareness which in turn enhances \nthe efficacy of communication between various stakeholders, such as policymakers, community-based groups, non-governmental organizations, and the general public with varied levels \nof visual literacy contributing to a participatory landscape design process (RAAPHORST et al. \n2019, RAAPHORST et al. 2020).  \n\nData visualizations are recognized as efficient tools for generating education around civic \ntopics as they help the public to understand the underlying process for formulating policies, \nand  to  arrive  at  informed  decisions  regarding  civic  issues  (WILLIAMS  2016).  Researchers \nfound that visualizations can overcome language barriers, address diverse cultural aspects \nand disparate education levels, and promote inclusivity. In addition, they can effectively communicate  complex  data,  helping  bridge  the  information  gap  between  diverse  stakeholders \n(SLEIGH & VAYENA 2021). Public dashboards are a promising tool to see how existing users \nengage with and conceptualize public spaces like trails, which can significantly shape efforts \nto address unequal access, exclusive norms, and safety concerns of such public spaces, and \nhow these intersect with societal inequalities along various axes such as race, class, and gender. Thus, data visualizations can become collaborative public engagement tools and reduce \nthe power differentials pertaining to the possession of knowledge (WILLIAMS 2016). \n\nVisualizing and Tagging Trail Data  \n\nSurvey of Trail Users and Maintainers \n\nThe framework put forth in this paper draws on data derived from an online survey about \ntrail use and trailwork conducted over several months in 2021. The survey was posted online \nand sent to various trail clubs, outdoor recreation groups, online forums, and other organizations with interests in trailwork and trail-based recreation. The data used here were collected \nuntil December 2021 and include ~570 responses from across the US. In this paper, we focus \nprimarily on responses to the freelist question: “What types of things do people do on trails? \n(List up to 10 things).” These open-ended text responses involve nuanced descriptions of trail \nactivities, allowing for emergent coding of themes in trail activities, which further revealing \nunderlying values, perceptions, and experiences of nature.  \n\nThe 559 survey respondents for the trail activity question listed a total of 1252 unique responses, which after text cleaning and standardization are grouped into 739 activity types. \nWe developed several distinct frameworks for classifying and interpreting trail activities, two \nwhich are represented in the dashboard figures and lexicon. For example, co-author KT manually  coded  activity  responses  according  to  whether  they  had  active,  passive,  or  negative \nimpacts on the environment. In another case, co-author MB assigned tags to individual words \ndrawn from activity responses to develop the lexicon described in the following section. Data \nwrangling, cleaning and analysis was done in R (R CORE TEAM 2022). Analyses of the significance of survey results and these methodological processes are ongoing. Here, we present \ntwo aspects of this project that are most relevant to the practice of digital landscape architecture: tagging lexicons for open science and digital dashboards for science communication. \nThis paper represents a reflective timestamp documenting the first phase of developing digital outputs for the broader project focused on trail management and use. \n\nLexicon of Trail Activities \n\nThe first contribution of this paper is an open-source semantic tagging lexicon for outdoor \nrecreation  and  nature  experiences.  The  text  analysis  lexicon  may  be  accessed  here: \n(https:\/\/trailusestudy.web.app\/assets\/traillexicon.html). At present the online lexicon is based \non annotations by co-author MB. Although these data are focused specifically on trail experiences, the lexicon is designed to also be applicable to experiences in other public spaces \nand natural environments. By releasing the tagging lexicon publicly, other scholars, communities, land managers, and planners interested in understanding behavior, perceptions, and \nexperiences of public lands are able to use and extend this framework. In particular, land \nmanagers  and  landscape  architects  who  work  with  social  media  or  other  digital  big  data \nsources may find this tagging library helpful as part of a reproducible text analysis workflow. \n\nThe  primary  objective  of  this  lexicon  is  providing  a  broadly  applicable  method  of  understanding human-nature interaction, not only within but also beyond trail spaces. Research \ndemonstrates that our relationship with the environment extends far beyond traditional forms \nof ‘usage’ (such as gardening or farming) into realms including, but not limited to, recreation; \nphysical and mental wellbeing; craftsmanship; social engagement; community building; and \neducation. Studies also indicate that in order to fully grasp the breadth of human-nature relationships, peoples’ experiences in nature must be understood as not only cognitive (mind and \nthought-related)  but  also  sensory  (relating  to  bodily  sensations)  and  affective  (relating  to \nmoods  and  emotions)  (PRAMOVA et  al.  2022).  For  example,  Temmes  (2022)  qualitatively \ncoded social media posts in order to understand the multisensory aspects of visiting protected \nareas. Building on these existing studies, we tagged words found in survey responses based \non themes that may be applicable to big data sets related to nature experiences. Here we draw \ninspiration from domain-specific lexicons developed in other fields (e. g. CODEN et al. 2005, \nOLTEANU et al. 2014) to develop tools for text analysis in conservation planning and nature-experience research. Developing and sharing tagged text datasets related to nature experiences and protected areas is an important step for linking text analysis, open science, and \nconservation planning. \n\nResearch surrounding peoples’ affective relationship with the environment is also being used \nto promote more frequent and sustainable human-nature interactions, such as through efforts \nto build a sense of ‘connectedness’ with nature via participatory approaches (RENOWDEN et \nal. 2022). People use trails not just for hiking, walking, or commuting, but also for exercise \n(e. g. running and biking); recreation (e. g. horse riding and rock climbing); promoting mental wellbeing (e. g. meditation and alone time); caring for the environment (e. g. picking up \ntrash or improving the trail); and more. Thus, recognizing the wide-ranging benefits of understanding trail users' experiences, especially with respect to designing landscapes and digital products extending beyond the scope of trails, we designed this lexicon to be applicable \nfor a wide variety of contexts related to nature and outdoor experiences. \n\nIn addition to classification based on the types of recreational activities, we developed a tagging library or lexicon for identifying the less tangible and more affective attributes of trail \nexperiences.  This  allows  for  individual  terms  within  a  text  to  be  systemically  tagged  for \nthemes. For example, if a survey response or social media post mentions “Going fishing with \nfriends to relax,” it might be tagged with themes such as mental health, socialization, engaging with wildlife, and fishing. On the other hand, text reading: “Going fishing alone to enjoy \nthe quiet” would result in a different set of tags including solitude and hearing. Tagging individual terms allows for greater nuance in interpreting how nature experiences are described \nwhen working with texts at a big data scale, since it becomes more difficult to manually tag \nentire phrases when working with larger datasets. The main lexicon presented here is developed along three nested scales. At the broadest level, there are 48 tags, which are grouped \ninto 18 categories, which are in turn grouped into five themes. For example, tags are developed for each of the senses. Additionally, words associated with physical and mental health, \nsocialization, contemplation, and community were compiled along with other tagging lists. \nCategories include conflict, health, senses, outdoor skills, social, and learning among others. \nThemes include: individual, social, trail, nature, and action. Groups are not mutually exclusive, meaning that a given term might be linked to several tags. \n\nDigital Dashboard of Trail Activities \n\nThe second contribution of this paper is a web application that visualizes and explains key \ninformation on trail use, experiences, and perceptions, based on the online survey and presented through charts and diagrams organized into panels. The dashboard can be accessed \nhere: https:\/\/trailusestudy.web.app\/. This online dashboard  allows viewers to explore relationships in the data by customizing which and\/or how variables are plotted. This may provide the general public and stakeholders with the ability to actively interact with the data in \na manner difficult to achieve through reports or articles. The interactive digital dashboard \npresents four aspects of the survey data: the impact of COVID-19 on trail experiences; the \ngeographic distribution of responses about trail activities; trail activities; and demographics \nand activities mentioned. Detailed descriptions of these components are as follows: \n\nCOVID-19 impact: this component of the dashboard is an interactive bar chart depicting the \nimpact of COVID-19 on various aspects of trail experience (e. g. places visited, frequency of \nuse, number of people on trails, frequency of volunteering, number of different activities; \ntrash along trails, and social trails). The chart is based on responses to the survey question: \n“To what extent has COVID-19 changed your experience of trails compared to during preCOVID times?” Data are presented here as they are of timely interest for understanding how \nCOVID-19 has impacted trails. Figure 1 shows two static images drawn from this component.  \n\nActivity distribution: this component is an interactive map depicting the frequency of survey response references to trail activities (including hiking, horseback riding, biking, running, and bird watching) across the U.S., by state. The interactive map displays the numeric \nfrequency of trail activities for each U.S. state. The map uses graduated color symbology, \nwhere darker shades represent greater frequency of the activity type. Viewers can see the \nspecific number of responses per activity type within each state by hovering over the state on \nthe map. Each activity type has a separate map to ensure readability.  \n\nTrail activities: this component is a sunburst diagram that depicts the types of activities that \nsurvey respondents reported as occurring on trails, as well as the frequency of each type of \nactivity (based on the number of responses for each type). This chart is based on responses \nto the question: “What types of things do people do on trails? (List up to 10 things).” The \nactivities data are grouped into nested levels. For example, the first level of grouping combines activities into precise categories (such as hiking, bird watching, and equestrian), while \nthe final level combined all activities into two overarching groups (human-centered and nature-centered). The sunburst diagram was thus used to display this data in a hierarchical structure: each subsequent ring of the diagram represents a broader level of grouping (with the \ncenter ring being the final, most comprehensive level). Users can click on categories across \neach level to reshuffle the view of the diagram. Hovering over the diagram also displays the \npercentage as well as the number of responses for each category. Figure 2 presents two views \nof the sunburst figure at different levels of specificity. These categories are not meant to be \nuniversal but  rather  to  open a  conversation  about how  to group  and differentiate  between \ndifferent kinds of things that people report doing on trails. \n\nDemographic factors: This component is an interactive chord diagram displaying two directions of relationships between trail activities and survey respondent demographics. This \ngraphic simultaneously allows the user to explore both the demographics of all respondents \nwho mentioned a particular activity, as well as all the types of activities named by individuals \nwithin a particular demographic group. \n\nDiscussion and Outlook \n\nTrail spaces are an important lens for understanding human-environment interactions. Many \ntrails are constructed in a way that renders them suitable for a wide variety of uses: whether \ntravel, recreation, education, or an escape from society. While trail spaces attract visitors of \nall demographic groups, there is also considerable variation in how each of these groups engage with these spaces; this ensures that trail usage data is not only broadly applicable but \nalso extremely useful for gaining deeper insights into the impact of demographic factors (i. e. \nrace, class and age) on human-nature interaction. Thus, research on trails may involve analyzing data and drawing conclusions that will prove useful for future applications in digital \nlandscape architecture extending beyond the scope of trail usage.  \n\nWe recognize that this study has limitations, which provide a basis for informing future directions for this line of research. First, the lexicons and text analysis tools developed are not \napplicable across all texts referring to human-nature interactions. This is a pilot version of \nthe lexicon, which will be expanded and improved through future validations and applications to new datasets. We plan to continue to update these tools as new development emerge. \n\nThe survey data used for creating the dashboard also has limitations, namely its exclusion of \nsome demographic information. Other aspects of identity or demographic information could \nbe of use to land managers and landscape architects who seek to expand trail and park accessibility. To better consider multiple demographic variables, studies into the perceptions of \ntrail use will require an inclusive method to understand values associated with trails. An additional limitation for generalizability of the results is that the trails survey was sent primarily \nto outdoor groups and NGOs focused on trails and conservation. This means that individuals \nwho are not part of these groups are less likely to have responded to the survey. As such, the \nresults are not representative of individuals in the US as a whole, but rather should be interpreted as perceptions of people who are involved in trail use or maintenance, particularly \nthrough formal groups. \n\nFinding new ways of analyzing and visualizing data is critical for the future of digital landscape architecture, conservation planning, and landscape design. This ability to capture and \nlink demographic data to a variety of qualitative user experiences has important future implications in analyzing questions of inequality, access, exclusion, and safety as they relate to \ndifferent user groups and identities such as race, class, sexuality, and gender. Furthermore, \nfuture directions for these research tools could further investigate different conceptions of \nstewardship, ownership, usage, access, and belonging in different kinds of public lands, as \ntrails traverse a wide range of property regimes, from federal, state, county, municipal, private, and tribal lands. Here we explored the potential for creating open-source text analysis \ntools for trail and nature-based experiences and developed an interactive dashboard of survey \nresults. Together, these digital tools offer platforms for engagement with text or other big \ndata sets related to protected areas and trails. \n\n